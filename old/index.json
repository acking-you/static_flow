[{"categories":["开发环境配置"],"content":"[C++]CLion集成vcpkg一键完成包管理","date":"2023-06-22","objectID":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/","tags":["[C++]CLion集成vcpkg一键完成包管理"],"title":"[C++]CLion集成vcpkg一键完成包管理——以使用imgui为例","uri":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/"},{"categories":["开发环境配置"],"content":"下载并集成vcpkg 首先，请确保 CLion 版本高于 2023.1。 按两下 shift 搜索 vcpkg ，并点击可进入 CLion 对 vcpkg 的支持模块。 如下图： CLion对vcpkg的操作模块如下： 上图中，由于我已经完成vcpkg的下载与集成，可以看到我已经安装的各种库。 如果你第一次来使用这个功能，那么可能需要点击左上角的四个按钮完成相应操作。 由于你目前还没有创建vcpkg的集成，所以需要先点击那个加号。 会出现下列选项： 上述图片中已经注释的很清楚了。 主要需要配置的是两类： vcpkg的下载： 如果你本地之前没有下载好vcpkg，那么可以填写url来帮你下载和初始化，但是需要注意，下载的文件夹需要是一个不存在的文件夹。 如果你本地之前现在过vcpkg，你想要手动指定它，那么只需要在 Directory 中指定已经下载好的vcpkg的目录即可，而上述url将无效。 将vcpkg集成到cmake项目： 这里就比较简单了，点击集成到你目前CLion中已经存在的cmake运行的配置项即可。 有关CLion中cmake的运行配置项是什么，可以查看我B站的视频讲解：https://www.bilibili.com/video/BV18R4y127UV 完成这两步，你的vcpkg算是已经集成到了你的cmake项目。 其他三个按键，比如第二个编辑按键其实就是重新编辑每个集成的 vcpkg 的这几个参数罢了。 ","date":"2023-06-22","objectID":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/:1:0","tags":["[C++]CLion集成vcpkg一键完成包管理"],"title":"[C++]CLion集成vcpkg一键完成包管理——以使用imgui为例","uri":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/"},{"categories":["开发环境配置"],"content":"引入第三方库 我们已经集成好了vcpkg，那么我们以一个较为复杂的第三方库引入来跑跑看？ 就决定是你了 Imgui ！ 我们直接到目前已经集成的vcpkg中搜索该库： 如上图所示，我们有两种方式去编译安装该库： 在项目根目录下提供 vcpkg.json 文件来描述需要依赖的库，在cmake执行后自动下载编译。 手动选择需要编译的环境，然后手动点击install来进行安装。 这两种方式在编译安装好后都会提示你如何引入该库，如下图我已经编译安装好imgui了： 你只需要把 main 替换为你的项目 target 即可完成库的引入。 ","date":"2023-06-22","objectID":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/:2:0","tags":["[C++]CLion集成vcpkg一键完成包管理"],"title":"[C++]CLion集成vcpkg一键完成包管理——以使用imgui为例","uri":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/"},{"categories":["开发环境配置"],"content":"集成cmake运行项目 现在你的项目虽然引入了 imgui ，但是你会发现你还是无法把官方的例子跑起来，因为官方的例子还依赖图形渲染引擎（这方面我也不是很懂）。 这里有作者对不同渲染引擎的例子如何跑起来的描述：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/README.txt#L160 介于我本机使用的是win11系统，所以我选择跑的是 example_win32_directx12 这个例子，描述如下： example_win32_directx12/\rDirectX12 example, Windows only.\r= main.cpp + imgui_impl_win32.cpp + imgui_impl_dx12.cpp\rThis is quite long and tedious, because: DirectX12.\r依照提示把 example 中的这三个 cpp 源文件以及对应头文件加入编译即可跑通该示例。 对应文件url如下： main.cpp：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/example_win32_directx12/main.cpp imgui_impl_win32.cpp：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/imgui_impl_win32.cpp imgui_impl_win32.h：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/imgui_impl_win32.h imgui_impl_dx12.cpp：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/imgui_impl_dx12.cpp imgui_impl_dx12.h：https://github.com/ocornut/imgui/blob/c206a193737811193a0b48ef2d35fe028fa0996e/examples/imgui_impl_dx12.h 如图所示项目目录以及cmake代码如下： 当我们兴高采烈的准备跑起整个示例项目的时候，我们得到了一个长达几十行的符号链接错误。。。 很明显，肯定是有些使用到的库没有链接，进行相关检索以及推理判断，我终于找出还需要链接的两个库：d3d12 dxgi 改为如下cmake，终于成功跑起来了！ cmake_minimum_required(VERSION 3.14)project(imgui_example)set(CMAKE_CXX_STANDARD 17)find_package(imgui CONFIG REQUIRED)add_executable(imgui_example main.cpp imgui_impl_dx12.cpp imgui_impl_win32.cpp)target_link_libraries(${PROJECT_NAME} PRIVATE imgui::imgui d3d12 dxgi)示例跑起来的样子： ","date":"2023-06-22","objectID":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/:3:0","tags":["[C++]CLion集成vcpkg一键完成包管理"],"title":"[C++]CLion集成vcpkg一键完成包管理——以使用imgui为例","uri":"/posts/c++clion%E9%9B%86%E6%88%90vcpkg%E4%B8%80%E9%94%AE%E5%AE%8C%E6%88%90%E5%8C%85%E7%AE%A1%E7%90%86/"},{"categories":["算法——滑动窗口"],"content":"使用Rabin-Karp算法替代KMP","date":"2023-06-04","objectID":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/","tags":["使用Rabin-Karp算法替代KMP"],"title":"使用Rabin-Karp算法替代KMP","uri":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/"},{"categories":["算法——滑动窗口"],"content":"Rabin-Karp算法 Rabin-Karp算法是利用滑动窗口的方式来计算哈希值，并通过该哈希值直接进行比较来判断字符串是否匹配，也就是消减了字符串比较的过程来实现 O(n) 的时间复杂度。 废话不多说了，直接上干货。 首先，我问你一个很基础的问题，给你输入一个字符串形式的正整数，如何把它转化成数字的形式？很简单，下面这段代码就可以做到： string s = \"8264\"; int number = 0; for (int i = 0; i \u003c s.size(); i++) { // 将字符转化成数字 number = 10 * number + (s[i] - '0'); print(number); } // 打印输出： // 8 // 82 // 826 // 8264 可以看到这个算法的核心思路就是不断向最低位（个位）添加数字，同时把前面的数字整体左移一位（乘以 10）。 为什么是乘以 10？因为我们默认探讨的是十进制数。这和我们操作二进制数的时候是一个道理，左移一位就是把二进制数乘以 2，右移一位就是除以 2。 上面这个场景是不断给数字添加最低位，那如果我想删除数字的最高位，怎么做呢？比如说我想把 8264 变成 264，应该如何运算？其实也很简单，让 8264 减去 8000 就得到 264 了。 这个 8000 是怎么来的？是 8 x 10^3 算出来的。8 是最高位的数字，10 是因为我们这里是十进制数，3 是因为 8264 去掉最高位后还剩三位数。 上述内容主要探讨了如何在数字的最低位添加数字以及如何删除数字的最高位，用R表示数字的进制数，用L表示数字的位数，就可以总结出如下公式： /* 在最低位添加一个数字 */ int number = 8264; // number 的进制 int R = 10; // 想在 number 的最低位添加的数字 int appendVal = 3; // 运算，在最低位添加一位 number = R * number + appendVal; // 此时 number = 82643 /* 在最高位删除一个数字 */ int number = 8264; // number 的进制 int R = 10; // number 最高位的数字 int removeVal = 8; // 此时 number 的位数 int L = 4; // 运算，删除最高位数字 number = number - removeVal * R^(L-1); // 此时 number = 264 如果你能理解这两个公式，那么 Rabin-Karp 算法就没有任何难度，算法就是这样，再高大上的技巧，都是在最简单最基本的原理之上构建的。不过在讲 Rabin-Karp 算法之前，我们先来看一道简单的力扣题目。 ","date":"2023-06-04","objectID":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/:0:0","tags":["使用Rabin-Karp算法替代KMP"],"title":"使用Rabin-Karp算法替代KMP","uri":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/"},{"categories":["算法——滑动窗口"],"content":"高效寻找重复子序列 看下力扣第 187 题「重复的 DNA 序列」，我简单描述下题目： DNA 序列由四种碱基A, G, C, T组成，现在给你输入一个只包含A, G, C, T四种字符的字符串s代表一个 DNA 序列，请你在s中找出所有重复出现的长度为 10 的子字符串。 比如下面的测试用例： 输入：s = \"AAAAACCCCCAAAAACCCCCCAAAAAGGGTTT\" 输出：[\"AAAAACCCCC\",\"CCCCCAAAAA\"] 解释：子串 \"AAAAACCCCC\" 和 \"CCCCCAAAAA\" 都重复出现了两次。 输入：s = \"AAAAAAAAAAAAA\" 输出：[\"AAAAAAAAAA\"] 函数签名如下： vector\u003cstring\u003e findRepeatedDnaSequences(string s); 这道题的拍脑袋解法比较简单粗暴，我直接穷举所有长度为 10 的子串，然后借助哈希集合寻找那些重复的子串就行了，代码如下： // 暴力解法 vector\u003cstring\u003e findRepeatedDnaSequences(string s) { int n = s.size(); // 记录出现过的子串 unordered_set\u003cstring\u003e seen; // 记录那些重复出现多次的子串 // 注意要用哈希集合，防止记录重复的结果 unordered_set\u003cstring\u003e res; for (int i = 0; i + 10 \u003c= n; i++) { auto subStr = s.substr(i, 10); if (seen.count(subStr)){ // 之前出现过，找到一个重复的 res.insert(subStr); } else { // 之前没出现过，加入集合 seen.insert(subStr); } } return {res.begin(),res.end()}; } 这个算法肯定是没问题的，只是时间复杂度略高。假设s的长度为N，目标子串的长度为L（本题L = 10），for 循环遍历s的O(N)个字符，对每个字符都要截取长度为L的子字符串，所以这个算法的时间复杂是O(NL)。 遍历整个s肯定是免不了的，问题是我们能不能不要每次都调用substr去截取子字符串？ 你注意我们这个匹配过程实际上就是维护了一个长度为L = 10的定长窗口在从左向右滑动，是否可以使用滑动窗口中的做法，只维护left, right指针来划定子字符串区间？ 其实可以的，直接套用前文给出的滑动窗口算法框架写出伪码思路： int L = 10; map\u003cstring\u003e seen; // 滑动窗口代码框架 CharWindow window; int left = 0, right = 0; while (right \u003c s.size()) { // 扩大窗口，移入字符 window.addRight(s[right]); right++; // 当子串的长度达到要求 if (right - left == L) { // 把窗口中的字符变成字符串，还是需要 O(L) 的时间 String windowStr = window.toString(); if (seen.contains(windowStr)) { print(\"找到一个重复子串: \", windowStr); } else { seen.add(windowHash); } // 缩小窗口，移出字符 window.removeLeft(); left++; } } 这段伪码直接套用了滑动窗口算法框架，你应该不难理解。但你注意这个解法依然需要将窗口中的字符转化成字符串然后去seen集合判断是否存在重复，你一旦想把字符转化成字符串，就难免需要O(L)的时间来操作。所以这个算法的时间复杂度还是没有降低，依然是O(NL)。 所以优化的关键在于，我们能不能不要真的把子字符串生成出来，而是用一些其他形式的唯一标识来表示滑动窗口中的子字符串，并且还能在窗口滑动的过程中快速更新？ 有办法的，回想一下本文开头我们讨论的那两个公式，现在你应该明白的用意了。 你把AGCT四种字符等价为0123四个数字，那么长度为L = 10的一个碱基序列其实就可以等价为一个十位数，这个数字可以唯一标识一个子串。而且窗口移动的过程，其实就是给这个数字的最低位添加数字，并删除最高位数字的过程，回顾之前的讲解，添加和删除数字的运算就是两个公式，可以在O(1)的时间完成。 然后，我们不要在哈希集合中直接存储子串了，而是存储子串对应的十位数。因为一个十位数可以唯一标识一个子串，所以也可以起到识别重复的作用。 这样，我们就避免了直接生成子串存入集合，而是生成一个十位数来表示子串，而且生成这个十位数的时间花费为O(1)，从而降低了匹配算法的时间复杂度。 其实你想下，你把一个字符串对象转化成了一个数字，这是什么？这就是你设计的一个哈希算法，生成的数字就可以认为是字符串的哈希值。在滑动窗口中快速计算窗口中元素的哈希值，叫做滑动哈希技巧。 上述优化思路的伪码思路如下： int L = 10; // 集合中不要存储字符串了，而是存储字符串对应的哈希值 HashSet\u003cInteger\u003e seen; // 滑动窗口代码框架 CharWindow window; int left = 0, right = 0; while (right \u003c s.size()) { // 扩大窗口，移入字符 window.addRight(s[right]); right++; // 当子串的长度达到要求 if (right - left == L) { // 获取当前窗口内字符串的哈希值，时间 O(1) int windowHash = window.hash(); // 根据哈希值判断是否曾经出现过相同的子串 if (seen.contains(windowHash)) { // 当前窗口中的子串是重复出现的 print(\"找到一个重复子串: \", window.toString()); } else { // 当前窗口中的子串之前没有出现过，记下来 seen.add(windowHash); } // 缩小窗口，移出字符 window.removeLeft(); left++; } } 进一步，我们用一个 10 位数来标识一个长度为 10 的碱基字符序列，这需要 long 类型存储，int 存不下 10 位数。但你注意这个 10 位数中所有的数字只会局限于0,1,2,3，是不是有些浪费？ 换句话说，我们需要存储的其实只是一个四进制下的十位数（共包含 4^10 个数字），却用了十进制的十位数（可以包含 10^10 个数字）来保存，显然是有些浪费的。 因为 4^10 = 1048576 \u003c 10^8，所以只要我们在四进制的运算规则下进行运算，十进制的八位数就能存下，这样的话 int 类型就够用了，不需要 long 类型。 具体来说，只要改变我们之前那两个公式的进制R就行了： /* 在最低位添加一个数字 */ // number 的进制 int R = 4; // 想在 number 的最低位添加的数字 int appendVal = 0~3 中的任意数字; // 运算，在最低位添加一位 number = R * number + appendVal; /* 在最高位删除一个数字 */ // number 的进制 int R = 4; // number 最高位的数字 int removeVal = 0~3 中的任意数字; // 此时 number 的位数 int L = ?; // 运算，删除最高位数字 number = number - removeVal * R^(L-1); 结合数字最高/最低位的处理技巧和滑动窗口代码框架，我们就可以轻松地写出最终的解法代码： vector\u003cstring\u003e findRepeatedDnaSequences(string s) { auto getNumber = [](char x)-\u003eint{ switch(x){ case 'A': return 0; case 'G': return 1; case 'C': return 2; case 'T': return 3; } return -1; } // 记录重复出现的哈希值 unordered_set\u003cint\u003e seen; // 记录重复出现的字符串结果 unordered_set\u003cstring\u003e res; // 数字位数 int L = 10; // 进制 int R = 4; // 存储 R^(L - 1) 的结果 int RL = (int) pow(R, L - 1); // 维护滑动窗口中字符串的哈希值 int windowHash = 0; // 滑动窗口代码框架，时间 O(N) int left = 0, right = 0; while (right \u003c nums.length) { // 扩大窗口，移入字符，并维护窗口哈希值（在最低位添加数字） windowHash = R * windowHash + getNumber(s[right++]); // 当子串的长度达到要求 if (right - left == L) { // 根据哈希值判断是否曾经出现过相同的子串 if (seen.count(windowHash)) { // 当前窗口中的子串是重复出现的 res.insert(s.substr(left, L)); } e","date":"2023-06-04","objectID":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/:1:0","tags":["使用Rabin-Karp算法替代KMP"],"title":"使用Rabin-Karp算法替代KMP","uri":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/"},{"categories":["算法——滑动窗口"],"content":"Rabin-Karp 算法 有了上面由浅入深的铺垫，你理解 Rabin-Karp 算法就非常容易了，因为上面这道题目的本质就是一个字符串匹配的问题。 字符串匹配算法大家都很熟悉，让你在文本串txt中搜索模式串pat的起始索引，暴力字符串匹配算法是这样的： // 在文本串 txt 中搜索模式串 pat 的起始索引 int search(string txt, string pat) { int N = txt.size(), L = pat.size(); for (int i = 0; i + L \u003c= N; i++) { auto subStr = txt.substr(i, L); if (subStr == pat){ // 在 txt 中找到模式串 pat，返回起始索引 return i; } } // txt 中不存在模式串 pat return -1; } 你可以发现，这个逻辑和上面讲的那道题的暴力解法非常类似，总的时间复杂度是O(LN)，优化的核心也是子串subStr和模式串pat匹配的部分。 那么借鉴上面的思路，我们不要每次都去一个字符一个字符地比较子串和模式串，而是维护一个滑动窗口，运用滑动哈希算法一边滑动一边计算窗口中字符串的哈希值，拿这个哈希值去和模式串的哈希值比较，这样就可以避免截取子串，从而把匹配算法降低为O(N)，这就是 Rabin-Karp 指纹字符串查找算法的核心逻辑。 那你可能会问，刚才我们处理的题目给你输入的只有AGCT四种字符，所以可以转化成数字，但面对五花八门的字符串，如何把他们转化成数字计算哈希值呢？其实很简单，字符本质上就是编码，而编码其实就是数字。 比方说以 ASCII 码为例，ASCII 码其实就是 0~255 这 256 个数字，分别对应所有英文字符和英文符号。那么一个长度为L的 ASCII 字符串，我们就可以等价理解成一个L位的 256 进制的数字，这个数字就可以唯一标识这个字符串，也就可以作为哈希值。 有了这个想法，我们就可以直接复制粘贴上一道题的大部分代码，写出 Rabin-Karp 算法的主要逻辑： // 文本串 string txt; // 模式串 string pat; // 需要寻找的子串长度为模式串 pat 的长度 int L = pat.size(); // 仅处理 ASCII 码字符串，可以理解为 256 进制的数字 int R = 256; // 存储 R^(L - 1) 的结果 int RL = (int) pow(R, L - 1); // 维护滑动窗口中字符串的哈希值 int windowHash = 0; // 计算模式串的哈希值 long patHash = 0; for (int i = 0; i \u003c pat.size(); i++) { patHash = R * patHash + pat[i]; } // 滑动窗口代码框架 int left = 0, right = 0; while (right \u003c txt.length()) { // 扩大窗口，移入字符（在最低位添加数字） windowHash = R * windowHash + txt[right]; right++; // 当子串的长度达到要求 if (right - left == L) { // 根据哈希值判断窗口中的子串是否匹配模式串 pat if (patHash == windowHash) { // 找到模式串 print(\"找到模式串，起始索引为\", left); return left; } // 缩小窗口，移出字符（删除最高位数字） windowHash = windowHash - txt[left] * RL; left++; } } // 没有找到模式串 return -1; 相对上一道题的解法，这段代码将进制数R改为了 256，同时计算了模式串pat的哈希值patHash用来和窗口中字符串的哈希值windowHash做对比，以便判断是否找到了模式串，其他的代码部分完全相同。 不过呢，这段代码实际运行的时候会有一个严重的问题，那就是整型溢出。 你想，上一道题给定的 DNA 序列字符串只包含AGCT四种字符，所以我们可以把 DNA 序列抽象成四进制的数字，即算法中R = 4。相同位数下，四进制包含的数字数量是小于十进制包含的数字数量的。比方说L = 10时，4^10 = 1048576 \u003c 10^8，即 10 位四进制数字用 8 位十进制数字就可以存下了。 但现在输入为 ASCII 码字符串，我们不得不把字符串抽象成 256 进制的数字，即算法中R = 256。而相同位数下，256 进制包含的数字数量显然是远大于十进制包含的数字数量的。比如L = 10时，256^10 = 1.2 x 10^24 \u003c 10 ^25，所以你需要一个 25 位的十进制数才能表示一个 10 位的 256 进制数。 可想而知，如果你真的把字符串对应的 256 进制数字的十进制表示作为该字符串的哈希值，那恐怕L稍微大一点，像RL, windowHash, patHash这些变量就超级大了，long 类型都远远装不下。 所以解决办法是什么呢？如何把一个很大的数字映射到一个较小的范围内呢？答案是求模（余数）。 无论一个数字多大，你让它除以Q，余数一定会落在[0, Q-1]的范围内。所以我们可以设置一个Q，用求模的方式让windowHash和patHash保持在[0, Q-1]之间，就可以有效避免整型溢出。 具体来说，对于一个字符串，我们不需要把完整的 256 进制数字存下来，而是对这个巨大的 256 进制数求Q的余数，然后把这个余数作为该字符串的哈希值即可。 好，整型溢出的问题倒是解决了，但新的问题又来了：求模之后的哈希值不能和原始字符串一一对应了，可能出现一对多的情况，即哈希冲突。 比方说 10 % 7 等于 3，而 17 % 7 也等于 3，所以如果你得到余数 3，你能确定原始数字就一定是 10 么？不能。 类似的，如果你发现windowHash == patHash，你也不敢完全肯定窗口中的字符串一定就和模式串pat匹配，有可能它俩不匹配，但恰好求模算出来的哈希值一样，这就产生了是「哈希冲突」。 对于 Rabin-Karp 算法来说，当发现windowHash == patHash时，使用暴力匹配算法检查一下窗口中的字符串和pat是否相同就可以避免哈希冲突了。因为希冲突出现的概率比较小，所以偶尔用一下暴力匹配算法是不影响总体的时间复杂度的。 明白了这些问题的解决方案，你就能很自然地写出 Rabin-Karp 算法了： // Rabin-Karp 指纹字符串查找算法 int rabinKarp(string str, string pattern) { const int64_t mod = (1ll \u003c\u003c 53) - 1; auto getNumber = [](char x) { return x - 'a'; }; int R = 26; int L = (int)pattern.size(); int64_t RL = 1; int64_t patternHash = 0; int64_t windowHash = 0; for (int i = 1; i \u003c L; i++) RL = (RL * R) % mod; for (int i = 0; i \u003c L; i++) patternHash = ((patternHash * R) % mod + getNumber(pattern[i])) % mod; int left = 0, right = 0; while (right \u003c str.size()) { windowHash = ((windowHash * R) % mod + getNumber(str[right++])) % mod; if (right - left == L) { if (windowHash == patternHash) { if (str.substr(left, L) == pattern) return left; } else { windowHash = (windowHash - (getNumber(str[left++]) * RL) % mod + mod) % mod; } } } return -1; } 上述代码可以在力扣28题验证。 有之前那么多铺垫，算法逻辑应该没啥可说的，就说一下模运算的两个运算法则吧： X % Q == (X + Q) % Q\r(X + Y) % Q == (X % Q + Y % Q) % Q\r稍微想一想就能理解这两个运算法则，在代码中但凡涉及到乘法和加法，都可能产生很大的结果，所以一有机会就可以运用上述法则对结果进行求模，以避免造成溢出。 Rabin-Karp 算法的时间复杂度是O(N + L)，N为文本串txt的长度，L为模式串pat的长度。当然，每次出现哈希冲突时会使用O(L)的时间进行暴力匹配，但考虑到只要Q设置的合理，哈希冲突的出现概率会很小，所以可以忽略不计。 最后说一下这个大素数Q的选择。 为什么要这个Q尽可能大呢？主要是为了降低哈希冲突的概率。 因为代码中你把这个Q作为除数，余数（哈希值）一定落在[0, Q-1]之间，所以Q越大，哈希值的空间就越大，就越不容易出现哈希冲突，整个算法的效率就会高一些。 为什么这个Q要是素数呢？依然是为了降低哈希冲突的概率。 举个极端一点的例子，你令Q = ","date":"2023-06-04","objectID":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/:2:0","tags":["使用Rabin-Karp算法替代KMP"],"title":"使用Rabin-Karp算法替代KMP","uri":"/posts/%E4%BD%BF%E7%94%A8rabin-karp%E7%AE%97%E6%B3%95%E6%9B%BF%E4%BB%A3kmp/"},{"categories":["C++好库推荐"],"content":"使用expected进行错误处理","date":"2023-06-02","objectID":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","tags":["使用expected进行错误处理"],"title":"使用expected进行错误处理","uri":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["C++好库推荐"],"content":"使用expected进行错误处理 C++ 中提供了很多中方式进行错误处理。无论是通过抛异常还是通过错误码，标准库都提供相应的调用。 通过 try catch 以抛出异常的方式进行处理，这种方式很多人会觉得看起来可读性很差（包括我），并且由于缺少异常规约（某些异常必须捕获），容易出现 bug，而且异常的传递很多时候可能伴随动态分配内存，这是一笔不小的开销。 通过 error_code 作为返回值判断，这种方式虽然看似简单易用，但是由于 C++ 中并未对 error_code 作过多的规范，使用起来并不方便，很多时候还是倾向于自定义一些枚举作为自己的 error_code，但是由于缺少多返回值的语法（当然如果C++版本比较高可用使用tuple以及解构的语法实现），如果把错误码作为返回值，那么原本的数据返回就只能函数参数传递引用的形式返回了。当然，如果不考虑函数返回值的具体错误信息，可以使用 C++17 的 optional 。 由于 optional 无法包含具体的错误信息，expected 横空出世，在 C++23 开始纳入标准。如果你的C++版本较低，可以使用第三方开源的 expected 库：https://github.com/TartanLlama/expected 关于 C++23 中的新特性，包括 expected 库的使用，大家可以观看 CppCon：How C++23 Changes the Way We Write Code 下面我会以一个例子把第三方库中的 expected 库的使用方式介绍给大家。 ","date":"2023-06-02","objectID":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/:1:0","tags":["使用expected进行错误处理"],"title":"使用expected进行错误处理","uri":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["C++好库推荐"],"content":"expected 使用实例 由于该第三方库是 head-only 的，所以你只需要进到GitHub仓库把对应的头文件复制过来，便可引入使用。 下面是示例代码，样例是 cppreference 中的。 #include \"expected.h\"#include \u003ciomanip\u003e#include \u003ciostream\u003e#include \u003cstring\u003e enum class parse_error { invalid_input, overflow }; tl::expected\u003cdouble, parse_error\u003e parse_number(std::string_view\u0026 str) { const char* begin = str.data(); char* end; double retval = std::strtod(begin, \u0026end); if (begin == end) return tl::unexpected(parse_error::invalid_input); else if (std::isinf(retval)) return tl::unexpected(parse_error::overflow); str.remove_prefix(end - begin); return retval; } int main() { auto process = [](std::string_view str) { std::cout \u003c\u003c \"str: \" \u003c\u003c std::quoted(str) \u003c\u003c \", \"; if (const auto num = parse_number(str); num) { std::cout \u003c\u003c \"value: \" \u003c\u003c *num \u003c\u003c '\\n'; // If num did not have a value, dereferencing num // would cause an undefined behavior, and // num.value() would throw std::bad_expected_access. // num.value_or(123) uses specified default value 123. } else if (num.error() == parse_error::invalid_input) { std::cout \u003c\u003c \"error: invalid input\\n\"; } else if (num.error() == parse_error::overflow) { std::cout \u003c\u003c \"error: overflow\\n\"; } else { std::cout \u003c\u003c \"unexpected!\\n\";// or invoke std::unreachable(); } }; for (auto src : {\"42\", \"42abc\", \"meow\", \"inf\"}) process(src); } 上面的代码如果想要跑通，情确保C++版本至少是C++17，因为其中用到了 string_view 以及更智能的自动类型推导（如果低于这个帮会导致unexpected需要指定明确的error类型）。 ","date":"2023-06-02","objectID":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/:1:1","tags":["使用expected进行错误处理"],"title":"使用expected进行错误处理","uri":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["C++好库推荐"],"content":"函数式的接口 and_then：传入一个回调，在没有错误的时候调用，该回调的返回值是新的 expected 值（可以控制err）。如果有错误返回原 expected 值。 or_else：传入一个回调，在有错误的时候调用，该回调的返回值是新的 expected 值（可以控制err），并且回调的参数是对应的错误类型。如果没有错误返回原 expected 值。 transform/map：transform 是C++23标准中规定的接口，而该第三方库作者又实现了一个名为map的接口，这两者效果是一致的。传入一个回调，在没有错误的时候调用，回调的参数和返回值都不牵扯 expected 值，只是作值的变换，所以无法控制新的 expected 的 err 值。如果有错误则返回原 expected 值。 transform_error/map_error：同上，但回调的调用时机和参数于 or_else 相同，但是需要注意的是，回调的返回值并不具备任何效用，也就是说如果 transform_error 中的回调被调用，那么返回的仍然是原本包含错误信息的 expected 值。 简单示例如下： #include \"expected.h\"#include \u003ciostream\u003e#include \u003cstring\u003e enum class parse_error { invalid_input, overflow }; tl::expected\u003cdouble, parse_error\u003e parse_number(std::string_view\u0026 str) { const char* begin = str.data(); char* end; double retval = std::strtod(begin, \u0026end); if (begin == end) return tl::unexpected(parse_error::invalid_input); else if (std::isinf(retval)) return tl::unexpected(parse_error::overflow); str.remove_prefix(end - begin); return retval; } int main() { auto sv = std::string_view{\"0\"}; auto result = parse_number(sv) .and_then([](double x) { return tl::expected\u003cdouble, parse_error\u003e(x + 1); }) .map([](double x) { return x + 1; }) .transform([](double x) { return x + 1; }); if (result) std::cout \u003c\u003c *result \u003c\u003c \"\\n\"; auto result2 = parse_number(sv) .and_then([](double x) { //自己构造了一个错误 tl::expected\u003cdouble, parse_error\u003e ret = tl::unexpected\u003cparse_error\u003e(parse_error::invalid_input); return ret; }) .or_else([](parse_error err) { if (err == parse_error::invalid_input) { std::cout \u003c\u003c \"invalid error\\n\"; } //自己构造了一个错误 tl::expected\u003cdouble, parse_error\u003e ret = tl::unexpected\u003cparse_error\u003e(parse_error::overflow); return ret; }) .transform_error([](parse_error err) { if (err == parse_error::overflow) { std::cout \u003c\u003c \"overflow error\\n\"; } return 32432.4324; }).map([](double x){ return x+1; }); if (result2) { std::cout \u003c\u003c *result2; } } ","date":"2023-06-02","objectID":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/:1:2","tags":["使用expected进行错误处理"],"title":"使用expected进行错误处理","uri":"/posts/%E4%BD%BF%E7%94%A8expected%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["MySQL日志篇"],"content":"MySQL日志：undo、redo、binlog","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"MySQL 日志：undo log、redo log、binlog 有什么用？ 我们知道一条查询语句经历的过程，这属于「读」一条记录的过程，如下图： 那么，执行一条 update 语句，期间发生了什么？，比如这一条 update 语句： UPDATEt_userSETname='xiaolin'WHEREid=1;查询语句的那一套流程，更新语句也是同样会走一遍： 客户端先通过连接器建立连接，连接器自会判断用户身份； 因为这是一条 update 语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空的，所以说查询缓存很鸡肋，在 MySQL 8.0 就被移除这个功能了； 解析器会通过词法分析识别出关键字 update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合 MySQL 语法； 预处理器会判断表和字段是否存在； 优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引； 执行器负责具体执行，找到这一行，然后更新。 不过，更新语句的流程会涉及到 undo log（回滚日志）、redo log（重做日志） 、binlog （归档日志）这三种日志： undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。 redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复； binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制； 所以这次就带着这个问题，看看这三种日志是怎么工作的。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:0:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#为什么需要 undo log？ 我们在执行执行一条“增删改”语句的时候，虽然没有输入 begin 开启事务和 commit 提交事务，但是 MySQL 会隐式开启事务来执行“增删改”语句的，执行完就自动提交事务的，这样就保证了执行完“增删改”语句后，我们可以及时在数据库表看到“增删改”的结果了。 执行一条语句是否自动提交事务，是由 autocommit 参数决定的，默认是开启。所以，执行一条 update 语句也是会使用事务的。 那么，考虑一个问题。一个事务在执行过程中，在还没有提交事务之前，如果 MySQL 发生了崩溃，要怎么回滚到事务之前的数据呢？ 如果我们每次在事务执行过程中，都记录下回滚时需要的信息到一个日志里，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据，我们可以通过这个日志回滚到事务之前的数据。 实现这一机制就是 undo log（回滚日志），它保证了事务的 ACID 特性 (opens new window)中的原子性（Atomicity）。 undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。如下图： 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作。 不同的操作，需要记录的内容也是不同的，所以不同类型的操作（修改、删除、新增）产生的 undo log 的格式也是不同的，具体的每一个操作的 undo log 的格式我就不详细介绍了，感兴趣的可以自己去查查。 一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id： 通过 trx_id 可以知道该记录是被哪个事务修改的； 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链； 版本链如下图： 另外，undo log 还有一个作用，通过 ReadView + undo log 实现 MVCC（多版本并发控制）。 对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列（trx_id 和 roll_pointer）」的比对，如果不满足可见行，就会顺着 undo log 版本链里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。具体的实现可以看我这篇文章：事务隔离级别是怎么实现的？(opens new window) 因此，undo log 两大作用： 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 TIP 很多人疑问 undo log 是如何刷盘（持久化到磁盘）的？ undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。 buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:1:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#为什么需要 Buffer Pool？ MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。那修改完这条记录是选择直接写回到磁盘，还是选择缓存起来呢？ 当然是缓存起来好，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。 为此，Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。 有了 Buffer Poo 后： 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:2:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#Buffer Pool 缓存什么？ InnoDB 会把存储的数据划分为若干个「页」，以页作为磁盘和内存交互的基本单位，一个页的默认大小为 16KB。因此，Buffer Pool 同样需要按「页」来划分。 在 MySQL 启动的时候，InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的16KB的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。 所以，MySQL 刚启动的时候，你会观察到使用的虚拟内存空间很大，而使用到的物理内存空间却很小，这是因为只有这些虚拟内存被访问后，操作系统才会触发缺页中断，申请物理内存，接着将虚拟地址和物理地址建立映射关系。 Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息等等。 Undo 页是记录什么？ 开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 查询一条记录，就只需要缓冲一条记录吗？ 不是的。 当我们查询一条记录时，InnoDB 是会把整个页的数据加载到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过页里的「页目录」去定位到某条具体的记录。 关于页结构长什么样和索引怎么查询数据的问题可以在这篇找到答案：换一个角度看 B+ 树(opens new window) ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:2:1","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#为什么需要 redo log ？ Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。 为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。 后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 过程如下图： 什么是 redo log？ redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 被修改 Undo 页面，需要记录对应 redo log 吗？ 需要的。 开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 不过，在内存修改该 Undo 页面后，需要记录对应的 redo log。 redo log 和 undo log 区别在哪？ 这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； 事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，如下图： 所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？ 写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。 磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。 针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。 可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。 至此， 针对为什么需要 redo log 这个问题我们有两个答案： 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。 产生的 redo log 是直接写入磁盘的吗？ 不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图： redo log buffer 默认大小 16 MB，可以通过 innodb_log_Buffer_size 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:3:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#redo log 什么时候刷盘？ 缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？ 主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。 innodb_flush_log_at_trx_commit 参数控制的是什么？ 单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式「顺序写」到磁盘。 上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。 除此之外，InnoDB 还提供了另外两种策略，由参数 innodb_flush_log_at_trx_commit 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下： 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache（如果你想了解 Page Cache，可以看这篇 (opens new window)），Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。 我画了一个图，方便大家理解： innodb_flush_log_at_trx_commit 为 0 和 2 的时候，什么时候才将 redo log 写入磁盘？ InnoDB 的后台线程每隔 1 秒： 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失; 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 加入了后台现线程后，innodb_flush_log_at_trx_commit 的刷盘时机如下图： 这三个参数的应用场景是什么？ 这三个参数的数据安全性和写入性能的比较如下： 数据安全性：参数 1 \u003e 参数 2 \u003e 参数 0 写入性能：参数 0 \u003e 参数 2\u003e 参数 1 所以，数据安全性和写入性能是熊掌不可得兼的，要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性。 在一些对数据安全性要求比较高的场景中，显然 innodb_flush_log_at_trx_commit 参数需要设置为 1。 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:3:1","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#redo log 文件写满了怎么办？ 默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：ib_logfile0 和 ib_logfile1 。 在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。 重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。 所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。 我们知道 redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。 redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图： 图中的： write pos 和 checkpoint 的移动都是顺时针方向； write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作； check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录； 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。 所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:3:2","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#为什么需要 binlog ？ 前面介绍的 undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。 MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。 binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 为什么有了 binlog， 还要有 redo log？ 这个问题跟 MySQL 的时间线有关系。 最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。 而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:4:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#redo log 和 binlog 有什么区别？ 这两个日志有四个区别。 1、适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 2、文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 3、写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 4、用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？ 不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:4:1","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#主从复制是怎么实现？ MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。 这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。 MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。 具体详细过程如下： MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。 从库是不是越多越好？ 不是的。 因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。 所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。 MySQL 主从复制还有哪些模型？ 主要有三种： 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:4:2","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#binlog 什么时候刷盘？ 事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。 一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。这是因为有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。 MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图： 虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件： 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。 MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N\u003e1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。 而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。 如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。 三个日志讲完了，至此我们可以先小结下，update 语句的执行过程。 当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。 具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:4:3","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#为什么需要两阶段提交？ 事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 举个例子，假设 id = 1 这行数据的字段 name 的值原本是 ‘jay’，然后执行 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性； 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性； 可以看到，在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。 MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。 两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。注意，不要把提交（Commit）阶段和 commit 语句混淆了，commit 语句执行的时候，会包含提交（Commit）阶段。 举个拳击比赛的例子，两位拳击手（参与者）开始比赛之前，裁判（协调者）会在中间确认两位拳击手的状态，类似于问你准备好了吗？ 准备阶段：裁判（协调者）会依次询问两位拳击手（参与者）是否准备好了，然后拳击手听到后做出应答，如果觉得自己准备好了，就会跟裁判说准备好了；如果没有自己还没有准备好（比如拳套还没有带好），就会跟裁判说还没准备好。 提交阶段：如果两位拳击手（参与者）都回答准备好了，裁判（协调者）宣布比赛正式开始，两位拳击手就可以直接开打；如果任何一位拳击手（参与者）回答没有准备好，裁判（协调者）会宣布比赛暂停，对应事务中的回滚操作。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:5:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#两阶段提交的过程是怎样的？ 在 MySQL 的 InnoDB 存储引擎中，开启 binlog 的情况下，MySQL 会同时维护 binlog 日志与 InnoDB 的 redo log，为了保证这两个日志的一致性，MySQL 使用了内部 XA 事务（是的，也有外部 XA 事务，跟本文不太相关，我就不介绍了），内部 XA 事务由 binlog 作为协调者，存储引擎是参与者。 当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，分两阶段来完成 XA 事务的提交，如下图： 从图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）； commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功； ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:5:1","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#异常重启会出现什么现象？ 我们来看看在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？下图中有时刻 A 和时刻 B 都有可能发生崩溃： 不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，此时的 redo log 都处于 prepare 状态。 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID： 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。 可以看到，对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。 所以说，两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计? binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。 也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。 有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？ 放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。 所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:5:2","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#两阶段提交有什么问题？ 两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响： 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 为什么两阶段提交的磁盘 I/O 次数会很高？ binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1： 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘； 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘； 可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。 为什么锁竞争激烈？ 在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。 通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。 #组提交 MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。 引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程： flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）； sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）； commit 阶段：各个事务按顺序做 InnoDB commit 操作； 上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。 对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。 有 binlog 组提交，那有 redo log 组提交吗？ 这个要看 MySQL 版本，MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。 在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。 所以在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。 这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。 接下来介绍每个阶段的过程，注意下面的过程针对的是“双 1” 配置（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）。 flush 阶段 第一个事务会成为 flush 阶段的 Leader，此时后面到来的事务都是 Follower ： 接着，获取队列中的事务组，由绿色事务组的 Leader 对 redo log 做一次 write + fsync，即一次将同组事务的 redolog 刷盘： 完成了 prepare 阶段后，将绿色这一组事务执行过程中产生的 binlog 写入 binlog 文件（调用 write，不会调用 fsync，所以不会刷盘，binlog 缓存在操作系统的文件系统中）。 从上面这个过程，可以知道 flush 阶段队列的作用是用于支撑 redo log 的组提交。 如果在这一步完成后数据库崩溃，由于 binlog 中没有该组事务的记录，所以 MySQL 会在重启后回滚该组事务。 sync 阶段 绿色这一组事务的 binlog 写入到 binlog 文件后，并不会马上执行刷盘的操作，而是会等待一段时间，这个等待的时长由 Binlog_group_commit_sync_delay 参数控制，目的是为了组合更多事务的 binlog，然后再一起刷盘，如下过程： 不过，在等待的过程中，如果事务的数量提前达到了 Binlog_group_commit_sync_no_delay_count 参数设置的值，就不用继续等待了，就马上将 binlog 刷盘，如下图： 从上面的过程，可以知道 sync 阶段队列的作用是用于支持 binlog 的组提交。 如果想提升 binlog 组提交的效果，可以通过设置下面这两个参数来实现： binlog_group_commit_sync_delay= N，表示在等待 N 微妙后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将「 binlog 文件」持久化到磁盘。 binlog_group_commit_sync_no_delay_count = N，表示如果队列中的事务数达到 N 个，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘。 如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据继续进行事务的提交。 commit 阶段 最后进入 commit 阶段，调用引擎的提交事务接口，将 redo log 状态设置为 commit。 commit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:5:3","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#MySQL 磁盘 I/O 很高，有什么优化的方法？ 现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率： 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。 ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:6:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["MySQL日志篇"],"content":"#总结 具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）： prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘； commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）； 至此，一条更新语句执行完成。 参考资料： 《MySQL 45 讲》 《MySQL 是怎样运行的？》 https://developer.aliyun.com/article/617776 http://mysql.taobao.org/monthly/2021/10/01/ https://www.cnblogs.com/Neeo/articles/13883976.html https://www.cnblogs.com/mengxinJ/p/14211427.html ","date":"2023-05-28","objectID":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/:7:0","tags":["MySQL日志：undo、redo、binlog"],"title":"MySQL日志：undo、redo、binlog","uri":"/posts/mysql%E6%97%A5%E5%BF%97undoredobinlog/"},{"categories":["Redis八股"],"content":"Redis常见面试题","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"Redis 常见面试题 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:0:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#认识 Redis ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:1:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#什么是 Redis？ 我们直接看 Redis 官方是怎么介绍自己的。 Redis 官方的介绍原版是英文的，我翻译成了中文后截图的，所以有些文字读起来会比较拗口，没关系，我会把里面比较重要的特性抽出来讲一下。 Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此读写速度非常快，常用于缓存，消息队列、分布式锁等场景。 Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是原子性的，因为执行命令由单线程负责的，不存在并发竞争的问题。 除此之外，Redis 还支持事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制等等。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:1:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 和 Memcached 有什么区别？ 很多人都说用 Redis 作为缓存，但是 Memcached 也是基于内存的数据库，为什么不选择它作为缓存呢？要解答这个问题，我们就要弄清楚 Redis 和 Memcached 的区别。 Redis 与 Memcached 共同点： 都是基于内存的数据库，一般都用来当做缓存使用。 都有过期策略。 两者的性能都非常高。 Redis 与 Memcached 区别： Redis 支持的数据类型更丰富（String、Hash、List、Set、ZSet），而 Memcached 只支持最简单的 key-value 数据类型； Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，而 Memcached 没有持久化功能，数据全部存在内存之中，Memcached 重启或者挂掉后，数据就没了； Redis 原生支持集群模式，Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据； Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持； ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:1:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#为什么用 Redis 作为 MySQL 的缓存？ 主要是因为 Redis 具备「高性能」和「高并发」两种特性。 1、Redis 具备高性能 假如用户第一次访问 MySQL 中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据缓存在 Redis 中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了，操作 Redis 缓存就是直接操作内存，所以速度相当快。 如果 MySQL 中的对应数据改变的之后，同步改变 Redis 缓存中相应的数据即可，不过这里会有 Redis 和 MySQL 双写一致性的问题，后面我们会提到。 2、 Redis 具备高并发 单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍，Redis 单机的 QPS 能轻松破 10w，而 MySQL 单机的 QPS 很难破 1w。 所以，直接访问 Redis 能够承受的请求是远远大于直接访问 MySQL 的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:1:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 数据结构 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:2:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 数据类型以及使用场景分别是什么？ Redis 提供了丰富的数据类型，常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）。 随着 Redis 版本的更新，后面又支持了四种数据类型： BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增）。 Redis 五种数据类型的应用场景： String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。 List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。 Hash 类型：缓存对象、购物车等。 Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。 Zset 类型：排序场景，比如排行榜、电话和姓名排序等。 Redis 后续版本又支持四种数据类型，它们的应用场景如下： BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等； HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等； GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车； Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。 TIP 想深入了解这 9 种数据类型，可以看这篇：2万字 + 20 张图 ｜ 细说 Redis 常见数据类型和应用场景(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:2:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#五种常见的 Redis 数据类型是怎么实现？ 我画了一张 Redis 数据类型和底层数据结构的对应关图，左边是 Redis 3.0版本的，也就是《Redis 设计与实现》这本书讲解的版本，现在看还是有点过时了，右边是现在 Redis 7.0 版本的。 String 类型内部实现 String 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串： SDS 不仅可以保存文本数据，还可以保存二进制数据。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。 SDS 获取字符串长度的时间复杂度是 O(1)。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。 Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。 List 类型内部实现 List 类型的底层数据结构是由双向链表或压缩列表实现的： 如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构； 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构； 但是在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表。 Hash 类型内部实现 Hash 类型的底层数据结构是由压缩列表或哈希表实现的： 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构； 如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 Set 类型内部实现 Set 类型的底层数据结构是由哈希表或整数集合实现的： 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构； 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构。 ZSet 类型内部实现 Zset 类型的底层数据结构是由压缩列表或跳表实现的： 如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构； 如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构； 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 TIP 想深入了解这 9 种数据结构，可以看这篇：2万字 + 40 张图 ｜ 细说 Redis 数据结构(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:2:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 线程模型 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 是单线程吗？ Redis 单线程指的是「接收客户端请求-\u003e解析请求 -\u003e进行数据读写等操作-\u003e发送数据给客户端」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。 但是，Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程（BIO）的： Redis 在 2.6 版本，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务； Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此我们应该使用 unlink 命令来异步删除大key。 之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。 后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可。 关闭文件、AOF 刷盘、释放内存这三个任务都有各自的任务队列： BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭； BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘， BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象； ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 单线程模式是怎样的？ Redis 6.0 版本之前的单线模式如下图： 图中的蓝色部分是一个事件循环，是由主线程负责的，可以看到网络 I/O 和命令处理都是单线程。 Redis 初始化的时候，会做下面这几件事情： 首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket； 然后，将调用 epoll_ctl() 将 listen socket 加入到 epoll，同时注册「连接事件」处理函数。 初始化完后，主线程就进入到一个事件循环函数，主要会做以下事情： 首先，先调用处理发送队列函数，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。 接着，调用 epoll_wait 函数等待事件的到来： 如果是连接事件到来，则会调用连接事件处理函数，该函数会做这些事情：调用 accpet 获取已连接的 socket -\u003e 调用 epoll_ctl 将已连接的 socket 加入到 epoll -\u003e 注册「读事件」处理函数； 如果是读事件到来，则会调用读事件处理函数，该函数会做这些事情：调用 read 获取客户端发送的数据 -\u003e 解析命令 -\u003e 处理命令 -\u003e 将客户端对象添加到发送队列 -\u003e 将执行结果写到发送缓存区等待发送； 如果是写事件到来，则会调用写事件处理函数，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。 以上就是 Redis 单线模式的工作方式，如果你想看源码解析，可以参考这一篇：为什么单线程的 Redis 如何做到每秒数万 QPS ？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 采用单线程为什么还这么快？ 官方使用基准测试的结果是，单线程的 Redis 吞吐量可以达到 10W/每秒，如下图所示： 之所以 Redis 采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因： Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了； Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。 Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 6.0 之前为什么使用单线程？ 我们都知道单线程的程序是无法利用服务器的多核 CPU 的，那么早期 Redis 版本的主要工作（网络 I/O 和执行命令）为什么还要使用单线程呢？我们不妨先看一下Redis官方给出的FAQ (opens new window)。 核心意思是：CPU 并不是制约 Redis 性能表现的瓶颈所在，更多情况下是受到内存大小和网络I/O的限制，所以 Redis 核心网络模型使用单线程并没有什么问题，如果你想要使用服务的多核CPU，可以在一台服务器上启动多个节点或者采用分片集群的方式。 除了上面的官方回答，选择单线程的原因也有下面的考虑。 使用了单线程后，可维护性高，多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:4","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 6.0 之后为什么引入了多线程？ 虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。 所以为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。**但是对于命令的执行，Redis 仍然使用单线程来处理，*所以大家*不要误解 Redis 有多线程同时执行命令。 Redis 官方表示，Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上。 Redis 6.0 版本支持的 I/O 多线程特性，默认情况下 I/O 多线程只针对发送响应数据（write client socket），并不会以多线程的方式处理读请求（read client socket）。要想开启多线程处理客户端读请求，就需要把 Redis.conf 配置文件中的 io-threads-do-reads 配置项设为 yes。 //读请求也使用io多线程 io-threads-do-reads yes 同时， Redis.conf 配置文件中提供了 IO 多线程个数的配置项。 // io-threads N，表示启用 N-1 个 I/O 多线程（主线程也算一个 I/O 线程） io-threads 4 关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。 因此， Redis 6.0 版本之后，Redis 在启动的时候，默认情况下会额外创建 6 个线程（这里的线程数不包括主线程）： Redis-server ： Redis的主线程，主要负责执行命令； bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务； io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:5","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 持久化 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 如何实现数据不丢失？ Redis 的读写操作都是在内存中，所以 Redis 性能才会高，但是当 Redis 重启后，内存中的数据就会丢失，那为了保证内存中的数据不会丢失，Redis 实现了数据持久化的机制，这个机制会把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据。 Redis 共有三种数据持久化的方式： AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点； ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#AOF 日志是如何实现的？ Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。 我这里以「set name xiaolin」命令作为例子，Redis 执行了这条命令后，记录在 AOF 日志里的内容如下图： 我这里给大家解释下。 「*3」表示当前命令有三个部分，每部分都是以「$+数字」开头，后面紧跟着具体的命令、键或值。然后，这里的「数字」表示这部分中的命令、键或值一共有多少字节。例如，「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。 为什么先执行命令，再把数据写入日志呢？ Reids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。 避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。 不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。 当然，这样做也会带来风险： 数据可能会丢失： 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。 可能阻塞其他操作： 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。 AOF 写回策略有几种？ 先来看看，Redis 写入 AOF 日志的过程，如下图： 具体说说： Redis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区； 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘； 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。 Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填： Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘； Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘； No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。 我也把这 3 个写回策略的优缺点总结成了一张表格： AOF 日志过大，会触发什么机制？ AOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。 所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。 AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。 举个例子，在没有使用重写机制前，假设前后执行了「set name xiaolin」和「set name xiaolincoding」这两个命令的话，就会将这两个命令记录到 AOF 文件。 但是在使用重写机制后，就会读取 name 最新的 value（键值对） ，然后用一条 「set name xiaolincoding」命令记录到新的 AOF 文件，之前的第一个命令就没有必要记录了，因为它属于「历史」命令，没有作用了。这样一来，一个键值对在重写日志中只用一条命令就行了。 重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。 重写 AOF 日志的过程是怎样的？ Redis 的重写 AOF 过程是由后台子进程 *bgrewriteaof* 来完成的，这么做可以达到两个好处： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 触发重写机制后，主进程就会创建重写 AOF 的子进程，此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF 文件）。 但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？ 为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。 在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。 也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作: 执行客户端发来的命令； 将执行后的写命令追加到 「AOF 缓冲区」； 将执行后的写命令追加到 「AOF 重写缓冲区」； 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。 主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作： 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 信号函数执行完后，主进程就可以继续像往常一样处理命令了。 TIP AOF 日志的内容就暂时提这些，想更详细了解 AOF 日志的工作原理，可以详细看这篇：AOF 持久化是怎么实现的(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#RDB 快照是如何实现的呢？ 因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。 为了解决这个问题，Redis 增加了 RDB 快照。所谓的快照，就是记录某一个瞬间东西，比如当我们给风景拍照时，那一个瞬间的画面和信息就记录到了一张照片。 所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。 因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。 RDB 做快照时会阻塞线程吗？ Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行： 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置： save 900 1 save 300 10 save 60 10000 别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是： 900 秒之内，对数据库进行了至少 1 次修改； 300 秒之内，对数据库进行了至少 10 次修改； 60 秒之内，对数据库进行了至少 10000 次修改。 这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。 RDB 在执行快照的时候，数据能修改吗？ 可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。 执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。 如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。 TIP RDB 快照的内容就暂时提这些，想更详细了解 RDB 快照的工作原理，可以详细看这篇：RDB 快照是怎么实现的？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#为什么会有混合持久化？ RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。 AOF 优点是丢失数据少，但是数据恢复不快。 为了集成了两者的优点， Redis 4.0 提出了混合使用 AOF 日志和内存快照，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。 混合持久化工作在 AOF 日志重写过程，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。 也就是说，使用了混合持久化，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。 这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。 加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。 混合持久化优点： 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。 混合持久化缺点： AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:4","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 集群 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:5:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 如何实现服务高可用？ 要想设计一个高可用的 Redis 服务，一定要从 Redis 的多服务节点来考虑，比如 Redis 的主从复制、哨兵模式、切片集群。 主从复制 主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。 也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。 注意，主从服务器之间的命令复制是异步进行的。 具体来说，在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。 所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的。 TIP 想更详细了解 Redis 主从复制的工作原理，可以详细看这篇：主从复制是怎么实现的？(opens new window) 哨兵模式 在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。 为了解决这个问题，Redis 增加了哨兵模式（Redis Sentinel），因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。 TIP 想更详细了解 Redis 哨兵的工作原理，可以详细看这篇：哨兵是怎么实现的？(opens new window) 切片集群模式 当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。 Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步： 根据键值对的 key，按照 CRC16 算法 (opens new window)计算一个 16 bit 的值。 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案： 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。 为了方便你的理解，我通过一张图来解释数据、哈希槽，以及节点三者的映射分布关系。 上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。 redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1 redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3 然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。 需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:5:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#集群脑裂导致数据丢失怎么办？ 什么是脑裂？ 先来理解集群的脑裂现象，这就好比一个人有两个大脑，那么到底受谁控制呢？ 那么在 Redis 中，集群脑裂产生数据丢失的现象是怎样的呢？ 在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。 如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被旧主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。 这时，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在「从节点」中选举出一个 leader 作为主节点，这时集群就有两个主节点了 —— 脑裂出现了。 然后，网络突然好了，哨兵因为之前已经选举出一个新主节点了，它就会把旧主节点降级为从节点（A），然后从节点（A）会向新主节点请求数据同步，因为第一次同步是全量同步的方式，此时的从节点（A）会清空掉自己本地的数据，然后再做全量同步。所以，之前客户端在过程 A 写入的数据就会丢失了，也就是集群产生脑裂数据丢失的问题。 总结一句话就是：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。 解决方案 当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。 在 Redis 的配置文件中有两个参数我们可以设置： min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。 min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。 我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。 这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的写请求了。 即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了。 等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。 再来举个例子。 假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。 同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。 这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:5:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 过期删除与内存淘汰 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 使用的过期删除策略是什么？ Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。 每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。 当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中： 如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。 什么是惰性删除策略？ 惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。 惰性删除的流程图如下： 惰性删除策略的优点： 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点： 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 什么是定期删除策略？ 定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。 Redis 的定期删除的流程： 从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。 定期删除的流程如下： 定期删除策略的优点： 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点： 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。 TIP Redis 的过期删除的内容就暂时提这些，想更详细了解的，可以详细看这篇：Redis 过期删除策略和内存淘汰策略有什么区别？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 持久化时，对过期键会如何处理的？ Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。 RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。 RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。 RDB 加载阶段 ：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况： 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响； 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。 AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。 AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。 AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 主从模式中，对过期键会如何处理？ 当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。 从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 内存满了，会发生什么？ 在 Redis 的运行内存达到了某个阀值，就会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:4","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 内存淘汰策略有哪些？ Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。 1、不进行数据淘汰的策略 noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。 2、进行数据淘汰的策略 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰： volatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰： allkeys-random：随机淘汰任意键值; allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:5","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#LRU 算法和 LFU 算法有什么区别？ 什么是 LRU 算法？ LRU 全称是 Least Recently Used 翻译为最近最少使用，会选择淘汰最近最少使用的数据。 传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。 Redis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题： 需要用链表管理所有的缓存数据，这会带来额外的空间开销； 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。 Redis 是如何实现 LRU 算法的？ Redis 实现的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间。 当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个。 Redis 实现的 LRU 算法的优点： 不用为所有的数据维护一个大链表，节省了空间占用； 不用在每次数据访问时都移动链表项，提升了缓存的性能； 但是 LRU 算法有一个问题，无法解决缓存污染问题，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。 因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。 什么是 LFU 算法？ LFU 全称是 Least Frequently Used 翻译为最近最不常用的，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。 所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。 Redis 是如何实现 LFU 算法的？ LFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下： typedef struct redisObject { ... // 24 bits，用于记录对象的访问信息 unsigned lru:24; ... } robj; Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。 在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。 在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次。 TIP Redis 的内存淘汰的内容就暂时提这些，想更详细了解的，可以详细看这篇：Redis 过期删除策略和内存淘汰策略有什么区别？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:6","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 缓存设计 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:7:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#如何避免缓存雪崩、缓存击穿、缓存穿透？ 如何避免缓存雪崩？ 通常我们为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存。 那么，当大量缓存数据在同一时间过期（失效）**时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩的问题。 对于缓存雪崩问题，我们可以采用两种方案解决。 将缓存失效时间随机打散： 我们可以在原有的失效时间基础上增加一个随机值（比如 1 到 10 分钟）这样每个缓存的过期时间都不重复了，也就降低了缓存集体失效的概率。 设置缓存不过期： 我们可以通过后台服务来更新缓存数据，从而避免因为缓存失效造成的缓存雪崩，也可以在一定程度上避免缓存并发问题。 如何避免缓存击穿？ 我们的业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。 如果缓存中的某个热点数据过期了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是缓存击穿的问题。 可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 应对缓存击穿可以采取前面说到两种方案： 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间； 如何避免缓存穿透？ 当发生缓存雪崩或击穿时，数据库中还是保存了应用要访问的数据，一旦缓存恢复相对应的数据，就可以减轻数据库的压力，而缓存穿透就不一样了。 当用户访问的数据，既不在缓存中，也不在数据库中，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是缓存穿透的问题。 缓存穿透的发生一般有这两种情况： 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据； 黑客恶意攻击，故意大量访问某些读取不存在数据的业务； 应对缓存穿透的方案，常见的方案有三种。 非法请求的限制：当有大量恶意请求访问不存在的数据的时候，也会发生缓存穿透，因此在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。 设置空值或者默认值：当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。 使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在：我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器的。 TIP 推荐阅读：什么是缓存雪崩、击穿、穿透？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:7:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#如何设计一个缓存策略，可以动态缓存热点数据呢？ 由于数据存储受限，系统并不是将所有数据都需要存放到缓存中的，而只是将其中一部分热点数据缓存起来，所以我们要设计一个热点数据动态缓存的策略。 热点数据动态缓存的策略总体思路：通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据。 以电商平台场景中的例子，现在要求只缓存用户经常访问的 Top 1000 的商品。具体细节如下： 先通过缓存系统做一个排序队列（比如存放 1000 个商品），系统会根据商品的访问时间，更新队列信息，越是最近访问的商品排名越靠前； 同时系统会定期过滤掉队列中排名最后的 200 个商品，然后再从数据库中随机读取出 200 个商品加入队列中； 这样当请求每次到达的时候，会先从队列中获取商品 ID，如果命中，就根据 ID 再从另一个缓存数据结构中读取实际的商品信息，并返回。 在 Redis 中可以用 zadd 方法和 zrange 方法来完成排序队列和获取 200 个商品的操作。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:7:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#说说常见的缓存更新策略？ 常见的缓存更新策略共有3种： Cache Aside（旁路缓存）策略； Read/Write Through（读穿 / 写穿）策略； Write Back（写回）策略； 实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略应用不了。 Cache Aside（旁路缓存）策略 Cache Aside（旁路缓存）策略是最常用的，应用程序直接与「数据库、缓存」交互，并负责对缓存的维护，该策略又可以细分为「读策略」和「写策略」。 写策略的步骤： 先更新数据库中的数据，再删除缓存中的数据。 读策略的步骤： 如果读取的数据命中了缓存，则直接返回数据； 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。 注意，写策略的步骤的顺序不能倒过来，即不能先删除缓存再更新数据库，原因是在「读+写」并发的时候，会出现缓存和数据库的数据不一致性的问题。 举个例子，假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。 最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致。 为什么「先更新数据库再删除缓存」不会有数据不一致的问题？ 继续用「读 + 写」请求的并发的场景来分析。 假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中。 最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库数据不一致。 从上面的理论上分析，先更新数据库，再删除缓存也是会出现数据不一致性的问题，但是在实际中，这个问题出现的概率并不高。 因为缓存的写入通常要远远快于数据库的写入，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。 Cache Aside 策略适合读多写少的场景，不适合写多的场景，因为当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案： 一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响； 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受。 Read/Write Through（读穿 / 写穿）策略 Read/Write Through（读穿 / 写穿）策略原则是应用程序只和缓存交互，不再和数据库交互，而是由缓存和数据库交互，相当于更新数据库的操作由缓存自己代理了。 1、Read Through 策略 先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用。 2、Write Through 策略 当有数据更新的时候，先查询要写入的数据在缓存中是否已经存在： 如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。 如果缓存中数据不存在，直接更新数据库，然后返回； 下面是 Read Through/Write Through 策略的示意图： Read Through/Write Through 策略的特点是由缓存节点而非应用程序来和数据库打交道，在我们开发过程中相比 Cache Aside 策略要少见一些，原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库和自动加载数据库中的数据的功能。而我们在使用本地缓存的时候可以考虑使用这种策略。 Write Back（写回）策略 Write Back（写回）策略在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行。 实际上，Write Back（写回）策略也不能应用到我们常用的数据库和缓存的场景中，因为 Redis 并没有异步更新数据库的功能。 Write Back 是计算机体系结构中的设计，比如 CPU 的缓存、操作系统中文件系统的缓存都采用了 Write Back（写回）策略。 Write Back 策略特别适合写多的场景，因为发生写操作的时候， 只需要更新缓存，就立马返回了。比如，写文件的时候，实际上是写入到文件系统的缓存就返回了，并不会写磁盘。 但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险，因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的。 这里贴一张 CPU 缓存与内存使用 Write Back 策略的流程图： 有没有觉得这个流程很熟悉？因为我在写 CPU 缓存文章 (opens new window)的时候提到过。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:7:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#如何保证缓存和数据库数据的一致性？ TIP 推荐阅读：数据库和缓存如何保证一致性？(opens new window) ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:7:4","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 实战 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:0","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 如何实现延迟队列？ 延迟队列是指把当前要做的事情，往后推迟一段时间再做。延迟队列的常见使用场景有以下几种： 在淘宝、京东等购物平台上下单，超过一定时间未付款，订单会自动取消； 打车的时候，在规定时间没有车主接单，平台会取消你的单并提醒你暂时没有车主接单； 点外卖的时候，如果商家在10分钟还没接单，就会自动取消订单； 在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间。 使用 zadd score1 value1 命令就可以一直往内存中生产消息。再利用 zrangebysocre 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:1","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 的大 key 如何处理？ 什么是 Redis 大 key？ 大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。 一般而言，下面这两种情况被称为大 key： String 类型的值大于 10 KB； Hash、List、Set、ZSet 类型的元素的个数超过 5000个； 大 key 会造成什么问题？ 大 key 会带来以下四种影响： 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。 如何找到大 key ？ 1、redis-cli –bigkeys 查找大key 可以通过 redis-cli –bigkeys 命令查找大 key： redis-cli -h 127.0.0.1 -p6379 -a \"password\" -- bigkeys 使用的时候注意事项： 最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点； 如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。 该方式的不足之处： 这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的 bigkey； 对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大； 2、使用 SCAN 命令查找大 key 使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。 对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。 对于集合类型来说，有两种方法可以获得它占用的内存大小： 如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：LLEN 命令；Hash 类型：HLEN 命令；Set 类型：SCARD 命令；Sorted Set 类型：ZCARD 命令； 如果不能提前知道写入集合的元素大小，可以使用 MEMORY USAGE 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间。 3、使用 RdbTools 工具查找大 key 使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。 比如，下面这条命令，将大于 10 kb 的 key 输出到一个表格文件。 rdb dump.rdb -c memory --bytes 10240 -f redis.csv 如何删除大 key？ 删除操作的本质是要释放键值对占用的内存空间，不要小瞧内存的释放过程。 释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序。 所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞，如果主线程发生了阻塞，其他所有请求可能都会超时，超时越来越多，会造成 Redis 连接耗尽，产生各种异常。 因此，删除大 key 这一个动作，我们要小心。具体要怎么做呢？这里给出两种方法： 分批次删除 异步删除（Redis 4.0版本以上） 1、分批次删除 对于删除大 Hash，使用 hscan 命令，每次获取 100 个字段，再用 hdel 命令，每次删除 1 个字段。 Python代码： def del_large_hash(): r = redis.StrictRedis(host='redis-host1', port=6379) large_hash_key =\"xxx\" #要删除的大hash键名 cursor = '0' while cursor != 0: # 使用 hscan 命令，每次获取 100 个字段 cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100) for item in data.items(): # 再用 hdel 命令，每次删除1个字段 r.hdel(large_hash_key, item[0]) 对于删除大 List，通过 ltrim 命令，每次删除少量元素。 Python代码： def del_large_list(): r = redis.StrictRedis(host='redis-host1', port=6379) large_list_key = 'xxx' #要删除的大list的键名 while r.llen(large_list_key)\u003e0: #每次只删除最右100个元素 r.ltrim(large_list_key, 0, -101) 对于删除大 Set，使用 sscan 命令，每次扫描集合中 100 个元素，再用 srem 命令每次删除一个键。 Python代码： def del_large_set(): r = redis.StrictRedis(host='redis-host1', port=6379) large_set_key = 'xxx' # 要删除的大set的键名 cursor = '0' while cursor != 0: # 使用 sscan 命令，每次扫描集合中 100 个元素 cursor, data = r.sscan(large_set_key, cursor=cursor, count=100) for item in data: # 再用 srem 命令每次删除一个键 r.srem(large_size_key, item) 对于删除大 ZSet，使用 zremrangebyrank 命令，每次删除 top 100个元素。 Python代码： def del_large_sortedset(): r = redis.StrictRedis(host='large_sortedset_key', port=6379) large_sortedset_key='xxx' while r.zcard(large_sortedset_key)\u003e0: # 使用 zremrangebyrank 命令，每次删除 top 100个元素 r.zremrangebyrank(large_sortedset_key,0,99) 2、异步删除 从 Redis 4.0 版本开始，可以采用异步删除法，用 unlink 命令代替 del 来删除。 这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。 除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，达到某些条件的时候自动进行异步删除。 主要有 4 种场景，默认都是关闭的： lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del noslave-lazy-flush no 它们代表的含义如下： lazyfree-lazy-eviction：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除； lazyfree-lazy-expire：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除； lazyfree-lazy-server-del：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除； slave-lazy-flush：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除。 建议开启其中的 lazyfree-lazy-eviction、lazyfree-lazy-expire、lazyfree-lazy-server-del 等配置，这样就可以有效的提高主线程的执行效率。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:2","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 管道有什么用？ 管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。 普通命令模式，如下图所示： 管道模式，如下图所示： 使用管道技术可以解决多个命令执行时的网络等待，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。 但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。 要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:3","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#Redis 事务支持回滚吗？ MySQL 在执行事务时，会提供回滚机制，当事务执行发生错误时，事务中的所有操作都会撤销，已经修改的数据也会被恢复到事务执行前的状态。 Redis 中并没有提供回滚机制，虽然 Redis 提供了 DISCARD 命令，但是这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。 下面是 DISCARD 命令用法： #读取 count 的值4 127.0.0.1:6379\u003e GET count \"1\" #开启事务 127.0.0.1:6379\u003e MULTI OK #发送事务的第一个操作，对count减1 127.0.0.1:6379\u003e DECR count QUEUED #执行DISCARD命令，主动放弃事务 127.0.0.1:6379\u003e DISCARD OK #再次读取a:stock的值，值没有被修改 127.0.0.1:6379\u003e GET count \"1\" 事务执行过程中，如果命令入队时没报错，而事务提交后，实际执行时报错了，正确的命令依然可以正常执行，所以这可以看出 Redis 并不一定保证原子性（原子性：事务中的命令要不全部成功，要不全部失败）。 比如下面这个例子： #获取name原本的值 127.0.0.1:6379\u003e GET name \"xiaolin\" #开启事务 127.0.0.1:6379\u003e MULTI OK #设置新值 127.0.0.1:6379(TX)\u003e SET name xialincoding QUEUED #注意，这条命令是错误的 # expire 过期时间正确来说是数字，并不是‘10s’字符串，但是还是入队成功了 127.0.0.1:6379(TX)\u003e EXPIRE name 10s QUEUED #提交事务，执行报错 #可以看到 set 执行成功，而 expire 执行错误。 127.0.0.1:6379(TX)\u003e EXEC 1) OK 2) (error) ERR value is not an integer or out of range #可以看到，name 还是被设置为新值了 127.0.0.1:6379\u003e GET name \"xialincoding\" 为什么Redis 不支持事务回滚？ Redis 官方文档 (opens new window)的解释如下： 大概的意思是，作者不支持事务回滚的原因有以下两个： 他认为 Redis 事务的执行时，错误通常都是编程错误造成的，这种错误通常只会出现在开发环境中，而很少会在实际的生产环境中出现，所以他认为没有必要为 Redis 开发事务回滚功能； 不支持事务回滚是因为这种复杂的功能和 Redis 追求的简单高效的设计主旨不符合。 这里不支持事务回滚，指的是不支持事务运行时错误的事务回滚。 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:4","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Redis八股"],"content":"#如何用 Redis 实现分布式锁的？ 分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。如下图所示： Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁，而且 Redis 的读写性能高，可以应对高并发的锁操作场景。 Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁： 如果 key 不存在，则显示插入成功，可以用来表示加锁成功； 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。 基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁； 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间； 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端； 满足这三个条件的分布式命令如下： SET lock_key unique_value NX PX 10000 lock_key 就是 key 键； unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作； NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作； PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。 而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。 可以看到，解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。 // 释放锁时，先比较 unique_value 是否相等，避免锁的误释放 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 这样一来，就通过使用 SET 命令和 Lua 脚本在 Redis 单节点上完成了分布式锁的加锁和解锁。 基于 Redis 实现分布式锁有什么优缺点？ 基于 Redis 实现分布式锁的优点： 性能高效（这是选择缓存实现分布式锁最核心的出发点）。 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。 基于 Redis 实现分布式锁的缺点： 超时时间不好设置 。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。 那么如何合理设置超时时间呢？ 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。 Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。 Redis 如何解决集群情况下分布式锁的可靠性？ 为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。 它是基于多个 Redis 节点的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。 Redlock 算法的基本思路，是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。 这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。 Redlock 算法加锁三个过程： 第一步是，客户端获取当前时间（t1）。 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作： 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 \u003c 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。 可以看到，加锁成功要同时满足两个条件（简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功）： 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁； 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。 加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。 加锁失败后，客户端向所有 Redis 节点发起释放锁的操作，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。 参考资料： 《Redis 设计与实现》 《Redis 实战》 《Redis 核心技术与实战》 《Redis 核心原理与实战 》 ","date":"2023-05-28","objectID":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:8:5","tags":["Redis常见面试题"],"title":"Redis常见面试题","uri":"/posts/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["心跳包设计"],"content":"TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？","date":"2023-05-28","objectID":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/","tags":["TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？"],"title":"TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？","uri":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/"},{"categories":["心跳包设计"],"content":"本文解释了 SO_KEEPALIVE 选项，和为什么要在应用层加入心跳包机制，以及心跳包机制如何设计的方方面面。 在实际开发中，我们需要处理下面两种情形中遇到的问题： 情形一： 一个客户端连接服务器以后，如果长期没有和服务器有数据来往，可能会被防火墙程序关闭连接，有时候我们并不想要被关闭连接。例如，对于一个即时通讯软件，如果服务器没有消息时，我们确实不会和服务器有任何数据交换，但是如果连接被关闭了，有新消息来时，我们再也没法收到了，这就违背了“即时通讯”的设计要求。 情形二：通常情况下，服务器与某个客户端一般不是位于同一个网络，其之间可能经过数个路由器和交换机，如果其中某个必经路由器或者交换器出现了故障，并且一段时间内没有恢复，导致这之间的链路不再畅通，而此时服务器与客户端之间也没有数据进行交换，由于 TCP 连接是状态机，对于这种情况，无论是客户端或者服务器都无法感知与对方的连接是否正常，这类连接我们一般称之为“死链”。 情形一：中的应用场景要求必须保持客户端与服务器之间的连接正常，就是我们通常所说的“保活“。如上文所述，当服务器与客户端一定时间内没有有效业务数据来往时，我们只需要给对端发送心跳包即可实现保活。 情形二中的死链，只要我们此时任意一端给对端发送一个数据包即可检测链路是否正常，这类数据包我们也称之为”心跳包”，这种操作我们称之为“心跳检测”。顾名思义，如果一个人没有心跳了，可能已经死亡了；一个连接长时间没有正常数据来往，也没有心跳包来往，就可以认为这个连接已经不存在，为了节约服务器连接资源，我们可以通过关闭 socket，回收连接资源。 根据上面的分析，让我再强调一下，心跳检测一般有两个作用： 保活 检测死链 ","date":"2023-05-28","objectID":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/:0:0","tags":["TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？"],"title":"TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？","uri":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/"},{"categories":["心跳包设计"],"content":"TCP keepalive 选项 操作系统的 TCP/IP 协议栈其实提供了这个的功能，即 keepalive 选项。在 Linux 操作系统中，我们可以通过代码启用一个 socket 的心跳检测（即每隔一定时间间隔发送一个心跳检测包给对端），代码如下： //on 是 1 表示打开 keepalive 选项，为 0 表示关闭，0 是默认值 int on = 1; setsockopt(fd, SOL_SOCKET, SO_KEEPALIVE, \u0026on, sizeof(on)); 但是，即使开启了这个选项，这个选项默认发送心跳检测数据包的时间间隔是 7200 秒（2 小时），这时间间隔实在是太长了，不具有实用性。 当然，我们可以通过继续设置 keepalive 相关的三个选项来改变这个时间间隔，它们分别是 TCP_KEEPIDLE、TCP_KEEPINTVL 和 TCP_KEEPCNT，示例代码如下： //发送 keepalive 报文的时间间隔 int val = 7200; setsockopt(fd, IPPROTO_TCP, TCP_KEEPIDLE, \u0026val, sizeof(val)); //两次重试报文的时间间隔 int interval = 75; setsockopt(fd, IPPROTO_TCP, TCP_KEEPINTVL, \u0026interval, sizeof(interval)); int cnt = 9; setsockopt(fd, IPPROTO_TCP, TCP_KEEPCNT, \u0026cnt, sizeof(cnt)); TCP_KEEPIDLE 选项设置了发送 keepalive 报文的时间间隔，发送时如果对端回复 ACK。则本端 TCP 协议栈认为该连接依然存活，继续等 7200 秒后再发送 keepalive 报文；如果对端回复 RESET，说明对端进程已经重启，本端的应用程序应该关闭该连接。 如果对端没有任何回复，则本端做重试，如果重试 9 次（TCP_KEEPCNT 值）（前后重试间隔为 75 秒（TCP_KEEPINTVL 值））仍然不可达，则向应用程序返回 ETIMEOUT（无任何应答）或 EHOST 错误信息。 我们可以使用如下命令查看 Linux 系统上的上述三个值的设置情况： [root@iZ238vnojlyZ ~]# sysctl -a | grep keepalive net.ipv4.tcp_keepalive_intvl = 75 net.ipv4.tcp_keepalive_probes = 9 net.ipv4.tcp_keepalive_time = 7200 在 Windows 系统设置 keepalive 及对应选项的代码略有不同： //开启 keepalive 选项 const char on = 1; setsockopt(socket, SOL_SOCKET, SO_KEEPALIVE, (char *)\u0026on, sizeof(on); // 设置超时详细信息 DWORD cbBytesReturned; tcp_keepalive klive; // 启用保活 klive.onoff = 1; klive.keepalivetime = 7200; // 重试间隔为10秒 klive.keepaliveinterval = 1000 * 10; WSAIoctl(socket, SIO_KEEPALIVE_VALS, \u0026klive, sizeof(tcp_keepalive), NULL, 0, \u0026cbBytesReturned, NULL, NULL); ","date":"2023-05-28","objectID":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/:1:0","tags":["TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？"],"title":"TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？","uri":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/"},{"categories":["心跳包设计"],"content":"应用层的心跳包机制设计 由于 keepalive 选项需要为每个连接中的 socket 开启，由于这不一定是必须的，可能会产生大量无意义的带宽浪费，且 keepalive 选项不能与应用层很好地交互，因此一般实际的服务开发中，还是建议读者在应用层设计自己的心跳包机制。那么如何设计呢？ 从技术来讲，心跳包其实就是一个预先规定好格式的数据包，在程序中启动一个定时器，定时发送即可，这是最简单的实现思路。但是，如果通信的两端有频繁的数据来往，此时到了下一个发心跳包的时间点了，此时发送一个心跳包。这其实是一个流量的浪费，既然通信双方不断有正常的业务数据包来往，这些数据包本身就可以起到保活作用，为什么还要浪费流量去发送这些心跳包呢？ 以我写的 NIO 网络库 netpoll-cpp 为例，为了检测死链，防止TCP短连接浪费大量资源，使用的策略是：把连接关闭资源释放这件事用一个回调来做，然后每次连接一旦有可读消息或者主动写入消息，都通过定时器添加一个回调，比如一有消息收发就添加30s后关闭连接的回调。 很明显如果直接每次定时器任务一触发就关闭连接，并不能起到延长生命周期的效果（因为后续添加的事件无法取消前面的事件），在 netpoll-cpp 中，这个定时任务其实是一个通过RAII封装的类，每次定时器里的任务队列只需要把该元素pop出来实现回调（调用析构函数），而析构函数是否调用就是另一个妙处了，我是通过引用计数来进行判断的，也就是说一个定时任务用 shared_ptr 进行封装，然后成员变量中留一份 weak_ptr ，这样就能够感知到该回调是否已经调用，如果调用了，那么引用计数将会为0，如果还没有调用，也就是延迟30s还没到，那么将 weak_ptr 升级为 shared_ptr 继续插入一个 30s 的任务，就算之前的任务触发，也不会调用回调，因为引用计数没有下降到0，这就是整个基于定时器的 C++ 中比较妙的心跳设计。 基于这个回调，我们只需要在这个类中加入一个 TCP 连接类的 weak_ptr 实例，只要 TCP 连接还存在，则关闭，否则啥也不做，这样同时实现了内存安全。 我们发现有了这样的底层回调的设计，灵活性和维护性将大大增加，但是定时器任务的触发存在比较大的消耗，但很明显，这个任务其实不需要很高精度的定时，所以 netpoll-cpp 中基于底层的定时器又加了一个时间轮实现的定时器，旨在将定时器任务的添加和任务的触发的性能消耗降到最低，具体可以看看我的博客 实现高性能时间轮用于踢出空闲连接 。 讲了这么多，最后总结具体检测死链和保活的心跳设计大家应该有很好的思路了，我的想法就是，把心跳的底层维护转化思路为延长连接的生命周期。比如需要检测死链，就只需要在每次收发数据包的时候延长生命周期即可，如果很久没有收发数据，那么该连接自然会被销毁。而保活也只需要你自己设计对应的心跳包，然后每次心跳包一旦触发就延长生命周期。 延长生命周期的代码逻辑如下： void TcpConnectionImpl::extendLife() { if (m_idleTimeout \u003e 0) { auto now = Timestamp::now(); if (now \u003c (m_lastTimingWheelUpdateTime + 1.0)) return; m_lastTimingWheelUpdateTime = now; auto entry = m_kickoffEntry.lock(); if (entry) { auto ptr = m_timingWheelWeakPtr.lock(); if (ptr) ptr-\u003einsertEntry(m_idleTimeout, entry); } } } 具体源代码可以查看：https://github.com/ACking-you/netpoll-cpp/blob/master/netpoll/net/inner/tcp_connection_impl.cc ","date":"2023-05-28","objectID":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/:2:0","tags":["TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？"],"title":"TCP中已有SO_KEEPALIVE选项，为什么还要在应用层加入心跳包机制？","uri":"/posts/tcp%E4%B8%AD%E5%B7%B2%E6%9C%89so_keepalive%E9%80%89%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E5%B1%82%E5%8A%A0%E5%85%A5%E5%BF%83%E8%B7%B3%E5%8C%85%E6%9C%BA%E5%88%B6/"},{"categories":["MySQL锁篇"],"content":"一、MySQL有哪些锁","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"MySQL 有哪些锁？ 在 MySQL 里，根据加锁的范围，可以分为全局锁、表级锁和行锁三类。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:0:0","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#全局锁 全局锁是怎么用的？ 要使用全局锁，则要执行这条命令： flushtableswithreadlock执行后，整个数据库就处于只读状态了，这时其他线程执行以下操作，都会被阻塞： 对数据的增删改操作，比如 insert、delete、update等语句； 对表结构的更改操作，比如 alter table、drop table 等语句。 如果要释放全局锁，则要执行这条命令： unlocktables当然，当会话断开了，全局锁会被自动释放。 全局锁应用场景是什么？ 全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。 举个例子大家就知道了。 在全库逻辑备份期间，假设不加全局锁的场景，看看会出现什么意外的情况。 如果在全库逻辑备份期间，有用户购买了一件商品，一般购买商品的业务逻辑是会涉及到多张数据库表的更新，比如在用户表更新该用户的余额，然后在商品表更新被购买的商品的库存。 那么，有可能出现这样的顺序： 先备份了用户表的数据； 然后有用户发起了购买商品的操作； 接着再备份商品表的数据。 也就是在备份用户表和商品表之间，有用户购买了商品。 这种情况下，备份的结果是用户表中该用户的余额并没有扣除，反而商品表中该商品的库存被减少了，如果后面用这个备份文件恢复数据库数据的话，用户钱没少，而库存少了，等于用户白嫖了一件商品。 所以，在全库逻辑备份期间，加上全局锁，就不会出现上面这种情况了。 加全局锁又会带来什么缺点呢？ 加上全局锁，意味着整个数据库都是只读状态。 那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？ 有的，如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。 因为在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的 Read View，这就是事务四大特性中的隔离性，这样备份期间备份的数据一直是在开启事务时的数据。 备份数据库的工具是 mysqldump，在使用 mysqldump 时加上 –single-transaction 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持「可重复读隔离级别的事务」的存储引擎。 InnoDB 存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。 但是，对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:1:0","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#表级锁 MySQL 表级锁有哪些？具体怎么用的。 MySQL 里面表级别的锁有这几种： 表锁； 元数据锁（MDL）; 意向锁； AUTO-INC 锁； ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:2:0","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#表锁 先来说说表锁。 如果我们想对学生表（t_student）加表锁，可以使用下面的命令： //表级别的共享锁，也就是读锁；locktablest_studentread;//表级别的独占锁，也就是写锁；locktablest_stuentwrite;需要注意的是，表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。 也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。 要释放表锁，可以使用下面这条命令，会释放当前会话的所有表锁： unlocktables另外，当会话退出后，也会释放所有表锁。 不过尽量避免在使用 InnoDB 引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能，InnoDB 牛逼的地方在于实现了颗粒度更细的行级锁。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:2:1","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#元数据锁 再来说说元数据锁（MDL）。 我们不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL： 对一张表进行 CRUD 操作时，加的是 MDL 读锁； 对一张表做结构变更操作的时候，加的是 MDL 写锁； MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。 当有线程在执行 select 语句（ 加 MDL 读锁）的期间，如果有其他线程要更改该表的结构（ 申请 MDL 写锁），那么将会被阻塞，直到执行完 select 语句（ 释放 MDL 读锁）。 反之，当有线程对表结构进行变更（ 加 MDL 写锁）的期间，如果有其他线程执行了 CRUD 操作（ 申请 MDL 读锁），那么就会被阻塞，直到表结构变更完成（ 释放 MDL 写锁）。 MDL 不需要显示调用，那它是在什么时候释放的? MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的。 那如果数据库有一个长事务（所谓的长事务，就是开启了事务，但是一直还没提交），那在对表结构做变更操作的时候，可能会发生意想不到的事情，比如下面这个顺序的场景： 首先，线程 A 先启用了事务（但是一直不提交），然后执行一条 select 语句，此时就先对该表加上 MDL 读锁； 然后，线程 B 也执行了同样的 select 语句，此时并不会阻塞，因为「读读」并不冲突； 接着，线程 C 修改了表字段，此时由于线程 A 的事务并没有提交，也就是 MDL 读锁还在占用着，这时线程 C 就无法申请到 MDL 写锁，就会被阻塞， 那么在线程 C 阻塞后，后续有对该表的 select 语句，就都会被阻塞，如果此时有大量该表的 select 语句的请求到来，就会有大量的线程被阻塞住，这时数据库的线程很快就会爆满了。 为什么线程 C 因为申请不到 MDL 写锁，而导致后续的申请读锁的查询操作也会被阻塞？ 这是因为申请 MDL 锁的操作会形成一个队列，队列中写锁获取优先级高于读锁，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。 所以为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了 MDL 读锁，如果可以考虑 kill 掉这个长事务，然后再做表结构的变更。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:2:2","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#意向锁 接着，说说意向锁。 在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」； 在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」； 也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。 而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。 不过，select 也是可以对记录加共享锁和独占锁的，具体方式如下： //先在表上加上意向共享锁，然后对读取的记录加共享锁select...lockinsharemode;//先表上加上意向独占锁，然后对读取的记录加独占锁select...forupdate;意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（*lock tables … read*）和独占表锁（*lock tables … write*）发生冲突。 表锁和行锁是满足读读共享、读写互斥、写写互斥的。 如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。 那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。 所以，意向锁的目的是为了快速判断表里是否有记录被加锁。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:2:3","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#AUTO-INC 锁 表里的主键通常都会设置成自增的，这是通过对主键字段声明 AUTO_INCREMENT 属性实现的。 之后可以在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过 AUTO-INC 锁实现的。 AUTO-INC 锁是特殊的表锁机制，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。 在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 AUTO_INCREMENT 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。 那么，一个事务在持有 AUTO-INC 锁的过程中，其他事务的如果要向该表插入语句都会被阻塞，从而保证插入数据时，被 AUTO_INCREMENT 修饰的字段的值是连续递增的。 但是， AUTO-INC 锁再对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。 因此， 在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种轻量级的锁来实现自增。 一样也是在插入数据的时候，会为被 AUTO_INCREMENT 修饰的字段加上轻量级锁，然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁。 InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁； 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。 当 innodb_autoinc_lock_mode = 1： 普通 insert 语句，自增锁在申请之后就马上释放； 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放； 当 innodb_autoinc_lock_mode = 2 是性能最高的方式，但是当搭配 binlog 的日志格式是 statement 一起使用的时候，在「主从复制的场景」中会发生数据不一致的问题。 举个例子，考虑下面场景： session A 往表 t 中插入了 4 行数据，然后创建了一个相同结构的表 t2，然后两个 session 同时执行向表 t2 中插入数据。 如果 innodb_autoinc_lock_mode = 2，意味着「申请自增主键后就释放锁，不必等插入语句执行完」。那么就可能出现这样的情况： session B 先插入了两个记录，(1,1,1)、(2,2,2)； 然后，session A 来申请自增 id 得到 id=3，插入了（3,5,5)； 之后，session B 继续执行，插入两条记录 (4,3,3)、 (5,4,4)。 可以看到，session B 的 insert 语句，生成的 id 不连续。 当「主库」发生了这种情况，binlog 面对 t2 表的更新只会记录这两个 session 的 insert 语句，如果 binlog_format=statement，记录的语句就是原始语句。记录的顺序要么先记 session A 的 insert 语句，要么先记 session B 的 insert 语句。 但不论是哪一种，这个 binlog 拿去「从库」执行，这时从库是按「顺序」执行语句的，只有当执行完一条 SQL 语句后，才会执行下一条 SQL。因此，在从库上「不会」发生像主库那样两个 session 「同时」执行向表 t2 中插入数据的场景。所以，在备库上执行了 session B 的 insert 语句，生成的结果里面，id 都是连续的。这时，主从库就发生了数据不一致。 要解决这问题，binlog 日志格式要设置为 row，这样在 binlog 里面记录的是主库分配的自增值，到备库执行的时候，主库的自增值是什么，从库的自增值就是什么。 所以，当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:2:4","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#行级锁 InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。 前面也提到，普通的 select 语句是不会对记录加锁的，因为它属于快照读。如果要在查询时对记录加行锁，可以使用下面这两个方式，这种查询会加锁的语句称为锁定读。 //对读取的记录加共享锁select...lockinsharemode;//对读取的记录加独占锁select...forupdate;上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。 共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。 行级锁的类型主要有三类： Record Lock，记录锁，也就是仅仅把一条记录锁上； Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身； Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:3:0","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#Record Lock Record Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的： 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）; 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。 举个例子，当一个事务执行了下面这条语句： mysql\u003ebegin;mysql\u003eselect*fromt_testwhereid=1forupdate;就是对 t_test 表中主键 id 为 1 的这条记录加上 X 型的记录锁，这样其他事务就无法对这条记录进行修改了。 当事务执行 commit 后，事务过程中生成的锁都会被释放。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:3:1","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#Gap Lock Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。 假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。 间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:3:2","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#Next-Key Lock Next-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。 所以，next-key lock 即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中。 next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。 比如，一个事务持有了范围为 (1, 10] 的 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，就会被阻塞。 虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:3:3","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL锁篇"],"content":"#插入意向锁 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。 如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。 举个例子，假设事务 A 已经对表加了一个范围 id 为（3，5）间隙锁。 当事务 A 还没提交的时候，事务 B 向该表插入一条 id = 4 的新记录，这时会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个插入意向锁，然后将锁的状态设置为等待状态（PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁），此时事务 B 就会发生阻塞，直到事务 A 提交了事务。 插入意向锁名字虽然有意向锁，但是它并不是意向锁，它是一种特殊的间隙锁，属于行级别锁。 如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一 个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。 插入意向锁与间隙锁的另一个非常重要的差别是：尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/:3:4","tags":["一、MySQL有哪些锁"],"title":"一、MySQL有哪些锁","uri":"/posts/%E4%B8%80mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%94%81/"},{"categories":["MySQL事务篇"],"content":"事务的隔离级别是怎么实现的？","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"事务隔离级别是怎么实现的？ 这是我的钱包，共有 100 万元。 今天我心情好，我决定给你的转账 100 万，最后的结果肯定是我的余额变为 0 元，你的余额多了 100 万元，是不是想到就很开心？ 转账这一动作在程序里会涉及到一系列的操作，假设我向你转账 100 万的过程是有下面这几个步骤组成的： 可以看到这个转账的过程涉及到了两次修改数据库的操作。 假设在执行第三步骤之后，服务器忽然掉电了，就会发生一个蛋疼的事情，我的账户扣了 100 万，但是钱并没有到你的账户上，也就是说这 100 万消失了！ 要解决这个问题，就要保证转账业务里的所有数据库的操作是不可分割的，要么全部执行成功 ，要么全部失败，不允许出现中间状态的数据。 数据库中的「事务（*Transaction*）」就能达到这样的效果。 我们在转账操作前先开启事务，等所有数据库操作执行完成后，才提交事务，对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，如果中途发生发生中断或错误，那么该事务期间对数据库所做的修改将会被回滚到没执行该事务之前的状态。 没错，今天就来图解 MySQL 事务啦，开车！ ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:0:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#事务有哪些特性？ 事务是由 MySQL 的引擎来实现的，我们常见的 InnoDB 引擎它是支持事务的。 不过并不是所有的引擎都能支持事务，比如 MySQL 原生的 MyISAM 引擎就不支持事务，也正是这样，所以大多数 MySQL 的引擎都是用 InnoDB。 事务看起来感觉简单，但是要实现事务必须要遵守 4 个特性，分别如下： 原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。 一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？ 持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 这次将重点介绍事务的隔离性，这也是面试时最常问的知识的点。 为什么事务要有隔离性，我们就要知道并发事务时会引发什么问题。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:1:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#并行事务会引发什么问题？ MySQL 服务端是允许多个客户端连接的，这意味着 MySQL 会出现同时处理多个事务的情况。 那么在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题。 接下来，通过举例子给大家说明，这些问题是如何发生的。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:2:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#脏读 如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。 举个栗子。 假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取余额数据，然后再执行更新操作，如果此时事务 A 还没有提交事务，而此时正好事务 B 也从数据库中读取余额数据，那么事务 B 读取到的余额数据是刚才事务 A 更新后的数据，即使没有提交事务。 因为事务 A 是还没提交事务的，也就是它随时可能发生回滚操作，如果在上面这种情况事务 A 发生了回滚，那么事务 B 刚才得到的数据就是过期的数据，这种现象就被称为脏读。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:2:1","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#不可重复读 在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。 举个栗子。 假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后继续执行代码逻辑处理，在这过程中如果事务 B 更新了这条数据，并提交了事务，那么当事务 A 再次读取该数据时，就会发现前后两次读到的数据是不一致的，这种现象就被称为不可重复读。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:2:2","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#幻读 在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。 举个栗子。 假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库查询账户余额大于 100 万的记录，发现共有 5 条，然后事务 B 也按相同的搜索条件也是查询出了 5 条记录。 接下来，事务 A 插入了一条余额超过 100 万的账号，并提交了事务，此时数据库超过 100 万余额的账号个数就变为 6。 然后事务 B 再次查询账户余额大于 100 万的记录，此时查询到的记录数量有 6 条，发现和前一次读到的记录数量不一样了，就感觉发生了幻觉一样，这种现象就被称为幻读。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:2:3","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#事务的隔离级别有哪些？ 前面我们提到，当多个事务并发执行时可能会遇到「脏读、不可重复读、幻读」的现象，这些现象会对事务的一致性产生不同程序的影响。 脏读：读到其他事务未提交的数据； 不可重复读：前后读取的数据不一致； 幻读：前后读取的记录数量不一致。 这三个现象的严重性排序如下： SQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下： 读未提交（*read uncommitted*），指一个事务还没提交时，它做的变更就能被其他事务看到； 读提交（*read committed*），指一个事务提交之后，它做的变更才能被其他事务看到； 可重复读（*repeatable read*），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别； 串行化（*serializable* ）；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行； 按隔离水平高低排序如下： 针对不同的隔离级别，并发事务时可能发生的现象也会不同。 也就是说： 在「读未提交」隔离级别下，可能发生脏读、不可重复读和幻读现象； 在「读提交」隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象； 在「可重复读」隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象； 在「串行化」隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。 所以，要解决脏读现象，就要升级到「读提交」以上的隔离级别；要解决不可重复读现象，就要升级到「可重复读」的隔离级别，要解决幻读现象不建议将隔离级别升级到「串行化」。 不同的数据库厂商对 SQL 标准中规定的 4 种隔离级别的支持不一样，有的数据库只实现了其中几种隔离级别，我们讨论的 MySQL 虽然支持 4 种隔离级别，但是与SQL 标准中规定的各级隔离级别允许发生的现象却有些出入。 MySQL 在「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生（注意是很大程度避免，并不是彻底避免），所以 MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生，因为使用「串行化」隔离级别会影响性能。 MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了，详见这篇文章 (opens new window)），解决的方案有两种： 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select … for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 接下来，举个具体的例子来说明这四种隔离级别，有一张账户余额表，里面有一条账户余额为 100 万的记录。然后有两个并发的事务，事务 A 只负责查询余额，事务 B 则会将我的余额改成 200 万，下面是按照时间顺序执行两个事务的行为： 在不同隔离级别下，事务 A 执行过程中查询到的余额可能会不同： 在「读未提交」隔离级别下，事务 B 修改余额后，虽然没有提交事务，但是此时的余额已经可以被事务 A 看见了，于是事务 A 中余额 V1 查询的值是 200 万，余额 V2、V3 自然也是 200 万了； 在「读提交」隔离级别下，事务 B 修改余额后，因为没有提交事务，所以事务 A 中余额 V1 的值还是 100 万，等事务 B 提交完后，最新的余额数据才能被事务 A 看见，因此额 V2、V3 都是 200 万； 在「可重复读」隔离级别下，事务 A 只能看见启动事务时的数据，所以余额 V1、余额 V2 的值都是 100 万，当事务 A 提交事务后，就能看见最新的余额数据了，所以余额 V3 的值是 200 万； 在「串行化」隔离级别下，事务 B 在执行将余额 100 万修改为 200 万时，由于此前事务 A 执行了读操作，这样就发生了读写冲突，于是就会被锁住，直到事务 A 提交后，事务 B 才可以继续执行，所以从 A 的角度看，余额 V1、V2 的值是 100 万，余额 V3 的值是 200万。 这四种隔离级别具体是如何实现的呢？ 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了； 对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问； 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View *来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。*「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View。 注意，执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是： 第一种：begin/start transaction 命令； 第二种：start transaction with consistent snapshot 命令； 这两种开启事务的命令，事务的启动时机是不同的： 执行了 begin/start transaction 命令后，并不代表事务启动了。只有在执行这个命令后，执行了增删查改操作的 SQL 语句，才是事务真正启动的时机； 执行了 start transaction with consistent snapshot 命令，就会马上启动事务。 接下来详细说下，Read View 在 MVCC 里如何工作的？ ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:3:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#Read View 在 MVCC 里如何工作的？ 我们需要了解两个知识： Read View 中四个字段作用； 聚簇索引记录中两个跟事务有关的隐藏列； 那 Read View 到底是个什么东西？ Read View 有四个重要的字段： m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。 min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。 max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1； creator_trx_id ：指的是创建该 Read View 的事务的事务 id。 知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。 假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下： 对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列： trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况： 一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况： 如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。 如果记录的 trx_id 值在 Read View 的 min_trx_id\r和 max_trx_id\r之间，需要判断 trx_id 是否在 m_ids 列表中： 如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。 如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。 这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:4:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#可重复读是如何工作的？ 可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View。 假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下： 事务 A 和 事务 B 的 Read View 具体内容如下： 在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，所以此时活跃事务的事务 id 列表就只有 51，活跃事务的事务 id 列表中最小的事务 id 是事务 A 本身，下一个事务 id 则是 52。 在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，所以此时活跃事务的事务 id 列表是 51 和 52，活跃的事务 id 中最小的事务 id 是事务 A，下一个事务 id 应该是 53。 接着，在可重复读隔离级别下，事务 A 和事务 B 按顺序执行了以下操作： 事务 B 读取账户余额记录，读到余额是 100 万； 事务 A 将账户余额记录修改成 200 万，并没有提交事务； 事务 B 读取账户余额记录，读到余额还是 100 万； 事务 A 提交事务； 事务 B 读取账户余额记录，读到余额依然还是 100 万； 接下来，跟大家具体分析下。 事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，此时发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的，也就是事务 B 可以获取到这条记录。 接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成版本链，如下图： 你可以在上图的「记录的字段」看到，由于事务 A 修改了该记录，以前的记录就变成旧版本记录了，于是最新记录和旧版本记录通过链表的方式串起来，而且最新记录的 trx_id 是事务 A 的事务 id（trx_id = 51）。 然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。 最后，当事务 A 提交事务后，由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200 万并提交了事务， 事务 B 第三次读取记录时，读到的记录都是小林余额是 100 万的这条记录。 就是通过这样的方式实现了，「可重复读」隔离级别下在事务期间读到的记录都是事务启动前的记录。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:5:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#读提交是如何工作的？ 读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。 也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 那读提交隔离级别是怎么工作呢？我们还是以前面的例子来聊聊。 假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，接着按顺序执行了以下操作： 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万； 事务 A 修改数据（还没提交事务），将小林的账户余额从 100 万修改成了 200 万； 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万； 事务 A 提交事务； 事务 B 读取数据（创建 Read View），小林的账户余额为 200 万； 那具体怎么做到的呢？我们重点看事务 B 每次读取数据时创建的 Read View。前两次 事务 B 读取数据时创建的 Read View 如下图： 我们来分析下为什么事务 B 第二次读数据时，读不到事务 A （还未提交事务）修改的数据？ 事务 B 在找到小林这条记录时，会看这条记录的 trx_id 是 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，接下来需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是，沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。 我们来分析下为什么事务 A 提交后，事务 B 就可以读到事务 A 修改的数据？ 在事务 A 提交后，由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建 Read View，此时事务 B 第三次读取数据时创建的 Read View 如下： 事务 B 在找到小林这条记录时，会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的。 正是因为在读提交隔离级别下，事务每次读数据时都重新创建 Read View，那么在事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:6:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL事务篇"],"content":"#总结 事务是在 MySQL 引擎层实现的，我们常见的 InnoDB 引擎是支持事务的，事务的四大特性是原子性、一致性、隔离性、持久性，我们这次主要讲的是隔离性。 当多个事务并发执行的时候，会引发脏读、不可重复读、幻读这些问题，那为了避免这些问题，SQL 提出了四种隔离级别，分别是读未提交、读已提交、可重复读、串行化，从左往右隔离级别顺序递增，隔离级别越高，意味着性能越差，InnoDB 引擎的默认隔离级别是可重复读。 要解决脏读现象，就要将隔离级别升级到读已提交以上的隔离级别，要解决不可重复读现象，就要将隔离级别升级到可重复读以上的隔离级别。 而对于幻读现象，不建议将隔离级别升级为串行化，因为这会导致数据库并发时性能很差。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种： 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select … for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列」的比对，来控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。 在可重复读隔离级别中，普通的 select 语句就是基于 MVCC 实现的快照读，也就是不会加锁的。而 select .. for update 语句就不是快照读了，而是当前读了，也就是每次读都是拿到最新版本的数据，但是它会对读到的记录加上 next-key lock 锁。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/:7:0","tags":["事务的隔离级别是怎么实现的？"],"title":"一、事务的隔离级别是怎么实现的？","uri":"/posts/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/"},{"categories":["MySQL索引篇"],"content":"一、初见索引","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"初见索引 面试中，MySQL 索引相关的问题基本都是一系列问题，都是先从索引的基本原理，再到索引的使用场景，比如： 索引底层使用了什么数据结构和算法？ 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？ 什么时候适用索引？ 什么时候不需要创建索引？ 什么情况下索引会失效？ 有什么优化索引的方法？ ….. 今天就带大家，夯实 MySQL 索引的知识点。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:0:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#什么是索引？ 当你想查阅书中某个知识的内容，你会选择一页一页的找呢？还是在书的目录去找呢？ 傻瓜都知道时间是宝贵的，当然是选择在书的目录去找，找到后再翻到对应的页。书中的目录，就是充当索引的角色，方便我们快速查找书中的内容，所以索引是以空间换时间的设计思想。 那换到数据库中，索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是索引是数据的目录。 所谓的存储引擎，说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。MySQL 存储引擎有 MyISAM 、InnoDB、Memory，其中 InnoDB 是在 MySQL 5.5 之后成为默认的存储引擎。 下图是 MySQL 的结构图，索引和数据就是位于存储引擎中： ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:1:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#索引的分类 你知道索引有哪些吗？大家肯定都能霹雳啪啦地说出聚簇索引、主键索引、二级索引、普通索引、唯一索引、hash索引、B+树索引等等。 然后再问你，你能将这些索引分一下类吗？可能大家就有点模糊了。其实，要对这些索引进行分类，要清楚这些索引的使用和实现方式，然后再针对有相同特点的索引归为一类。 我们可以按照四个角度来分类索引。 按「数据结构」分类：B+tree索引、Hash索引、Full-text索引。 按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）。 按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引。 按「字段个数」分类：单列索引、联合索引。 接下来，按照这些角度来说说各类索引的特点。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:2:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#按数据结构分类 从数据结构的角度来看，MySQL 常见索引有 B+Tree 索引、HASH 索引、Full-Text 索引。 每一种存储引擎支持的索引类型不一定相同，我在表中总结了 MySQL 常见的存储引擎 InnoDB、MyISAM 和 Memory 分别支持的索引类型。 InnoDB 是在 MySQL 5.5 之后成为默认的 MySQL 存储引擎，B+Tree 索引类型也是 MySQL 存储引擎采用最多的索引类型。 在创建表时，InnoDB 存储引擎会根据不同的场景选择不同的列作为索引： 如果有主键，默认会使用主键作为聚簇索引的索引键（key）； 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）； 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）； 其它索引都属于辅助索引（Secondary Index），也被称为二级索引或非聚簇索引。创建的主键索引和二级索引默认使用的是 B+Tree 索引。 为了让大家理解 B+Tree 索引的存储和查询的过程，接下来我通过一个简单例子，说明一下 B+Tree 索引在存储数据中的具体实现。 先创建一张商品表，id 为主键，如下： CREATETABLE`product`(`id`int(11)NOTNULL,`product_no`varchar(20)DEFAULTNULL,`name`varchar(255)DEFAULTNULL,`price`decimal(10,2)DEFAULTNULL,PRIMARYKEY(`id`)USINGBTREE)CHARACTERSET=utf8COLLATE=utf8_general_ciROW_FORMAT=Dynamic;商品表里，有这些行数据： 这些行数据，存储在 B+Tree 索引时是长什么样子的？ B+Tree 是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据是按主键顺序存放的。每一层父节点的索引值都会出现在下层子节点的索引值中，因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。 主键索引的 B+Tree 如图所示（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）： #通过主键查询商品数据的过程 比如，我们执行了下面这条查询语句： select*fromproductwhereid=5;这条语句使用了主键索引查询 id 号为 5 的商品。查询过程是这样的，B+Tree 会自顶向下逐层进行查找： 将 5 与根节点的索引数据 (1，10，20) 比较，5 在 1 和 10 之间，所以根据 B+Tree的搜索逻辑，找到第二层的索引数据 (1，4，7)； 在第二层的索引数据 (1，4，7)中进行查找，因为 5 在 4 和 7 之间，所以找到第三层的索引数据（4，5，6）； 在叶子节点的索引数据（4，5，6）中进行查找，然后我们找到了索引值为 5 的行数据。 数据库的索引和数据都是存储在硬盘的，我们可以把读取一个节点当作一次磁盘 I/O 操作。那么上面的整个查询过程一共经历了 3 个节点，也就是进行了 3 次 I/O 操作。 B+Tree 存储千万级的数据只需要 3-4 层高度就可以满足，这意味着从千万级的表查询目标数据最多需要 3-4 次磁盘 I/O，所以B+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。 #通过二级索引查询商品数据的过程 主键索引的 B+Tree 和二级索引的 B+Tree 区别如下： 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里； 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。 我这里将前面的商品表中的 product_no （商品编码）字段设置为二级索引，那么二级索引的 B+Tree 如下图（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）。 其中非叶子的 key 值是 product_no（图中橙色部分），叶子节点存储的数据是主键值（图中绿色部分）。 如果我用 product_no 二级索引查询商品，如下查询语句： select*fromproductwhereproduct_no='0002';会先检二级索引中的 B+Tree 的索引值（商品编码，product_no），找到对应的叶子节点，然后获取主键值，然后再通过主键索引中的 B+Tree 树查询到对应的叶子节点，然后获取整行数据。这个过程叫「回表」，也就是说要查两个 B+Tree 才能查到数据。如下图（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）： 不过，当查询的数据是能在二级索引的 B+Tree 的叶子节点里查询到，这时就不用再查主键索引查，比如下面这条查询语句： selectidfromproductwhereproduct_no='0002';这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据。 #为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？ 前面已经讲了 B+Tree 的索引原理，现在就来回答一下 B+Tree 相比于 B 树、二叉树或 Hash 索引结构的优势在哪儿？ 之前我也专门写过一篇文章，想详细了解的可以看这篇：「女朋友问我：为什么 MySQL 喜欢 B+ 树？我笑着画了 20 张图 (opens new window)」，这里就简单做个比对。 1、B+Tree vs B Tree B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。 另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。 2、B+Tree vs 二叉树 对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。 在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。 而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。 3、B+Tree vs Hash Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:2:1","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#按物理存储分类 从物理存储的角度来看，索引分为聚簇索引（主键索引）、二级索引（辅助索引）。 这两个区别在前面也提到了： 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里； 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。 所以，在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:2:2","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#按字段特性分类 从字段特性的角度来看，索引分为主键索引、唯一索引、普通索引、前缀索引。 #主键索引 主键索引就是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多只有一个主键索引，索引列的值不允许有空值。 在创建表时，创建主键索引的方式如下： CREATETABLEtable_name(....PRIMARYKEY(index_column_1)USINGBTREE);#唯一索引 唯一索引建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值。 在创建表时，创建唯一索引的方式如下： CREATETABLEtable_name(....UNIQUEKEY(index_column_1,index_column_2,...));建表后，如果要创建唯一索引，可以使用这面这条命令： CREATEUNIQUEINDEXindex_nameONtable_name(index_column_1,index_column_2,...);#普通索引 普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为 UNIQUE。 在创建表时，创建普通索引的方式如下： CREATETABLEtable_name(....INDEX(index_column_1,index_column_2,...));建表后，如果要创建普通索引，可以使用这面这条命令： CREATEINDEXindex_nameONtable_name(index_column_1,index_column_2,...);#前缀索引 前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。 使用前缀索引的目的是为了减少索引占用的存储空间，提升查询效率。 在创建表时，创建前缀索引的方式如下： CREATETABLEtable_name(column_list,INDEX(column_name(length)));建表后，如果要创建前缀索引，可以使用这面这条命令： CREATEINDEXindex_nameONtable_name(column_name(length));","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:2:3","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#按字段个数分类 从字段个数的角度来看，索引分为单列索引、联合索引（复合索引）。 建立在单列上的索引称为单列索引，比如主键索引； 建立在多列上的索引称为联合索引； #联合索引 通过将多个字段组合成一个索引，该索引就被称为联合索引。 比如，将商品表中的 product_no 和 name 字段组合成联合索引(product_no, name)，创建联合索引的方式如下： CREATEINDEXindex_product_no_nameONproduct(product_no,name);联合索引(product_no, name) 的 B+Tree 示意图如下（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）。 可以看到，联合索引的非叶子节点用两个字段的值作为 B+Tree 的 key 值。当在联合索引查询数据时，先按 product_no 字段比较，在 product_no 相同的情况下再按 name 字段比较。 也就是说，联合索引查询的 B+Tree 是先按 product_no 进行排序，然后再 product_no 相同的情况再按 name 字段排序。 因此，使用联合索引时，存在最左匹配原则，也就是按照最左优先的方式进行索引的匹配。在使用联合索引进行查询的时候，如果不遵循「最左匹配原则」，联合索引会失效，这样就无法利用到索引快速查询的特性了。 比如，如果创建了一个 (a, b, c) 联合索引，如果查询条件是以下这几种，就可以匹配上联合索引： where a=1； where a=1 and b=2 and c=3； where a=1 and b=2； 需要注意的是，因为有查询优化器，所以 a 字段在 where 子句的顺序并不重要。 但是，如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效: where b=2； where c=3； where b=2 and c=3； 上面这些查询条件之所以会失效，是因为(a, b, c) 联合索引，是先按 a 排序，在 a 相同的情况再按 b 排序，在 b 相同的情况再按 c 排序。所以，b 和 c 是全局无序，局部相对有序的，这样在没有遵循最左匹配原则的情况下，是无法利用到索引的。 我这里举联合索引（a，b）的例子，该联合索引的 B+ Tree 如下（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）。 可以看到，a 是全局有序的（1, 2, 2, 3, 4, 5, 6, 7 ,8），而 b 是全局是无序的（12，7，8，2，3，8，10，5，2）。因此，直接执行where b = 2这种查询条件没有办法利用联合索引的，利用索引的前提是索引里的 key 是有序的。 只有在 a 相同的情况才，b 才是有序的，比如 a 等于 2 的时候，b 的值为（7，8），这时就是有序的，这个有序状态是局部的，因此，执行where a = 2 and b = 7是 a 和 b 字段能用到联合索引的，也就是联合索引生效了。 #联合索引范围查询 联合索引有一些特殊情况，并不是查询过程使用了联合索引查询，就代表联合索引中的所有字段都用到了联合索引进行索引查询，也就是可能存在部分字段用到联合索引的 B+Tree，部分字段没有用到联合索引的 B+Tree 的情况。 这种特殊情况就发生在范围查询。联合索引的最左匹配原则会一直向右匹配直到遇到「范围查询」就会停止匹配。也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。 范围查询有很多种，那到底是哪些范围查询会导致联合索引的最左匹配原则会停止匹配呢？ 接下来，举例几个范围查例子。 Q1: select * from t_table where a \u003e 1 and b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ 由于联合索引（二级索引）是先按照 a 字段的值排序的，所以符合 a \u003e 1 条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合 a \u003e 1 条件的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录不符合 a \u003e 1 条件位置。所以 a 字段可以在联合索引的 B+Tree 中进行索引查询。 但是在符合 a \u003e 1 条件的二级索引记录的范围里，b 字段的值是无序的。比如前面图的联合索引的 B+ Tree 里，下面这三条记录的 a 字段的值都符合 a \u003e 1 查询条件，而 b 字段的值是无序的： a 字段值为 5 的记录，该记录的 b 字段值为 8； a 字段值为 6 的记录，该记录的 b 字段值为 10； a 字段值为 7 的记录，该记录的 b 字段值为 5； 因此，我们不能根据查询条件 b = 2 来进一步减少需要扫描的记录数量（b 字段无法利用联合索引进行索引查询的意思）。 所以在执行 Q1 这条查询语句的时候，对应的扫描区间是 (2, + ∞)，形成该扫描区间的边界条件是 a \u003e 1，与 b = 2 无关。 因此，Q1 这条查询语句只有 a 字段用到了联合索引进行索引查询，而 b 字段并没有使用到联合索引。 我们也可以在执行计划中的 key_len 知道这一点，在使用联合索引进行查询的时候，通过 key_len 我们可以知道优化器具体使用了多少个字段的搜索条件来形成扫描区间的边界条件。 举例个例子 ，a 和 b 都是 int 类型且不为 NULL 的字段，那么 Q1 这条查询语句执行计划如下，可以看到 key_len 为 4 字节（如果字段允许为 NULL，就在字段类型占用的字节数上加 1，也就是 5 字节），说明只有 a 字段用到了联合索引进行索引查询，而且可以看到，即使 b 字段没用到联合索引，key 为 idx_a_b，说明 Q1 查询语句使用了 idx_a_b 联合索引。 通过 Q1 查询语句我们可以知道，a 字段使用了 \u003e 进行范围查询，联合索引的最左匹配原则在遇到 a 字段的范围查询（ \u003e）后就停止匹配了，因此 b 字段并没有使用到联合索引。 Q2: select * from t_table where a \u003e= 1 and b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ Q2 和 Q1 的查询语句很像，唯一的区别就是 a 字段的查询条件「大于等于」。 由于联合索引（二级索引）是先按照 a 字段的值排序的，所以符合 \u003e= 1 条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合 \u003e= 1 条件的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录不符合 a\u003e= 1 条件位置。所以 a 字段可以在联合索引的 B+Tree 中进行索引查询。 虽然在符合 a\u003e= 1 条件的二级索引记录的范围里，b 字段的值是「无序」的，但是对于符合 a = 1 的二级索引记录的范围里，b 字段的值是「有序」的（因为对于联合索引，是先按照 a 字段的值排序，然后在 a 字段的值相同的情况下，再按照 b 字段的值进行排序）。 于是，在确定需要扫描的二级索引的范围时，当二级索引记录的 a 字段值为 1 时，可以通过 b = 2 条件减少需要扫描的二级索引记录范围（b 字段可以利用联合索引进行索引查询的意思）。也就是说，从符合 a = 1 and b = 2 条件的第一条记录开始扫描，而不需要从第一个 a 字段值为 1 的记录开始扫描。 所以，Q2 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询。 我们也可以在执行计划中的 key_len 知道这一点。执行计划如下，可以看到 key_len 为 8 字节，说明优化器使用了 2 个字段的查询条件来形成扫描区间的边界条件，也就是 a 和 b 字段都用到了联合索引进行索引查询。 通过 Q2 查询语句我们可以知道，虽然 a 字段使用了 \u003e= 进行范围查询，但是联合索引的最左匹配原则并没有在遇到 a 字段的范围查询（ \u003e=）后就停止匹配了，b 字段还是可以用到了联合索引的。 Q3: SELECT * FROM t_table WHERE a BETWEEN 2 AND 8 AND b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？ Q3 查询条件中 a BETWEEN 2 AND 8 的意思是查询 a 字段的值在 2 和 8 之间的记录。不同的数据库对 BETWEEN … AND 处理方式是有差异的。在 MySQL 中，BETWEEN 包含了 value1 和 value2 边界值，类似于 \u003e= and =\u003c。而有的数据库则不包含 value1 和 value2 边界值（类似于 \u003e and \u003c）。 这里我们只讨论 MySQL。由于 MySQL 的 BETWEEN 包含 value1 和 value2 边界值，所以类似于 Q2 查询语句，因此 Q3 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询。 我们也可以在执行计划中的 key_len 知道这一点。执行计划如下，可以看到 key_len 为 8 字节，说明优化器使用了 2 个字段的查询条件来形成扫描区间的边界条件，也就是 a 和 b 字段都用到了联合索引进行索引查询。 通过 Q3 查询语句我们可以知道，虽然 a 字段使用了 BETWEEN 进行范围查询，但是联合索引的最左匹配原则并没有在遇到 a 字段的范围查询（ BETWEEN）后","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:2:4","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#什么时候需要 / 不需要创建索引？ 索引最大的好处是提高查询速度，但是索引也是有缺点的，比如： 需要占用物理空间，数量越大，占用空间越大； 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增大； 会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护。 所以，索引不是万能钥匙，它也是根据场景来使用的。 #什么时候适用索引？ 字段有唯一性限制的，比如商品编码； 经常用于 WHERE 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。 经常用于 GROUP BY 和 ORDER BY 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。 #什么时候不需要创建索引？ WHERE 条件，GROUP BY，ORDER BY 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。 字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。 表数据太少的时候，不需要创建索引； 经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:3:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#有什么优化索引的方法？ 这里说一下几种常见优化索引的方法： 前缀索引优化； 覆盖索引优化； 主键索引最好是自增的； 防止索引失效； ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#前缀索引优化 前缀索引顾名思义就是使用某个字段中字符串的前几个字符建立索引，那我们为什么需要使用前缀来建立索引呢？ 使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。 不过，前缀索引有一定的局限性，例如： order by 就无法使用前缀索引； 无法把前缀索引用作覆盖索引； ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:1","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#覆盖索引优化 覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作。 假设我们只需要查询商品的名称、价格，有什么方式可以避免回表呢？ 我们可以建立一个联合索引，即「商品ID、名称、价格」作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。 所以，使用覆盖索引的好处就是，不需要查询出包含整行记录的所有信息，也就减少了大量的 I/O 操作。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:2","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#主键索引最好是自增的 我们在建表的时候，都会默认将主键索引设置为自增的，具体为什么要这样做呢？又什么好处？ InnoDB 创建主键索引默认为聚簇索引，数据被存放在了 B+Tree 的叶子节点上。也就是说，同一个叶子节点内的各个数据是按主键顺序存放的，因此，每当有一条新的数据插入时，数据库会根据主键将其插入到对应的叶子节点中。 如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。 如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 举个例子，假设某个数据页中的数据是1、3、5、9，且数据页满了，现在准备插入一个数据7，则需要把数据页分割为两个数据页： 出现页分裂时，需要将一个页的记录移动到另外一个页，性能会受到影响，同时页空间的利用率下降，造成存储空间的浪费。 而如果记录是顺序插入的，例如插入数据11，则只需开辟新的数据页，也就不会发生页分裂： 因此，在使用 InnoDB 存储引擎时，如果没有特别的业务需求，建议使用自增字段作为主键。 另外，主键字段的长度不要太大，因为主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:3","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#索引最好设置为 NOT NULL 为了更好的利用索引，索引列要设置为 NOT NULL 约束。有两个原因： 第一原因：索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为NULL 的行。 第二个原因：NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题，因为 InnoDB 存储记录的时候，如果表中存在允许为 NULL 的字段，那么行格式 (opens new window)中至少会用 1 字节空间存储 NULL 值列表，如下图的紫色部分： ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:4","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#防止索引失效 用上了索引并不意味着查询的时候会使用到索引，所以我们心里要清楚有哪些情况会导致索引失效，从而避免写出索引失效的查询语句，否则这样的查询效率是很低的。 我之前写过索引失效的文章，想详细了解的可以去看这篇文章：谁还没碰过索引失效呢?(opens new window) 这里简单说一下，发生索引失效的情况： 当我们使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx%这两种方式都会造成索引失效； 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效； 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。 我上面说的是常见的索引失效场景，实际过程中，可能会出现其他的索引失效场景，这时我们就需要查看执行计划，通过执行计划显示的数据判断查询语句是否使用了索引。 如下图，就是一个没有使用索引，并且是一个全表扫描的查询语句。 对于执行计划，参数有： possible_keys 字段表示可能用到的索引； key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引； key_len 表示索引的长度； rows 表示扫描的数据行数。 type 表示数据扫描类型，我们需要重点看这个。 type 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的执行效率从低到高的顺序为： All（全表扫描）； index（全索引扫描）； range（索引范围扫描）； ref（非唯一索引扫描）； eq_ref（唯一索引扫描）； const（结果只有一条的主键或唯一索引扫描）。 在这些情况里，all 是最坏的情况，因为采用了全表扫描的方式。index 和 all 差不多，只不过 index 对索引表进行全扫描，这样做的好处是不再需要对数据进行排序，但是开销依然很大。所以，要尽量避免全表扫描和全索引扫描。 range 表示采用了索引范围扫描，一般在 where 子句中使用 \u003c 、\u003e、in、between 等关键词，只检索给定范围的行，属于范围查找。从这一级别开始，索引的作用会越来越明显，因此我们需要尽量让 SQL 查询可以使用到 range 这一级别及以上的 type 访问方式。 ref 类型表示采用了非唯一索引，或者是唯一索引的非唯一性前缀，返回数据返回可能是多条。因为虽然使用了索引，但该索引列的值并不唯一，有重复。这样即使使用索引快速查找到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。但它的好处是它并不需要扫全表，因为索引是有序的，即便有重复值，也是在一个非常小的范围内扫描。 eq_ref 类型是使用主键或唯一索引时产生的访问方式，通常使用在多表联查中。比如，对两张表进行联查，关联条件是两张表的 user_id 相等，且 user_id 是唯一索引，那么使用 EXPLAIN 进行执行计划查看的时候，type 就会显示 eq_ref。 const 类型表示使用了主键或者唯一索引与常量值进行比较，比如 select name from product where id=1。 需要说明的是 const 类型和 eq_ref 都使用了主键或唯一索引，不过这两个类型有所区别，const 是与常量进行比较，查询效率会更快，而 eq_ref 通常用于多表联查中。 除了关注 type，我们也要关注 extra 显示的结果。 这里说几个重要的参考指标： Using filesort ：当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。 Using temporary：使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。 Using index：所需数据只需在索引即可全部获得，不须要再到表中取数据，也就是使用了覆盖索引，避免了回表操作，效率不错。 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:4:5","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL索引篇"],"content":"#总结 这次主要介绍了索引的原理、分类和使用。我把重点总结在了下面这个表格 ","date":"2023-05-28","objectID":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/:5:0","tags":["一、初见索引"],"title":"一、初见索引","uri":"/posts/%E4%B8%80%E5%88%9D%E8%A7%81%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL事务篇"],"content":"二、可重复读隔离级别完全解决幻读了吗","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"MySQL 可重复读隔离级别，完全解决幻读了吗？ 在上一篇文章中提到，MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种： 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select … for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 这两个解决方案是很大程度上解决了幻读现象，但是还是有个别的情况造成的幻读现象是无法解决的。 这次，就跟大家好好聊这个问题。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:0:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#什么是幻读？ 首先来看看 MySQL 文档是怎么定义幻读（Phantom Read）的: The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 翻译：当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。 举个例子，假设一个事务在 T1 时刻和 T2 时刻分别执行了下面查询语句，途中没有执行其他任何语句： SELECT*FROMt_testWHEREid\u003e100;只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题，比如： T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 6 条行记录，那就发生了幻读的问题。 T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 4 条行记录，也是发生了幻读的问题。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:1:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#快照读是如何避免幻读的？ 可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出 来这条数据的，所以就很好了避免幻读问题。 做个实验，数据库表 t_stu 如下，其中 id 为主键。 然后在可重复读隔离级别下，有两个事务的执行顺序如下： 从这个实验结果可以看到，即使事务 B 中途插入了一条记录，事务 A 前后两次查询的结果集都是一样的，并没有出现所谓的幻读现象。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:2:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#当前读是如何避免幻读的？ MySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。 这很好理解，假设你要 update 一个记录，另一个事务已经 delete 这条记录并且提交事务了，这样不是会产生冲突吗，所以 update 的时候肯定要知道最新的数据。 另外，select ... for update 这种查询语句是当前读，每次执行的时候都是读取最新的数据。 接下来，我们假设select ... for update当前读是不会加锁的（实际上是会加锁的），在做一遍实验。 这时候，事务 B 插入的记录，就会被事务 A 的第二条查询语句查询到（因为是当前读），这样就会出现前后两次查询的结果集合不一样，这就出现了幻读。 所以，Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了间隙锁。 假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。 举个具体例子，场景如下： 事务 A 执行了这面这条锁定读语句后，就在对表中的记录加上 id 范围为 (2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）。 然后，事务 B 在执行插入语句的时候，判断到插入的位置被事务 A 加了 next-key lock，于是事物 B 会生成一个插入意向锁，同时进入等待状态，直到事务 A 提交了事务。这就避免了由于事务 B 插入新记录而导致事务 A 发生幻读的现象。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:3:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#幻读被完全解决了吗？ 可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。 我举例一个可重复读隔离级别发生幻读现象的场景。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:4:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#第一个发生幻读现象的场景 还是以这张表作为例子： 事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。 #事务Amysql\u003ebegin;QueryOK,0rowsaffected(0.00sec)mysql\u003eselect*fromt_stuwhereid=5;Emptyset(0.01sec)然后事务 B 插入一条 id = 5 的记录，并且提交了事务。 #事务Bmysql\u003ebegin;QueryOK,0rowsaffected(0.00sec)mysql\u003einsertintot_stuvalues(5,'小美',18);QueryOK,1rowaffected(0.00sec)mysql\u003ecommit;QueryOK,0rowsaffected(0.00sec)此时，事务 A 更新 id = 5 这条记录，对没错，事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录，事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场景。 #事务Amysql\u003eupdatet_stusetname='小林coding'whereid=5;QueryOK,1rowaffected(0.01sec)Rowsmatched:1Changed:1Warnings:0mysql\u003eselect*fromt_stuwhereid=5;+----+--------------+------+ |id|name|age|+----+--------------+------+ |5|小林coding|18|+----+--------------+------+ 1rowinset(0.00sec)整个发生幻读的时序图如下： 在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。 因为这种特殊现象的存在，所以我们认为 MySQL Innodb 中的 MVCC 并不能完全避免幻读现象。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:4:1","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#第二个发生幻读现象的场景 除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。 T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id \u003e 100 得到了 3 条记录。 T2 时刻：事务 B 往插入一个 id= 200 的记录并提交； T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id \u003e 100 for update 就会得到 4 条记录，此时也发生了幻读现象。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select … for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:4:2","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["MySQL事务篇"],"content":"#总结 MySQL InnoDB 引擎的可重复读隔离级别（默认隔离级），根据不同的查询方式，分别提出了避免幻读的方案： 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读。 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。 我举例了两个发生幻读场景的例子。 第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。 第二个例子：对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。 所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select … for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。 ","date":"2023-05-28","objectID":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/:5:0","tags":["二、可重复读隔离级别完全解决幻读了吗"],"title":"二、可重复读隔离级别完全解决幻读了吗","uri":"/posts/%E4%BA%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97/"},{"categories":["业务常见数据结构"],"content":"布隆过滤器","date":"2023-05-28","objectID":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","tags":["布隆过滤器"],"title":"布隆过滤器","uri":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":["业务常见数据结构"],"content":"在程序的世界中，布隆过滤器是程序员的一把利器，利用它可以快速地解决项目中一些比较棘手的问题。如网页 URL 去重、垃圾邮件识别、大集合中重复元素的判断和缓存穿透等问题。 布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。 ","date":"2023-05-28","objectID":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/:0:0","tags":["布隆过滤器"],"title":"布隆过滤器","uri":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":["业务常见数据结构"],"content":"一、布隆过滤器简介 当你往简单数组或列表中插入新数据时，将不会根据插入项的值来确定该插入项的索引值。这意味着新插入项的索引值与数据值之间没有直接关系。这样的话，当你需要在数组或列表中搜索相应值的时候，你必须遍历已有的集合。若集合中存在大量的数据，就会影响数据查找的效率。 针对这个问题，你可以考虑使用哈希表。利用哈希表你可以通过对 “值” 进行哈希处理来获得该值对应的键或索引值，然后把该值存放到列表中对应的索引位置。这意味着索引值是由插入项的值所确定的，当你需要判断列表中是否存在该值时，只需要对值进行哈希处理并在相应的索引位置进行搜索即可，这时的搜索速度是非常快的。 根据定义，布隆过滤器可以检查值是 “可能在集合中” 还是 “绝对不在集合中”。“可能” 表示有一定的概率，也就是说可能存在一定为误判率。那为什么会存在误判呢？下面我们来分析一下具体的原因。 布隆过滤器（Bloom Filter）本质上是由长度为 m 的位向量或位列表（仅包含 0 或 1 位值的列表）组成，最初所有的值均设置为 0，如下图所示。 为了将数据项添加到布隆过滤器中，我们会提供 K 个不同的哈希函数，并将结果位置上对应位的值置为 “1”。在前面所提到的哈希表中，我们使用的是单个哈希函数，因此只能输出单个索引值。而对于布隆过滤器来说，我们将使用多个哈希函数，这将会产生多个索引值。 如上图所示，当输入 “semlinker” 时，预设的 3 个哈希函数将输出 2、4、6，我们把相应位置 1。假设另一个输入 ”kakuqo“，哈希函数输出 3、4 和 7。你可能已经注意到，索引位 4 已经被先前的 “semlinker” 标记了。此时，我们已经使用 “semlinker” 和 ”kakuqo“ 两个输入值，填充了位向量。当前位向量的标记状态为： 当对值进行搜索时，与哈希表类似，我们将使用 3 个哈希函数对 ”搜索的值“ 进行哈希运算，并查看其生成的索引值。假设，当我们搜索 ”fullstack“ 时，3 个哈希函数输出的 3 个索引值分别是 2、3 和 7： 从上图可以看出，相应的索引位都被置为 1，这意味着我们可以说 ”fullstack“ 可能已经插入到集合中。事实上这是误报的情形，产生的原因是由于哈希碰撞导致的巧合而将不同的元素存储在相同的比特位上。幸运的是，布隆过滤器有一个可预测的误判率（FPP）： n 是已经添加元素的数量； k 哈希的次数； m 布隆过滤器的长度（如比特数组的大小）； 极端情况下，当布隆过滤器没有空闲空间时（满），每一次查询都会返回 true 。这也就意味着 m 的选择取决于期望预计添加元素的数量 n ，并且 m 需要远远大于 n 。 实际情况中，布隆过滤器的长度 m 可以根据给定的误判率（FFP）的和期望添加的元素个数 n 的通过如下公式计算： 了解完上述的内容之后，我们可以得出一个结论，当我们搜索一个值的时候，若该值经过 K 个哈希函数运算后的任何一个索引位为 ”0“，那么该值肯定不在集合中。但如果所有哈希索引值均为 ”1“，则只能说该搜索的值可能存在集合中。 ","date":"2023-05-28","objectID":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/:0:1","tags":["布隆过滤器"],"title":"布隆过滤器","uri":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":["业务常见数据结构"],"content":"二、布隆过滤器应用 在实际工作中，布隆过滤器常见的应用场景如下： 网页爬虫对 URL 去重，避免爬取相同的 URL 地址； 反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱； Google Chrome 使用布隆过滤器识别恶意 URL； Medium 使用布隆过滤器避免推荐给用户已经读过的文章； Google BigTable，Apache HBbase 和 Apache Cassandra 使用布隆过滤器减少对不存在的行和列的查找。 除了上述的应用场景之外，布隆过滤器还有一个应用场景就是解决缓存穿透的问题。所谓的缓存穿透就是服务调用方每次都是查询不在缓存中的数据，这样每次服务调用都会到数据库中进行查询，如果这类请求比较多的话，就会导致数据库压力增大，这样缓存就失去了意义。 利用布隆过滤器我们可以预先把数据查询的主键，比如用户 ID 或文章 ID 缓存到过滤器中。当根据 ID 进行数据查询的时候，我们先判断该 ID 是否存在，若存在的话，则进行下一步处理。若不存在的话，直接返回，这样就不会触发后续的数据库查询。需要注意的是缓存穿透不能完全解决，我们只能将其控制在一个可以容忍的范围内。 ","date":"2023-05-28","objectID":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/:0:2","tags":["布隆过滤器"],"title":"布隆过滤器","uri":"/posts/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":["深入Linux内核"],"content":"[内核源码]epoll实现原理","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"文章主要对 tcp 通信进行 epoll 源码走读。 引发我对 epoll 源码感兴趣的原因在于知乎上的一个提问，ET和LT模式在源码中到底怎么实现的 Linux 源码：Linux 5.7 版本。epoll 核心源码：eventpoll.h / eventpoll.c。 搭建 epoll 内核调试环境视频：vscode + gdb 远程调试 linux (EPOLL) 内核源码 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:0:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"1. 应用场景 epoll 应用，适合海量用户，一个时间段内部分活跃的用户群体。 例如 app，正常用户并不是 24 小时都拿起手机玩个不停，可能玩一下，又去干别的事，回头又玩一下，断断续续地操作。即便正在使用 app 也不是连续产生读写通信事件，可能手指点击几下页面，页面产生需要的内容，用户就去浏览内容，不再操作了。换句话说，在海量用户里，同一个时间段内，很可能只有一小部分用户正在活跃，而在这一小部分活跃用户里，又只有一小撮人同时点击页面上的操作。那 epoll 管理海量用户，只需要将这一小撮人产生的事件，及时通知 appserver 处理逻辑即可。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:1:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"2. 预备知识 走读 epoll 源码前，先熟悉内核相关工作流程：[epoll 源码走读] epoll 源码实现-预备知识。 走读源码过程中，可以通过 Linux 文档 搜索 epoll 相关知识。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:2:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"3. 使用 接口。 接口 描述 epoll_create 创建 epoll。 epoll_ctl fd 事件注册函数，用户通过这个函数关注 fd 读写事件。 epoll_wait 阻塞等待 fd 事件发生。 使用流程。 图片来源：《epoll 多路复用 I/O工作流程》 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:3:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"4. 事件 常用事件注释可以请参考 epoll_ctl 文档。 // eventpoll.h #define EPOLLIN (__force __poll_t)0x00000001 #define EPOLLOUT (__force __poll_t)0x00000004 #define EPOLLERR (__force __poll_t)0x00000008 #define EPOLLHUP (__force __poll_t)0x00000010 #define EPOLLRDHUP (__force __poll_t)0x00002000 #define EPOLLEXCLUSIVE ((__force __poll_t)(1U \u003c\u003c 28)) #define EPOLLET ((__force __poll_t)(1U \u003c\u003c 31)) 事件 描述 EPOLLIN 有可读数据到来。 EPOLLOUT 有数据要写。 EPOLLERR 该文件描述符发生错误。 EPOLLHUP 该文件描述符被挂断。常见 socket 被关闭（read == 0）。 EPOLLRDHUP 对端已关闭链接，或者用 shutdown 关闭了写链接。 EPOLLEXCLUSIVE 唯一唤醒事件，主要为了解决 epoll_wait 惊群问题。多线程下多个 epoll_wait 同时等待，只唤醒一个 epoll_wait 执行。 该事件只支持 epoll_ctl 添加操作 EPOLL_CTL_ADD。 EPOLLET 边缘触发模式。 通过 tcp_poll 函数，可以看到 socket 事件对应的相关事件逻辑。 // tcp.c /* * Wait for a TCP event. * * Note that we don't need to lock the socket, as the upper poll layers * take care of normal races (between the test and the event) and we don't * go look at any of the socket buffers directly. */ __poll_t tcp_poll(struct file *file, struct socket *sock, poll_table *wait) { __poll_t mask; struct sock *sk = sock-\u003esk; const struct tcp_sock *tp = tcp_sk(sk); int state; // fd 添加等待事件，关联事件回调。 sock_poll_wait(file, sock, wait); // socket 对应事件逻辑。 state = inet_sk_state_load(sk); if (state == TCP_LISTEN) return inet_csk_listen_poll(sk); /* Socket is not locked. We are protected from async events * by poll logic and correct handling of state changes * made by other threads is impossible in any case. */ mask = 0; /* * EPOLLHUP is certainly not done right. But poll() doesn't * have a notion of HUP in just one direction, and for a * socket the read side is more interesting. * * Some poll() documentation says that EPOLLHUP is incompatible * with the EPOLLOUT/POLLWR flags, so somebody should check this * all. But careful, it tends to be safer to return too many * bits than too few, and you can easily break real applications * if you don't tell them that something has hung up! * * Check-me. * * Check number 1. EPOLLHUP is _UNMASKABLE_ event (see UNIX98 and * our fs/select.c). It means that after we received EOF, * poll always returns immediately, making impossible poll() on write() * in state CLOSE_WAIT. One solution is evident --- to set EPOLLHUP * if and only if shutdown has been made in both directions. * Actually, it is interesting to look how Solaris and DUX * solve this dilemma. I would prefer, if EPOLLHUP were maskable, * then we could set it on SND_SHUTDOWN. BTW examples given * in Stevens' books assume exactly this behaviour, it explains * why EPOLLHUP is incompatible with EPOLLOUT. --ANK * * NOTE. Check for TCP_CLOSE is added. The goal is to prevent * blocking on fresh not-connected or disconnected socket. --ANK */ if (sk-\u003esk_shutdown == SHUTDOWN_MASK || state == TCP_CLOSE) mask |= EPOLLHUP; if (sk-\u003esk_shutdown \u0026 RCV_SHUTDOWN) mask |= EPOLLIN | EPOLLRDNORM | EPOLLRDHUP; /* Connected or passive Fast Open socket? */ if (state != TCP_SYN_SENT \u0026\u0026 (state != TCP_SYN_RECV || rcu_access_pointer(tp-\u003efastopen_rsk))) { int target = sock_rcvlowat(sk, 0, INT_MAX); if (READ_ONCE(tp-\u003eurg_seq) == READ_ONCE(tp-\u003ecopied_seq) \u0026\u0026 !sock_flag(sk, SOCK_URGINLINE) \u0026\u0026 tp-\u003eurg_data) target++; if (tcp_stream_is_readable(tp, target, sk)) mask |= EPOLLIN | EPOLLRDNORM; if (!(sk-\u003esk_shutdown \u0026 SEND_SHUTDOWN)) { if (sk_stream_is_writeable(sk)) { mask |= EPOLLOUT | EPOLLWRNORM; } else { /* send SIGIO later */ sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk); set_bit(SOCK_NOSPACE, \u0026sk-\u003esk_socket-\u003eflags); /* Race breaker. If space is freed after * wspace test but before the flags are set, * IO signal will be lost. Memory barrier * pairs with the input side. */ smp_mb__after_atomic(); if (sk_stream_is_writeable(sk)) mask |= EPOLLOUT | EPOLLWRNORM; } } else mask |= EPOLLOUT | EPOLLWRNORM; if (tp-\u003eurg_data \u0026 TCP_URG_VALID) mask |= EPOLLPRI; } else if (state == TCP_SYN_SENT \u0026\u0026 inet_sk(sk)-\u003edefer_connect) { /* Active TCP fastopen socket with defer_connect * Return EPOLLOUT so application can call write() * in order for kernel to generate SYN+data */ mask |","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:4:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"5. 源码工作流程 图片来源：tcp + epoll 内核睡眠唤醒工作流程 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:5:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6. 数据结构 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.1. eventpoll /* * This structure is stored inside the \"private_data\" member of the file * structure and represents the main data structure for the eventpoll * interface. */ struct eventpoll { /* * This mutex is used to ensure that files are not removed * while epoll is using them. This is held during the event * collection loop, the file cleanup path, the epoll file exit * code and the ctl operations. */ struct mutex mtx; /* Wait queue used by sys_epoll_wait() */ wait_queue_head_t wq; /* Wait queue used by file-\u003epoll() */ wait_queue_head_t poll_wait; /* List of ready file descriptors */ struct list_head rdllist; /* Lock which protects rdllist and ovflist */ rwlock_t lock; /* RB tree root used to store monitored fd structs */ struct rb_root_cached rbr; /* * This is a single linked list that chains all the \"struct epitem\" that * happened while transferring ready events to userspace w/out * holding -\u003elock. */ struct epitem *ovflist; /* wakeup_source used when ep_scan_ready_list is running */ struct wakeup_source *ws; /* The user that created the eventpoll descriptor */ struct user_struct *user; struct file *file; /* used to optimize loop detection check */ int visited; struct list_head visited_list_link; #ifdef CONFIG_NET_RX_BUSY_POLL /* used to track busy poll napi_id */ unsigned int napi_id; #endif }; 成员 描述 mtx 互斥变量，避免在遍历 epi 节点时（例如 ep_send_events），epi 被删除。 wq 等待队列，当 epoll_wait 没发现就绪事件需要处理，添加等待事件，需要睡眠阻塞等待唤醒进程。 poll_wait 等待队列，当epoll_ctl 监听的是另外一个 epoll fd 时使用。 rdllist 就绪列表，产生了用户注册的 fd读写事件的 epi 链表。 ovflist 单链表，当 rdllist 被锁定遍历，向用户空间发送数据时，rdllist 不允许被修改，新触发的就绪 epitem 被 ovflist 串联起来，等待 rdllist 被处理完了，重新将 ovflist 数据写入 rdllist。 详看 ep_scan_ready_list 逻辑。 user 创建 eventpoll 的用户结构信息。 lock 锁，保护 rdllist 和 ovflist 。 rbr 红黑树根结点，管理 fd 结点。 file eventpoll 对应的文件结构，Linux 一切皆文件，用 vfs 管理数据。 napi_id 应用于中断缓解技术。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:1","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.2. epitem fd 事件管理节点。可以添加到红黑树，也可以串联成就绪列表或其它列表。 /* * Each file descriptor added to the eventpoll interface will * have an entry of this type linked to the \"rbr\" RB tree. * Avoid increasing the size of this struct, there can be many thousands * of these on a server and we do not want this to take another cache line. */ struct epitem { union { /* RB tree node links this structure to the eventpoll RB tree */ struct rb_node rbn; /* Used to free the struct epitem */ struct rcu_head rcu; }; /* List header used to link this structure to the eventpoll ready list */ struct list_head rdllink; /* * Works together \"struct eventpoll\"-\u003eovflist in keeping the * single linked chain of items. */ struct epitem *next; /* The file descriptor information this item refers to */ struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; /* List containing poll wait queues */ struct list_head pwqlist; /* The \"container\" of this item */ struct eventpoll *ep; /* List header used to link this item to the \"struct file\" items list */ struct list_head fllink; /* wakeup_source used when EPOLLWAKEUP is set */ struct wakeup_source __rcu *ws; /* The structure that describe the interested events and the source fd */ struct epoll_event event; }; 成员 描述 rbn 连接红黑树结构节点。 rdllink 就绪队列节点，用于将 epitem 串联成就绪队列列表。 next 指向下一个单链表节点的指针。配合 eventpoll 的 ovflist 使用。 ffd 记录节点对应的 fd 和 file 文件信息。 nwait 等待队列个数。 pwqlist 等待事件回调队列。当数据进入网卡，底层中断执行 ep_poll_callback。 ep eventpoll 指针，epitem 关联 eventpoll。 fllink epoll 文件链表结点，与 epoll 文件链表进行关联 file.f_ep_links。参考 fs.h, struct file 结构。 ws EPOLLWAKEUP 模式下使用。 event 用户关注的事件。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:2","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.3. epoll_filefd fd 对应 file 文件结构，Linux 一切皆文件，采用了 vfs （虚拟文件系统）管理文件或设备。 struct epoll_filefd { struct file *file; int fd; } __packed; ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:3","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.4. epoll_event 用户关注的 epoll 事件结构。 struct epoll_event { __poll_t events; __u64 data; } EPOLL_PACKED; 成员 描述 events 事件集合 data fd ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:4","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.5. poll_table_struct 就绪事件处理结构。 /* poll.h * Do not touch the structure directly, use the access functions * poll_does_not_wait() and poll_requested_events() instead. */ typedef struct poll_table_struct { poll_queue_proc _qproc; __poll_t _key; } poll_table; /* * structures and helpers for f_op-\u003epoll implementations */ typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *); 成员 描述 _qproc 处理函数，可以指向 ep_ptable_queue_proc 函数，或者空。 _key 事件组合。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:5","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"6.6. ep_pqueue 包装就绪事件处理结构，关联 epitem。 /* Wrapper struct used by poll queueing */\rstruct ep_pqueue {\rpoll_table pt;\rstruct epitem *epi;\r};\r 成员 描述 pt 就绪事件处理结构。 epi epitem 对应节点。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:6:6","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"7. 关键函数 函数 描述 eventpoll_init 初始化 epoll 模块。eventpoll 作为 Linux 内核的一部分，模块化管理。 do_epoll_create 为 eventpoll 结构分配资源。 do_epoll_ctl epoll 管理 fd 事件接口。 do_epoll_wait 有条件阻塞等待 fd 事件发生，返回对fd 和对应事件数据。 ep_item_poll 获取 fd 就绪事件，并关联 fd 和事件触发回调函数 ep_poll_callback。 ep_poll_callback fd 事件回调函数。当底层收到数据，中断调用 fd 关联的 ep_poll_callback 回调函数，如果事件是用户关注的事件，会将 fd 对应的 epi 结点添加进就绪队列，然后唤醒阻塞等待的 epoll_wait 处理。 ep_send_events 遍历就绪列表，拷贝内核空间就绪数据到用户空间。结合 ep_scan_ready_list 和 ep_send_events_proc 使用。 ep_scan_ready_list 遍历就绪列表。当 fd 收到数据，回调 ep_poll_callback，如果事件是用户关注的，那么将 fd 对应的 epi 结点添加到就绪队列，ep_scan_ready_list 会遍历这个就绪列表，将数据从内核空间拷贝到用户空间，或者其它操作。 ep_send_events_proc 内核将就绪列表数据，发送到用户空间。结合 ep_scan_ready_list 使用。LT/ET 模式在这个函数里实现。 ep_ptable_queue_proc 添加 fd 的等待事件到等待队列，关联 fd 与回调函数 ep_poll_callback。 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:7:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8. 核心源码 ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.1. 初始化 添加 epoll 模块到内核，slab 算法为 epoll 分配资源。 static int __init eventpoll_init(void) { struct sysinfo si; ... /* Allocates slab cache used to allocate \"struct epitem\" items */ epi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT, NULL); /* Allocates slab cache used to allocate \"struct eppoll_entry\" */ pwq_cache = kmem_cache_create(\"eventpoll_pwq\", sizeof(struct eppoll_entry), 0, SLAB_PANIC|SLAB_ACCOUNT, NULL); return 0; } fs_initcall(eventpoll_init); ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:1","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.2. epoll_create 创建 eventpoll 对象，关联文件资源。 static int do_epoll_create(int flags) { int error, fd; struct eventpoll *ep = NULL; struct file *file; ... // slab 算法为 eventpoll 结构分配内存，并初始化 eventpoll 成员数据。 error = ep_alloc(\u0026ep); if (error \u003c 0) return error; // 分配一个空闲的文件描述符。 fd = get_unused_fd_flags(O_RDWR | (flags \u0026 O_CLOEXEC)); if (fd \u003c 0) { error = fd; goto out_free_ep; } // slab 分配一个新的文件结构对象（struct file *） file = anon_inode_getfile(\"[eventpoll]\", \u0026eventpoll_fops, ep, O_RDWR | (flags \u0026 O_CLOEXEC)); if (IS_ERR(file)) { error = PTR_ERR(file); goto out_free_fd; } ep-\u003efile = file; // fd 与 file* 结构进行绑定。 fd_install(fd, file); return fd; ... } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:2","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.3. epoll_ctl fd 对应的事件管理（增删改）。 添加 fd 事件管理流程：fd 关联回调 ep_poll_callback。 fd -\u003e socket -\u003e poll -\u003e ep_ptable_queue_proc -\u003e wait_queue -\u003e ep_poll_callback 触发了 fd 关注的事件回调处理。 driver -\u003e ep_poll_callback -\u003e waitup -\u003e epoll_wait(wake up) SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event) { struct epoll_event epds; // 为了 event 数据的安全性，将数据进行拷贝，再进行逻辑处理。 if (ep_op_has_event(op) \u0026\u0026 copy_from_user(\u0026epds, event, sizeof(struct epoll_event))) return -EFAULT; return do_epoll_ctl(epfd, op, fd, \u0026epds, false); } int do_epoll_ctl(int epfd, int op, int fd, struct epoll_event *epds, bool nonblock) { int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct eventpoll *tep = NULL; ... // 检查参数合法性。 ... // 在 do_epoll_create 实现里 anon_inode_getfile 将 private_data 与 eventpoll 关联。 ep = f.file-\u003eprivate_data; ... // 红黑树检查 fd 是否已经被添加。 epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) { case EPOLL_CTL_ADD: if (!epi) { /* epoll 如果没有添加过该 fd，就添加到红黑树进行管理。 * 事件默认关注异常处理(EPOLLERR | EPOLLHUP)。*/ epds-\u003eevents |= EPOLLERR | EPOLLHUP; error = ep_insert(ep, epds, tf.file, fd, full_check); } else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; case EPOLL_CTL_DEL: if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; case EPOLL_CTL_MOD: if (epi) { if (!(epi-\u003eevent.events \u0026 EPOLLEXCLUSIVE)) { epds-\u003eevents |= EPOLLERR | EPOLLHUP; error = ep_modify(ep, epi, epds); } } else error = -ENOENT; break; } ... return error; } static int ep_insert(struct eventpoll *ep, const struct epoll_event *event, struct file *tfile, int fd, int full_check) { // epoll 管理 fd 和对应事件节点 epitem 数据结构。 struct epitem *epi; struct ep_pqueue epq; ... epq.epi = epi; // 初始化就绪事件处理函数调用。poll() 接口调用 ep_ptable_queue_proc。 init_poll_funcptr(\u0026epq.pt, ep_ptable_queue_proc); // 添加等待队列，如果 fd 有用户关注的事件发生，返回对应 fd 关注的事件 revents。 revents = ep_item_poll(epi, \u0026epq.pt, 1); ... // 将当前节点，添加到 epoll 文件钩子，将 epoll 文件与 fd 对应文件串联起来。 list_add_tail_rcu(\u0026epi-\u003efllink, \u0026tfile-\u003ef_ep_links); // 将节点添加进二叉树 ep_rbtree_insert(ep, epi); // 如果有关注的事件发生，将节点关联到就绪事件列表。 if (revents \u0026\u0026 !ep_is_linked(epi)) { list_add_tail(\u0026epi-\u003erdllink, \u0026ep-\u003erdllist); ep_pm_stay_awake(epi); /* 如果进程正在睡眠等待，唤醒它去处理就绪事件。睡眠事件 ep-\u003ewq 在 epoll_wait 中添加*/ if (waitqueue_active(\u0026ep-\u003ewq)) // 唤醒进程 wake_up(\u0026ep-\u003ewq); // 如果监控的是另外一个 epoll_create 的 fd，有就绪事件，也唤醒进程。 if (waitqueue_active(\u0026ep-\u003epoll_wait)) pwake++; } ... if (pwake) ep_poll_safewake(\u0026ep-\u003epoll_wait); return 0; } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:3","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.4. ep_item_poll fd 节点就绪事件处理。 static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt, int depth) { struct eventpoll *ep; bool locked; pt-\u003e_key = epi-\u003eevent.events; if (!is_file_epoll(epi-\u003effd.file)) { // 非 epoll fd，tcp_poll 检查 socket 就绪事件，fd 关联回调函数 ep_poll_callback。 return vfs_poll(epi-\u003effd.file, pt) \u0026 epi-\u003eevent.events; } else { // epoll 嵌套。epoll_ctl 添加关注了另外一个 epoll 的 fd(epfd)。 ep = epi-\u003effd.file-\u003eprivate_data; poll_wait(epi-\u003effd.file, \u0026ep-\u003epoll_wait, pt); locked = pt \u0026\u0026 (pt-\u003e_qproc == ep_ptable_queue_proc); return ep_scan_ready_list(epi-\u003effd.file-\u003eprivate_data, ep_read_events_proc, \u0026depth, depth, locked) \u0026 epi-\u003eevent.events; } } // vfs - Virtual Filesystem Switch（Linux 虚拟文件系统） // poll.h 就绪事件处理函数。 static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt) { if (unlikely(!file-\u003ef_op-\u003epoll)) return DEFAULT_POLLMASK; // 这里的 poll 函数指针指向 tcp_poll 函数。 return file-\u003ef_op-\u003epoll(file, pt); } // tcp.c // tcp 就绪事件获取函数。 __poll_t tcp_poll(struct file *file, struct socket *sock, poll_table *wait) { __poll_t mask; struct sock *sk = sock-\u003esk; const struct tcp_sock *tp = tcp_sk(sk); int state; /* 添加等待队列和关联事件回调函数 ep_poll_callback *（只有 epoll_ctl EPOLL_CTL_ADD 的情况下，才会添加等待事件，否则 wait == NULL）*/ sock_poll_wait(file, sock, wait); // 检查 fd 是否有事件发生。 state = inet_sk_state_load(sk); if (state == TCP_LISTEN) return inet_csk_listen_poll(sk); ... } // socket.h static inline void sock_poll_wait(struct file *filp, struct socket *sock, poll_table *p) { // ep_insert 调用 ep_item_poll 才会插入等待事件。 if (!poll_does_not_wait(p)) { poll_wait(filp, \u0026sock-\u003ewq.wait, p); ... } } // poll.h static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p) { if (p \u0026\u0026 p-\u003e_qproc \u0026\u0026 wait_address) // _qproc ---\u003e ep_ptable_queue_proc p-\u003e_qproc(filp, wait_address, p); } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:4","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.5. ep_ptable_queue_proc socket 的等待队列关联回调函数 ep_poll_callback static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead, poll_table *pt) { struct epitem *epi = ep_item_from_epqueue(pt); struct eppoll_entry *pwq; if (epi-\u003enwait \u003e= 0 \u0026\u0026 (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) { // 关联等待队列和ep_poll_callback。 init_waitqueue_func_entry(\u0026pwq-\u003ewait, ep_poll_callback); // whead ---\u003e socket-\u003ewq.wait pwq-\u003ewhead = whead; pwq-\u003ebase = epi; /* 等待事件，添加到等待队列。EPOLLEXCLUSIVE 为了解决 epoll_wait 惊群问题。 * 如果多线程同时调用 epoll_wait，那么 fd 应该设置 EPOLLEXCLUSIVE 事件。 */ if (epi-\u003eevent.events \u0026 EPOLLEXCLUSIVE) { add_wait_queue_exclusive(whead, \u0026pwq-\u003ewait); } else { add_wait_queue(whead, \u0026pwq-\u003ewait); } /* 等待事件，关联 epitem。epitem 为什么要有一个等待队列呢， * 因为有可能一个进程里存在多个 epoll 实例同时 epoll_ctl 关注一个 fd。*/ list_add_tail(\u0026pwq-\u003ellink, \u0026epi-\u003epwqlist); epi-\u003enwait++; } else { /* We have to signal that an error occurred */ epi-\u003enwait = -1; } } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:5","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.6. epoll_wait SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout) { return do_epoll_wait(epfd, events, maxevents, timeout); } static int do_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, int timeout) { ... // timeout 阻塞等待处理并返回就绪事件。 error = ep_poll(ep, events, maxevents, timeout); ... } static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout) { int res = 0, eavail, timed_out = 0; u64 slack = 0; bool waiter = false; wait_queue_entry_t wait; ktime_t expires, *to = NULL; // 计算 timeout 睡眠时间。如果有就绪事件，处理并发送到用户空间。 ... fetch_events: if (!ep_events_available(ep)) // napi 中断缓解技术，避免网卡频繁中断 cpu，提高数据获取的效率。这里为了积攒网络数据进行返回。 ep_busy_loop(ep, timed_out); // 检查就绪队列是否有数据。 eavail = ep_events_available(ep); if (eavail) // 如果有就绪事件了，就直接不用睡眠等待了，进入发送环节。 goto send_events; ... // 没有就绪事件发生，需要睡眠等待。 if (!waiter) { waiter = true; // 等待事件，关联当前进程。 init_waitqueue_entry(\u0026wait, current); spin_lock_irq(\u0026ep-\u003ewq.lock); // 添加等待事件。（为了解决惊群效应，所以等待事件添加了 WQ_FLAG_EXCLUSIVE 标识。查看 __wake_up_common 实现。） __add_wait_queue_exclusive(\u0026ep-\u003ewq, \u0026wait); spin_unlock_irq(\u0026ep-\u003ewq.lock); } for (;;) { /* * We don't want to sleep if the ep_poll_callback() sends us * a wakeup in between. That's why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ // 设置当前进程状态为等待状态，可以被信号解除等待。 set_current_state(TASK_INTERRUPTIBLE); /* * Always short-circuit for fatal signals to allow * threads to make a timely exit without the chance of * finding more events available and fetching * repeatedly. */ // 信号中断，不要执行睡眠了。 if (fatal_signal_pending(current)) { res = -EINTR; break; } // 检查就绪队列。 eavail = ep_events_available(ep); if (eavail) break; // 信号中断，不要执行睡眠了。 if (signal_pending(current)) { res = -EINTR; break; } // 进程进入睡眠状态。 if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) { timed_out = 1; break; } } // 进程等待超时，或者被唤醒，设置进程进入运行状态，等待内核调度运行。 __set_current_state(TASK_RUNNING); send_events: /* * Try to transfer events to user space. In case we get 0 events and * there's still timeout left over, we go trying again in search of * more luck. */ // 有就绪事件就发送到用户空间，否则继续获取数据直到超时。 if (!res \u0026\u0026 eavail \u0026\u0026 !(res = ep_send_events(ep, events, maxevents)) \u0026\u0026 !timed_out) goto fetch_events; // 从等待队列中，删除等待事件。 if (waiter) { spin_lock_irq(\u0026ep-\u003ewq.lock); __remove_wait_queue(\u0026ep-\u003ewq, \u0026wait); spin_unlock_irq(\u0026ep-\u003ewq.lock); } return res; } /* Used by the ep_send_events() function as callback private data */ struct ep_send_events_data { int maxevents; struct epoll_event __user *events; int res; }; static int ep_send_events(struct eventpoll *ep, struct epoll_event __user *events, int maxevents) { struct ep_send_events_data esed; esed.maxevents = maxevents; esed.events = events; // 遍历事件就绪列表，发送就绪事件到用户空间。 ep_scan_ready_list(ep, ep_send_events_proc, \u0026esed, 0, false); return esed.res; } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:6","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.7. ep_scan_ready_list 遍历就绪列表，处理 sproc 函数。这里 sproc 函数指针的使用，是为了减少代码冗余，将 ep_scan_ready_list 做成一个通用的函数。 // static __poll_t ep_scan_ready_list(struct eventpoll *ep, __poll_t (*sproc)(struct eventpoll *, struct list_head *, void *), void *priv, int depth, bool ep_locked) { __poll_t res; struct epitem *epi, *nepi; LIST_HEAD(txlist); ... // 将就绪队列分片链接到 txlist 链表中。 list_splice_init(\u0026ep-\u003erdllist, \u0026txlist); res = (*sproc)(ep, \u0026txlist, priv); ... // 在处理 sproc 回调处理过程中，可能产生新的就绪事件被写入 ovflist，将 ovflist 回写 rdllist。 for (nepi = READ_ONCE(ep-\u003eovflist); (epi = nepi) != NULL; nepi = epi-\u003enext, epi-\u003enext = EP_UNACTIVE_PTR) { if (!ep_is_linked(epi)) { list_add(\u0026epi-\u003erdllink, \u0026ep-\u003erdllist); ep_pm_stay_awake(epi); } } ... // txlist 在 epitem 回调中，可能没有完全处理完，那么重新放回到 rdllist，下次处理。 list_splice(\u0026txlist, \u0026ep-\u003erdllist); ... } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:7","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.8. ep_send_events_proc 处理就绪列表，将数据从内核空间拷贝到用户空间。 static __poll_t ep_send_events_proc(struct eventpoll *ep, struct list_head *head, void *priv) { struct ep_send_events_data *esed = priv; __poll_t revents; struct epitem *epi, *tmp; struct epoll_event __user *uevent = esed-\u003eevents; struct wakeup_source *ws; poll_table pt; init_poll_funcptr(\u0026pt, NULL); ... // 遍历处理 txlist（原 ep-\u003erdllist 数据）就绪队列结点，获取事件拷贝到用户空间。 list_for_each_entry_safe (epi, tmp, head, rdllink) { if (esed-\u003eres \u003e= esed-\u003emaxevents) break; ... // 先从就绪队列中删除 epi，如果是 LT 模式，就绪事件还没处理完，再把它添加回去。 list_del_init(\u0026epi-\u003erdllink); // 获取 epi 对应 fd 的就绪事件。 revents = ep_item_poll(epi, \u0026pt, 1); if (!revents) // 如果没有就绪事件就返回（这时候，epi 已经从就绪列表中删除了。） continue; // 内核空间向用户空间传递数据。__put_user 成功拷贝返回 0。 if (__put_user(revents, \u0026uevent-\u003eevents) || __put_user(epi-\u003eevent.data, \u0026uevent-\u003edata)) { // 如果拷贝失败，继续保存在就绪列表里。 list_add(\u0026epi-\u003erdllink, head); ep_pm_stay_awake(epi); if (!esed-\u003eres) esed-\u003eres = -EFAULT; return 0; } // 成功处理就绪事件的 fd 个数。 esed-\u003eres++; uevent++; if (epi-\u003eevent.events \u0026 EPOLLONESHOT) // #define EP_PRIVATE_BITS (EPOLLWAKEUP | EPOLLONESHOT | EPOLLET | EPOLLEXCLUSIVE) epi-\u003eevent.events \u0026= EP_PRIVATE_BITS; else if (!(epi-\u003eevent.events \u0026 EPOLLET)) { /* * If this file has been added with Level * Trigger mode, we need to insert back inside * the ready list, so that the next call to * epoll_wait() will check again the events * availability. At this point, no one can insert * into ep-\u003erdllist besides us. The epoll_ctl() * callers are locked out by * ep_scan_ready_list() holding \"mtx\" and the * poll callback will queue them in ep-\u003eovflist. */ /* lt 模式下，当前事件被处理完后，不会从就绪列表中删除，留待下一次 epoll_wait * 调用，再查看是否还有事件没处理，如果没有事件了就从就绪列表中删除。 * 在遍历事件的过程中，不能写 ep-\u003erdllist，因为已经上锁，只能把新的就绪信息 * 添加到 ep-\u003eovflist */ list_add_tail(\u0026epi-\u003erdllink, \u0026ep-\u003erdllist); ep_pm_stay_awake(epi); } } return 0; } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:8","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"8.9. ep_poll_callback fd 事件回调。当 fd 有网络事件发生，就会通过等待队列，进行回调。参考 __wake_up_common，如果事件是用户关注的事件，回调会唤醒进程进行处理。 static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key) { int pwake = 0; struct epitem *epi = ep_item_from_wait(wait); struct eventpoll *ep = epi-\u003eep; __poll_t pollflags = key_to_poll(key); unsigned long flags; int ewake = 0; // 禁止本地中断并获得指定读锁。 read_lock_irqsave(\u0026ep-\u003elock, flags); ep_set_busy_poll_napi_id(epi); // #define EP_PRIVATE_BITS (EPOLLWAKEUP | EPOLLONESHOT | EPOLLET | EPOLLEXCLUSIVE) // 如果 fd 没有关注除了 EP_PRIVATE_BITS 之外的事件，那么走解锁流程。 if (!(epi-\u003eevent.events \u0026 ~EP_PRIVATE_BITS)) goto out_unlock; // 如果回调的事件，不是用户关注的 fd 事件，那么走解锁流程。 if (pollflags \u0026\u0026 !(pollflags \u0026 epi-\u003eevent.events)) goto out_unlock; /* * If we are transferring events to userspace, we can hold no locks * (because we're accessing user memory, and because of linux f_op-\u003epoll() * semantics). All the events that happen during that period of time are * chained in ep-\u003eovflist and requeued later on. */ // 当内核空间向用户空间拷贝数据时，不添加 epi 到 rdllist，将它添加到 ovflist。 if (READ_ONCE(ep-\u003eovflist) != EP_UNACTIVE_PTR) { if (epi-\u003enext == EP_UNACTIVE_PTR \u0026\u0026 chain_epi_lockless(epi)) ep_pm_stay_awake_rcu(epi); goto out_unlock; } // epi 已经加入就绪链表就不需要添加了。 if (!ep_is_linked(epi) \u0026\u0026 list_add_tail_lockless(\u0026epi-\u003erdllink, \u0026ep-\u003erdllist)) { ep_pm_stay_awake_rcu(epi); } // 当回调事件是用户关注的事件，那么需要唤醒进程处理。 // ep-\u003ewq 在 epoll_wait 时添加，当没有就绪事件，epoll_wait 进行睡眠等待唤醒。 if (waitqueue_active(\u0026ep-\u003ewq)) { if ((epi-\u003eevent.events \u0026 EPOLLEXCLUSIVE) \u0026\u0026 !(pollflags \u0026 POLLFREE)) { // #define EPOLLINOUT_BITS (EPOLLIN | EPOLLOUT) switch (pollflags \u0026 EPOLLINOUT_BITS) { case EPOLLIN: if (epi-\u003eevent.events \u0026 EPOLLIN) ewake = 1; break; case EPOLLOUT: if (epi-\u003eevent.events \u0026 EPOLLOUT) ewake = 1; break; case 0: ewake = 1; break; } } wake_up(\u0026ep-\u003ewq); } // ep-\u003epoll_wait 是 epoll 监控另外一个 epoll fd 的等待队列。如果触发事件，也需要唤醒进程处理。 if (waitqueue_active(\u0026ep-\u003epoll_wait)) pwake++; out_unlock: read_unlock_irqrestore(\u0026ep-\u003elock, flags); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(\u0026ep-\u003epoll_wait); if (!(epi-\u003eevent.events \u0026 EPOLLEXCLUSIVE)) ewake = 1; if (pollflags \u0026 POLLFREE) { /* * If we race with ep_remove_wait_queue() it can miss * -\u003ewhead = NULL and do another remove_wait_queue() after * us, so we can't use __remove_wait_queue(). */ list_del_init(\u0026wait-\u003eentry); /* * -\u003ewhead != NULL protects us from the race with ep_free() * or ep_remove(), ep_remove_wait_queue() takes whead-\u003elock * held by the caller. Once we nullify it, nothing protects * ep/epi or even wait. */ smp_store_release(\u0026ep_pwq_from_wait(wait)-\u003ewhead, NULL); } return ewake; } ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:8:9","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"9. 参考 vscode + gdb 远程调试 linux (EPOLL) 内核源码 Linux下的I/O复用与epoll详解 epoll实现探究 Buddy memory allocation (伙伴内存分配器) Linux内存管理，内存寻址 EPOLL内核原理极简图文解读 彻底理解epoll 《UNIX 环境高级编程》3.2 文件描述符 Linux内核空间内存申请函数kmalloc、kzalloc、vmalloc的区别 Linux内核笔记–深入理解文件描述符 epoll_ctl 文档 epoll的原理过程讲解 socket—proto_ops—inetsw_array等基本结构 epoll高效IO复用 Epoll技术扩展 Linux网络包收发总体过程 epoll源码分析 TASK_INTERRUPTIBLE 和 TASK_UNINTERRUPTIBLE NAPI(New API)的一些浅见 NAPI 技术在 Linux 网络驱动上的应用和完善 EPOLL 源码分析 用户空间和内核空间传递数据 《Linux内核设计与实现》读书笔记（十）- 内核同步方法 虚拟文件系统VFS epoll用法【整理】 再谈Linux epoll惊群问题的原因和解决方案 从linux源码看epoll ","date":"2023-05-27","objectID":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/:9:0","tags":["[内核源码]epoll实现原理"],"title":"[内核源码]epoll实现原理","uri":"/posts/%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"Linux进程、线程文件、描述符的底层原理","date":"2023-05-27","objectID":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","tags":["Linux进程、线程文件、描述符的底层原理"],"title":"Linux进程、线程文件、描述符的底层原理","uri":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"说到进程，恐怕面试中最常见的问题就是线程和进程的关系了，那么先说一下答案：在 Linux 系统中，进程和线程几乎没有区别。 Linux 中的进程就是一个数据结构，看明白就可以理解文件描述符、重定向、管道命令的底层工作原理，最后我们从操作系统的角度看看为什么说线程和进程基本没有区别。 ","date":"2023-05-27","objectID":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/:0:0","tags":["Linux进程、线程文件、描述符的底层原理"],"title":"Linux进程、线程文件、描述符的底层原理","uri":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"进程是什么 首先，抽象地来说，我们的计算机就是这个东西： 这个大的矩形表示计算机的内存空间，其中的小矩形代表进程，左下角的圆形表示磁盘，右下角的图形表示一些输入输出设备，比如鼠标键盘显示器等等。另外，注意到内存空间被划分为了两块，上半部分表示用户空间，下半部分表示内核空间。 用户空间装着用户进程需要使用的资源，比如你在程序代码里开一个数组，这个数组肯定存在用户空间；内核空间存放内核进程需要加载的系统资源，这一些资源一般是不允许用户访问的。但是注意有的用户进程会共享一些内核空间的资源，比如一些动态链接库等等。 我们用 C 语言写一个 hello 程序，编译后得到一个可执行文件，在命令行运行就可以打印出一句 hello world，然后程序退出。在操作系统层面，就是新建了一个进程，这个进程将我们编译出来的可执行文件读入内存空间，然后执行，最后退出。 你编译好的那个可执行程序只是一个文件，不是进程，可执行文件必须要载入内存，包装成一个进程才能真正跑起来。进程是要依靠操作系统创建的，每个进程都有它的固有属性，比如进程号（PID）、进程状态、打开的文件等等，进程创建好之后，读入你的程序，你的程序才被系统执行。 那么，操作系统是如何创建进程的呢？对于操作系统，进程就是一个数据结构，我们直接来看 Linux 的源码： struct task_struct { // 进程状态 long state; // 虚拟内存结构体 struct mm_struct *mm; // 进程号 pid_t pid; // 指向父进程的指针 struct task_struct __rcu *parent; // 子进程列表 struct list_head children; // 存放文件系统信息的指针 struct fs_struct *fs; // 一个数组，包含该进程打开的文件指针 struct files_struct *files; }; task_struct就是 Linux 内核对于一个进程的描述，也可以称为「进程描述符」。源码比较复杂，我这里就截取了一小部分比较常见的。 其中比较有意思的是mm指针和files指针。mm指向的是进程的虚拟内存，也就是载入资源和可执行文件的地方；files指针指向一个数组，这个数组里装着所有该进程打开的文件的指针。 ","date":"2023-05-27","objectID":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/:1:0","tags":["Linux进程、线程文件、描述符的底层原理"],"title":"Linux进程、线程文件、描述符的底层原理","uri":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"文件描述符是什么 先说files，它是一个文件指针数组。一般来说，一个进程会从files[0]读取输入，将输出写入files[1]，将错误信息写入files[2]。 举个例子，以我们的角度 C 语言的printf函数是向命令行打印字符，但是从进程的角度来看，就是向files[1]写入数据；同理，scanf函数就是进程试图从files[0]这个文件中读取数据。 每个进程被创建时，files的前三位被填入默认值，分别指向标准输入流、标准输出流、标准错误流。我们常说的「文件描述符」就是指这个文件指针数组的索引，所以程序的文件描述符默认情况下 0 是输入，1 是输出，2 是错误。 我们可以重新画一幅图： 对于一般的计算机，输入流是键盘，输出流是显示器，错误流也是显示器，所以现在这个进程和内核连了三根线。因为硬件都是由内核管理的，我们的进程需要通过「系统调用」让内核进程访问硬件资源。 PS：不要忘了，Linux 中一切都被抽象成文件，设备也是文件，可以进行读和写。 如果我们写的程序需要其他资源，比如打开一个文件进行读写，这也很简单，进行系统调用，让内核把文件打开，这个文件就会被放到files的第 4 个位置： 明白了这个原理，输入重定向就很好理解了，程序想读取数据的时候就会去files[0]读取，所以我们只要把files[0]指向一个文件，那么程序就会从这个文件中读取数据，而不是从键盘。 管道符其实也是异曲同工，把一个进程的输出流和另一个进程的输入流接起一条「管道」，数据就在其中传递，不得不说这种设计思想真的很优美： cmd1 | cmd2 | cmd3 到这里，你可能也看出「Linux 中一切皆文件」设计思路的高明了，不管是设备、另一个进程、socket 套接字还是真正的文件，全部都可以读写，统一装进一个简单的files数组，进程通过简单的文件描述符访问相应资源，具体细节交于操作系统，有效解耦，优美高效。 ","date":"2023-05-27","objectID":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/:2:0","tags":["Linux进程、线程文件、描述符的底层原理"],"title":"Linux进程、线程文件、描述符的底层原理","uri":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"categories":["深入Linux内核"],"content":"线程是什么 首先要明确的是，多进程和多线程都是并发，都可以提高处理器的利用效率，所以现在的关键是，多线程和多进程有啥区别。 为什么说 Linux 中线程和进程基本没有区别呢，因为从 Linux 内核的角度来看，并没有把线程和进程区别对待。 我们知道系统调用fork()可以新建一个子进程，函数pthread()可以新建一个线程。但无论线程还是进程，都是用task_struct结构表示的，唯一的区别就是共享的数据区域不同。 换句话说，线程看起来跟进程没有区别，只是线程的某些数据区域和其父进程是共享的，而子进程是拷贝副本，而不是共享。就比如说，mm结构和files结构在线程中都是共享的，我画两张图你就明白了： 所以说，我们的多线程程序要利用锁机制，避免多个线程同时往同一区域写入数据，否则可能造成数据错乱。 那么你可能问，既然进程和线程差不多，而且多进程数据不共享，即不存在数据错乱的问题，为什么多线程的使用比多进程普遍得多呢？ 因为现实中数据共享的并发更普遍呀，比如十个人同时从一个账户取十元，我们希望的是这个共享账户的余额正确减少一百元，而不是希望每人获得一个账户的拷贝，每个拷贝账户减少十元。 当然，必须要说明的是，只有 Linux 系统将线程看做共享数据的进程，不对其做特殊看待，其他的很多操作系统是对线程和进程区别对待的，线程有其特有的数据结构，我个人认为不如 Linux 的这种设计简洁，增加了系统的复杂度。 在 Linux 中新建线程和进程的效率都是很高的，对于新建进程时内存区域拷贝的问题，Linux 采用了 copy-on-write 的策略优化，也就是并不真正复制父进程的内存空间，而是等到需要写操作时才去复制。所以 Linux 中新建进程和新建线程都是很迅速的。 ","date":"2023-05-27","objectID":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/:3:0","tags":["Linux进程、线程文件、描述符的底层原理"],"title":"Linux进程、线程文件、描述符的底层原理","uri":"/posts/linux%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"categories":["MySQL基础篇"],"content":"一、执行一条select语句，期间发生了什么","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"执行一条 select 语句，期间发生了什么？ 学习 SQL 的时候，大家肯定第一个先学到的就是 select 查询语句了，比如下面这句查询语句： //在product表中，查询id=1的记录select*fromproductwhereid=1;但是有没有想过，MySQL 执行一条 select 查询语句，在 MySQL 中期间发生了什么？ 带着这个问题，我们可以很好的了解 MySQL 内部的架构，所以这次小林就带大家拆解一下 MySQL 内部的结构，看看内部里的每一个“零件”具体是负责做什么的。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#MySQL 执行流程是怎样的？ 先来一个上帝视角图，下面就是 MySQL 执行一条 SQL 查询语句的流程，也从图中可以看到 MySQL 内部架构里的各个功能模块。 可以看到， MySQL 的架构共分为两层：Server 层和存储引擎层， Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。 存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。 好了，现在我们对 Server 层和存储引擎层有了一个简单认识，接下来，就详细说一条 SQL 查询语句的执行流程，依次看看每一个功能模块的作用。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:1:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#第一步：连接器 如果你在 Linux 操作系统里要使用 MySQL，那你第一步肯定是要先连接 MySQL 服务，然后才能执行 SQL 语句，普遍我们都是使用下面这条命令进行连接： # -h 指定 MySQL 服务得 IP 地址，如果是连接本地的 MySQL服务，可以不用这个参数； # -u 指定用户名，管理员角色名为 root； # -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里面输入密码 mysql -h$ip -u$user -p 连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的，如果 MySQL 服务并没有启动，则会收到如下的报错： 如果 MySQL 服务正常运行，完成 TCP 连接的建立后，连接器就要开始验证你的用户名和密码，如果用户名或密码不对，就收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。 如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断。 所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 如何查看 MySQL 服务被多少个客户端连接了？ 如果你想知道当前 MySQL 服务被多少个客户端连接了，你可以执行 show processlist 命令进行查看。 比如上图的显示结果，共有两个用户名为 root 的用户连接了 MySQL 服务，其中 id 为 6 的用户的 Command 列的状态为 Sleep ，这意味着该用户连接完 MySQL 服务就没有再执行过任何命令，也就是说这是一个空闲的连接，并且空闲的时长是 736 秒（ Time 列）。 空闲连接会一直占用着吗？ 当然不是了，MySQL 定义了空闲连接的最大空闲时长，由 wait_timeout 参数控制的，默认值是 8 小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开。 mysql\u003eshowvariableslike'wait_timeout';+---------------+-------+ |Variable_name|Value|+---------------+-------+ |wait_timeout|28800|+---------------+-------+ 1rowinset(0.00sec)当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。 mysql\u003ekillconnection+6;QueryOK,0rowsaffected(0.00sec)一个处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。 MySQL 的连接数有限制吗？ MySQL 服务支持的最大连接数由 max_connections 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。 mysql\u003eshowvariableslike'max_connections';+-----------------+-------+ |Variable_name|Value|+-----------------+-------+ |max_connections|151|+-----------------+-------+ 1rowinset(0.00sec)MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，它们的区别如下： // 短连接 连接 mysql 服务（TCP 三次握手） 执行sql 断开 mysql 服务（TCP 四次挥手） // 长连接 连接 mysql 服务（TCP 三次握手） 执行sql 执行sql 执行sql .... 断开 mysql 服务（TCP 四次挥手） 可以看到，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接。 但是，使用长连接后可能会占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象。 怎么解决长连接占用内存的问题？ 有两种解决方式。 第一种，定期断开长连接。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。 第二种，客户端主动重置连接。MySQL 5.7 版本实现了 mysql_reset_connection() 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 至此，连接器的工作做完了，简单总结一下： 与客户端进行 TCP 三次握手建立连接； 校验客户端的用户名和密码，如果用户名或密码不对，则会报错； 如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限； ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:2:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#第二步：查询缓存 连接器得工作完成后，客户端就可以向 MySQL 服务发送 SQL 语句了，MySQL 服务收到 SQL 语句后，就会解析出 SQL 语句的第一个字段，看看是什么类型的语句。 如果 SQL 是查询语句（select 语句），MySQL 就会先去查询缓存（ Query Cache ）里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果。 如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。如果查询的语句没有命中查询缓存中，那么就要往下继续执行，等执行完后，查询的结果就会被存入查询缓存中。 这么看，查询缓存还挺有用，但是其实查询缓存挺鸡肋的。 对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。如果刚缓存了一个查询结果很大的数据，还没被使用的时候，刚好这个表有更新操作，查询缓冲就被清空了，相当于缓存了个寂寞。 所以，MySQL 8.0 版本直接将查询缓存删掉了，也就是说 MySQL 8.0 开始，执行一条 SQL 查询语句，不会再走到查询缓存这个阶段了。 对于 MySQL 8.0 之前的版本，如果想关闭查询缓存，我们可以通过将参数 query_cache_type 设置成 DEMAND。 TIP 这里说的查询缓存是 server 层的，也就是 MySQL 8.0 版本移除的是 server 层的查询缓存，并不是 Innodb 存储引擎中的 buffer pool。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:3:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#第三步：解析 SQL 在正式执行 SQL 查询语句之前， MySQL 会先对 SQL 语句做解析，这个工作交由「解析器」来完成。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:4:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#解析器 解析器会做如下两件事情。 第一件事情，词法分析。MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。 第二件事情，语法分析。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果我们输入的 SQL 语句语法不对，就会在解析器这个阶段报错。比如，我下面这条查询语句，把 from 写成了 form，这时 MySQL 解析器就会给报错。 但是注意，表不存在或者字段不存在，并不是在解析器里做的，《MySQL 45 讲》说是在解析器做的，但是经过我和朋友看 MySQL 源码（5.7和8.0）得出结论是解析器只负责构建语法树和检查语法，但是不会去查表或者字段存不存在。 那到底谁来做检测表和字段是否存在的工作呢？别急，接下来就是了。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:4:1","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#第四步：执行 SQL 经过解析器后，接着就要进入执行 SQL 查询语句的流程了，每条SELECT 查询语句流程主要可以分为下面这三个阶段： prepare 阶段，也就是预处理阶段； optimize 阶段，也就是优化阶段； execute 阶段，也就是执行阶段； ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:5:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#预处理器 我们先来说说预处理阶段做了什么事情。 检查 SQL 查询语句中的表或者字段是否存在； 将 select * 中的 * 符号，扩展为表上的所有列； 我下面这条查询语句，test 这张表是不存在的，这时 MySQL 就会在执行 SQL 查询语句的 prepare 阶段中报错。 mysql\u003eselect*fromtest;ERROR1146(42S02):Table'mysql.test'doesn't exist 这里贴个 MySQL 8.0 源码来证明表或字段是否存在的判断，不是在解析器里做的，而是在 prepare 阶段。（PS：下图是公众号「一树一溪」老哥帮我分析的，这位老哥专门写 MySQL 源码文章，感兴趣的朋友，可以微信搜索关注） 上面的中间部分是 MySQL 报错表不存在时的函数调用栈，可以看到表不存在的错误是在get_table_share() 函数里报错的，而这个函数是在 prepare 阶段调用的。 不过，对于 MySQL 5.7 判断表或字段是否存在的工作，是在词法分析\u0026语法分析之后，prepare 阶段之前做的。结论都一样，不是在解析器里做的。代码我就不放了，正因为 MySQL 5.7 代码结构不好，所以 MySQL 8.0 代码结构变化很大，后来判断表或字段是否存在的工作就被放入到 prepare 阶段做了。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:5:1","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#优化器 经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。 优化器主要负责将 SQL 查询语句的执行方案确定下来，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。 当然，我们本次的查询语句（select * from product where id = 1）很简单，就是选择使用主键索引。 要想知道优化器选择了哪个索引，我们可以在查询语句最前面加个 explain 命令，这样就会输出这条 SQL 语句的执行计划，然后执行计划中的 key 就表示执行过程中使用了哪个索引，比如下图的 key 为 PRIMARY 就是使用了主键索引。 如果查询语句的执行计划里的 key 为 null 说明没有使用索引，那就会全表扫描（type = ALL），这种查询扫描的方式是效率最低档次的，如下图： 这张 product 表只有一个索引就是主键，现在我在表中将 name 设置为普通索引（二级索引）。 这时 product 表就有主键索引（id）和普通索引（name）。假设执行了这条查询语句： selectidfromproductwhereid\u003e1andnamelike'i%';这条查询语句的结果既可以使用主键索引，也可以使用普通索引，但是执行的效率会不同。这时，就需要优化器来决定使用哪个索引了。 很显然这条查询语句是覆盖索引，直接在二级索引就能查找到结果（因为二级索引的 B+ 树的叶子节点的数据存储的是主键值），就没必要在主键索引查找了，因为查询主键索引的 B+ 树的成本会比查询二级索引的 B+ 的成本大，优化器基于查询成本的考虑，会选择查询代价小的普通索引。 在下图中执行计划，我们可以看到，执行过程中使用了普通索引（name），Exta 为 Using index，这就是表明使用了覆盖索引优化。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:5:2","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#执行器 经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。 接下来，用三种方式执行过程，跟大家说一下执行器和存储引擎的交互过程（PS ：为了写好这一部分，特地去看 MySQL 源码，也是第一次看哈哈）。 主键索引查询 全表扫描 索引下推 #主键索引查询 以本文开头查询语句为例，看看执行器是怎么工作的。 select*fromproductwhereid=1;这条查询语句的查询条件用到了主键索引，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的： 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 id = 1 交给存储引擎，让存储引擎定位符合条件的第一条记录。 存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器； 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。 执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。 至此，这个语句就执行完成了。 #全表扫描 举个全表扫描的例子： select * from product where name = 'iphone'; 这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的： 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录； 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。 执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端； 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息； 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。 至此，这个语句就执行完成了。 #索引下推 在这部分非常适合讲索引下推（MySQL 5.6 推出的查询优化策略），这样大家能清楚的知道，「下推」这个动作，下推到了哪里。 索引下推能够减少二级索引在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。 举一个具体的例子，方便大家理解，这里一张用户表如下，我对 age 和 reward 字段建立了联合索引（age，reward）： 现在有下面这条查询语句： select*fromt_userwhereage\u003e20andreward=100000;联合索引当遇到范围查询 (\u003e、\u003c) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。具体原因这里可以看这篇：索引常见面试题(opens new window) 那么，不使用索引下推（MySQL 5.6 之前的版本）时，执行器与存储引擎的执行流程是这样的： Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age \u003e 20 的第一条记录； 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给 Server 层； Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录； 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层； 如此往复，直到存储引擎把表中的所有记录读完。 可以看到，没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 reward 是否等于 100000。 而使用索引下推后，判断记录的 reward 是否等于 100000 的工作交给了存储引擎层，过程如下 ： Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age \u003e 20 的第一条记录； 存储引擎定位到二级索引后，先不执行回表操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。 Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。 如此往复，直到存储引擎把表中的所有记录读完。 可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。 当你发现执行计划里的 Extr 部分显示了 “Using index condition”，说明使用了索引下推。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:5:3","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["MySQL基础篇"],"content":"#总结 执行一条 SQL 查询语句，期间发生了什么？ 连接器：建立连接，管理连接、校验用户身份； 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块； 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型； 执行 SQL：执行 SQL 共有三个阶段： 预处理阶段：检查表或字段是否存在；将 select * 中的 * 符号扩展为表上的所有列。 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划； 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端； 怎么样？现在再看这张图，是不是很清晰了。 参考资料： 《MySQL 45 讲》 《MySQL是怎样运行的：从根儿上理解MySQL》 https://gohalo.me/post/mysql-executor.html http://www.iskm.org/mysql56/sql__executor_8cc_source.html https://tangocc.github.io/2018/10/11/mysql-sourcecode/ ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:6:0","tags":["一、执行一条select语句，期间发生了什么"],"title":"一、执行一条select语句，期间发生了什么","uri":"/posts/%E4%B8%80%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1select%E8%AF%AD%E5%8F%A5%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["进程管理"],"content":"一、进程、线程基础知识","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程、线程基础知识 先来看看一则小故事 我们写好的一行行代码，为了让其工作起来，我们还得把它送进城（进程）里，那既然进了城里，那肯定不能胡作非为了。 城里人有城里人的规矩，城中有个专门管辖你们的城管（操作系统），人家让你休息就休息，让你工作就工作，毕竟摊位不多，每个人都要占这个摊位来工作，城里要工作的人多着去了。 所以城管为了公平起见，它使用一种策略（调度）方式，给每个人一个固定的工作时间（时间片），时间到了就会通知你去休息而换另外一个人上场工作。 另外，在休息时候你也不能偷懒，要记住工作到哪了，不然下次到你工作了，你忘记工作到哪了，那还怎么继续？ 有的人，可能还进入了县城（线程）工作，这里相对轻松一些，在休息的时候，要记住的东西相对较少，而且还能共享城里的资源。 “哎哟，难道本文内容是进程和线程？” 可以，聪明的你猜出来了，也不枉费我瞎编乱造的故事了。 进程和线程对于写代码的我们，真的天天见、日日见了，但见的多不代表你就熟悉它们，比如简单问你一句，你知道它们的工作原理和区别吗？ 不知道没关系，今天就要跟大家讨论操作系统的进程和线程。 TIP 先强调一下，我们本篇讲的主要都是操作系统理论知识，偏大学计算机专业课上的那种，并不是讲解 Linux 或 Windows 操作系统的实现方式，所以大家要区别一下。 想让了解 Linux 或 Windows 操作系统的具体实现，得去看这些操作系统的实现原理或者源码书籍。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:0:0","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程 我们编写的代码只是一个存储在硬盘的静态文件，通过编译后就会生成二进制可执行文件，当我们运行这个可执行文件后，它会被装载到内存中，接着 CPU 会执行程序中的每一条指令，那么这个运行中的程序，就被称为「进程」（Process）。 现在我们考虑有一个会读取硬盘文件数据的程序被执行了，那么当运行到读取文件的指令时，就会去从硬盘读取数据，但是硬盘的读写速度是非常慢的，那么在这个时候，如果 CPU 傻傻的等硬盘返回数据的话，那 CPU 的利用率是非常低的。 做个类比，你去煮开水时，你会傻傻的等水壶烧开吗？很明显，小孩也不会傻等。我们可以在水壶烧开之前去做其他事情。当水壶烧开了，我们自然就会听到“嘀嘀嘀”的声音，于是再把烧开的水倒入到水杯里就好了。 所以，当进程要从硬盘读取数据时，CPU 不需要阻塞等待数据的返回，而是去执行另外的进程。当硬盘数据返回时，CPU 会收到个中断，于是 CPU 再继续运行这个进程。 这种多个程序、交替执行的思想，就有 CPU 管理多个进程的初步想法。 对于一个支持多进程的系统，CPU 会从一个进程快速切换至另一个进程，其间每个进程各运行几十或几百个毫秒。 虽然单核的 CPU 在某一个瞬间，只能运行一个进程。但在 1 秒钟期间，它可能会运行多个进程，这样就产生并行的错觉，实际上这是并发。 并发和并行有什么区别？ 一图胜千言。 进程与程序的关系的类比 到了晚饭时间，一对小情侣肚子都咕咕叫了，于是男生见机行事，就想给女生做晚饭，所以他就在网上找了辣子鸡的菜谱，接着买了一些鸡肉、辣椒、香料等材料，然后边看边学边做这道菜。 突然，女生说她想喝可乐，那么男生只好把做菜的事情暂停一下，并在手机菜谱标记做到哪一个步骤，把状态信息记录了下来。 然后男生听从女生的指令，跑去下楼买了一瓶冰可乐后，又回到厨房继续做菜。 这体现了，CPU 可以从一个进程（做菜）切换到另外一个进程（买可乐），在切换前必须要记录当前进程中运行的状态信息，以备下次切换回来的时候可以恢复执行。 所以，可以发现进程有着「运行 - 暂停 - 运行」的活动规律。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:1:0","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程的状态 在上面，我们知道了进程有着「运行 - 暂停 - 运行」的活动规律。一般说来，一个进程并不是自始至终连续不停地运行的，它与并发执行中的其他进程的执行是相互制约的。 它有时处于运行状态，有时又由于某种原因而暂停运行处于等待状态，当使它暂停的原因消失后，它又进入准备运行状态。 所以，在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。 上图中各个状态的意义： 运行状态（Running）：该时刻进程占用 CPU； 就绪状态（Ready）：可运行，由于其他进程处于运行状态而暂时停止运行； 阻塞状态（Blocked）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行； 当然，进程还有另外两个基本状态： 创建状态（new）：进程正在被创建时的状态； 结束状态（Exit）：进程正在从系统中消失时的状态； 于是，一个完整的进程状态的变迁如下图： 再来详细说明一下进程的状态变迁： NULL -\u003e 创建状态：一个新进程被创建时的第一个状态； 创建状态 -\u003e 就绪状态：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的； 就绪态 -\u003e 运行状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程； 运行状态 -\u003e 结束状态：当进程已经运行完成或出错时，会被操作系统作结束状态处理； 运行状态 -\u003e 就绪状态：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行； 运行状态 -\u003e 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件； 阻塞状态 -\u003e 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态； 如果有大量处于阻塞状态的进程，进程可能会占用着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占用着物理内存就一种浪费物理内存的行为。 所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。 那么，就需要一个新的状态，来描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态。这跟阻塞状态是不一样，阻塞状态是等待某个事件的返回。 另外，挂起状态可以分为两种： 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现； 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行； 这两种挂起状态加上前面的五种状态，就变成了七种状态变迁（留给我的颜色不多了），见如下图： 导致进程挂起的原因不只是因为进程所使用的内存空间不在物理内存，还包括如下情况： 通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。 用户希望挂起一个程序的执行，比如在 Linux 中用 Ctrl+Z 挂起进程； ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:1:1","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程的控制结构 在操作系统中，是用进程控制块（process control block，PCB）数据结构来描述进程的。 那 PCB 是什么呢？打开知乎搜索你就会发现这个东西并不是那么简单。 打住打住，我们是个正经的人，怎么会去看那些问题呢？是吧，回来回来。 PCB 是进程存在的唯一标识，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。 PCB 具体包含什么信息呢？ 进程描述信息： 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符； 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务； 进程控制和管理信息： 进程当前状态，如 new、ready、running、waiting 或 blocked 等； 进程优先级：进程抢占 CPU 时的优先级； 资源分配清单： 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。 CPU 相关信息： CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。 可见，PCB 包含信息还是比较多的。 每个 PCB 是如何组织的呢？ 通常是通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列。比如： 将所有处于就绪状态的进程链在一起，称为就绪队列； 把所有因等待某事件而处于等待状态的进程链在一起就组成各种阻塞队列； 另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。 那么，就绪队列和阻塞队列链表的组织形式如下图： 除了链接的组织方式，还有索引方式，它的工作原理：将同一状态的进程组织在一个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。 一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:1:2","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程的控制 我们熟知了进程的状态变迁和进程的数据结构 PCB 后，再来看看进程的创建、终止、阻塞、唤醒的过程，这些过程也就是进程的控制。 01 创建进程 操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。 创建进程的过程如下： 申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等； 为该进程分配运行时所必需的资源，比如内存资源； 将 PCB 插入到就绪队列，等待被调度运行； 02 终止进程 进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 kill 掉）。 当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。 终止进程的过程如下： 查找需要终止的进程的 PCB； 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程； 如果其还有子进程，则应将该进程的子进程交给 1 号进程接管； 将该进程所拥有的全部资源都归还给操作系统； 将其从 PCB 所在队列中删除； 03 阻塞进程 当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。 阻塞进程的过程如下： 找到将要被阻塞进程标识号对应的 PCB； 如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行； 将该 PCB 插入到阻塞队列中去； 04 唤醒进程 进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。 如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。 唤醒进程的过程如下： 在该事件的阻塞队列中找到相应进程的 PCB； 将其从阻塞队列中移出，并置其状态为就绪状态； 把该 PCB 插入到就绪队列中，等待调度程序调度； 进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:1:3","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程的上下文切换 各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，那么这个一个进程切换到另一个进程运行，称为进程的上下文切换。 在详细说进程上下文切换前，我们先来看看 CPU 上下文切换 大多数操作系统都是多任务，通常支持大于 CPU 数量的任务同时运行。实际上，这些任务并不是同时运行的，只是因为系统在很短的时间内，让各个任务分别在 CPU 运行，于是就造成同时运行的错觉。 任务是交给 CPU 运行的，那么在每个任务运行前，CPU 需要知道任务从哪里加载，又从哪里开始运行。 所以，操作系统需要事先帮 CPU 设置好 CPU 寄存器和程序计数器。 CPU 寄存器是 CPU 内部一个容量小，但是速度极快的内存（缓存）。我举个例子，寄存器像是你的口袋，内存像你的书包，硬盘则是你家里的柜子，如果你的东西存放到口袋，那肯定是比你从书包或家里柜子取出来要快的多。 再来，程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。 所以说，CPU 寄存器和程序计数是 CPU 在运行任何任务前，所必须依赖的环境，这些环境就叫做 CPU 上下文。 既然知道了什么是 CPU 上下文，那理解 CPU 上下文切换就不难了。 CPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 系统内核会存储保持下来的上下文信息，当此任务再次被分配给 CPU 运行时，CPU 会重新加载这些上下文，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 上面说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下文切换分成：进程上下文切换、线程上下文切换和中断上下文切换。 进程的上下文切换到底是切换什么呢？ 进程是由内核管理和调度的，所以进程的切换只能发生在内核态。 所以，进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。 通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行，如下图所示： 大家需要注意，进程的上下文开销是很关键的，我们希望它的开销越小越好，这样可以使得进程可以把更多时间花费在执行程序上，而不是耗费在上下文切换。 发生进程上下文切换有哪些场景？ 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行； 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行； 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度； 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行； 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序； 以上，就是发生进程上下文切换的常见场景了。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:1:4","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"线程 在早期的操作系统中都是以进程作为独立运行的基本单位，直到后面，计算机科学家们又提出了更小的能独立运行的基本单位，也就是线程。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:0","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"为什么使用线程？ 我们举个例子，假设你要编写一个视频播放器软件，那么该软件功能的核心模块有三个： 从视频文件当中读取数据； 对读取的数据进行解压缩； 把解压缩后的视频数据播放出来； 对于单进程的实现方式，我想大家都会是以下这个方式： 对于单进程的这种方式，存在以下问题： 播放出来的画面和声音会不连贯，因为当 CPU 能力不够强的时候，Read 的时候可能进程就等在这了，这样就会导致等半天才进行数据解压和播放； 各个函数之间不是并发执行，影响资源的使用效率； 那改进成多进程的方式： 对于多进程的这种方式，依然会存在问题： 进程之间如何通信，共享数据？ 维护进程的系统开销较大，如创建进程时，分配资源、建立 PCB；终止进程时，回收资源、撤销 PCB；进程切换时，保存当前进程的状态信息； 那到底如何解决呢？需要有一种新的实体，满足以下特性： 实体之间可以并发运行； 实体之间共享相同的地址空间； 这个新的实体，就是线程( *Thread* )，线程之间可以并发运行且共享相同的地址空间。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:1","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"什么是线程？ 线程是进程当中的一条执行流程。 同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。 线程的优缺点？ 线程的优点： 一个进程中可以同时存在多个线程； 各个线程之间可以并发执行； 各个线程之间可以共享地址空间和文件等资源； 线程的缺点： 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃，具体分析原因可以看这篇：线程崩溃了，进程也会崩溃吗？ (opens new window)）。 举个例子，对于游戏的用户设计，则不应该使用多线程的方式，否则一个用户挂了，会影响其他同个进程的线程。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:2","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"线程与进程的比较 线程与进程的比较如下： 进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位； 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈； 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系； 线程能减少并发执行的时间和空间开销； 对于，线程相比进程能减少开销，体现在： 线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们； 线程的终止时间比进程快，因为线程释放的资源相比进程少很多； 同一个进程内的线程切换比进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的； 由于同一进程的各线程间共享内存和文件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了； 所以，不管是时间效率，还是空间效率线程比进程都要高。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:3","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"线程的上下文切换 在前面我们知道了，线程与进程最大的区别在于：线程是调度的基本单位，而进程则是资源拥有的基本单位。 所以，所谓操作系统的任务调度，实际上的调度对象是线程，而进程只是给线程提供了虚拟内存、全局变量等资源。 对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程； 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下文切换时是不需要修改的； 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 线程上下文切换的是什么？ 这还得看线程是不是属于同一个进程： 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样； 当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据； 所以，线程的上下文切换相比进程，开销要小很多。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:4","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"线程的实现 主要有三种线程的实现方式： 用户线程（*User Thread*）：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理； 内核线程（*Kernel Thread*）：在内核中实现的线程，是由内核管理的线程； 轻量级进程（*LightWeight Process*）：在内核中来支持用户线程； 那么，这还需要考虑一个问题，用户线程和内核线程的对应关系。 首先，第一种关系是多对一的关系，也就是多个用户线程对应同一个内核线程： 第二种是一对一的关系，也就是一个用户线程对应一个内核线程： 第三种是多对多的关系，也就是多个用户线程对应到多个内核线程： 用户线程如何理解？存在什么优势和缺陷？ 用户线程是基于用户态的线程管理库来实现的，那么线程控制块（*Thread Control Block, TCB*） 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。 所以，用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。 用户级线程的模型，也就类似前面提到的多对一的关系，即多个用户线程对应同一个内核线程，如下图所示： 用户线程的优点： 每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统； 用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快； 用户线程的缺点： 由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。 当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。 由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会比较慢； 以上，就是用户线程的优缺点了。 那内核线程如何理解？存在什么优势和缺陷？ 内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。 内核线程的模型，也就类似前面提到的一对一的关系，即一个用户线程对应一个内核线程，如下图所示： 内核线程的优点： 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行； 分配给线程，多线程的进程获得更多的 CPU 运行时间； 内核线程的缺点： 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB； 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大； 以上，就是内核线程的优缺点了。 最后的轻量级进程如何理解？ 轻量级进程（*Light-weight process，LWP*）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度。 在大多数系统中，LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息。一般来说，一个进程代表程序的一个实例，而 LWP 代表程序的执行线程，因为一个执行线程不像进程那样需要那么多状态信息，所以 LWP 也不带有这样的信息。 在 LWP 之上也是可以使用用户线程的，那么 LWP 与用户线程的对应关系就有三种： 1 : 1，即一个 LWP 对应 一个用户线程； N : 1，即一个 LWP 对应多个用户线程； M : N，即多个 LWP 对应多个用户线程； 接下来针对上面这三种对应关系说明它们优缺点。先看下图的 LWP 模型： 1 : 1 模式 一个线程对应到一个 LWP 再对应到一个内核线程，如上图的进程 4，属于此模型。 优点：实现并行，当一个 LWP 阻塞，不会影响其他 LWP； 缺点：每一个用户线程，就产生一个内核线程，创建线程的开销较大。 N : 1 模式 多个用户线程对应一个 LWP 再对应一个内核线程，如上图的进程 2，线程管理是在用户空间完成的，此模式中用户的线程对操作系统不可见。 优点：用户线程要开几个都没问题，且上下文切换发生用户空间，切换的效率较高； 缺点：一个用户线程如果阻塞了，则整个进程都将会阻塞，另外在多核 CPU 中，是没办法充分利用 CPU 的。 M : N 模式 根据前面的两个模型混搭一起，就形成 M:N 模型，该模型提供了两级控制，首先多个用户线程对应到多个 LWP，LWP 再一一对应到内核线程，如上图的进程 3。 优点：综合了前两种优点，大部分的线程上下文发生在用户空间，且多个线程又可以充分利用多核 CPU 的资源。 组合模式 如上图的进程 5，此进程结合 1:1 模型和 M:N 模型。开发人员可以针对不同的应用特点调节内核线程的数目来达到物理并行性和逻辑并行性的最佳方案。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:2:5","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"调度 进程都希望自己能够占用 CPU 进行工作，那么这涉及到前面说过的进程上下文切换。 一旦操作系统把进程切换到运行状态，也就意味着该进程占用着 CPU 在执行，但是当操作系统把进程切换到其他状态时，那就不能在 CPU 中执行了，于是操作系统会选择下一个要运行的进程。 选择一个进程运行这一功能是在操作系统中完成的，通常称为调度程序（scheduler）。 那到底什么时候调度进程，或以什么原则来调度进程呢？ TIP 我知道很多人会问，线程不是操作系统的调度单位吗？为什么这里参与调度的是进程？ 先提前说明，这里的进程指只有主线程的进程，所以调度主线程就等于调度了整个进程。 那为什么干脆不直接取名线程调度？主要是操作系统相关书籍，都是用进程调度这个名字，所以我也沿用了这个名字。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:3:0","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"调度时机 在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。 比如，以下状态的变化都会触发操作系统的调度： 从就绪态 -\u003e 运行态：当进程被创建时，会进入到就绪队列，操作系统会从就绪队列选择一个进程运行； 从运行态 -\u003e 阻塞态：当进程发生 I/O 事件而阻塞时，操作系统必须选择另外一个进程运行； 从运行态 -\u003e 结束态：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行； 因为，这些状态变化的时候，操作系统需要考虑是否要让新的进程给 CPU 运行，或者是否让当前进程从 CPU 上退出来而换另一个进程运行。 另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断 ，把调度算法分为两类： 非抢占式调度算法挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。 抢占式调度算法挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便把 CPU 控制返回给调度程序进行调度，也就是常说的时间片机制。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:3:1","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"调度原则 原则一：如果运行的程序，发生了 I/O 事件的请求，那 CPU 使用率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，为了提高 CPU 利用率，在这种发送 I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择一个进程来运行。 原则二：有的程序执行某个任务花费的时间会比较长，如果这个程序一直占用着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，要提高系统的吞吐率，调度程序要权衡长任务和短任务进程的运行完成数量。 原则三：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运行时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越小越好，如果进程的等待时间很长而运行时间很短，那周转时间就很长，这不是我们所期望的，调度程序应该避免这种情况发生。 原则四：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执行。所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。 原则五：对于鼠标、键盘这种交互式比较强的应用，我们当然希望它的响应时间越快越好，否则就会影响用户体验了。所以，对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则。 针对上面的五种调度原则，总结成如下： CPU 利用率：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率； 系统吞吐量：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量； 周转时间：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好； 等待时间：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意； 响应时间：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。 说白了，这么多调度原则，目的就是要使得进程要「快」。 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:3:2","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"调度算法 不同的调度算法适用的场景也是不同的。 接下来，说说在单核 CPU 系统中常见的调度算法。 01 先来先服务调度算法 最简单的一个调度算法，就是非抢占式的先来先服务（*First Come First Serve, FCFS*）算法了。 顾名思义，先来后到，每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。 这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。 FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。 02 最短作业优先调度算法 最短作业优先（*Shortest Job First, SJF*）调度算法同样也是顾名思义，它会优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。 这显然对长作业不利，很容易造成一种极端现象。 比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。 03 高响应比优先调度算法 前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。 那么，高响应比优先 （*Highest Response Ratio Next, HRRN*）调度算法主要是权衡了短作业和长作业。 每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，「响应比优先级」的计算公式： 从上面的公式，可以发现： 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行； 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会； TIP 很多人问怎么才能知道一个进程要求服务的时间？这不是不可预知的吗？ 对的，这是不可预估的。所以，高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的。 04 时间片轮转调度算法 最古老、最简单、最公平且使用最广的算法就是时间片轮转（*Round Robin, RR*）调度算法。 每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程； 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换； 另外，时间片的长度就是一个很关键的点： 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率； 如果设得太长又可能引起对短作业进程的响应时间变长。将 一般来说，时间片设为 20ms~50ms 通常是一个比较合理的折中值。 05 最高优先级调度算法 前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。 但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（*Highest Priority First，HPF*）调度算法。 进程的优先级可以分为，静态优先级和动态优先级： 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化； 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是随着时间的推移增加等待进程的优先级。 该算法也有两种处理优先级高的方法，非抢占式和抢占式： 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。 但是依然有缺点，可能会导致低优先级的进程永远不会运行。 06 多级反馈队列调度算法 多级反馈队列（*Multilevel Feedback Queue*）调度算法是「时间片轮转算法」和「最高优先级算法」的综合和发展。 顾名思义： 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列； 来看看，它是如何工作的： 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从高到低，同时优先级越高时间片越短； 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成； 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行； 可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也变更长了，所以该算法很好的兼顾了长短作业，同时有较好的响应时间。 看的迷迷糊糊？那我拿去银行办业务的例子，把上面的调度算法串起来，你还不懂，你锤我！ 办理业务的客户相当于进程，银行窗口工作人员相当于 CPU。 现在，假设这个银行只有一个窗口（单核 CPU ），那么工作人员一次只能处理一个业务。 那么最简单的处理方式，就是先来的先处理，后面来的就乖乖排队，这就是先来先服务（*FCFS*）调度算法。但是万一先来的这位老哥是来贷款的，这一谈就好几个小时，一直占用着窗口，这样后面的人只能干等，或许后面的人只是想简单的取个钱，几分钟就能搞定，却因为前面老哥办长业务而要等几个小时，你说气不气人？ 有客户抱怨了，那我们就要改进，我们干脆优先给那些几分钟就能搞定的人办理业务，这就是短作业优先（*SJF*）调度算法。听起来不错，但是依然还是有个极端情况，万一办理短业务的人非常的多，这会导致长业务的人一直得不到服务，万一这个长业务是个大客户，那不就捡了芝麻丢了西瓜 那就公平起见，现在窗口工作人员规定，每个人我只处理 10 分钟。如果 10 分钟之内处理完，就马上换下一个人。如果没处理完，依然换下一个人，但是客户自己得记住办理到哪个步骤了。这个也就是时间片轮转（*RR*）调度算法。但是如果时间片设置过短，那么就会造成大量的上下文切换，增大了系统开销。如果时间片过长，相当于退化成 FCFS 算法了。 既然公平也可能存在问题，那银行就对客户分等级，分为普通客户、VIP 客户、SVIP 客户。只要高优先级的客户一来，就第一时间处理这个客户，这就是最高优先级（*HPF*）调度算法。但依然也会有极端的问题，万一当天来的全是高级客户，那普通客户不是没有被服务的机会，不把普通客户当人是吗？那我们把优先级改成动态的，如果客户办理业务时间增加，则降低其优先级，如果客户等待时间增加，则升高其优先级。 那有没有兼顾到公平和效率的方式呢？这里介绍一种算法，考虑的还算充分的，多级反馈队列（*MFQ*）调度算法，它是时间片轮转算法和优先级算法的综合和发展。它的工作方式： 银行设置了多个排队（就绪）队列，每个队列都有不同的优先级，各个队列优先级从高到低，同时每个队列执行时间片的长度也不同，优先级越高的时间片越短。 新客户（进程）来了，先进入第一级队列的末尾，按先来先服务原则排队等待被叫号（运行）。如果时间片用完客户的业务还没办理完成，则让客户进入到下一级队列的末尾，以此类推，直至客户业务办理完成。 当第一级队列没人排队时，就会叫号二级队列的客户。如果客户办理业务过程中，有新的客户加入到较高优先级的队列，那么此时办理中的客户需要停止办理，回到原队列的末尾等待再次叫号，因为要把窗口让给刚进入较高优先级队列的客户。 可以发现，对于要办理短业务的客户来说，可以很快的轮到并解决。对于要办理长业务的客户，一下子解决不了，就可以放到下一个队列，虽然等待的时间稍微变长了，但是轮到自己的办理时间也变长了，也可以接受，不会造成极端的现象，可以说是综合上面几种算法的优点 ","date":"2023-05-27","objectID":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/:3:3","tags":["一、进程、线程基础知识"],"title":"一、进程、线程基础知识","uri":"/posts/%E4%B8%80%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"categories":["进程管理"],"content":"进程间通信方式","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"进程间有哪些通信方式？ 直接开讲！ 每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。 Linux 内核提供了不少进程间通信的机制，我们来一起瞧瞧有哪些？ ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:0:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#管道 如果你学过 Linux 命令，那你肯定很熟悉「|」这个竖线。 $ ps auxf | grep mysql 上面命令行里的「|」竖线就是一个管道，它的功能是将前一个命令（ps auxf）的输出，作为后一个命令（grep mysql）的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行。 同时，我们得知上面这种管道是没有名字，所以「|」表示的管道称为匿名管道，用完了就销毁。 管道还有另外一个类型是命名管道，也被叫做 FIFO，因为数据是先进先出的传输方式。 在使用命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字： $ mkfifo myPipe myPipe 就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用 ls 看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思： $ ls -l prw-r--r--. 1 root root 0 Jul 17 02:45 myPipe 接下来，我们往 myPipe 这个管道写入数据： $ echo \"hello\" \u003e myPipe // 将数据写进管道 // 停住了 ... 你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。 于是，我们执行另外一个命令来读取这个管道里的数据： $ cat \u003c myPipe // 读取管道里的数据 hello 可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。 我们可以看出，管道这种通信方式效率低，不适合进程间频繁地交换数据。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。 那管道如何创建呢，背后原理是什么？ 匿名管道的创建，需要通过下面这个系统调用： int pipe(int fd[2]) 这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 fd[0]，另一个是管道的写入端描述符 fd[1]。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。 其实，所谓的管道，就是内核里面的一串缓存。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。 看到这，你可能会有疑问了，这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？ 我们可以使用 fork 创建子进程，创建的子进程会复制父进程的文件描述符，这样就做到了两个进程各有两个「 fd[0] 与 fd[1]」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。 管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是： 父进程关闭读取的 fd[0]，只保留写入的 fd[1]； 子进程关闭写入的 fd[1]，只保留读取的 fd[0]； 所以说如果需要双向通信，则应该创建两个管道。 到这里，我们仅仅解析了使用管道进行父进程与子进程之间的通信，但是在我们 shell 里面并不是这样的。 在 shell 里面执行 A | B命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。 所以说，在 shell 里通过「|」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程，那么在我们编写 shell 脚本时，能使用一个管道搞定的事情，就不要多用一个管道，这样可以减少创建子进程的系统开销。 我们可以得知，对于匿名管道，它的通信范围是存在父子关系的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。 另外，对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。 不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循先进先出原则，不支持 lseek 之类的文件定位操作。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:1:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#消息队列 前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。 对于这个问题，消息队列的通信模式就可以解决。比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。 再来，消息队列是保存在内核中的消息链表，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。 消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。 消息这种模型，两个进程之间的通信就像平时发邮件一样，你来一封，我回一封，可以频繁沟通了。 但邮件的通信方式存在不足的地方有两点，一是通信不及时，二是附件也有大小限制，这同样也是消息队列通信不足的点。 消息队列不适合比较大数据的传输，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 MSGMAX 和 MSGMNB，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。 消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:2:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#共享内存 消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那共享内存的方式，就很好的解决了这一问题。 现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。 共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:3:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#信号量 用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。 为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信号量就实现了这一保护机制。 信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。 信号量表示资源的数量，控制信号量的方式有两种原子操作： 一个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 \u003c 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 \u003e= 0，则表明还有资源可使用，进程可正常继续执行。 另一个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 \u003c= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 \u003e 0，则表明当前没有阻塞中的进程； P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。 接下来，举个例子，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 1。 具体的过程如下： 进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。 可以发现，信号初始化为 1，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。 另外，在多进程里，每个进程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个进程能密切合作，以实现一个共同的任务。 例如，进程 A 是负责生产数据，而进程 B 是负责读取数据，这两个进程是相互合作、相互依赖的，进程 A 必须先生产了数据，进程 B 才能读取到数据，所以执行是有前后顺序的。 那么这时候，就可以用信号量来实现多进程同步的方式，我们可以初始化信号量为 0。 具体过程： 如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待； 接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B； 最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。 可以发现，信号初始化为 0，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执行。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:4:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#信号 上面说的进程间通信，都是常规状态下的工作模式。对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。 信号跟信号量虽然名字相似度 66.66%，但两者用途完全不一样，就好像 Java 和 JavaScript 的区别。 在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 kill -l 命令，查看所有的信号： $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如 Ctrl+C 产生 SIGINT 信号，表示终止该进程； Ctrl+Z 产生 SIGTSTP 信号，表示停止该进程，但还未结束； 如果进程在后台运行，可以通过 kill 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如： kill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，用来立即结束该进程； 所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。 信号是进程间通信机制中唯一的异步通信机制，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。 1.执行默认操作。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。 2.捕捉信号。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。 3.忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:5:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#Socket 前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。 实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。 我们来看看创建 socket 的系统调用： int socket(int domain, int type, int protocal) 三个参数分别代表： domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机； type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字； protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可； 根据创建 socket 类型的不同，通信的方式也就不同： 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM； 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM； 实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket； 接下来，简单说一下这三种通信的编程模式。 针对 TCP 协议通信的 socket 编程模型 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将绑定在 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务器端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。 所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。 成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 针对 UDP 协议通信的 socket 编程模型 UDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。 对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。 另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。 针对本地进程间通信的 socket 编程模型 本地 socket 被用于在同一台主机上进程间通信的场景： 本地 socket 的编程接口和 IPv4 、IPv6 套接字编程接口是一致的，可以支持「字节流」和「数据报」两种协议； 本地 socket 的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报 socket 实现； 对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。 对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是绑定一个本地文件，这也就是它们之间的最大区别。 ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:6:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["进程管理"],"content":"#总结 由于每个进程的用户空间都是独立的，不能相互访问，这时就需要借助内核空间来实现进程间通信，原因很简单，每个进程都是共享一个内核空间。 Linux 内核提供了不少进程间通信的方式，其中最简单的方式就是管道，管道分为「匿名管道」和「命名管道」。 匿名管道顾名思义，它没有名字标识，匿名管道是特殊文件只存在于内存，没有存在于文件系统中，shell 命令中的「|」竖线就是匿名管道，通信的数据是无格式的流并且大小受限，通信的方式是单向的，数据只能在一个方向上流动，如果要双向通信，需要创建两个管道，再来匿名管道是只能用于存在父子关系的进程间通信，匿名管道的生命周期随着进程创建而建立，随着进程终止而消失。 命名管道突破了匿名管道只能在亲缘关系进程间的通信限制，因为使用命名管道的前提，需要在文件系统创建一个类型为 p 的设备文件，那么毫无关系的进程就可以通过这个设备文件进行通信。另外，不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循先进先出原则，不支持 lseek 之类的文件定位操作。 消息队列克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型，发送数据时，会被分成一个一个独立的消息体，当然接收数据时，也要与发送方发送的消息体的数据类型保持一致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程。 共享内存可以解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，它直接分配一个共享空间，每个进程都可以直接访问，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，享有最快的进程间通信方式之名。但是便捷高效的共享内存通信，带来新的问题，多进程竞争同个共享资源会造成数据的错乱。 那么，就需要信号量来保护共享资源，以确保任何时刻只能有一个进程访问共享资源，这种方式就是互斥访问。信号量不仅可以实现访问的互斥性，还可以实现进程间的同步，信号量其实是一个计数器，表示的是资源个数，其值可以通过两个原子操作来控制，分别是 P 操作和 V 操作。 与信号量名字很相似的叫信号，它俩名字虽然相似，但功能一点儿都不一样。信号是异步通信机制，信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），一旦有信号发生，进程有三种方式响应信号 1. 执行默认操作、2. 捕捉信号、3. 忽略信号。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SIGSTOP，这是为了方便我们能在任何时候结束或停止某个进程。 前面说到的通信机制，都是工作于同一台主机，如果要与不同主机的进程间通信，那么就需要 Socket 通信了。Socket 实际上不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 TCP 协议的通信方式，一个是基于 UDP 协议的通信方式，一个是本地进程间通信方式。 以上，就是进程间通信的主要机制了。你可能会问了，那线程通信间的方式呢？ 同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，比如全局变量，所以对于线程间关注的不是通信方式，而是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步： 互斥的方式，可保证任意时刻只有一个线程访问共享资源； 同步的方式，可保证线程 A 应在线程 B 之前执行； ","date":"2023-05-27","objectID":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/:7:0","tags":["进程间通信方式"],"title":"三、进程间通信方式","uri":"/posts/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/"},{"categories":["MySQL基础篇"],"content":"二、MySQL一条记录怎么存储的？","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"MySQL 一行记录是怎么存储的？ 有人被问到这么个问题： 如果你知道 MySQL 一行记录的存储结构，那么这个问题对你没什么难度。 如果你不知道也没关系，这次我跟大家聊聊 MySQL 一行记录是怎么存储的？ 知道了这个之后，除了能应解锁前面这道面试题，你还会解锁这些面试题： MySQL 的 NULL 值会占用空间吗？ MySQL 怎么知道 varchar(n) 实际占用数据的大小？ varchar(n) 中 n 最大取值为多少？ 行溢出后，MySQL 是怎么处理的？ 这些问题看似毫不相干，其实都是在围绕「 MySQL 一行记录的存储结构」这一个知识点，所以攻破了这个知识点后，这些问题就引刃而解了。 好了，话不多说，发车！ ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:0:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#MySQL 的数据存放在哪个文件？ 大家都知道 MySQL 的数据都是保存在磁盘的，那具体是保存在哪个文件呢？ MySQL 存储的行为是由存储引擎实现的，MySQL 支持多种存储引擎，不同的存储引擎保存的文件自然也不同。 InnoDB 是我们常用的存储引擎，也是 MySQL 默认的存储引擎。所以，本文主要以 InnoDB 存储引擎展开讨论。 先来看看 MySQL 数据库的文件存放在哪个目录？ mysql\u003eSHOWVARIABLESLIKE'datadir';+---------------+-----------------+ |Variable_name|Value|+---------------+-----------------+ |datadir|/var/lib/mysql/|+---------------+-----------------+ 1rowinset(0.00sec)我们每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。 比如，我这里有一个名为 my_test 的 database，该 database 里有一张名为 t_order 数据库表。 然后，我们进入 /var/lib/mysql/my_test 目录，看看里面有什么文件？ [root@xiaolin ~]#ls /var/lib/mysql/my_test db.opt t_order.frm t_order.ibd 可以看到，共有三个文件，这三个文件分别代表着： db.opt，用来存储当前数据库的默认字符集和字符校验规则。 t_order.frm ，t_order 的表结构会保存在这个文件。在 MySQL 中建立一张表都会生成一个.frm 文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。 t_order.ibd，t_order 的表数据会保存在这个文件。表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）。这个行为是由参数 innodb_file_per_table 控制的，若设置了参数 innodb_file_per_table 为 1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从 MySQL 5.6.6 版本开始，它的默认值就是 1 了，因此从这个版本之后， MySQL 中每一张表的数据都存放在一个独立的 .ibd 文件。 好了，现在我们知道了一张数据库表的数据是保存在「 表名字.ibd 」的文件里的，这个文件也称为独占表空间文件。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:1:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#表空间文件的结构是怎么样的？ 表空间由段（segment）、区（extent）、页（page）、行（row）组成，InnoDB存储引擎的逻辑存储结构大致如下图： 下面我们从下往上一个个看看。 #1、行（row） 数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。 后面我们详细介绍 InnoDB 存储引擎的行格式，也是本文重点介绍的内容。 #2、页（page） 记录是按照行来存储的，但是数据库的读取并不以「行」为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。 因此，InnoDB 的数据是按「页」为单位来读写的，也就是说，当需要读一条记录的时候，并不是将这个行记录从磁盘读出来，而是以页为单位，将其整体读入内存。 默认每个页的大小为 16KB，也就是最多能保证 16KB 的连续存储空间。 页是 InnoDB 存储引擎磁盘管理的最小单元，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。 页的类型有很多，常见的有数据页、undo 日志页、溢出页等等。数据表中的行记录是用「数据页」来管理的，数据页的结构这里我就不讲细说了，之前文章有说过，感兴趣的可以去看这篇文章：换一个角度看 B+ 树(opens new window) 总之知道表中的记录存储在「数据页」里面就行。 #3、区（extent） 我们知道 InnoDB 存储引擎是用 B+ 树来组织数据的。 B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。 解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 了，那么在范围查询（扫描叶子节点）的时候性能就会很高。 那具体怎么解决呢？ 在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了。 #4、段（segment） 表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。 索引段：存放 B + 树的非叶子节点的区的集合； 数据段：存放 B + 树的叶子节点的区的集合； 回滚段：存放的是回滚数据的区的集合，之前讲事务隔离 (opens new window)的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。 好了，终于说完表空间的结构了。接下来，就具体讲一下 InnoDB 的行格式了。 之所以要绕一大圈才讲行记录的格式，主要是想让大家知道行记录是存储在哪个文件，以及行记录在这个表空间文件中的哪个区域，有一个从上往下切入的视角，这样理解起来不会觉得很抽象。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:1:1","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#InnoDB 行格式有哪些？ 行格式（row_format），就是一条记录的存储结构。 InnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式。 Redundant 是很古老的行格式了， MySQL 5.0 版本之前用的行格式，现在基本没人用了。 由于 Redundant 不是一种紧凑的行格式，所以 MySQL 5.0 之后引入了 Compact 行记录存储方式，Compact 是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的行记录，从 MySQL 5.1 版本之后，行格式默认设置成 Compact。 Dynamic 和 Compressed 两个都是紧凑的行格式，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西。从 MySQL5.7 版本之后，默认使用 Dynamic 行格式。 Redundant 行格式我这里就不讲了，因为现在基本没人用了，这次重点介绍 Compact 行格式，因为 Dynamic 和 Compressed 这两个行格式跟 Compact 非常像。 所以，弄懂了 Compact 行格式，之后你们在去了解其他行格式，很快也能看懂。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:2:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#COMPACT 行格式长什么样？ 先跟 Compact 行格式混个脸熟，它长这样： 可以看到，一条完整的记录分为「记录的额外信息」和「记录的真实数据」两个部分。 接下里，分别详细说下。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:3:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#记录的额外信息 记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。 #1. 变长字段长度列表 varchar(n) 和 char(n) 的区别是什么，相信大家都非常清楚，char 是定长的，varchar 是变长的，变长字段实际存储的数据的长度（大小）不固定的。 所以，在存储数据的时候，也要把数据占用的大小存起来，存到「变长字段长度列表」里面，读取数据的时候才能根据这个「变长字段长度列表」去读取对应长度的数据。其他 TEXT、BLOB 等变长字段也是这么实现的。 为了展示「变长字段长度列表」具体是怎么保存「变长字段的真实数据占用的字节数」，我们先创建这样一张表，字符集是 ascii（所以每一个字符占用的 1 字节），行格式是 Compact，t_user 表中 name 和 phone 字段都是变长字段： CREATETABLE`t_user`(`id`int(11)NOTNULL,`name`VARCHAR(20)DEFAULTNULL,`phone`VARCHAR(20)DEFAULTNULL,`age`int(11)DEFAULTNULL,PRIMARYKEY(`id`)USINGBTREE)ENGINE=InnoDBDEFAULTCHARACTERSET=asciiROW_FORMAT=COMPACT;现在 t_user 表里有这三条记录： 接下来，我们看看看看这三条记录的行格式中的 「变长字段长度列表」是怎样存储的。 先来看第一条记录： name 列的值为 a，真实数据占用的字节数是 1 字节，十六进制 0x01； phone 列的值为 123，真实数据占用的字节数是 3 字节，十六进制 0x03； age 列和 id 列不是变长字段，所以这里不用管。 这些变长字段的真实数据占用的字节数会按照列的顺序逆序存放（等下会说为什么要这么设计），所以「变长字段长度列表」里的内容是「 03 01」，而不是 「01 03」。 同样的道理，我们也可以得出第二条记录的行格式中，「变长字段长度列表」里的内容是「 04 02」，如下图： 第三条记录中 phone 列的值是 NULL，NULL 是不会存放在行格式中记录的真实数据部分里的，所以「变长字段长度列表」里不需要保存值为 NULL 的变长字段的长度。 为什么「变长字段长度列表」的信息要按照逆序存放？ 这个设计是有想法的，主要是因为「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。 「变长字段长度列表」中的信息之所以要逆序存放，是因为这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率。 同样的道理， NULL 值列表的信息也需要逆序存放。 如果你不知道什么是 CPU Cache，可以看这篇文章 (opens new window)，这属于计算机组成的知识。 每个数据库表的行格式都有「变长字段字节数列表」吗？ 其实变长字段字节数列表不是必须的。 当数据表没有变长字段的时候，比如全部都是 int 类型的字段，这时候表里的行格式就不会有「变长字段长度列表」了，因为没必要，不如去掉以节省空间。 所以「变长字段长度列表」只出现在数据表有变长字段的时候。 #2. NULL 值列表 表中的某些列可能会存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中会比较浪费空间，所以 Compact 行格式把这些值为 NULL 的列存储到 NULL值列表中。 如果存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。 二进制位的值为1时，代表该列的值为NULL。 二进制位的值为0时，代表该列的值不为NULL。 另外，NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 0。 还是以 t_user 表的这三条记录作为例子： 接下来，我们看看看看这三条记录的行格式中的 NULL 值列表是怎样存储的。 先来看第一条记录，第一条记录所有列都有值，不存在 NULL 值，所以用二进制来表示是酱紫的： 但是 InnoDB 是用整数字节的二进制位来表示 NULL 值列表的，现在不足 8 位，所以要在高位补 0，最终用二进制来表示是酱紫的： 所以，对于第一条数据，NULL 值列表用十六进制表示是 0x00。 接下来看第二条记录，第二条记录 age 列是 NULL 值，所以，对于第二条数据，NULL值列表用十六进制表示是 0x04。 最后第三条记录，第三条记录 phone 列 和 age 列是 NULL 值，所以，对于第三条数据，NULL 值列表用十六进制表示是 0x06。 我们把三条记录的 NULL 值列表都填充完毕后，它们的行格式是这样的： 每个数据库表的行格式都有「NULL 值列表」吗？ NULL 值列表也不是必须的。 当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。 所以在设计数据库表的时候，通常都是建议将字段设置为 NOT NULL，这样可以至少节省 1 字节的空间（NULL 值列表至少占用 1 字节空间）。 「NULL 值列表」是固定 1 字节空间吗？如果这样的话，一条记录有 9 个字段值都是 NULL，这时候怎么表示？ 「NULL 值列表」的空间不是固定 1 字节的。 当一条记录有 9 个字段值都是 NULL，那么就会创建 2 字节空间的「NULL 值列表」，以此类推。 #3. 记录头信息 记录头信息中包含的内容很多，我就不一一列举了，这里说几个比较重要的： delete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1。 next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。 record_type：表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:3:1","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#记录的真实数据 记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer，我们来看下这三个字段是什么。 row_id 如果我们建表的时候指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。 trx_id 事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。 roll_pointer 这条记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节。 如果你熟悉 MVCC 机制，你应该就清楚 trx_id 和 roll_pointer 的作用了，如果你还不知道 MVCC 机制，可以看完这篇文章 (opens new window)，一定要掌握，面试也很经常问 MVCC 是怎么实现的。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:3:2","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#varchar(n) 中 n 最大取值为多少？ 我们要清楚一点，MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。 也就是说，一行记录除了 TEXT、BLOBs 类型的列，限制最大为 65535 字节，注意是一行的总长度，不是一列。 知道了这个前提之后，我们再来看看这个问题：「varchar(n) 中 n 最大取值为多少？」 varchar(n) 字段类型的 n 代表的是最多存储的字符数量，并不是字节大小哦。 要算 varchar(n) 最大能允许存储的字节数，还要看数据库表的字符集，因为字符集代表着，1个字符要占用多少字节，比如 ascii 字符集， 1 个字符占用 1 字节，那么 varchar(100) 意味着最大能允许存储 100 字节的数据。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:4:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#单字段的情况 前面我们知道了，一行记录最大只能存储 65535 字节的数据。 那假设数据库表只有一个 varchar(n) 类型的列且字符集是 ascii，在这种情况下， varchar(n) 中 n 最大取值是 65535 吗？ 不着急说结论，我们先来做个实验验证一下。 我们定义一个 varchar(65535) 类型的字段，字符集为 ascii 的数据库表。 CREATETABLEtest(`name`VARCHAR(65535)NULL)ENGINE=InnoDBDEFAULTCHARACTERSET=asciiROW_FORMAT=COMPACT;看能不能成功创建一张表： 可以看到，创建失败了。 从报错信息就可以知道一行数据的最大字节数是 65535（不包含 TEXT、BLOBs 这种大对象类型），其中包含了 storage overhead。 问题来了，这个 storage overhead 是什么呢？其实就是「变长字段长度列表」和 「NULL 值列表」，也就是说一行数据的最大字节数 65535，其实是包含「变长字段长度列表」和 「NULL 值列表」所占用的字节数的。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去 storage overhead 占用的字节数。 这是因为我们存储字段类型为 varchar(n) 的数据时，其实分成了三个部分来存储： 真实数据 真实数据占用的字节数 NULL 标识，如果不允许为NULL，这部分不需要 本次案例中，「NULL 值列表」所占用的字节数是多少？ 前面我创建表的时候，字段是允许为 NULL 的，所以会用 1 字节来表示「NULL 值列表」。 本次案例中，「变长字段长度列表」所占用的字节数是多少？ 「变长字段长度列表」所占用的字节数 = 所有「变长字段长度」占用的字节数之和。 所以，我们要先知道每个变长字段的「变长字段长度」需要用多少字节表示？具体情况分为： 条件一：如果变长字段允许存储的最大字节数小于等于 255 字节，就会用 1 字节表示「变长字段长度」； 条件二：如果变长字段允许存储的最大字节数大于 255 字节，就会用 2 字节表示「变长字段长度」； 我们这里字段类型是 varchar(65535) ，字符集是 ascii，所以代表着变长字段允许存储的最大字节数是 65535，符合条件二，所以会用 2 字节来表示「变长字段长度」。 因为我们这个案例是只有 1 个变长字段，所以「变长字段长度列表」= 1 个「变长字段长度」占用的字节数，也就是 2 字节。 因为我们在算 varchar(n) 中 n 最大值时，需要减去 「变长字段长度列表」和 「NULL 值列表」所占用的字节数的。所以，在数据库表只有一个 varchar(n) 字段且字符集是 ascii 的情况下，varchar(n) 中 n 最大值 = 65535 - 2 - 1 = 65532。 我们先来测试看看 varchar(65533) 是否可行？ 可以看到，还是不行，接下来看看 varchar(65532) 是否可行？ 可以看到，创建成功了。说明我们的推论是正确的，在算 varchar(n) 中 n 最大值时，需要减去 「变长字段长度列表」和 「NULL 值列表」所占用的字节数的。 当然，我上面这个例子是针对字符集为 ascii 情况，如果采用的是 UTF-8，varchar(n) 最多能存储的数据计算方式就不一样了： 在 UTF-8 字符集下，一个字符最多需要三个字节，varchar(n) 的 n 最大取值就是 65532/3 = 21844。 上面所说的只是针对于一个字段的计算方式。 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:4:1","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#多字段的情况 如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 \u003c= 65535。 这里举个多字段的情况的例子（感谢@Emoji同学提供的例子） ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:4:2","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#行溢出后，MySQL 是怎么处理的？ MySQL 中磁盘和内存交互的基本单位是页，一个页的大小一般是 16KB，也就是 16384字节，而一个 varchar(n) 类型的列最多可以存储 65532字节，一些大对象如 TEXT、BLOB 可能存储更多的数据，这时一个页可能就存不了一条记录。这个时候就会发生行溢出，多的数据就会存到另外的「溢出页」中。 如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。在一般情况下，InnoDB 的数据都是存放在 「数据页」中。但是当发生行溢出时，溢出的数据会存放到「溢出页」中。 当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。大致如下图所示。 上面这个是 Compact 行格式在发生行溢出后的处理。 Compressed 和 Dynamic 这两个行格式和 Compact 非常类似，主要的区别在于处理行溢出数据时有些区别。 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中，看起来就像下面这样： ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:5:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["MySQL基础篇"],"content":"#总结 MySQL 的 NULL 值是怎么存放的？ MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。 NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。 MySQL 怎么知道 varchar(n) 实际占用数据的大小？ MySQL 的 Compact 行格式中会用「变长字段长度列表」存储变长字段实际占用的数据大小。 varchar(n) 中 n 最大取值为多少？ 一行记录最大能存储 65535 字节的数据，但是这个是包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。 如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。 计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。 如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 \u003c= 65535。 行溢出后，MySQL 是怎么处理的？ 如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。 Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。 Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。 参考资料： 《MySQL 是怎样运行的》 《MySQL技术内幕 InnoDB存储引擎》 ","date":"2023-05-27","objectID":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/:6:0","tags":["二、MySQL一条记录怎么存储的？"],"title":"二、MySQL一条记录怎么存储的？","uri":"/posts/%E4%BA%8Cmysql%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%E7%9A%84/"},{"categories":["进程管理"],"content":"多线程冲突了怎么办","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"多线程冲突了怎么办？ 先来看看虚构的小故事 已经晚上 11 点了，程序员小明的双手还在键盘上飞舞着，眼神依然注视着的电脑屏幕。 没办法这段时间公司业绩增长中，需求自然也多了起来，加班自然也少不了。 天气变化莫测，这时窗外下起了蓬勃大雨，同时闪电轰鸣。 但这一丝都没有影响到小明，始料未及，突然一道巨大的雷一闪而过，办公楼就这么停电了，随后整栋楼都在回荡着的小明那一声撕心裂肺的「卧槽」。 此时，求小明的心里面积有多大？ 等小明心里平复后，突然肚子非常的痛，想上厕所，小明心想肯定是晚上吃的某堡王有问题。 整栋楼都停了电，小明两眼一抹黑，啥都看不见，只能靠摸墙的方法，一步一步的来到了厕所门口。 到了厕所（共享资源），由于实在太急，小明直接冲入了厕所里，用手摸索着刚好第一个门没锁门，便夺门而入。 这就荒唐了，这个门里面正好小红在上着厕所，正好这个厕所门是坏了的，没办法锁门。 黑暗中，小红虽然看不见，但靠着声音，发现自己面前的这扇门有动静，觉得不对劲，于是铆足了力气，用她穿着高跟鞋脚，用力地一脚踢了过去。 小明很幸运，被踢中了「命根子」，撕心裂肺地喊出了一个字「痛」！ 故事说完了，扯了那么多，实际上是为了说明，对于共享资源，如果没有上锁，在多线程的环境里，那么就可能会发生翻车现场。 接下来，用 30+ 张图，带大家走进操作系统中避免多线程资源竞争的互斥、同步的方法。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:0:0","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#竞争与协作 在单核 CPU 系统里，为了实现多个程序同时运行的假象，操作系统通常以时间片调度的方式，让每个进程执行每次执行一个时间片，时间片用完了，就切换下一个进程运行，由于这个时间片的时间很短，于是就造成了「并发」的现象。 另外，操作系统也为每个进程创建巨大、私有的虚拟内存的假象，这种地址空间的抽象让每个程序好像拥有自己的内存，而实际上操作系统在背后秘密地让多个地址空间「复用」物理内存或者磁盘。 如果一个程序只有一个执行流程，也代表它是单线程的。当然一个程序可以有多个执行流程，也就是所谓的多线程程序，线程是调度的基本单位，进程则是资源分配的基本单位。 所以，线程之间是可以共享进程的资源，比如代码段、堆空间、数据段、打开的文件等资源，但每个线程都有自己独立的栈空间。 那么问题就来了，多个线程如果竞争共享资源，如果不采取有效的措施，则会造成共享数据的混乱。 我们做个小实验，创建两个线程，它们分别对共享变量 i 自增 1 执行 10000 次，如下代码（虽然说是 C++ 代码，但是没学过 C++ 的同学也是看到懂的）： 按理来说，i 变量最后的值应该是 20000，但很不幸，并不是如此。我们对上面的程序执行一下： 运行了两次，发现出现了 i 值的结果是 15173，也会出现 20000 的 i 值结果。 每次运行不但会产生错误，而且得到不同的结果。在计算机里是不能容忍的，虽然是小概率出现的错误，但是小概率事件它一定是会发生的，「墨菲定律」大家都懂吧。 为什么会发生这种情况？ 为了理解为什么会发生这种情况，我们必须了解编译器为更新计数器 i 变量生成的代码序列，也就是要了解汇编指令的执行顺序。 在这个例子中，我们只是想给 i 加上数字 1，那么它对应的汇编指令执行过程是这样的： 可以发现，只是单纯给 i 加上数字 1，在 CPU 运行的时候，实际上要执行 3 条指令。 设想我们的线程 1 进入这个代码区域，它将 i 的值（假设此时是 50 ）从内存加载到它的寄存器中，然后它向寄存器加 1，此时在寄存器中的 i 值是 51。 现在，一件不幸的事情发生了：时钟中断发生。因此，操作系统将当前正在运行的线程的状态保存到线程的线程控制块 TCB。 现在更糟的事情发生了，线程 2 被调度运行，并进入同一段代码。它也执行了第一条指令，从内存获取 i 值并将其放入到寄存器中，此时内存中 i 的值仍为 50，因此线程 2 寄存器中的 i 值也是 50。假设线程 2 执行接下来的两条指令，将寄存器中的 i 值 + 1，然后将寄存器中的 i 值保存到内存中，于是此时全局变量 i 值是 51。 最后，又发生一次上下文切换，线程 1 恢复执行。还记得它已经执行了两条汇编指令，现在准备执行最后一条指令。回忆一下， 线程 1 寄存器中的 i 值是51，因此，执行最后一条指令后，将值保存到内存，全局变量 i 的值再次被设置为 51。 简单来说，增加 i （值为 50 ）的代码被运行两次，按理来说，最后的 i 值应该是 52，但是由于不可控的调度，导致最后 i 值却是 51。 针对上面线程 1 和线程 2 的执行过程，我画了一张流程图，会更明确一些： ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:1:0","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#互斥的概念 上面展示的情况称为竞争条件（*race condition*），当多线程相互竞争操作共享变量时，由于运气不好，即在执行过程中发生了上下文切换，我们得到了错误的结果，事实上，每次运行都可能得到不同的结果，因此输出的结果存在不确定性（*indeterminate*）。 由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为临界区（*critical section*），它是访问共享资源的代码片段，一定不能给多线程同时执行。 我们希望这段代码是互斥（*mutualexclusion*）的，也就说保证一个线程在临界区执行时，其他线程应该被阻止进入临界区，说白了，就是这段代码执行过程中，最多只能出现一个线程。 另外，说一下互斥也并不是只针对多线程。在多进程竞争共享资源的时候，也同样是可以使用互斥的方式来避免资源竞争造成的资源混乱。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:1:1","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#同步的概念 互斥解决了并发进程/线程对临界区的使用问题。这种基于临界区控制的交互作用是比较简单的，只要一个进程/线程进入了临界区，其他试图想进入临界区的进程/线程都会被阻塞着，直到第一个进程/线程离开了临界区。 我们都知道在多线程里，每个线程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个线程能密切合作，以实现一个共同的任务。 例子，线程 1 是负责读入数据的，而线程 2 是负责处理数据的，这两个线程是相互合作、相互依赖的。线程 2 在没有收到线程 1 的唤醒通知时，就会一直阻塞等待，当线程 1 读完数据需要把数据传给线程 2 时，线程 1 会唤醒线程 2，并把数据交给线程 2 处理。 所谓同步，就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。 举个生活的同步例子，你肚子饿了想要吃饭，你叫妈妈早点做菜，妈妈听到后就开始做菜，但是在妈妈没有做完饭之前，你必须阻塞等待，等妈妈做完饭后，自然会通知你，接着你吃饭的事情就可以进行了。 注意，同步与互斥是两种不同的概念： 同步就好比：「操作 A 应在操作 B 之前执行」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等； 互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」； ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:1:2","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#互斥与同步的实现和使用 在进程/线程并发执行的过程中，进程/线程之间存在协作的关系，例如有互斥、同步的关系。 为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种： 锁：加锁、解锁操作； 信号量：P、V 操作； 这两个都可以方便地实现进程/线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程/线程同步。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:2:0","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#锁 使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。 任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。 根据锁的实现不同，可以分为「忙等待锁」和「无忙等待锁」。 我们先来看看「忙等待锁」的实现 在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊原子操作指令 —— 测试和置位（*Test-and-Set*）指令。 如果用 C 代码表示 Test-and-Set 指令，形式如下： 测试并设置指令做了下述事情: 把 old_ptr 更新为 new 的新值 返回 old_ptr 的旧值； 当然，关键是这些代码是原子执行。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作「测试并设置」。 那什么是原子操作呢？原子操作就是要么全部执行，要么都不执行，不能出现执行到一半的中间状态 我们可以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下： 我们来确保理解为什么这个锁能工作： 第一个场景是，首先假设一个线程在运行，调用 lock()，没有其他线程持有锁，所以 flag 是 0。当调用 TestAndSet(flag, 1) 方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为1，标志锁已经被持有。当线程离开临界区，调用 unlock() 将 flag 清理为 0。 第二种场景是，当某一个线程已经持有锁（即 flag 为1）。本线程调用 lock()，然后调用 TestAndSet(flag, 1)，这一次返回 1。只要另一个线程一直持有锁，TestAndSet() 会重复返回 1，本线程会一直忙等。当 flag 终于被改为 0，本线程会调用 TestAndSet()，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。 很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为自旋锁（*spin lock*）。 这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。 再来看看「无等待锁」的实现 无等待锁顾明思议就是获取不到锁的时候，不用自旋。 既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。 本次只是提出了两种简单锁的实现方式。当然，在具体操作系统实现中，会更复杂，但也离不开本例子两个基本元素。 如果你想要对锁的更进一步理解，推荐大家可以看《操作系统导论》第 28 章锁的内容，这本书在「微信读书」就可以免费看。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:2:1","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#信号量 信号量是操作系统提供的一种协调共享资源访问的方法。 通常信号量表示资源的数量，对应的变量是一个整型（sem）变量。 另外，还有两个原子操作的系统调用函数来控制信号量的，分别是： P 操作：将 sem 减 1，相减后，如果 sem \u003c 0，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞； V 操作：将 sem 加 1，相加后，如果 sem \u003c= 0，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞； TIP 很多人问，V 操作 中 sem \u003c= 0 的判断是不是写反了？ 没写反，我举个例子，如果 sem = 1，有三个线程进行了 P 操作： 第一个线程 P 操作后，sem = 0； 第二个线程 P 操作后，sem = -1； 第三个线程 P 操作后，sem = -2； 这时，第一个线程执行 V 操作后， sem 是 -1，因为 sem \u003c= 0，所以要唤醒第二或第三个线程。 P 操作是用在进入临界区之前，V 操作是用在离开临界区之后，这两个操作是必须成对出现的。 举个类比，2 个资源的信号量，相当于 2 条火车轨道，PV 操作如下图过程： 操作系统是如何实现 PV 操作的呢？ 信号量数据结构与 PV 操作的算法描述如下图： PV 操作的函数是由操作系统管理和实现的，所以操作系统已经使得执行 PV 函数时是具有原子性的。 PV 操作如何使用的呢？ 信号量不仅可以实现临界区的互斥访问控制，还可以线程间的事件同步。 我们先来说说如何使用信号量实现临界区的互斥访问。 为每类共享资源设置一个信号量 s，其初值为 1，表示该临界资源未被占用。 只要把进入临界区的操作置于 P(s) 和 V(s) 之间，即可实现进程/线程互斥： 此时，任何想进入临界区的线程，必先在互斥信号量上执行 P 操作，在完成对临界资源的访问后再执行 V 操作。由于互斥信号量的初始值为 1，故在第一个线程执行 P 操作后 s 值变为 0，表示临界资源为空闲，可分配给该线程，使之进入临界区。 若此时又有第二个线程想进入临界区，也应先执行 P 操作，结果使 s 变为负值，这就意味着临界资源已被占用，因此，第二个线程被阻塞。 并且，直到第一个线程执行 V 操作，释放临界资源而恢复 s 值为 0 后，才唤醒第二个线程，使之进入临界区，待它完成临界资源的访问后，又执行 V 操作，使 s 恢复到初始值 1。 对于两个并发线程，互斥信号量的值仅取 1、0 和 -1 三个值，分别表示： 如果互斥信号量为 1，表示没有线程进入临界区； 如果互斥信号量为 0，表示有一个线程进入临界区； 如果互斥信号量为 -1，表示一个线程进入临界区，另一个线程等待进入。 通过互斥信号量的方式，就能保证临界区任何时刻只有一个线程在执行，就达到了互斥的效果。 再来，我们说说如何使用信号量实现事件同步。 同步的方式是设置一个信号量，其初值为 0。 我们把前面的「吃饭-做饭」同步的例子，用代码的方式实现一下： 妈妈一开始询问儿子要不要做饭时，执行的是 P(s1) ，相当于询问儿子需不需要吃饭，由于 s1 初始值为 0，此时 s1 变成 -1，表明儿子不需要吃饭，所以妈妈线程就进入等待状态。 当儿子肚子饿时，执行了 V(s1)，使得 s1 信号量从 -1 变成 0，表明此时儿子需要吃饭了，于是就唤醒了阻塞中的妈妈线程，妈妈线程就开始做饭。 接着，儿子线程执行了 P(s2)，相当于询问妈妈饭做完了吗，由于 s2 初始值是 0，则此时 s2 变成 -1，说明妈妈还没做完饭，儿子线程就等待状态。 最后，妈妈终于做完饭了，于是执行 V(s2)，s2 信号量从 -1 变回了 0，于是就唤醒等待中的儿子线程，唤醒后，儿子线程就可以进行吃饭了。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:2:2","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#生产者-消费者问题 生产者-消费者问题描述： 生产者在生成数据后，放在一个缓冲区中； 消费者从缓冲区取出数据处理； 任何时刻，只能有一个生产者或消费者可以访问缓冲区； 我们对问题分析可以得出： 任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，需要互斥； 缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者需要同步。 那么我们需要三个信号量，分别是： 互斥信号量 mutex：用于互斥访问缓冲区，初始化值为 1； 资源信号量 fullBuffers：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）； 资源信号量 emptyBuffers：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）； 具体的实现代码： 如果消费者线程一开始执行 P(fullBuffers)，由于信号量 fullBuffers 初始值为 0，则此时 fullBuffers 的值从 0 变为 -1，说明缓冲区里没有数据，消费者只能等待。 接着，轮到生产者执行 P(emptyBuffers)，表示减少 1 个空槽，如果当前没有其他生产者线程在临界区执行代码，那么该生产者线程就可以把数据放到缓冲区，放完后，执行 V(fullBuffers) ，信号量 fullBuffers 从 -1 变成 0，表明有「消费者」线程正在阻塞等待数据，于是阻塞等待的消费者线程会被唤醒。 消费者线程被唤醒后，如果此时没有其他消费者线程在读数据，那么就可以直接进入临界区，从缓冲区读取数据。最后，离开临界区后，把空槽的个数 + 1。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:2:3","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#经典同步问题 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:3:0","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#哲学家就餐问题 当初我在校招的时候，面试官也问过「哲学家就餐」这道题目，我当时听的一脸懵逼，无论面试官怎么讲述这个问题，我也始终没听懂，就莫名其妙的说这个问题会「死锁」。 当然，我这回答槽透了，所以当场 game over，残酷又悲惨故事，就不多说了，反正当时菜就是菜。 时至今日，看我来图解这道题。 先来看看哲学家就餐的问题描述： 5 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面； 巧就巧在，这个桌子只有 5 支叉子，每两个哲学家之间放一支叉子； 哲学家围在一起先思考，思考中途饿了就会想进餐； 奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐； 吃完后，会把两支叉子放回原处，继续思考； 那么问题来了，如何保证哲 学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？ 方案一 我们用信号量的方式，也就是 PV 操作来尝试解决它，代码如下： 上面的程序，好似很自然。拿起叉子用 P 操作，代表有叉子就直接用，没有叉子时就等待其他哲学家放回叉子。 不过，这种解法存在一个极端的问题：假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 P(fork[(i + 1) % N ]) 这条语句阻塞了，很明显这发生了死锁的现象。 方案二 既然「方案一」会发生同时竞争左边叉子导致死锁的现象，那么我们就在拿叉子前，加个互斥信号量，代码如下： 上面程序中的互斥信号量的作用就在于，只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。 方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。 方案三 那既然方案二使用互斥信号量，会导致只能允许一个哲学家就餐，那么我们就不用它。 另外，方案一的问题在于，会出现所有哲学家同时拿左边刀叉的可能性，那我们就避免哲学家可以同时拿左边的刀叉，采用分支结构，根据哲学家的编号的不同，而采取不同的动作。 即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。 上面的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉子的顺序不同。另外，V 操作是不需要分支的，因为 V 操作是不会阻塞的。 方案三即不会出现死锁，也可以两人同时进餐。 方案四 在这里再提出另外一种可行的解决方案，我们用一个数组 state 来记录每一位哲学家的三个状态，分别是在进餐状态、思考状态、饥饿状态（正在试图拿叉子）。 那么，一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。 第 i 个哲学家的左邻右舍，则由宏 LEFT 和 RIGHT 定义： LEFT : ( i + 5 - 1 ) % 5 RIGHT : ( i + 1 ) % 5 比如 i 为 2，则 LEFT 为 1，RIGHT 为 3。 具体代码实现如下： 上面的程序使用了一个信号量数组，每个信号量对应一位哲学家，这样在所需的叉子被占用时，想进餐的哲学家就被阻塞。 注意，每个进程/线程将 smart_person 函数作为主代码运行，而其他 take_forks、put_forks 和 test 只是普通的函数，而非单独的进程/线程。 方案四同样不会出现死锁，也可以两人同时进餐。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:3:1","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["进程管理"],"content":"#读者-写者问题 前面的「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I/O 设备）一类的建模过程十分有用。 另外，还有个著名的问题是「读者-写者」，它为数据库访问建立了一个模型。 读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。 读者-写者的问题描述： 「读-读」允许：同一时刻，允许多个读者同时读 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写 「写-写」互斥：没有其他写者时，写者才能写 接下来，提出几个解决方案来分析分析。 方案一 使用信号量的方式来尝试解决： 信号量 wMutex：控制写操作的互斥信号量，初始值为 1 ； 读者计数 rCount：正在进行读操作的读者个数，初始化为 0； 信号量 rCountMutex：控制对 rCount 读者计数器的互斥修改，初始值为 1； 接下来看看代码的实现： 上面的这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。 方案二 那既然有读者优先策略，自然也有写者优先策略： 只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞； 如果有写者持续不断写入，则读者就处于饥饿； 在方案一的基础上新增如下变量： 信号量 rMutex：控制读者进入的互斥信号量，初始值为 1； 信号量 wDataMutex：控制写者写操作的互斥信号量，初始值为 1； 写者计数 wCount：记录写者数量，初始值为 0； 信号量 wCountMutex：控制 wCount 互斥修改，初始值为 1； 具体实现如下代码： 注意，这里 rMutex 的作用，开始有多个读者读数据，它们全部进入读者队列，此时来了一个写者，执行了 P(rMutex) 之后，后续的读者由于阻塞在 rMutex 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先。 同时，第一个写者执行了 P(rMutex) 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 V(wDataMutex) 唤醒写者的写操作。 方案三 既然读者优先策略和写者优先策略都会造成饥饿的现象，那么我们就来实现一下公平策略。 公平策略： 优先级相同； 写者、读者互斥访问； 只能一个写者访问临界区； 可以有多个读者同时访问临界资源； 具体代码实现： 看完代码不知你是否有这样的疑问，为什么加了一个信号量 flag，就实现了公平竞争？ 对比方案一的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进入读者队列， 而写者必须等待，直到没有读者到达。 没有读者到达会导致读者队列为空，即 rCount==0，此时写者才可以进入临界区执行写操作。 而这里 flag 的作用就是阻止读者的这种特殊权限（特殊权限是只要读者到达，就可以进入读者队列）。 比如：开始来了一些读者读数据，它们全部进入读者队列，此时来了一个写者，执行 P(falg) 操作，使得后续到来的读者都阻塞在 flag 上，不能进入读者队列，这会使得读者队列逐渐为空，即 rCount 减为 0。 这个写者也不能立马开始写（因为此时读者队列不为空），会阻塞在信号量 wDataMutex 上，读者队列中的读者全部读取结束后，最后一个读者进程执行 V(wDataMutex)，唤醒刚才的写者，写者则继续开始进行写操作。 ","date":"2023-05-27","objectID":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/:3:2","tags":["多线程冲突了怎么办"],"title":"二、多线程冲突了怎么办","uri":"/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["CS144"],"content":"lab0-实现ByteStream","date":"2023-04-13","objectID":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/","tags":["lab0-实现ByteStream"],"title":"lab0-实现ByteStream","uri":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/"},{"categories":["CS144"],"content":"CS144 lab0 lab0具体的相关事宜可以查看博客:https://kiprey.github.io/2021/11/cs144-lab0/ 完整项目代码: CS144 ","date":"2023-04-13","objectID":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/:1:0","tags":["lab0-实现ByteStream"],"title":"lab0-实现ByteStream","uri":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/"},{"categories":["CS144"],"content":"缓冲区实现 固定大小 lab0要求实现的缓冲区就是固定大小的,具体要求是需要实现一个可读可写的缓冲区.我们先讲讲实现的逻辑,如下图: 由于大小固定,我们通过模运算得到每次需要读取或写入的实际位置,而 readIdx 和 writeIdx 则一直+1即可,每次在需要读取的时候判断当前可读区域的大小,我们始终只需要通过 readIdx 和 writeIdx 的相对位置得到刻度区域的大小,而 writeIdx 永远是大于 readIdx ,只有实际读写的时候通过模运算获得需要读写的具体位置,这样就可以避免 writeIdx 在具体逻辑中可能在 readIdx 之后的情况,readIdx 和 writeIdx 是直接存入的取模的结果那么就需要特殊处理可读区域为空以及可写区域为空两种情况(因为这个时候都满足 readIdx==writeIdx).所以我们是通过一直自增,存取的时候再取模的方式完美的避开了无法判空和判满的情况,每次只要计算出可读区域的大小 writeIdx - readIdx ,可写区域的大小就确定了 (capacity-可读区域大小),然后我们就只管根据 writeIdx 或者 readIdx 模运算得到起始位置,然后一直+1再取模得到具体的值即可完成对应操作. 并且这种方式实现的缓冲区, writeIdx 直接表示总共写入多少个字节,readIdx 表示总共读取多少个字节,正好就对应我们需要实现的 bytes_written 方法和 bytes_read 方法. 动态大小 当缓冲区需要动态大小的时候,上述策略将变得毫无意义,因为每次扩容的时候需要重新拷贝之前的数据,所以我们需要改变复用内存的模运算策略. 这种动态大小的缓冲区,一般都是在需要扩容的时候,把整体的可读区域重新迁移到最前面,再更新可写指针,这样我们就只需要拷贝部分数据了. 具体的应用有很多,比如muduo网络库种或go标准库种的buffer都是这样实现的. 我写的一个网络库 netpoll-cpp 也有类似的实现: netpoll-cpp/netpoll/util/message_buffer.h netpoll-cpp/netpoll/util/message_buffer.cc ","date":"2023-04-13","objectID":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/:1:1","tags":["lab0-实现ByteStream"],"title":"lab0-实现ByteStream","uri":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/"},{"categories":["CS144"],"content":"ByteStream实现 声明 前面对固定大小缓冲区的实现方式已经描述完毕,现在我们来看看需要实现的函数声明: class ByteStream { private: bool _error{}; //!\u003c Flag indicating that the stream suffered an error. bool _eof{}; size_t _readIdx{}; size_t _writeIdx{}; std::vector\u003cchar\u003e _buffer; public: explicit ByteStream(size_t capacity); size_t write(const std::string \u0026data); size_t remaining_capacity() const; void end_input(); //! Indicate that the stream suffered an error. void set_error() { _error = true; } std::string peek_output(const size_t len) const; //! Remove bytes from the buffer void pop_output(const size_t len); std::string read(const size_t len); //! \\returns `true` if the stream input has ended bool input_ended() const; //! \\returns `true` if the stream has suffered an error bool error() const { return _error; } //! \\returns the maximum amount that can currently be read from the stream size_t buffer_size() const; //! \\returns `true` if the buffer is empty bool buffer_empty() const; //! \\returns `true` if the output has reached the ending bool eof() const; size_t bytes_written() const; //! Total number of bytes popped size_t bytes_read() const; //!@} }; 实现 ByteStream::ByteStream(const size_t capacity) : _buffer(capacity, 0) {} size_t ByteStream::write(const string \u0026data) { assert(buffer_size() \u003c= _buffer.size()); if (input_ended()) return 0; size_t canWrite = _buffer.size() - buffer_size(); size_t realWrite = min(canWrite, data.size()); for (size_t i = 0; i \u003c realWrite; i++) { _buffer[_writeIdx++ % _buffer.size()] = data[i]; } return realWrite; } //! \\param[in] len bytes will be copied from the output side of the buffer string ByteStream::peek_output(const size_t len) const { assert(buffer_size() \u003c= _buffer.size()); size_t canPeek = min(buffer_size(), len); string ret; for (size_t i = 0; i \u003c canPeek; i++) { ret += _buffer[(_readIdx + i) % _buffer.size()]; } return ret; } //! \\param[in] len bytes will be removed from the output side of the buffer void ByteStream::pop_output(const size_t len) { assert(buffer_size() \u003c= _buffer.size()); if (len \u003e buffer_size()) { set_error(); return; } _readIdx += len; } std::string ByteStream::read(const size_t len) { assert(buffer_size() \u003c= _buffer.size()); auto ret = peek_output(len); pop_output(len); if (_error) return {}; return ret; } void ByteStream::end_input() { _eof = true; } bool ByteStream::input_ended() const { return _eof; } size_t ByteStream::buffer_size() const { assert(_writeIdx \u003e= _readIdx); return _writeIdx - _readIdx; } bool ByteStream::buffer_empty() const { return _writeIdx == _readIdx; } bool ByteStream::eof() const { return _eof \u0026\u0026 buffer_empty(); } size_t ByteStream::bytes_written() const { return _writeIdx; } size_t ByteStream::bytes_read() const { return _readIdx; } size_t ByteStream::remaining_capacity() const { return _buffer.size() - buffer_size(); } ","date":"2023-04-13","objectID":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/:1:2","tags":["lab0-实现ByteStream"],"title":"lab0-实现ByteStream","uri":"/posts/lab0-%E5%AE%9E%E7%8E%B0bytestream/"},{"categories":["CS144"],"content":"lab1-实现StreamReassembler","date":"2023-04-13","objectID":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/","tags":["lab1-实现StreamReassembler"],"title":"lab1-实现StreamReassembler","uri":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/"},{"categories":["CS144"],"content":"CS144 lab1 lab1具体的相关事宜可以查看博客:https://kiprey.github.io/2021/11/cs144-lab1/ 完整项目代码: CS144 ","date":"2023-04-13","objectID":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/:1:0","tags":["lab1-实现StreamReassembler"],"title":"lab1-实现StreamReassembler","uri":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/"},{"categories":["CS144"],"content":"StreamReassembler实现 StreamReassembler 类需要完成对底层发送来的数据的顺序进行正确的重组,如下图: 具体来说你只需要实现下列的方法: class StreamReassembler { private: // Your code here -- add private members as necessary. size_t _unass_base; //!\u003c The index of the first unassembled byte size_t _unass_size; //!\u003c The number of bytes in the substrings stored but //!\u003c not yet reassembled bool _eof; //!\u003c The last byte has arrived std::string _buffer; //!\u003c The unassembled strings std::vector\u003cbool\u003e _bitmap; //!\u003c buffer bitmap ByteStream _output; //!\u003c The reassembled in-order byte stream size_t _capacity; //!\u003c The maximum number of bytes public: explicit StreamReassembler(size_t capacity); void push_substring(const std::string \u0026data, uint64_t index, bool eof); [[nodiscard]] const ByteStream \u0026stream_out() const { return _output; } ByteStream \u0026stream_out() { return _output; } size_t unassembled_bytes() const; bool empty() const; size_t ack_index() const; }; 方法有很多,具体最重要的方法是 push_substring(const string\u0026 data,uint64_t index,bool eof) ,该方法传入需要重排的数据,以及这个数据开始位置的绝对序号(index),还有表示是否是最后一块数据(eof). 我这里实现重排的逻辑如下: 首先需要一个 bitmap 记录每个序号的数据是否已经存入需要被重排的相应位置中,这个结构记录的数据作用有二: 对应序号的数据是否被加入重排队列. 已经重排的数据的下一个序号是否存在连续可重排数据. 第一个作用大家都能理解,那么第二个作用是什么意思呢? 每次把数据push进去之后,需要判断是否存在一个连续的需要重排的数据,且这个连续数据的第一个位置是已经重排数据的下一个位置,如果存在,那么需要写入已重排的缓冲区中. 整个数据重排的过程可以用下图进行表示: 具体实现 我们先设计如下成员变量: size_t _unass_base; //!\u003c The index of the first unassembled byte size_t _unass_size; //!\u003c The number of bytes in the substrings stored but //!\u003c not yet reassembled bool _eof; //!\u003c The last byte has arrived std::string _buffer; //!\u003c The unassembled strings std::vector\u003cbool\u003e _bitmap; //!\u003c buffer bitmap ByteStream _output; //!\u003c The reassembled in-order byte stream size_t _capacity; //!\u003c The maximum number of bytes _unass_base 变量用于表示当前已经重排过多少数据,该值可以结合用户传入的 index 算出用户的数据在待重排队列中的 offset . _unass_size 变量表示待重排数据的长度. _eof 表示传入的数据块是最后一个. _buffer 是待重排的队列. _output 是已重排的队列. _capacity 待重排和已重拍队列的最大容量,且待重排和已重拍(未读取)的数据不能超过这个数值. 对于每个数据的重排,分为下列三种情况: index-_unass_base\u003ecapacity 数据的标号顺序已经超过了当前重排器的最大范围.这个情况直接跳过暂时不进行重排. 在满足不超过capacity的情况下, index \u003e= _unass_base ,需要计算 offset 然后插入待重排队列. 在满足不超过capacity的情况下,index + len \u003e _unass_base ,同样计算offset插入队列,但这个offset的计算过程以及插入的位置均与第二种情况不同. 画图表示如下: 在执行完插入待重排队列的操作后,我们还需要检查是重排队列的队头是否存在待重排数据,如果存在,则把整个连续的部分都插入到已重排的队列中,当然整个插入的过程就好像溪水流入的过程一样,需要将这些连续的数据删除,并让后续的数据流到前面来,最后不够capacity的部分补0即可. 讲完整个实现逻辑,我们发现这两个队列非常适合用C++的 std::deque 容器来实现,它既可以按下标访问,又能够支持队列的所有操作. 但是我这里的实现并没有使用 deque 而是普通的 vector 和 string,我建议可以使用deque,网络上的几乎所有实现也都是用的deque. 实现源码如下: //! \\details This functions calls just after pushing a substring into the //! _output stream. It aims to check if there exists any contiguous substrings //! recorded earlier can be push into the stream. void StreamReassembler::check_contiguous() { size_t len = 0; if (_bitmap.front()) { while (len \u003c _capacity \u0026\u0026 _bitmap[len]) ++len; } if (len \u003e 0) { _unass_base += len; _unass_size -= len; _output.write(_buffer.substr(0, len)); // 把流往前推进 for (size_t i = 0; i \u003c _capacity; i++) { if (i + len \u003c _capacity) { _bitmap[i] = _bitmap[i + len]; _buffer[i] = _buffer[i + len]; } else { _bitmap[i] = false; } } } } //! \\details This function accepts a substring (aka a segment) of bytes, //! possibly out-of-order, from the logical stream, and assembles any newly //! contiguous substrings and writes them into the output stream in order. void StreamReassembler::push_substring(const string \u0026data, const size_t index, const bool eof) { if (eof) { _eof = true; } size_t len = data.length(); if (len == 0 \u0026\u0026 _eof \u0026\u0026 _unass_size == 0) { _output.end_input(); return; } // ignore invalid index if (index \u003e= _unass_base + _capacity) return; // 未重排数据的序号大于等于已经重排数据的下一个序号 if (index \u003e= _unass_base) { int offset = index - _unass_base; // 减去_output.buffer_size()是为了确保滑动窗口的大小(如果已经重排好的数据没有被读取,那么不一直接收更多的未重排数据) size_t real_len = min(len, _capacity - _output.buffer_size() - offset); if (real_len \u003c len) { _eof = false; } for (size_t i = 0; i \u003c real_len; i++) { if (_bitmap[i + offset]) continue; _buffer[i + offset] = data[i]; _bitmap[i + offset] = tru","date":"2023-04-13","objectID":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/:1:1","tags":["lab1-实现StreamReassembler"],"title":"lab1-实现StreamReassembler","uri":"/posts/lab1-%E5%AE%9E%E7%8E%B0streamreassembler/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"TcpConnectionImpl如何高效且统一处理IO事件","date":"2023-04-12","objectID":"/posts/tcpconnectionimpl%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E4%B8%94%E7%BB%9F%E4%B8%80%E5%A4%84%E7%90%86io%E4%BA%8B%E4%BB%B6/","tags":["TcpConnectionImpl如何高效且统一处理IO事件"],"title":"TcpConnectionImpl如何高效且统一处理IO事件","uri":"/posts/tcpconnectionimpl%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E4%B8%94%E7%BB%9F%E4%B8%80%E5%A4%84%E7%90%86io%E4%BA%8B%E4%BB%B6/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"实现高性能时间轮用于踢出空闲连接","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"完整代码实现： netpoll/net/inner/timing_wheel.h netpoll/net/inner/timing_wheel.cc ","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:0:0","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"实现契机 在网络框架的设计中,有一个环节是踢出空闲的连接,但是我觉得这个过程并不是一个很紧急的过程,有没有一种可以损失定时任务精度,但追求更小的时间消耗的方式呢? 我想到在原本的定时器上层封装一个时间轮,这样可以让那些不怎么重要的任务迅速添加,且即便在并发量很大的时候也能够防止过多的系统调用,因为你只需要和中间层打交道,并且时间轮本身的插入复杂度就是 O1 级别的. 这样每个线程维护一个每秒一转的时间轮来处理不重要的定时任务,可以减少整个系统在繁忙时不必要的开销. 如下图: ","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:1:0","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"框架设计 我们这里的时间轮由于是直接调用已经实现好的定时器来进行轮转，所以不需要考虑定时轮转的问题。只需要关注实现时间轮采取的数据结构。 可以先考虑一种最简单的时间轮实现：使用一个数组结构，然后保存一个下标用于存储每次轮询到的位置，每次触发轮询都只需要把下标+1即可，然后通过模运算得到数组下标获取对应需要执行的任务。 但这样实现有一个很明显的问题：如果需要支持较长的定时任务，需要大量的内存。 为了优化这个内存的问题,我们采取多个不同精度的时间轮同时推进,如下图: 我们假设从左往右的任务队列依次是 Q4/Q3/Q2/Q1 . 假设全局时间轮一共轮转了 112 次,我们可以把上述队列的各个位置看成是10进制数的各个位置,那么四个队列从左到右分别代表 0 1 1 2 这四个值,每个队列都是自己单独的每隔对应的 10^n 推进一次,而这个数字就记录了它们目前已经推进过多少次了(进位后不算),这个数值可以度量挨着的高位队列距离下次推进需要多久. 当我们需要往对应的队列中插入任务时,需要加上这个值(当前队列的前一个)作为初值. 对于上述描述,我们举一个例子比如当前时间轮计数器已经是 123 ,当前我需要添加一个 100s 后的任务,根据 123%10=3 可知还需要 7 次 Q1 的推进,Q2 才推进,以此类推 Q2 还需要推进 10-12%10=8 次,Q3 才推进一格……而我们目前需要添加一个延时 100s 的任务,首先计算需要在第一个队列添加到什么位置,如果任务需要的时间少于 10s ,那么只需要放入 Q1 中对应的位置即可,但是本例中需要添加 100s 后的任务,则优先往 Q2 添加任务,Q1 就只需要记录 Q2 无法表示的精度即可,计算在 Q2 中的位置首先需要 100+3=103 得到初值, 然后得到 103%10=3 是 Q2 不能表示的精度,也就是需要使用 Q1 来表示 3s 的精度,103/10=10 得到在 Q2 需要的精度,该数值正好小于等于 10 ,那么直接在 Q2 的相应位置插入任务即可. 最后我们可以验算一下,计数器是 123 的时候,添加100s 后的任务,我们在 Q1 添加了 3s 的任务,在 Q2 添加了 10*10-7=93s 的任务,请注意,我们对于这个 3s 的任务并不是立马就添加,而是在 Q2 中的任务执行结束后再添加,所以最终成功完成了延时 100s 后执行任务. 我们再次计算上述逻辑实现的时间轮最多可以添加多长的延时任务呢? $$ 10^310+10^210+10^110+10^010=11111s $$ 以上都是以 10 进制为基底的情况,实际的实现中,我默认是以 100 进制为基底,如果按照上述图中有 4 个队列,我算了算大概是可以表示 100000101s 大概是 3.17 年. 假设每个任务需要 32byte 内存,那么时间轮队列的内存消耗也只有 32*100*4=12800=12.5KB ,如果采用的是朴素的实现方式,最终需要的内存可能是 2.98GB ,这波优化确实是好很多. ","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:2:0","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"源码实现 ","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:3:0","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"任务队列实现 对于时间轮中队列的设计使用 std::deque ,队列中每个元素是一个 std::unordered_set ,这个集合中包含多个任务,每个任务是一个 void* 指针,使用shared_ptr 进行内存管理,具体每个任务的调用是通过将队列头部的智能指针给 pop 出去,然后尾部继续插入空的指针,如果被 pop 的智能指针对应的引用计数减少为0,那么就调用对应的析构,而析构函数才是真正需要调用的任务. 这种设计逻辑是为了简化对同一个任务的延时逻辑,如果之前已经添加的延时任务马上就要被调用了,但是我还是想重置这个延时,那么只要这个引用计数不为零,真正的任务都不会被调用.这样设计特别适合踢出空闲的 TCP 连接. using EntryPtr = std::shared_ptr\u003cvoid\u003e; using EntryBucket = std::unordered_set\u003cEntryPtr\u003e; using BucketQueue = std::deque\u003cEntryBucket\u003e; class CallbackEntry { public: explicit CallbackEntry(std::function\u003cvoid()\u003e cb) : m_cb(std::move(cb)) {} ~CallbackEntry() { m_cb(); } private: std::function\u003cvoid()\u003e m_cb; }; ","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:3:1","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["从零开始实现NIO高性能网络库"],"content":"时间轮逻辑实现 时间轮定时模块 时间轮的具体逻辑就是之前所提到的,但是在源码的具体实现中需要分离一些内容方便自定义配置. 比如下面的这几个变量: float m_ticksInterval; //时间轮每次轮转经历的时间,比如每秒转一次 size_t m_QueueNum; //一共有多少个任务队列,比如之前演示的时候就是4个 size_t m_bucketsNumPerQueue;//每个任务队列长度为多少,也就是之前所说的基底 默认情况下,m_ticksInterval 的值的 1.0s, m_bucketsNumPerQueue 的值是 100 ,而 m_QueueNum 则需要根据你最多需要多长的延时时间来确定. 下面是时间轮对应的构造函数, loop 表示该时间轮所用到的 loop (底层计时器), maxTimeout 是最大需要的延时时间. TimingWheel(EventLoop *loop, size_t maxTimeout, float ticksInterval = TIMING_TICK_INTERVAL, size_t bucketsNumPerQueue = TIMING_BUCKET_NUM_PER_WHEEL); 构造函数的具体实现如下: TimingWheel::TimingWheel(EventLoop* loop, size_t maxTimeout, float ticksInterval, size_t bucketsNumPerQueue) : m_loop(loop), m_ticksInterval(ticksInterval), m_bucketsNumPerQueue(bucketsNumPerQueue) { assert(maxTimeout \u003e 1); assert(ticksInterval \u003e 0); assert(m_bucketsNumPerQueue \u003e 1); auto maxTickNum = static_cast\u003csize_t\u003e(maxTimeout / ticksInterval); auto ticksNum = bucketsNumPerQueue; m_QueueNum = 1; // Find out how many task queue of different accuracy are needed. while (maxTickNum \u003e ticksNum) { ++m_QueueNum; ticksNum *= m_bucketsNumPerQueue; } m_taskQueues.resize(m_QueueNum); for (size_t i = 0; i \u003c m_QueueNum; ++i) { m_taskQueues[i].resize(m_bucketsNumPerQueue); } auto cb = [this](TimerId _) { ++m_ticksCounter; size_t t = m_ticksCounter; size_t pow = 1; // bucketsNumPerQueue is used as a base for counting. For example, in // base 100, suppose there are 4 task queues: Q1, Q2, Q3, and Q4. // Q1 is advanced once every revolution.Q2 is advanced once // every 100 revolutions. Q3 is 100^2 and 100^4. for (size_t i = 0; i \u003c m_QueueNum; ++i) { if ((t % pow) == 0) { EntryBucket tmp; { // use tmp val to make this critical area as short as // possible. m_taskQueues[i].front().swap(tmp); m_taskQueues[i].pop_front(); m_taskQueues[i].emplace_back(); } } pow = pow * m_bucketsNumPerQueue; } }; // Mark the lowest priority and rotate every m_ticksInterval. m_timerId = m_loop-\u003erunEvery(m_ticksInterval, cb, false, true); } 上述代码实现两个逻辑: 根据 maxTimeout 计算出 m_QueueNum ,并初始化对应的队列内存. 因为之前介绍过这个时间轮的实现逻辑,比如 10 为基底有4个队列,并且每次tick需要1s的时候,最多可以:10+10^2+10^3+10^4 . 这是最好的情况,最坏的情况是: 9+9*10^1+9*10^2+9*10^3+1 = 10^4 得出最多可以的延时时间和队列数量的关系是 base^num ,根据这个计算出num即可. 往 loop 中注册一个每隔 m_ticksInterval 时间后都执行的任务. 这个任务每次执行需要将时间轮的计时器+1,根据这个计数之前说过可以算出当前各个队列是否需要向前推进,而队列的向前推进过程就是把 front() 元素 swap 出来,然后 pop ,最后在 push_back 补齐长度. 时间轮的任务插入 虽然通过多个任务队列解决了时间轮的内存消耗问题,但是同样也增加了插入代码的书写复杂程度. 我们需要解决的问题有: 如何计算不同的耗时插入到哪个精度的任务队列? 如何让一个任务按照顺序插入不同精度的任务队列? 第一个问题很好理解,比如 101s 后的延时任务该怎么去插入到不同精度的队列,这个复杂的关键原因在于当你分成的不同精度之后,不同的任务队列当前执行到的位置是不同的,比如一个需要 100s 推进一次的任务队列,当你在时间轮开始轮转23s后插入这个延时101s的任务,那么这个精度为100s的任务队列距离下次任务推进的时间并不是100s而是 100-23 = 77s ,所以我们需要将原来的延时任务 +23s 才能平衡为完整的 100s ,这样就方便查找我需要插入的位置,比如 (101+23)/100=1 得出需要在该任务队列的第1个位置插入一个延时任务,通过 (101+23)%100=24 得出需要在前一个低精度的任务队列的第24个位置插入延时任务. 通过这种计算方式就解决了第一个问题. 但是我们不应该同时加入这两个任务,因为这两个任务的计时是并行的,所以无法进行时间的累加.为了解决这个问题,我们可以手动通过函数套娃封装去添加任务,比如上述有两个任务需要添加,我们先添加延时较长的那个任务,这个任务的内容就是添加后续较短的任务,以此类推.这样就保证了其实只添加了一个任务,当当这个任务被执行的时候再去添加下一个任务,到最后的截止时间才去调用真正的任务. 源代码如下: void TimingWheel::insertEntryInLoop(size_t delay, EntryPtr entryPtr) { m_loop-\u003eassertInLoopThread(); // If delay is not a multiple of the rotation interval, then the number of // rotations needs plus one. delay = static_cast\u003csize_t\u003e(delay / m_ticksInterval) + (delay % static_cast\u003csize_t\u003e(m_ticksInterval) ? 1 : 0); size_t t = m_ticksCounter; for (size_t i = 0; i \u003c m_QueueNum; ++i) { // The number of rotations required is less than or equal to the maximum // number of rotations of the TimingWheel with the current accuracy. if (delay \u003c= m_bucketsNumPerQueue) { m_taskQueues[i][delay - 1].insert(entryPtr); break; } if (i \u003c (m_QueueNum - 1)) { entryPtr = std::make_shared\u003cCallbackEntry\u003e([this, delay, i, t, entryPtr]() { if (delay \u003e 0) { m_taskQueues[i][(delay + (t % m_bucketsNumPerQueue) - 0) % m_bucketsNumPerQueue] .insert(entryPtr); } }); } else { // delay is too long to put entry at valid position in wheels. m_taskQueues[i][m_bucketsNumPerQueue - 1].insert(entryPtr); } delay = (delay + (t %","date":"2023-02-25","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/:3:2","tags":["实现高性能时间轮用于踢出空闲连接"],"title":"实现高性能时间轮用于踢出空闲连接","uri":"/posts/%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%94%A8%E4%BA%8E%E8%B8%A2%E5%87%BA%E7%A9%BA%E9%97%B2%E8%BF%9E%E6%8E%A5/"},{"categories":["个人项目"],"content":"elog4cpp官方文档","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"代码仓库：https://github.com/ACking-you/elog4cpp elog4cpp ：意味着这是一个使用上非常 easy，同时性能上也非常 efficiency c++ log 日志库。 支持c++11及以上，并且完全的跨平台。 使用 easy 体现在： api简单，你只需要关注一个 elog::Log 类，或者静态方法 Log::\u003cLEVEL\u003e ，又或是宏定义 ELG_\u003cLEVEL\u003e。 格式化输出简单，因为格式化输出使用的 fmt 库。 自定义格式化方式简单，支持自定义 formatter，而且已经预置四种 formatter，包括 defaultFormatter、colorfulFormatter、jsonFormatter、customFormatter。 配置简单，支持通过 json 文件一键读取配置项。 引入简单，支持 cmake 命令一键引入项目并使用。 性能 efficiency 体现在： 同步输出一条日志的延迟只需 180ns ，异步只需 120ns，是 spdlog 至少4倍的性能。 对于benchmark，可以参考tests/bench_start.cc ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:0:0","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"快速开始 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:1:0","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"要求 C++11及以上，是跨全平台的 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:1:1","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"安装与引入 推荐用以下两种方式进行引入： 方法一：通过cmake中的 FetchContent 模块引入： 在项目的cmake中添加下列代码进行引入，国内如果因为网络问题无法使用可以换这个gitee的镜像源：https://gitee.com/acking-you/elog4cpp.git include(FetchContent)FetchContent_Declare( elog4cpp GIT_REPOSITORY https://github.com/ACking-you/elog4cpp.git GIT_TAG v2.2 GIT_SHALLOW TRUE)FetchContent_MakeAvailable(elog4cpp) 在需要使用该库的目标中链接 elog 即可。 target_link_libraries(target elog) 方法二：手动下载源代码，然后通过cmake命令引入： 通过git命令下载项目源码 git clone https://github.com/ACking-you/elog4cpp.git 将该项目添加到子项目中： add_subdirectory(elog4cpp) 在需要使用该库的目标中链接 elog 即可。 target_link_libraries(target elog) ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:1:2","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"开始使用 在不经过任何配置的情况下，我们可以直接调用静态方法输出到终端，代码如下： #include \u003celog/logger.h\u003eusing namespace elog; int main() { Log::trace(\"hello elog4cpp\"); Log::debug(\"hello elog4cpp\"); Log::info(\"hello elog4cpp\"); Log::warn(\"hello elog4cpp\"); Log::error(\"hello elog4cpp\"); Log::fatal(\"hello elog4cpp\"); } 输出结果如下： 通过上述例子，我们需要明白以下三点： 本日志库的输出等级一共有 trace 、debug 、info 、warn 、error 、fatal 。 默认的输出最低输出等级为 debug ，也就是 trace 等级并不输出。 在进行 fatal 等级的输出时会抛出异常。 实际上在 error 或 fatal 等级输出时，如果 errno 存在错误，那么会输出对应的错误，这在进行系统编程的时候很有用。 前面的例子中，我们无法输出 trace 等级的日志，现在我们尝试着改变它的最低输出等级，然后将输出的内容增加更多的信息（比如文件名、行号、函数名等），并且将输出的内容以颜色高亮的形式输出。 代码如下： #include \u003celog/logger.h\u003eusing namespace elog; int main() { GlobalConfig::Get() .setLevel(Levels::kTrace) .setFormatter(formatter::colorfulFormatter); Log::trace(loc::current(),\"hello elog4cpp\"); Log::debug(loc::current(),\"hello elog4cpp\"); Log::info(loc::current(),\"hello elog4cpp\"); Log::warn(loc::current(),\"hello elog4cpp\"); Log::error(loc::current(),\"hello elog4cpp\"); } 输出效果如下： 从上面的示例代码中，我们发现，如果需要获得文件名等信息，需要在第一个参数中传入 loc::current() ，显然大多数时候我们会觉得这样使用起来会很麻烦，所以我们可以通过宏去解决这个问题，你可以像下面这样，在引入 \u003celog/logger.h\u003e 之前定义 ENABLE_ELG_LOG 宏来使用更简短的宏定义 ELG_\u003cLEVEL\u003e。 #define ENABLE_ELG_LOG #include \u003celog/logger.h\u003eusing namespace elog; int main() { GlobalConfig::Get() .setLevel(Levels::kTrace) .setFormatter(formatter::colorfulFormatter); ELG_TRACE(\"hello elog4cpp\"); ELG_DEBUG(\"hello elog4cpp\"); ELG_INFO(\"hello elog4cpp\"); ELG_WARN(\"hello elog4cpp\"); ELG_ERROR(\"hello elog4cpp\"); } 这个宏定义生成的代码与之前的示例中的代码等效。 之前的例子都只是输出到控制台，我们现在把内容输出到文件中去。 代码如下： #define ENABLE_ELG_LOG #include \u003celog/logger.h\u003eusing namespace elog; int main() { GlobalConfig::Get() .setFilepath(\"../log/\") .setLevel(Levels::kTrace) .setFormatter(formatter::colorfulFormatter); ELG_TRACE(\"hello elog4cpp\"); ELG_DEBUG(\"hello elog4cpp\"); ELG_INFO(\"hello elog4cpp\"); ELG_WARN(\"hello elog4cpp\"); ELG_ERROR(\"hello elog4cpp\"); } 上述代码的输出的结果既会输出到文件中也会输出到控制台中，setFilepath(\"../log/\") 指定了输出文件的文件夹为上一层级的 log 文件夹。注意这里传递的文件路径只能是输出文件的文件夹路径，也就是说文件输出只支持滚动日志。如果参数换为 \"../log\" 则表示输出文件夹路径为上级目录，且输出文件的名字前面都带有 log 。输出文件夹的命名格式为：.\u003cDATE TIME\u003e.\u003cUSERNAME\u003e.\u003cPID\u003e.log 。 如果需要禁用输出到控制台，则只需要添加下列配置：GlobalConfig::Get().enableConsole(false) 。同理如果不需要输出到文件，则需要保持 log_filepath 的值为默认值 nullptr 即可。 经过以上三次实践，大家应该对本库的基本使用已经有所了解，接下来如果需要详细了解对应的使用方式，则可以以继续深入了解如下内容： 如何配置 详细接口描述 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:1:3","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"如何配置 所有的配置都基于 Config 类或者 GlobalConfig 类。请注意这两个类的关系，Config 类作为 GlobalConfig 的基类，Config 中含有一些输出的通用配置，一般用于局部配置，GlobalConfig 中含有一些特殊的一次性配置，一般作为全局单例用于全局配置。 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:2:0","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"全局配置 如果没有设置局部配置，默认使用的都是全局配置。如果是使用静态方法或宏进行日志打印，那就只能使用通过全局配置进行配置。 配置方式 所有的全局配置都是通过一个全局的单例 GlobalConfig 来进行配置，可以调用 GlobalConfig::Get 来获取该单例，共有以下两种方式对该单例进行配置： 调用 GlobalConfig 的方法来配置，具体情况如下。 特有的方法如下： GlobalConfig::setRollSize(int size)：设置单个文件最大超过多大，就创建新的文件进行日志打印，以 mb 为基本单位。 GlobalConfig::setFlushInterval(int flushInterval)：设置每过多长时间进行一次刷新日志到磁盘，以秒为基本单位。 GlobalConfig::setFilepath(const char* basedir)：设置滚动日志的输出路径，注意传入的并不是单个文件路径，而是一个文件夹路径，本日志库只支持滚动日志。 GlobalConfig::enableConsole(bool s)：设置是否输出到控制台。 继承自 Config 的方法如下： Config::setFlag(Flags flag)：设置 flags ，该 flags 可以用于更精细的控制日志输出的内容，对于Flags有以下枚举。 kDate：是否输出日期。 kTime：是否输出时间。 kLongname：是否输出长文件名。 kShortname：是否输出短文件名。 kLine：是否输出行号。 kFuncName：是否输出函数名。 kThreadId：是否输出线程id。 kStdFlags：代表 kDate | kTime | kShortname | kLine | kFuncName ，里面的或运算代表开启对应的功能。 Config::setLevel(Levels level)：设置最低的日志输出等级。 Config::setName(const char* name)：设置日志器的名字，会在输出的时候添加该内容。 Config::setBefore(callback_t const\u0026 cb)：设置发生在格式化之前的回调函数。 Config::setAfter(callback_t const\u0026 cb)：设置发生在格式化之后的回调函数。 Config::setFormatter(formatter_t const\u0026 formatter)：设置格式化器，默认已经写好了如下格式化器： defaultFormatter ：默认的格式化器。 colorfulFormatter：在默认格式化器的基础上，在控制台台的输出中带上颜色。 jsonFormatter：以json格式进行输出。 customFromString(str) -\u003e formatter_t：这是一个可以你传入的字符串获取自定义的格式化器，具体的是使用方式请查看后续的描述。 通过传入json配置文件来配置，具体情况如下。 你除了调用对应的方法来进行配置以外，还可以通过外部的json文件进行配置，关键方法在于 loadFromJSON 和 loadToJSON ，分别用于从 json 文件中读取信息设置 GlobalConfig 的变量值和根据 GlobalConfig 变量值反过来生成对应的 json 文件。 具体的 json 配置文件如下，所有的使用方式均在 comments 中有说明： { \"comments\": [ \"下面的数值都是默认生成的注释，用于说明参数填写的注意事项\", \"name:可选参数，默认不填则日志输出无name\", \"roll_size:滚动日志的阈值，以mb为单位\", \"flush_interval:日志后台刷盘的时间，以秒为单位\", \"out_console:是否开启输出控制台，是bool值\", \"out_file:是否开启输出日志文件，不开启请使用null值，开启请用一个文件夹目录\", \"flag:用于开启日志对应输出的数据内容，有date,time,line,file,short_file,tid,func七种，可以通过+号来同时开启，当然也可直接使用default，它表示除tid以外的所有选项\", \"level:用于规定全局的最低输出等级，有trace,debug,info,warn,error,fatal,默认使用debug\", \"formatter:用于规定全局的日志格式化方式，有default,colorful,custom这三种，默认采取default，如果使用custom，则需要添加fmt_string\", \"fmt_string:仅当formatter选择custom后用于设定自定义的formatter，对应的数据表示如下：%T:time,%t:tid,%F:filepath,%f:func,%e:error info,%L:long levelText,%l:short levelText,%v:message ,%c color start %C color end\" ], \"elog\": { \"flag\": \"default\", \"flush_interval\": 3, \"formatter\": \"default\", \"level\": \"debug\", \"out_console\": true, \"out_file\": \"null\", \"roll_size\": 20 } } 使用示例 两个简单完整使用示例如下： 通过 GlobalConfig 的方法进行配置。 #define ENABLE_ELG_LOG #include \u003celog/logger.h\u003eusing namespace elog; int main() { GlobalConfig::Get() .setRollSize(4) .setFlushInterval(3) .setFilepath(\"../log/\") .enableConsole(true) .setFlag(kStdFlags + kThreadId) .setLevel(kTrace) .setName(\"elog\") .setBefore([](output_buf_t\u0026 buf) { buf.append(\"before\"); }) .setAfter([](output_buf_t\u0026 buf) { buf.append(\"after\"); }) .setFormatter(formatter::customFromString(\"%c[%L][%T][tid:%t][name:%n][file:%F][func:%f]:%v%C\")); ELG_TRACE(\"hello elog4cpp\"); ELG_DEBUG(\"hello elog4cpp\"); ELG_INFO(\"hello elog4cpp\"); ELG_WARN(\"hello elog4cpp\"); ELG_ERROR(\"hello elog4cpp\"); } 打印结果如下： 同理可以直接使用等效的 json 配置文件直接加载对应的配置项。 配置项如下： { \"elog\": { \"flag\": \"default+tid\", \"flush_interval\": 3, \"name\": \"elog\", \"formatter\": \"custom\", \"fmt_string\": \"%c[%L][%T][tid:%t][name:%n][file:%F][func:%f]:%v%C\", \"level\": \"trace\", \"out_console\": true, \"out_file\": \"../log/\", \"roll_size\": 4 } } 代码如下： #define ENABLE_ELG_LOG #include \u003celog/logger.h\u003eusing namespace elog; int main() { GlobalConfig::Get() .loadFromJSON(\"../config.json\") .setBefore([](output_buf_t\u0026 buf) { buf.append(\"before\"); }) .setAfter([](output_buf_t\u0026 buf) { buf.append(\"after\"); }); ELG_TRACE(\"hello elog4cpp\"); ELG_DEBUG(\"hello elog4cpp\"); ELG_INFO(\"hello elog4cpp\"); ELG_WARN(\"hello elog4cpp\"); ELG_ERROR(\"hello elog4cpp\"); } ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:2:1","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"局部配置 局部配置指的是可以通过单独创建一个类，来使用一个单独的 Config 配置。某些配置只提供了全局，具体有roolSize 、flushInterval、outConsole、outFile，因为后端负责输出的线程只能是一个单例，所以这些配置也只能配置一次。 关于如何使用局部配置的步骤如下： 创建 Config 结构体并初始化对应的配置。 创建 Log 对象并传入当前 日志输出的等级以及 Config 指针参数。 使用 Log 对象，调用它对应的 println 和 printf 方法进行打印。 示例代码如下： #include \u003celog/logger.h\u003e#include \u003cmemory\u003e#include \u003cvector\u003eusing namespace elog; void config_global() { GlobalConfig::Get() .loadFromJSON(\"../config.json\") .setBefore([](output_buf_t\u0026 buf) { buf.append(\"before\"); }) .setAfter([](output_buf_t\u0026 buf) { buf.append(\"after\"); }); } std::unique_ptr\u003cConfig\u003e make_config() { auto config = make_unique\u003cConfig\u003e(); config-\u003elog_formatter = formatter::colorfulFormatter; config-\u003elog_name = \"local_config\"; config-\u003elog_level = kTrace; config-\u003elog_flag = kStdFlags + kThreadId; config-\u003elog_before = [](output_buf_t\u0026 buf) { buf.append(\"before\"); }; config-\u003elog_after = [](output_buf_t\u0026 buf) { buf.append(\"after\"); }; return config; } int main() { config_global(); //创建Log对象，并设置对应的Config和level auto trace = Log(kTrace, make_config()); trace.printf(\"hello {}\", \"world\"); trace.println(\"hello \", std::vector\u003cint\u003e{1, 2, 32}); //改变日志输出等级 trace.set_level(kDebug); trace.printf(\"hello {}\", \"world\"); trace.println(\"hello \", std::vector\u003cint\u003e{1, 2, 32}); //移动构造到新对象 auto info = std::move(trace); info.set_level(kInfo); info.printf(\"hello {}\", \"world\"); info.println(\"hello \", std::vector\u003cint\u003e{1, 2, 32}); } 观察上述源码，我们发现 Log 对象是不可复制的，它只能移动，而且每个 Log 对象也只能独占一份 Config ，所以是完全线程安全的。 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:2:2","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"详细接口描述 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:3:0","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"Formatter 内置formatter 如果你看过前面的内容，那么对 formatter 的作用应该有了一定的了解，他是一个用于控制格式化输出的接口实现，本日志库内部已经实现的 formatter 有如下几种： defaultFormatter ：这是默认的formatter，格式固定。 jsonFormatter：以json格式输出，格式固定。 colorfulFormatter：输出的格式与formatter相同，但输出到控制台的时候有颜色高亮。 customFromString：可以根据用户传入的字符串自定义输出格式。 具体描述： %T：表示整个日期时间，还包括时区。 %t：表示线程id。 %F：表示该条日志输出来自哪个文件。 %f：表示该条日志输出来自哪个函数。 %e：表示如果 errno 存在错误则表示该错误信息，否则表示空。 %n：表示当前日志器的名字，如果不存在，则表示为空。 %L：表示长的代表日志等级的字符串，比如 TRACE 。 %l：表示短的代表日志等级的字符串，比如 TRC 。 %v：表示日志输出的内容。 %c和%C：表示颜色的开始与结束，只在支持 \\033 的终端中有效。 自定义formatter 既然想要 formatter ，那么就必须清楚 formatter 在本日志库中到底被设计成了什么。实际上 formatter 在本日志库中只是一个简单的回调函数，函数签名如下： using formatter_t = std::function\u003cvoid(Config* config, context const\u0026 ctx, buffer_t\u0026buf,Appenders apender_type)\u003e; 各个参数的含义如下： config：当前日志输出使用的配置。 ctx：当前日志输出的相关内容，包括需要输出的日志内容、日志等级和行号等等信息。 buf：当前日志最终格式化后需要输出到的 buffer 。 appder_type：这是一个枚举代表当前日志格式化输出的目的地，具体有 文件 或 控制台 两种。 根据上述解释，如果想要实现一个自己的 formatter ，就可以通过 config 的配置信息和 ctx 的输出信息以及 appender_type 的目的地信息来定制化的格式化输出内容到 buf 中。 有了上述理解，我们就能够通过观察原本已经实现的 formatter 来仿照的实现自己的 formatter 了，例如 defaultFormatter 的源码链接如下：src/formatter.cc ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:3:1","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"Micros ENABLE_ELG_LOG 为了使得文件位置信息的输出不需要手动的添加 loc::current() 这一参数，本日志库提供了 ELG_\u003cLEVEL\u003e 来简化这一过程，所以如果需要文件位置信息可以将 Log::\u003cLevel\u003e 替换为下面的宏： ELG_TRACE ELG_DEBUG ELG_INFO ELG_WARN ELG_ERROR ELG_FATAL 注意：使用上述宏之前，需要在 #include\u003celog/logger.h\u003e 之前定义 ENABLE_ELG_LOG 宏。 ENABLE_ELG_CHECK 通过定义 ENABLE_ELG_CHECK 宏，我们可以使用到如下的宏定义来更方便的检查值之间的关系： 断言宏，不满足条件，则抛出异常。 ELG_CHECK_EQ(a,b) 等价于 ELG_ASSERT_IF(a == b). ELG_CHECK_NQ(a,b) 等价于 ELG_ASSERT_IF(a != b). ELG_CHECK_GE(a,b) 等价于 ELG_ASSERT_IF(a \u003e b). ELG_CHECK_GT(a,b) 等价于 ELG_ASSERT_IF(a \u003e= b). ELG_CHECK_LE(a,b) 等价于 ELG_ASSERT_IF(a \u003c b). ELG_CHECK_LT(a,b) 等价于 ELG_ASSERT_IF(a \u003c= b). ELG_CHECK_NOTNULL(a) 等价于 ELG_ASSERT_IF(a != nullptr). 判断断言，自定义打印。 在传入判断条件后，会返回一个对象供你进行打印提示信息，可以选择不同的等级进行打印，如下面的例子是打印 trace 等级。 ELG_CHECK(1 == 2).trace(\"1 != 2!\"); ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:3:2","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"其他杂项 还有如下提供方便的函数： elog::Ptr：用于将任意指针强制转化为 void* ，这是为了方便直接打印指针的值。 elog::WaitForDone：等待后台线程将日志信息刷入磁盘，这在某些时候很有用。 ","date":"2023-02-13","objectID":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/:3:3","tags":["elog4cpp官方文档"],"title":"elog4cpp官方文档","uri":"/posts/elog4cpp%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"},{"categories":["开发环境配置"],"content":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"单元测试框架 google test是一个C++中常用且历史悠久的测试框架，其他类似且较新的测试框架有 catch2 或 doetest等，这两个测试框架的优势在于引入简单，是完全 head only 的，但是也正是因为 head only 导致编译速度很慢，当然 doctest 还是挺快的，但 catch2 真的编译太慢了。而 googletest 引入就需要我们自行编译了，当然用cmake的话是可以简化这个过程的，gtest 的使用和引入其实也很简单，由于是直接链接编译好的库，所以编译速度是比较快的（最近测试了下，同样以链接库的方式比 doctest 慢一些… ）。我现在更推荐使用 doctest 而不是 gtest 了。 本来是想讲 Google test 的使用的（看我开篇就知道），但是使用了 doctest 后，我现在完全放弃了 Googletest，对我而言有以下几点非常好： 文件轻量，非常轻量，就几个文件，而且代码量好像就7000行，编译速度奇快，而相对的 Googletest 里包含的东西有点多，比如 gmock，对比起来略显重量级。 CLion 对 doctest 的支持更好，每次我用 Googletest 的时候，CLion都需要重新我当前使用的测试框架，而使用doctest后，则完全没有这方面困扰，反应奇快，这也是轻量带来的好处。 api超级友好，用过就真的回不去。虽然断言宏不是很多，但核心观点是它分解了比较表达式，所以会比其他框架用起来方便太多。 功能丰富（比如支持对模板进行批量测试），尽管代码轻量，但是功能也毫不含糊，感觉比googletest更好用。 ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:1:0","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何引入 正如上述所说，doctest 是head-only的，所以仅仅只需要一个 .h 文件即可，但是我建议不要这样，这样编译速度会慢一些，建议使用编译库再链接的方式，这种方式在cmake里面也很简单，如果不懂cmake，可以看看我这期视频：cmake入门 。 你只需在cmake项目中添加下列代码： include(FetchContent)FetchContent_Declare( doctest GIT_REPOSITORY https://github.com/doctest/doctest.git GIT_TAG v2.4.9 GIT_SHALLOW TRUE )FetchContent_MakeAvailable(doctest)target_link_libraries(target doctest_with_main)这里的仓库链接由于是GitHub上，如果你不会科学上网的话，建议可以去手动下载GitHub上的代码然后 add_subdirectory() 也是一样的。当然也可以把对应的仓库在gitee上创建一个镜像，那么你就可以直接把上面的 GIT_REPOSITORY 换成你镜像的地址了，比如我拉了一个镜像地址如下：https://gitee.com/acking-you/doctest.git 替换即可。 ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:1:1","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何使用 开始前，你可以直接去看官方文档，写的也挺详细：官方文档 首先，我们要清楚，一个测试框架，你需要注意的就只有两点： 如何组织测试 -\u003e 测试宏 如何进行测试断言 -\u003e 断言宏 通过下面这个简单的测试进行一个简单的讲解： //这个宏如果是通过链接的方式引入库的话千万不要加，如果是通过直接的include头文件引入的则需要加入 //#define DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN #include \"doctest.h\" int factorial(int number) { return number \u003c= 1 ? number : factorial(number - 1) * number; } TEST_CASE(\"testing the factorial function\") { CHECK(factorial(1) == 1); CHECK(factorial(2) == 2); CHECK(factorial(3) == 6); CHECK(factorial(10) == 3628800); } 上述代码是在测试斐波那契数列的值。 通过 TEST_CASE 这个宏来组织一个测试，参数是该测试的名字是一个字符串值，在CLion中会以这个名字来标识这个测试。与 googletest 相比，对应的是 TEST 宏，但不同的是 googletest 需要传两个参数，两个都不是字符串，而且必须符合C++变量命名的字符规则，所以不能以空格或者其他非字母数字的任何符号放在其中，这点其实很不方便。 通过 CHECK 宏来进行断言判断，如果失败了CLion中会有对应的提示。参数是一个判断表达式，不要小看这个宏，它是默认支持几乎所有内置的类型，并且包括stl容器。对应的 googletest 一般使用 EXPECT_EQ() 传递两个参数来进行比较，默认不支持 const char* 类型，需要使用 EXPECT_STREQ ，而 doctest 则不需要有这方面的考虑，只需要关注这个 CHECK 宏即可，当然它也有对应的 CHECK_XX 宏。 测试相关 经过一个小demo的讲解，那么大家对于测试宏有了一定的了解，下面将继续介绍更多的测试宏。 SUBCASE 这个宏用于在TEST_CASE中继续产生更小的分组，然后你可以安全的捕获到外界的变量来使用。因为每个SUBCASE都是完全独立的重新执行，而不是在同一次执行，比如我将下面的代码块分为1、2、3，那么第一个SUBCASE的顺序将会是 1-\u003e2-\u003e结束 ，第二个SUBCASE的执行顺序将会是 1-\u003e3-\u003e结束 。如果最外层的代码在 SUBCASE 后面，那么不会被执行，所有的 SUBCASE 执行情况，我们可以看作是从一个树的根节点到子节点的简单遍历，但每次遍历没有前后文关系（也就是每次遍历都是重新执行的） TEST_CASE(\"vectors can be sized and resized\") { std::vector\u003cint\u003e v(5); //1 REQUIRE(v.size() == 5); REQUIRE(v.capacity() \u003e= 5); SUBCASE(\"adding to the vector increases it's size\") { //2 v.push_back(1); CHECK(v.size() == 6); CHECK(v.capacity() \u003e= 6); } SUBCASE(\"reserving increases just the capacity\") { //3 v.reserve(6); CHECK(v.size() == 5); CHECK(v.capacity() \u003e= 6); } } 例如下面这个例子将会输出： TEST_CASE(\"lots of nested subcases\") { cout \u003c\u003c endl \u003c\u003c \"root\" \u003c\u003c endl; SUBCASE(\"\") { cout \u003c\u003c \"1\" \u003c\u003c endl; SUBCASE(\"\") { cout \u003c\u003c \"1.1\" \u003c\u003c endl; } } SUBCASE(\"\") { cout \u003c\u003c \"2\" \u003c\u003c endl; SUBCASE(\"\") { cout \u003c\u003c \"2.1\" \u003c\u003c endl; } SUBCASE(\"\") { cout \u003c\u003c \"2.2\" \u003c\u003c endl; SUBCASE(\"\") { cout \u003c\u003c \"2.2.1\" \u003c\u003c endl; SUBCASE(\"\") { cout \u003c\u003c \"2.2.1.1\" \u003c\u003c endl; } SUBCASE(\"\") { cout \u003c\u003c \"2.2.1.2\" \u003c\u003c endl; } } } SUBCASE(\"\") { cout \u003c\u003c \"2.3\" \u003c\u003c endl; } SUBCASE(\"\") { cout \u003c\u003c \"2.4\" \u003c\u003c endl; } } } TEST_SUITE test suite表示测试集，顾名思义，就是可以把 test case 分组。 比如可以这样写： TEST_SUITE(\"math\") { TEST_CASE(\"\") {} // part of the math test suite TEST_CASE(\"\") {} // part of the math test suite } 也可以分开用 TEST_SUITE_BEGIN 和 TEST_SUITE_END 宏来实现： TEST_SUITE_BEGIN(\"utils\"); TEST_CASE(\"\") {} // part of the utils test suite TEST_SUITE_END(); TEST_CASE(\"\") {} // not part of any test suite 分组后的好处当然是可以直接分组执行了。 TEST_CASE_FIXTURE 这个宏是用来直接测试某个类的方法的，相当于是通过继承的方式创建了一个新的类，所以 protect 修饰的东西都能访问，比如： class UniqueTestsFixture { private: static int uniqueID; protected: int conn; public: UniqueTestsFixture() : conn(10) { } protected: static int getID() { return ++uniqueID; } }; int UniqueTestsFixture::uniqueID = 0; TEST_CASE_FIXTURE(UniqueTestsFixture, \"test get ID\") { REQUIRE(getID() == conn); } TEST_CASE_TEMPLATE 这个宏是用来测试模板的，如果需要测试的模板功能有共通性，只是类型不一致，那么你可以减少重复劳动，直接用这个宏来帮忙实例化再测试。 比如下列代码测试了 std::any 对于接收字符串类型和整数类型的情况测试： TEST_CASE_TEMPLATE(\"test std::any as integer\", T, char, short, int, long long int) { auto v = T(); std::any var = T(); CHECK(std::any_cast\u003cT\u003e(var)==v); } TEST_CASE_TEMPLATE(\"test std::any as string\", T, const char*, std::string_view, std::string) { T v = \"hello world\"; std::any var = v; CHECK(std::any_cast\u003cT\u003e(var)==v); } 也可用 TEST_CASE_TEMPLATE_DEFINE 先定义一个模板测试，后面再用 TEST_CASE_TEMPLATE_INVOKE 来决定实例化模板的类型： TEST_CASE_TEMPLATE_DEFINE(\"test std::any as integer\", T,integer) { auto v = T(); std::any var = T(); CHECK(std::any_cast\u003cT\u003e(var)==v); } TEST_CASE_TEMPLATE_DEFINE(\"test std::any as string\", T,string) { T v = \"hello world\"; std::any var = v; CHECK(std::any_cast\u003cT\u003e(var)==v); } TEST_CASE_TEMPLATE_INVOKE(integer, char, short, int, long long int); TEST_CASE_TEMPLATE_INVOKE(string, const char*, std::string_view, std::string); 断言相关 doctest的断言宏是很有规律的，它的设计我之前也提到过，是一种尽量以表达式的方式去简化对api的记忆，你只需要清楚三个断言的等级即可，当然如果想要直接通过对应的类似于 gtest 的 EXPECT_XXX 之类的api来进行断言，实际上也是有的。 断言宏一共有以下三个等级： REQUIRE：这个等级算是最高的，如果断言失败，不仅会标记为测试不通过，而且会强制退出测试（也就是后续的测试将不","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:1:2","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"benchmark框架 关于benchmark，我建议使用 nanobench ，同样也是因为引入简单轻量，使用简单且 head only 。 官方文档如下：https://nanobench.ankerl.com/tutorial.html#usage ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:2:0","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何引入 其实官方文档已经介绍了如何引入，它也是推荐使用下面的方式进行引入： cmake_minimum_required(VERSION 3.14)set(CMAKE_CXX_STANDARD 17)project( CMakeNanobenchExample VERSION 1.0 LANGUAGES CXX)include(FetchContent)FetchContent_Declare( nanobench GIT_REPOSITORY https://github.com/martinus/nanobench.git GIT_TAG v4.1.0 GIT_SHALLOW TRUE)FetchContent_MakeAvailable(nanobench)add_executable(MyExample my_example.cpp)target_link_libraries(MyExample PRIVATE nanobench)","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:2:1","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何使用 使用非常简单，不依赖于宏，而是使用对应的类的成员函数。 比如： #include \u003cnanobench.h\u003e #include \u003catomic\u003e int main() { int y = 0; std::atomic\u003cint\u003e x(0); ankerl::nanobench::Bench().run(\"compare_exchange_strong\", [\u0026] { x.compare_exchange_strong(y, 0); }); } 输出如下： 可以看得出来，上述的输出结果其实可以直接copy到markdown中，会被渲染为表格。 ns/op：每个bench内容需要经历的时间（ns为单位）。 op/s：每秒可以执行多少次操作。 err%：运行多次测试的波动情况（误差）。 ins/op：每次操作需要多少条指令。 cyc/op：每次操作需要多少次时钟周期。 bra/op：每次操作有多少次分支预判。 miss%：分支预判的miss率。 total：本次消耗的总时间。 benchmark：对应的名字。 对于不同的机器上述的指标支持程度略有不同，官方的描述为： CPU statistics like instructions, cycles, branches, branch misses are only available on Linux, through perf events. On some systems you might need to change permissions through perf_event_paranoid or use ACL. 防止被优化 如下示例： #include \u003cnanobench.h\u003e#include \u003cthirdparty/doctest/doctest.h\u003e TEST_CASE(\"tutorial_fast_v1\") { uint64_t x = 1; ankerl::nanobench::Bench().run(\"++x\", [\u0026]() { ++x; }); } 可能无法输出结果，因为x被优化了，所以可以改为下面这样： #include \u003cnanobench.h\u003e#include \u003cdoctest/doctest.h\u003e TEST_CASE(\"tutorial_fast_v2\") { uint64_t x = 1; ankerl::nanobench::Bench().run(\"++x\", [\u0026]() { ankerl::nanobench::doNotOptimizeAway(x += 1); }); } 优化不稳定 有些时候输出结果会提示你测试不稳定，你可以按照提示增加 minEpochIterations 。 比如： #include \u003cnanobench.h\u003e#include \u003cdoctest/doctest.h\u003e #include \u003crandom\u003e TEST_CASE(\"tutorial_fluctuating_v1\") { std::random_device dev; std::mt19937_64 rng(dev()); ankerl::nanobench::Bench().run(\"random fluctuations\", [\u0026] { // each run, perform a random number of rng calls auto iterations = rng() \u0026 UINT64_C(0xff); for (uint64_t i = 0; i \u003c iterations; ++i) { ankerl::nanobench::doNotOptimizeAway(rng()); } }); } 输出如下： 我们按照提示修改代码如下： #include \u003cnanobench.h\u003e#include \u003cdoctest/doctest.h\u003e #include \u003crandom\u003e TEST_CASE(\"tutorial_fluctuating_v2\") { std::random_device dev; std::mt19937_64 rng(dev()); ankerl::nanobench::Bench().minEpochIterations(5000).run( \"random fluctuations\", [\u0026] { // each run, perform a random number of rng calls auto iterations = rng() \u0026 UINT64_C(0xff); for (uint64_t i = 0; i \u003c iterations; ++i) { ankerl::nanobench::doNotOptimizeAway(rng()); } }); } 结果果然稳定了。 比较测试结果 有时候我们需要对很多测试结果进行比较，在 nanobench 中，很容易做到，只要共用同一个 Bench 对象即可，在开始的时候调用对应的方法。 比如官方给出了一个对比不同随机数生成器的性能的例子：完整代码：example_random_number_generators.cpp private: static constexpr uint64_t rotl(uint64_t x, unsigned k) noexcept { return (x \u003c\u003c k) | (x \u003e\u003e (64U - k)); } uint64_t stateA; uint64_t stateB; }; namespace { // Benchmarks how fast we can get 64bit random values from Rng. template \u003ctypename Rng\u003e void bench(ankerl::nanobench::Bench* bench, char const* name) { std::random_device dev; Rng rng(dev()); bench-\u003erun(name, [\u0026]() { auto r = std::uniform_int_distribution\u003cuint64_t\u003e{}(rng); ankerl::nanobench::doNotOptimizeAway(r); }); } } // namespace TEST_CASE(\"example_random_number_generators\") { // perform a few warmup calls, and since the runtime is not always stable // for each generator, increase the number of epochs to get more accurate // numbers. ankerl::nanobench::Bench b; b.title(\"Random Number Generators\") .unit(\"uint64_t\") .warmup(100) .relative(true); b.performanceCounters(true); // sets the first one as the baseline bench\u003cstd::default_random_engine\u003e(\u0026b, \"std::default_random_engine\"); bench\u003cstd::mt19937\u003e(\u0026b, \"std::mt19937\"); bench\u003cstd::mt19937_64\u003e(\u0026b, \"std::mt19937_64\"); bench\u003cstd::ranlux24_base\u003e(\u0026b, \"std::ranlux24_base\"); bench\u003cstd::ranlux48_base\u003e(\u0026b, \"std::ranlux48_base\"); bench\u003cstd::ranlux24\u003e(\u0026b, \"std::ranlux24_base\"); bench\u003cstd::ranlux48\u003e(\u0026b, \"std::ranlux48\"); bench\u003cstd::knuth_b\u003e(\u0026b, \"std::knuth_b\"); bench\u003cWyRng\u003e(\u0026b, \"WyRng\"); bench\u003cNasamRng\u003e(\u0026b, \"NasamRng\"); bench\u003cSfc4\u003e(\u0026b, \"Sfc4\"); bench\u003cRomuTrio\u003e(\u0026b, \"RomuTrio\"); bench\u003cRomuDuo\u003e(\u0026b, \"RomuDuo\"); bench\u003cRomuDuoJr\u003e(\u0026b, \"RomuDuoJr\"); bench\u003cOrbit\u003e(\u0026b, \"Orbit\"); bench\u003cankerl::nanobench::Rng\u003e(\u0026b, \"ankerl::nanobench::Rng\"); } 我们需要注意的几个关键方法： unit ：用于将原本默认的 xx/op 中的 op 替换为自定义的字符串。 warmup ：在测试开始之前进行预热的次数，也就是先执行这么些次数，不会计入最终数据。 relative ：设置为 true 之后，再run，之后的所有run都会以这个为基准线做对比。 performanceCounters ：是否测试 ins/op 、bra/op 、miss%。 上述测试结果如下： relative ns/uint64_t uint64_t/s err% ins/uint64_t bra","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:2:2","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"CLion中查看测试覆盖率 关于测试覆盖率，截一段chatgpt的对话： 其实测试覆盖率是几乎所有编译器自带的功能（CLion中暂时不支持msvc，mingw是支持的），但是需要在编译的时候加入对应的参数，但不同的编译器参数很多很繁杂，CLion就为我们提供了便利性，使用CLion你只需要点击两下鼠标就行了。 第一次点击鼠标：用于CLion帮我们生成对应的coverage配置项。 第二次点击鼠标：用于运行coverage配置项生成覆盖率数据，然后CLion将会有图形化显示。 已经生成好了对应的配置项，用对应配置项去运行这个测试即可得出结果如下： 当然你也可以自己在配置项里面添加对应的编译器flag，下面是gcc编译器的flag，别的编译器有所不同，所以CLion提供了自动帮我们生成配置项的功能。 -DCMAKE_CXX_FLAGS=\"--coverage\" ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:3:0","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"CLion中使用sanitizers检测内存错误 关于sanitizer是什么，可以看下面这段chatgpt的截图： 现在其实在clang/gcc中已经自带了这个功能，只需要在编译时加入编译选项 -fsanitize 即可（亲测Windows下的mingw里的gcc并不支持）。 整个所有的 sanitize 功能如下： AddressSanitizer (ASan)：检测内存访问错误，如越界访问和使用释放的内存。它通过在程序执行期间在内存中插入虚拟填充来实现这一点，并在程序试图访问这些填充时生成错误消息。 LeakSanitizer (LSan)：检测内存泄漏，即程序未释放的内存。它通过跟踪程序中的每个动态分配来实现这一点，并在程序结束时生成报告。 ThreadSanitizer (TSan)：检测多线程程序中的数据竞争。它通过在程序执行期间跟踪线程之间的共享变量访问来实现这一点，并在发现竞争时生成错误消息。 UndefinedBehaviorSanitizer (UBSan)：检测未定义行为，如类型转换错误和溢出。它通过在程序执行期间插入检查代码来实现这一点，并在发现错误时生成错误消息。 MemorySanitizer (MSan)：检测未初始化内存的使用，这是一个非常隐蔽的错误，它通过在程序中所有未初始化内存插入值来实现这一点，并在程序试图使用这些值时生成错误消息。 其实上述的 memory 和 ub 问题的检测，在CLion中你还未编译时就已经给出了提示。 ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:4:0","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"环境准备 官方文档在这：https://www.jetbrains.com/help/clion/google-sanitizers.html，如果是windows环境可以通过安装clang-cl编译器来得到 AddressSanitizer 的能力，具体操作在这个文档中：https://www.jetbrains.com/help/clion/quick-tutorial-on-configuring-clion-on-windows.html#clang-cl 如果是 Linux/wsl/macos 环境使用 gcc/clang 都可以得到 AddressSanitizer 、LeakSanitizer 、ThreadSanitizer、 UndefinedBehaviorSanitizer 的能力。 我推荐使用clang，至少在我的wsl上gcc的 ThreadSanitizer 能力是错误的。 如何安装clang环境就非常简单了，apt install即可。 ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:4:1","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何使用 其实使用对应的能力很简单，只需要在编译选项中加入对应的参数即可。 set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=[sanitizer_name] [-g] [-OX]\")下面这些是 [sanitizer_name] 对应的选项： address ：表示开启 AddressSanitizer leak ： 表示开启LeakSanitizer thread： 表示开启 ThreadSanitizer undefined： 表示开启UndefinedBehaviorSanitizer (other options are also available, see the UBSan section) memory： 表示开启MemorySanitizer -g 是用来生成调试信息的，建议不要再选项里面加，因为CLion会根据cmake配置项里的 Release/Debug 模式来自动加上，所以你不要画蛇添足，这样会出错，加入调试信息可以让最终收集到的信息有具体的源码位置，方便我们查看分析的结果。 -ox 是优化选项，比如 -o1 -o2之类的，这个也不用管，CLion同样也是根据cmake配置项里的信息生成，比如Release就是-o3，Debug就是-o2。 设置哈对应的编译参数后，我们对需要分析的程序运行一次即可，然后CLion中就会出现图形化的结果。 内存泄漏检测（leak） 比如我现在有下面这段内存泄漏代码，我开了 -fsanitize=leak 并且为Debug模式 ： int main(){ auto* p = new int(3234); (void)p; } 最终的结果图如下： 请不要同时开启多个选项，可能会报错，如果没有报错，也可能只出现一个效果。 内存访问错误检测（address） 加入 -fsanitize=address 并设置为Debug模式。 同样我有下面这段代码（请在C++17及以上进行编译）： #include \u003ciostream\u003e#include \u003csstream\u003e#include \u003cstring\u003e#include \u003cstring_view\u003e#include \u003cvector\u003e struct str_helper { std::string* data; str_helper() : data(new std::string()) {} ~str_helper() { delete data; } }; std::vector\u003cstd::string_view\u003e split_by_line_v1(std::string const\u0026 str) { std::stringstream ss(str); std::vector\u003cstd::string_view\u003e ret; str_helper line; while (std::getline(ss, *line.data, '\\n')) { ret.push_back(*line.data); } return ret; } std::vector\u003cstd::string_view\u003e split_by_line_v2(std::string const\u0026 str) { size_t pos = str.find('\\n'), pre_pos = 0; auto ret = std::vector\u003cstd::string_view\u003e(); while (pos != std::string::npos) { ret.emplace_back(str.data() + pre_pos, pos - pre_pos); pre_pos = pos + 1; pos = str.find('\\n', pre_pos); } if (pre_pos + 1 \u003c str.size()) { ret.emplace_back(str.data() + pre_pos, str.size() - pre_pos); } return ret; } std::vector\u003cstd::string\u003e split_by_line_v3(std::string_view str) { std::stringstream ss; ss \u003c\u003c str; std::vector\u003cstd::string\u003e list; std::string line; while (std::getline(ss, line, '\\n')) { list.emplace_back(std::move(line)); } return list; } int main() { std::string data = \"你好\\n你好2\\n哈哈哈哈\"; for (const auto\u0026 v : split_by_line_v1(data)) { std::cout \u003c\u003c v \u003c\u003c \"\\n\"; } } 这段代码有三个版本的split实现，很明显，第一个版本有空悬指针的问题，还有我解释为什么我第一个版本要专门再写一个 str_helper ，因为如果直接用 std::string 的话，它是检查不出来问题的，必须要使用new和delete进行内存的申请与释放才能被检测到，而标准库容器中使用的是 std::acllocator 。 第二个版本，没有内存安全问题，且不存在拷贝，但是使用的时候需要注意生命周期的问题，因为都是浅拷贝(string_view)。 第三个版本，没有内存安全问题，且不需要注意生命周期问题，但是有深拷贝和堆内存创建的性能损耗。 使用第一个版本检测出的情况如下图： 多线程数据竞争检测（thread） 加入 -fsanitize=thread 并设置为Debug模式。 有下列简单代码： #include \u003cthread\u003e int s_count; void count_plus(int times) { for (int i = 0; i \u003c times; i++) ++s_count; } int main() { std::thread th1(count_plus,100); std::thread th2(count_plus,100); th1.join(); th2.join(); } 检测结果如下： ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:4:2","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"CLion中使用perf生成火焰图 同样截取一段chatgpt的回答： 说白了就是分析软件的性能瓶颈，具体是通过查看各个函数调用所占用的时间或cpu消耗等等。 这个功能需要下载 perf 工具，而 perf 需要Linux环境，所以Windows可以使用wsl2来实现，但是我的wsl2无法直接使用需要手动去下载wsl2内核源代码然后编译安装，安装好后，我使用后发现还是有bug（无法显示非系统调用函数），所以这个还是只适合在 Linux/macos 使用。官方文档在：https://www.jetbrains.com/help/clion/2022.3/cpu-profiler.html ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:5:0","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"环境准备 我这里就偷个懒直接把官方文档的中文翻译截图放这里了，建议自己去看官方文档： ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:5:1","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["开发环境配置"],"content":"如何使用 使用的话，只需要像下面这样配置即可： 在CLion中运行的时候按下这个按钮即可： 运行后等一会儿，然后CLion里会有个通知告诉你可以查看profiler了，我的结果如下图（有bug，只能显示出系统调用的函数）： ","date":"2023-01-27","objectID":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/:5:2","tags":["C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测"],"title":"C++工程实践必备：测试、基准测试、覆盖测试、性能分析、内存泄漏检测","uri":"/posts/c++%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BF%85%E5%A4%87%E6%B5%8B%E8%AF%95%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%A6%86%E7%9B%96%E6%B5%8B%E8%AF%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%A3%80%E6%B5%8B/"},{"categories":["个人项目"],"content":"ejson4cpp使用文档","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"代码仓库：https://github.com/ACking-you/ejson4cpp ejosn4cpp ：意味着这是一个使用上非常 easy，同时性能上也非常 efficiency c++ json解析库。 支持c++11及以上，并且完全的跨平台。 使用 easy 体现在： api简单，你只需要关注两个函数（FromJSON、ToJSON），且支持一键json结构体互转。 引入简单，支持cmake命令一键引入项目并使用。 错误定位简单，无论是解析json还是序列化为json，任何错误的操作都会有详细的报错信息（模拟打印了堆栈信息），让错误定位更简单。 性能 efficiency 体现在： 本机benchmark(3000行json)结果如图： 反序列化（Parse)性能明显领先 nlohmann-json 和 jsoncpp，但只有 rapidjson 的一半性能。 序列化（Stringify）性能遥遥领先其他所有json库一个数量级。 查找（FindMember）：由于看过 rapidjson 源码，发现其内部每个元素的节点是以数组的形式组织的，并没有用到其他高深的数据结构，故专门对他进行成员查找测试，发现确实是 O(n) 级别的查找性能。 benchmark的代码仓库：https://github.com/ACking-you/bench_json4cpp ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:0:0","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"快速开始 ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:1:0","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"要求 C++11及以上，是跨全平台的 ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:1:1","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"安装与引入 推荐用以下两种方式进行引入： 方法一：通过cmake中的 FetchContent 模块引入 在项目的cmake中添加下列代码进行引入，国内如果因为网络问题无法使用可以换这个gitee的镜像源：https://gitee.com/acking-you/ejson4cpp.git include(FetchContent)FetchContent_Declare( ejson4cpp GIT_REPOSITORY https://github.com/ACking-you/ejson4cpp.git GIT_TAG v1.5.2 GIT_SHALLOW TRUE)FetchContent_MakeAvailable(ejson4cpp) 在需要使用该库的目标中链接 ejson 即可。 target_link_libraries(target ejson) 方法二：手动下载包，然后通过cmake命令引入 通过git命令下载项目源码 git clone https://github.com/ACking-you/ejson4cpp.git 将该项目添加到子项目中： add_subdirectory(ejson4cpp) 在需要使用该库的目标中链接 ejson 即可。 target_link_libraries(target ejson) ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:1:2","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"开始使用 这里以解析 json 的配置文件映射到 C++ 结构体为例子来进行讲解。 假设有redis、mysql、日志服务需要通过配置文件来进行配置，我们先写下结构体如下： struct server { int port{}; std::string host; }; struct log { std::string level; std::string filedir; std::string formatter; }; struct config { log logger; server redis; server mysql; }; 一个模拟的json配置文件如下： { \"logger\": { \"filedir\": \"home/project/1\", \"formatter\": \"default\", \"level\": \"debug\" }, \"mysql\": { \"host\": \"192.31.1.1\", \"port\": 1314 }, \"redis\": { \"host\": \"127.0.0.1\", \"port\": 1444 } } 现在要实现的功能是读取json配置文件的数据将 config 结构体进行初始化，我们可以按照下面的步骤进行： 完整代码请看 example/example1.cc 让server、log、config这几个自定义类型支持 json 序列化，添加下列宏定义即可： // auto generate log/server/config to_json and from_json AUTO_GEN_NON_INTRUSIVE(log, level, filedir, formatter) AUTO_GEN_NON_INTRUSIVE(server, host, port) AUTO_GEN_NON_INTRUSIVE(config, logger, redis, mysql) 定义config变量，调用 FromFile 函数，即可完成需求： struct config s_config; // init config from config.json Parser::FromFile(CONFIG_PATH, s_config); 如果需要重新写回文件，则可调用 ToFile 函数： // write config to file Parser::ToFile(CONFIG_PATH, s_config); 如果读取json字符串的数据并初始化对应的变量（反序列化）则可以调用 FromJSON 函数： // init config struct from json string Parser::FromJSON(j, s_config); 如果需要将变量转化为json字符串（序列化），则可调用 ToJSON 函数： auto json_str = Parser::ToJSON(s_config); 好的，经过以上两步，你已经学会了整个库的核心用法，没错，这个库提倡使用直接的函数而不是类来实现对应的功能，这样能减少你的记忆和思考过程。当然如果需要更为细致的使用它，你可以去了解 JObject 类的相关用法，在API介绍里面写的很详细。 ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:1:3","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"常见用法 在进行后端开发的过程中，前端传来的数据很多时候是 json 数据，那么我们现在就使用该库来模拟一个简单的后端业务。 比如一个视频平台的评论区，首先映入眼帘的是一条条评论，然后是发出该条评论的用户。 那么我们可以抽离出 comment 和 user_info 这两个结构体表示前端需要展示的消息，那么它在我们C++后端的请看可能是下面这样的结构体： 完整代码在 example/example2.cc struct user_info { bool is_follow{};//是否关注 int64_t id{};//id信息 int64_t follow_count{};//关注的博主数量 int64_t follower_count{};//粉丝数量 std::string name;//用户名 }; struct comment { int64_t id{};//id信息 int64_t user_id{};//用户id信息 std::string created_date;//创建时间 std::string content;//评论内容 }; 那么我们的后端逻辑可能会经历下面的过程： 从前端获取json数据（中间一般有鉴权的过程）。 接收json数据并其初始化为对应的C++结构体。 进行该次接口调用的业务逻辑处理。 保存数据到数据库。 那么我们用模拟数据来模拟上述过程： 前端的数据： const char* comment_json = \"{\\n\" \" \"content\": \"这是一条\\\"测试\\\"评论\",\\n\" \" \"created_date\": \"2023-01-16\",\\n\" \" \"id\": 1,\\n\" \" \"user_id\": 10\\n\" \"}\"; const char* user_info_json = \"{\\n\" \" \"follow_count\": 12,\\n\" \" \"follower_count\": 23,\\n\" \" \"id\": 1,\\n\" \" \"is_follow\": false,\\n\" \" \"name\": \"某人名字\"\\n\" \"}\"; 将数据转为C++的结构体： 需要先添加下列宏让对应的结构支持json互转 AUTO_GEN_NON_INTRUSIVE(user_info, is_follow, id, follow_count, follower_count,name) AUTO_GEN_NON_INTRUSIVE(comment, id, user_id, created_date, content) 然后调用对应函数即可转化 comment cmt; user_info uinfo; Parser::FromJSON(comment_json, cmt); Parser::FromJSON(user_info_json, uinfo); 处理业务逻辑，这个跳过。 保存数据到数据库，这个模拟为保存数据到文件： 我们可以创建一个 dict_t 类型的 JObject ，然后把刚才的结构体以 key-value 对的形式放进去，最后再调用 ToFile 函数。 // 4.save data to database(we simulate it to local file) auto object = JObject::Dict(); object.at(\"comment\").get_from(cmt); object.at(\"user_info\").get_from(uinfo); ejson::Parser::ToFile(DATA_PATH, object); 最终得到下列json数据到文件中： { \"comment\": { \"content\": \"这是一条\"测试\"评论\", \"created_date\": \"2023-01-16\", \"id\": 1, \"user_id\": 10 }, \"user_info\": { \"follow_count\": 12, \"follower_count\": 23, \"id\": 1, \"is_follow\": false, \"name\": \"某人名字\" } } ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:2:0","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"API介绍 对所有类成员的描述的信息，可以点开 doc/html/index.html 进行查看。如果需要其他语言版本的文档，可以自己通过 Doxygen 进行生成。 ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:0","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"通过命名风格识别API 所有对外暴露的静态成员函数均以 PascalCase 风格命名。如下： namespace ejson { class Parser { static JObject FromJSON(const str_t \u0026content,bool skip_comment=false); template \u003cclass T\u003e static void FromJSON(string_view const \u0026src, T \u0026dst,bool skip_comment=false); static JObject FromFile(string_view const \u0026filename,bool skip_comment=false); template \u003cclass T\u003e static void FromFile(string_view const \u0026filename, T \u0026dst); template \u003cclass T\u003e static std::string ToJSON(T \u0026\u0026src,const int indent = -1, const char indent_char = ' ', bool is_esc = false); template \u003cclass T\u003e static void ToFile(string_view const \u0026filename, T const \u0026src, const int indent = -1, const char indent_char = ' ', bool is_esc = false); static void ToFile(string_view const \u0026filename, JObject const \u0026src, const int indent = -1, const char indent_char = ' ', bool is_esc = false) }; class JObject { static auto Dict() -\u003e JObject; static auto List() -\u003e JObject; }; } // namespace ejson 所有想要暴露的普通成员函数均以 snack_case 风格命名，如下： namespace ejson { class JObject { auto type() const -\u003e Type; auto at(const str_t \u0026key) const -\u003e ObjectRef; auto to_string(int indent = -1, char indent_char = ' ', bool is_esc = false) const -\u003e string; void push_back(JObject item); void pop_back(); auto has_key(const str_t \u0026key) const -\u003e bool; template \u003cclass T\u003e auto cast() const -\u003e T; }; struct ObjectRef { template \u003cclass T\u003e auto get_from(T const \u0026src) -\u003e ObjectRef \u0026; template \u003cclass T\u003e void get_to(T \u0026src); }; } // namespace ejson 其余还有两个函数，如下： namespace ejson_literals { auto operator\"\"_json(const char *json, size_t len) -\u003e JObject; auto float_d(int d) -\u003e int; } // namespace ejson_literals ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:1","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"宏定义 利用宏定义可以方便且迅速的让自定义类型支持 FromJSON 和 ToJSON 系列函数。 实际上自定义类型在使用 FromJSON 时，只需要定义对应的 from_json 函数，使用 ToJSON 时，只需定义对应的 to_json 函数。 下列是 from_json 和 to_json 的函数签名： void from_json(const ejson::JObject\u0026 ejson_j, T\u0026 ejson_t); void to_json(ejson::JObject\u0026 ejson_j, const T\u0026 ejson_t); 你可以像下面这样自己实现上面这两个函数来让自定义类型支持 FromJSON 和 ToJSON 函数。 struct student { int id; int score; std::string name; }; void to_json(ejson::JObject\u0026 ejson_j, const student\u0026 ejson_t) { ejson_j.at(\"id\").get_from(ejson_t.id); ejson_j.at(\"score\").get_from(ejson_t.score); ejson_j.at(\"name\").get_from(ejson_t.name); } void from_json(const ejson::JObject\u0026 ejson_j, student\u0026 ejson_t) { ejson_j.at(\"id\").get_to(ejson_t.id); ejson_j.at(\"score\").get_to(ejson_t.score); ejson_j.at(\"name\").get_to(ejson_t.name); } 如果属性是 private 的，那么可以像下面这样侵入式的定义： struct student { friend void to_json(ejson::JObject\u0026 ejson_j, const student\u0026 ejson_t) { ejson_j.at(\"id\").get_from(ejson_t.id); ejson_j.at(\"score\").get_from(ejson_t.score); ejson_j.at(\"name\").get_from(ejson_t.name); } private: int id; int score; std::string name; }; FROM_JSON_FUNC\u0026FROM_JSON_FRIEND_FUNC 用于简化 from_json 函数定义的书写，例如前面对于 strudent 类型的 from_json 函数可以这样写： struct student { int id; int score; std::string name; }; //非侵入式 FROM_JSON_FUNC(student, ejson_j, ejson_t) { ejson_j.at(\"id\").get_to(ejson_t.id); ejson_j.at(\"score\").get_to(ejson_t.score); ejson_j.at(\"name\").get_to(ejson_t.name); } struct student { //侵入式 FROM_JSON_FRIEND_FUNC(student,ejson_j,ejson_t) { ejson_j.at(\"id\").get_to(ejson_t.id); ejson_j.at(\"score\").get_to(ejson_t.score); ejson_j.at(\"name\").get_to(ejson_t.name); } private: int id; int score; std::string name; }; TO_JSON_FUNC\u0026TO_JSON_FRIEND_FUNC 用于简化 to_json 函数定义的书写，例如前面对于 strudent 类型的 to_json 函数可以这样写： struct student { int id; int score; std::string name; }; //非侵入式 TO_JSON_FUNC(student, ejson_j, ejson_t) { ejson_j.at(\"id\").get_from(ejson_t.id); ejson_j.at(\"score\").get_from(ejson_t.score); ejson_j.at(\"name\").get_from(ejson_t.name); } struct student { //侵入式 TO_JSON_FRIEND_FUNC(student,ejson_j,ejson_t) { ejson_j.at(\"id\").get_from(ejson_t.id); ejson_j.at(\"score\").get_from(ejson_t.score); ejson_j.at(\"name\").get_from(ejson_t.name); } private: int id; int score; std::string name; }; AUTO_GEN_NON_INTRUSIVE\u0026AUTO_GEN_INTRUSIVE 这两个宏可以帮助你一键生成之前例子中的 to_json 和 from_json 函数。 前面的代码可以替换为： struct student { int id; int score; std::string name; }; //非侵入式 AUTO_GEN_NON_INTRUSIVE(student,id,score,name) struct student { //侵入式 AUTO_GEN_INTRUSIVE(student,id,score,name) private: int id; int score; std::string name; }; ENABLE_JSON_COUT 自动生成对应类型的 operator\u003c\u003c(ostream\u0026,T) 运算符重载，用于将对应类型支持 cout 打印出json格式。该宏可以为多个类型生成。 struct student { int id; int score; std::string name; }; struct info { int id; std::string msg; }; //让对应类型支持json格式化 AUTO_GEN_NON_INTRUSIVE(student,id,score,name) AUTO_GEN_NON_INTRUSIVE(info,id,msg) //支持json格式cout打印 ENABLE_JSON_COUT(student,info) ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:2","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"FromJSON系列函数 参数说明 static JObject Parser::FromJSON(const str_t \u0026content, bool skip_comment = false); 根据json字符串内容反序列化为JObject结构。 参数说明： content：需要解析的json资源，这是一个string_view类型的参数，支持C风格字符串和 std::string。 skip_comment：是否需要支持跳过注释，默认为false，未开启。 返回值： 返回解析完的 JObject 类型。 template \u003cclass T\u003e static void Parser::FromJSON(string_view const \u0026src, T \u0026dst,bool skip_comment = false) 根据json字符串内容反序列化数据到 dst。 参数说明： src：需要解析的json资源，这是一个string_view类型的参数，支持C风格字符串和 std::string。 dst：需要初始化的变量，可以是自定义类型。 skip_comment：是否需要支持跳过注释，默认为false，未开启。 static JObject\u0026 Parser::FromFile(string_view const \u0026filename, bool skip_comment = false); 根据文件中的 json 数据获取 JObject\u0026，这个JObject是thread_local变量，也就是每个线程共用一个JObject。所以请注意，当您调用此函数时，将更新这个共用的 JObject 的值。 参数说明： filename ：json文件路径。 skip_comment ：是否需要支持跳过注释，默认为false，未开启。 返回值： 同一个线程共用的JObject\u0026。 static void Parser::FromFile(string_view const \u0026filename, T \u0026dst,bool skip_comment = false); 根据文件中的json数据设置 dst 的值。 参数说明： filename ：json文件路径。 dst ：需要初始化的变量。 skip_comment ：是否需要支持跳过注释，默认为false，未开启。 使用示例 #include \u003cejson/parser.h\u003e#include \u003ciostream\u003eusing namespace ejson; struct Score { double p; }; struct student { int id{}; std::string name; Score score{}; }; //为Score类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(Score, p) //为student类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(student, id, name, score) //重载方便cout打印数据 ENABLE_JSON_COUT(Score,student) int main(){ const char *json1 = R\"({\"id\":324,\"name\":\"刘xx\",\"score\":{\"p\":2342343243242.124}})\"; student stu; //使用FromJSON初始化stu变量 Parser::FromJSON(json1,stu); //使用FromFile初始化stu变量 Parser::FromFile(\"json文件路径\",stu); std::cout\u003c\u003cstu; } ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:3","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"ToJSON系列函数 参数说明 template \u003cclass T\u003e static auto Parser::ToJSON(T \u0026\u0026src,const int indent = -1, const char indent_char = ' ', bool is_esc = false) -\u003e std::string; 将任意类型序列化为json字符串返回。 参数说明： src ：需要序列化为json字符串的数据。 indent ：是否需要美化json输出，小于0表示美化，其余情况为美化时的缩进长度，默认不美化。 indent_char ：美化时填入缩进的字符，默认为 ' ' 。 is_esc ：是否需要识别转义字符，默认不识别。 返回值： json字符串。 template \u003cclass T\u003e static void ToFile(string_view const \u0026filename, T const \u0026src, const int indent = -1, const char indent_char = ' ', bool is_esc = false); 根据 src 中的数据序列化为json数据到文件中。 参数说明： filename ：需要写入的文件路径。 src ：需要序列化的变量。 indent ：是否需要美化json输出，小于0表示美化，其余情况为美化时的缩进长度，默认不美化。 indent_char ：美化时填入缩进的字符，默认为 ' ' 。 is_esc ：是否需要识别转义字符，默认不识别。 static void ToFile(string_view const \u0026filename, JObject const \u0026src const int indent = -1, const char indent_char = ' ', bool is_esc = false) 将JObject中的数据转为json写入到文件中。 参数说明： filename ：需要写入的文件路径。 src ：JObject变量。 indent ：是否需要美化json输出，小于0表示美化，其余情况为美化时的缩进长度，默认不美化。 indent_char ：美化时填入缩进的字符，默认为 ' ' 。 is_esc ：是否需要识别转义字符，默认不识别。 使用示例 #include \u003cejson/parser.h\u003e#include \u003ciostream\u003eusing namespace ejson; struct Score { double p; }; struct student { int id{}; std::string name; Score score{}; }; //为Score类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(Score, p) //为student类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(student, id, name, score) //重载方便cout打印数据 ENABLE_JSON_COUT(Score,student) int main(){ student stu; stu.id = 324; stu.name = \"刘xx\"; stu.score.p = 2342343243242.124; //使用ToJSON进行序列化 auto json_data = Parser::ToJSON(stu); //使用ToFile将数据序列化到文件 Parser::ToFile(\"文件路径\",stu); std::cout\u003c\u003cstu; } ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:4","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"JObject系列函数 JObject的构造函数 只需要清楚以下几点： JObject有无参构造，但是无参构造产生的JObject为null类型，无法进行正常使用。 JObject的构造函数可以直接接受大部分类型，且包括自定义类型和部分stl容器。 JObject本身不支持拷贝构造，只支持移动构造。 前面的 ToJSON API完全可以用下列方式替代，因为所有的序列化过程其实都是构造JObject来进行： #include \u003cejson/parser.h\u003e#include \u003ciostream\u003eusing namespace ejson; struct Score { double p; }; struct student { int id{}; std::string name; Score score{}; }; //为Score类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(Score, p) //为student类型自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(student, id, name, score) //重载方便cout打印数据 ENABLE_JSON_COUT(Score,student) int main(){ student stu; stu.id = 324; stu.name = \"刘xx\"; stu.score.p = 2342343243242.124; //构造JObject并使用其成员函数 auto json_data = JObject(stu).to_string(); std::cout\u003c\u003cstu; } JObject的成员函数 Type JObject::type() const 返回当前JObject对象的类型，具体的类型有下列情况： kNull ：值为null类型 kBool：值为bool类型 kInt：值为整数类型 kDouble：值为浮点类型 kStr：值为字符串类型 kList：值为列表类型 kDict：值为字典类型（或叫做对象类型） kDict 当你的JObject的类型为 kDict 时，下列成员函数可供使用： bool JObject::has_key(const str_t\u0026 key) const 判断 JObject 中是否存在包含 key 的映射。 ObjectRef JObject::at(const str_t\u0026 key) const 根据key取出对应映射的value，value以 ObjectRef 类型的方式提供。 而ObjectRef类型有这两个关键的成员函数： ObjectRef\u0026 JObject::ObjectRef::get_from(T\u0026\u0026 src) 从 `src` 中获取数据填充到 `JObject` 中，若 `src` 为自定义类型需要自定义 `to_json` 方法。\r* ```cpp\rObjectRef\u0026 JObject::ObjectRef::get_from(T\u0026 dst)\r从 `JObject` 中获取数据填充到 `dst` 中，若 `dst` 为自定义类型需要自定义 `from_json` 方法。\r kList 当JObject类型为 kList 时，下列成员函数可用： void JObject::push_back(JObject item); 插入一个值到 JObject 列表的尾部，可以插入任意类型，但都需要显式的转化为 JObject 类型，如 JObject(324234)。 void JObject::pop_back() 删除列表中末尾的值。 to_string string JObject::to_string( int indent = -1, char indent_char = ' ', bool is_esc = false )const 序列化最终调用的API，将JObject序列化为json字符串返回。 参数说明： indent ：用于判断json解析是否需要美化，如果需要美化，则该值为缩进的长度。该值小于0时不进行美化，默认值为-1。 indent_char：缩进填入的字符，默认为 ' ' 。 is_esc：是否需要对转义字符进行处理，默认不开启。 返回值： 序列化后端json字符串。 static 为了方便快速创建 dict_t 类型和 list_t 类型的 JObject ，定义了下列静态函数： static JObject JObject::Dict() 使用如下： #include \u003cejson/parser.h\u003e#include \u003ciostream\u003eusing namespace ejson; int main(){ //构造JObject并使用其成员函数 auto json = JObject::Dict(); json.at(\"a\").get_from(\"bc\"); json.at(\"d\").get_from(\"ef\"); std::cout\u003c\u003cjson.to_string(); } static JObject JObject::List() 使用如下： #include \u003cejson/parser.h\u003e#include \u003ciostream\u003e#include \u003cvector\u003eusing namespace ejson; struct custom_type{ int id; std::vector\u003cint\u003e data; }; //自动生成to_json和from_json函数 AUTO_GEN_NON_INTRUSIVE(custom_type, id,data) int main(){ //构造JObject并使用其成员函数 auto json = JObject::List(); json.push_back(JObject(\"abc\")); custom_type v{1,{2,3,3}}; json.push_back(JObject(v)); std::cout\u003c\u003cjson.to_string(); } ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:3:5","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"注意事项 在使用本库时需要注意几点： 本库不对你使用的字符编码进行验证，加入你使用 gbk 编码的json数据进行解析得到还是gbk编码的，所以这点需要注意，建议都使用utrf8编码。 本库只支持对 `` 和 \" 的转义，其他的诸如 \\u \\b 等功能暂时不支持，我还是建议在json数据中不要存储二进制文件，如果需要存储二进制文件，后续的版本中可能会增base64编码的支持。 本库对所有字符串解析到 JObject 中的过程都是浅拷贝（只拷贝了指针），故如果需要使用原生 JObject 进行数据的存储需要额外的注意内存的所有权和生命周期，建议使用 JObject 进行短期的解析而不是长期的存储，在大多数情况下直接使用函数是最好的选择，后续可能会出一个深拷贝的 JObject 那样的结构才适合存储。 本库所有的错误均以异常抛出，好处在于可以模拟递归调用栈的栈信息进行打印，坏处当然是要写 try catch。 ","date":"2023-01-17","objectID":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/:4:0","tags":["ejson4cpp使用文档"],"title":"ejson4cpp使用文档","uri":"/posts/ejson4cpp%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"},{"categories":["开发环境配置"],"content":"CLion开发环境配置完全解析","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"下载与安装 关于CLion的安装我推荐安装最新的CLion 2022.3之后的版本，因为从这个版本后性能提升了很多，而且还增加了对cmake代码的调试功能，并且对wsl的支持得到进一步的增强，现在有了CLion在windows上使用Linux环境进行开发将是非常简单且舒适的过程。 建议先下载JetBrains家的toolbox然后再进行对应 CLion 的下载，这样也方便你管理与统一配置JB家的所有IDE。 官网链接如下：https://www.jetbrains.com/zh-cn/toolbox-app/ 下载完toolbox后，点开安装CLion 2022.3(或更新的版本) 即可。当然如果你不设置的话，下载完CLion后你会发现它默认安装在C盘，不要慌，toolbox也考虑到了这样的问题。 所以你只需要像下面这样操作： 更改完这个路径后，它会自动的把数据移动到这个文件夹中，之后下载的所有IDE数据也都会在这个文件夹中了。 我们发现下面还有一个选项是代理，这对某些情况显然是很有用的，如果IDE中的某些东西的下载需要代理，那么可以在这里设置代理。 现在CLion已经下载好了，点开后可能有些人没法正常使用，毕竟大家都是白嫖党()，其实作为学生的我们也完全没有必要花这笔钱，我们去官网申请一个资格即可，之后所有的JB家的产品都可以免费使用一年了，每年需要重新申请一次，申请的网站如下：https://www.jetbrains.com/shop/eform/students 申请前请用邮箱注册一个jb的账号，注册链接如下：https://account.jetbrains.com/login 申请有多种方式，学校邮箱如果有的话就很简单了，但是很多大学生没有这个，那么大家可以使用官方文件的方式申请，这种方式稍微麻烦一点，需要去学信网弄一个验证，具体流程如下： 上述邮箱同样也需要填入jb账号对应的邮箱，申请完后，一般7天内就会有邮箱通知你成功了，之后所有的jb家的产品都可以通过该账号免费使用。而且这个账号似乎可以供6/7个人同时使用（我不知道上限，但是我给了至少6/7个人）。 至此，CLion应该能够正常打开和使用了，但是打开的界面估计默认是中文，我建议去设置里面调成中文。 方式如下：1.点开设置 2.点开插件 3.点开已安装的插件 4.点击中文的那个插件禁用即可。 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:1:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"主题推荐 我推荐使用新UI + onedark的组合（我目前所使用的）。 效果如下： 如果你使用的是旧版本的UI，我之前经常使用的主题有以下三个： Atom Material Icons 效果大概如下，这个一个用于将文件夹颜色更明亮的插件，新UI暂时不可用 material-theme-ui：插件主页 https://plugins.jetbrains.com/plugin/8006-material-theme-ui one-dark-theme：插件主页 https://plugins.jetbrains.com/plugin/11938-one-dark-theme ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:2:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"编辑器与clang-format设置 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:3:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"鼠标滚轮改变字体大小 Editor -\u003e General -\u003e Change font size with Ctrl+Mouse Wheel 打上勾就行。 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:3:1","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"clang-format的使用 关于clang-format是什么，我截一段chatgpt的回答。 CLion是自带clang-format的，你只需要开启即可，他还自动扫描项目根目录下的clang-format文件进行相应的格式化，开启后你每次创建一个新项目他也会自动生产一个clang-format文件到项目根目录，这个文件配置是根据CLion默认的格式化的格式来的，如果想要更改格式化风格，只需要更改clang-formt配置文件即可。 一般来说，我们只需要配置基本风格就行，一个.clang-format文件大概长这样： 一般来说没有什么特殊需求，那么就只需要填写 BasedOnStyle 即可，是Google风格还是LLVM风格还是Microsoft风格，都取决于上述的前三行代码，后面的都可以不用写。我这个配置文件是想要使用Google的格式化风格，但是Googel风格默认的代码边距太短了，所以进行了一系列的调整。 下面是CLion如何开启clang-format，开启后CLion对代码的格式化将会以你项目根目录的clang-format文件为主。 我的 .clang-format 配置 BasedOnStyle: Google Language: Cpp AccessModifierOffset: -4 AlignAfterOpenBracket: Align AlignConsecutiveMacros: AcrossComments AlignConsecutiveAssignments: AcrossComments AlignConsecutiveDeclarations: AcrossComments AlignEscapedNewlines: Left AlignOperands: true AlignTrailingComments: true AllowAllArgumentsOnNextLine: true AllowAllConstructorInitializersOnNextLine: false AllowAllParametersOfDeclarationOnNextLine: true AllowShortBlocksOnASingleLine: Never AllowShortCaseLabelsOnASingleLine: true AllowShortFunctionsOnASingleLine: All AllowShortLambdasOnASingleLine: All AllowShortIfStatementsOnASingleLine: WithoutElse AllowShortLoopsOnASingleLine: true AlwaysBreakAfterDefinitionReturnType: None AlwaysBreakAfterReturnType: None AlwaysBreakBeforeMultilineStrings: false AlwaysBreakTemplateDeclarations: Yes BinPackArguments: true BinPackParameters: true BraceWrapping: AfterCaseLabel: false AfterClass: true AfterControlStatement: Always AfterEnum: true AfterFunction: true AfterNamespace: false AfterObjCDeclaration: false AfterStruct: true AfterUnion: true AfterExternBlock: true BeforeCatch: true BeforeElse: true BreakBeforeBinaryOperators: None BreakBeforeBraces: Custom BreakBeforeInheritanceComma: false BreakInheritanceList: BeforeColon BreakBeforeTernaryOperators: true BreakConstructorInitializersBeforeComma: false BreakConstructorInitializers: BeforeColon BreakAfterJavaFieldAnnotations: false BreakStringLiterals: true ColumnLimit: 80 CommentPragmas: \"^ NOLINT:\" CompactNamespaces: false ConstructorInitializerAllOnOneLineOrOnePerLine: true ConstructorInitializerIndentWidth: 4 ContinuationIndentWidth: 4 Cpp11BracedListStyle: true DeriveLineEnding: true DerivePointerAlignment: true DisableFormat: false ExperimentalAutoDetectBinPacking: false FixNamespaceComments: true ForEachMacros: - foreach - Q_FOREACH - BOOST_FOREACH IncludeBlocks: Regroup IncludeCategories: - Regex: '^\u003cext/.*\\.h\u003e' Priority: 2 SortPriority: 0 - Regex: '^\u003c.*\\.h\u003e' Priority: 1 SortPriority: 0 - Regex: \"^\u003c.*\" Priority: 2 SortPriority: 0 - Regex: \".*\" Priority: 3 SortPriority: 0 IncludeIsMainRegex: \"([-_](test|unittest))?$\" IncludeIsMainSourceRegex: \"\" IndentCaseLabels: true IndentGotoLabels: true IndentPPDirectives: None IndentWidth: 4 IndentWrappedFunctionNames: false JavaScriptQuotes: Leave JavaScriptWrapImports: true KeepEmptyLinesAtTheStartOfBlocks: false MacroBlockBegin: \"\" MacroBlockEnd: \"\" MaxEmptyLinesToKeep: 1 NamespaceIndentation: None ObjCBinPackProtocolList: Never ObjCBlockIndentWidth: 2 ObjCSpaceAfterProperty: false ObjCSpaceBeforeProtocolList: true PenaltyBreakAssignment: 2 PenaltyBreakBeforeFirstCallParameter: 1 PenaltyBreakComment: 300 PenaltyBreakFirstLessLess: 120 PenaltyBreakString: 1000 PenaltyBreakTemplateDeclaration: 10 PenaltyExcessCharacter: 1000000 PenaltyReturnTypeOnItsOwnLine: 200 PointerAlignment: Right RawStringFormats: - Language: Cpp Delimiters: - cc - CC - cpp - Cpp - CPP - \"c++\" - \"C++\" CanonicalDelimiter: \"\" BasedOnStyle: google - Language: TextProto Delimiters: - pb - PB - proto - PROTO EnclosingFunctions: - EqualsProto - EquivToProto - PARSE_PARTIAL_TEXT_PROTO - PARSE_TEST_PROTO - PARSE_TEXT_PROTO - ParseTextOrDie - ParseTextProtoOrDie CanonicalDelimiter: \"\" BasedOnStyle: google ReflowComments: true SortIncludes: CaseInsensitive SortUsingDeclarations: false SpaceAfterCStyleCast: false SpaceAfterLogicalNot: false SpaceAfterTemplateKeyword: true SpaceBeforeAssignmentOperators: true SpaceBeforeCpp11BracedList: false SpaceBeforeCtorInitializerC","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:3:2","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"编译工具链设置 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:4:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"编译工具链的添加与解释 这个配置是进行C++开发的关键，因为这个编译工具链就意味着C++的编译环境。 按下图点开对应的信息，如果你任何编译工具链都没有添加，由于新版本的CLion它会自带一个mingw的编译套件，所以默认会有一个CLion自带的mingw编译工具链，如下图所示我的编译工具链稍微有点丰富，有 msvc、g++、clang++、mingw，作为一个刚刚入门学编程的新手，我建议编译工具链这一块暂时就没必要了解了，但在CLion中编译的具体配置流程我认为还是有必要讲清楚。CLion中添加编译工具链非常简单，你本机把对应工具链的路径加入到了环境变量，那么在你点击 + 对应编译链类型后，会自动扫描到，如果实在没有扫描到，那么也可以自己填入对应的路径，整个编译链包括： cmake，用于跨平台以及简化底层编译脚本的工具。 cmake生成更底层的编译命令(对应上述的Build Tool)，比如gmake也就是解析.makefile文件进行命令执行，比如 ninja 解析 .ninja文件进行命令执行（编译速度比makefile更快，亲身体验）。 C语言的编译器(clang/gcc/cl等等)。 C++的编译器(clang++/g++/cl等等)。 如果是mingw，那么上述的一套都是包含的，只需要把 Toolset 这个选项选择为mingw对应的目录即可，选择好后，CLion会自动识别上述四件套的位置。 接下来简单介绍如何添加一些工具链： 安装msvc编译工具链：直接到官网下载VS2022，然后安装对应C++环境，打开CLion后添加msvc环境时就会自动识别。官网：https://visualstudio.microsoft.com/zh-hans/vs/ 其实对于我们C++程序员而言，最需要的就是一个Linux环境，因为很多底层的系统调用是不在C++标准之内的，C++想要做到跨平台很难，所以我们需要把开发环境切换到Linux系统，正好windows提供了Linux子系统，也就是wsl，完美的解决了这个问题，不要考虑日常的使用和开发环境我们到底选哪个了，我全都要！ 而CLion对wsl的适配程度和正常的本机开发几乎没有任何区别，我们只需要现在Windows上安装wsl2后，CLion便可以自动识别你本机的wsl环境了，但是你有了wsl，并不意味着你有了对应的编译链，之前说了，编译链是包括四个东西的，你需要一一手动再wsl上先安装好，CLion会自动识别到的，如果识别不到，由于是通过 apt install 命令安装的，大家应该都清楚在具体那个目录下，实在不清楚可以使用 which 命令。 安装wsl2：其实wsl2的安装已经被简化到了极致，在powershell中 wsl --install 即可。 具体的官方文档如下 https://learn.microsoft.com/zh-cn/windows/wsl/install ，一篇非常简练的博客教程：https://zhuanlan.zhihu.com/p/438255467，如果安装遇到问题（大概率网络问题），请自行谷歌或百度或bing或chatgpt。 如果需要使用CLion进行Qt开发，可以查看视频讲解：https://www.bilibili.com/video/BV18q4y1i7kV/ ，对应的配置信息：https://gitee.com/yuexingqin/template_qtclion 如果需要使用CLion进行STM32开发，那么可以查看稚晖君在知乎写的博客教程：https://zhuanlan.zhihu.com/p/145801160 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:4:1","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"cmake配置项的添加与解释 解释完上述编译链后，我们发现CLion中有个很明显的 default 字眼，这个有什么用呢？ 如果排在第一个的编译链，会被设置为默认编译链，如果想要其他的编译链为默认，点击上移即可。 至于具体的作用，别急，先等我讲完CLion中cmake配置项的处理。 如上图所示，第二个 CMake 选项就是我们现在要讲的东西，而这两个正好也是整个开发环境中最重要的东西，第一个编译工具链决定了CLion中已经识别了本机有哪些编译环境，而第二个 CMake 选项，则是用于配置 cmake 基于哪些配置项生成。 所以我们现在应该了解了CLion是如何去编译项目生成可执行文件的了。 通过cmake配置选项运行整个项目的CMakeList.txt 生成makefile或其他底层脚本后再通过对应的工具去执行这个脚本 运行编译好的程序 而我们现在讲的就是添加cmake配置选项，如果你手动敲cmake命令的话，那样对应的就是命令行参数了。 上述图片中已经解释了一些配置的作用。这些配置项一般是不常改动，使用默认值就行，比如 Build options 是执行最后的脚本所用的参数，默认为 -j 12，比如如果是makefile，那么就是 make -j12。 下面是大家可能需要进行一些配置的选项： Build type：这是程序最终编译的类型，意味着编译器该以何种程度对源代码进行优化，比如Debug版本一般再gcc中对应o2的优化，release版本对应o3的优化，两者一般存在10倍左右的性能差距。 Toolchain：这是前面所说的编译工具链，一般来说，想要切换编译器，你切换这个选项就行了，默认使用default工具链。 Generator：这是前面所说的工具链中的较为底层的脚本的运行工具，可以是makefile或者ninja，不选的话也是默认工具链里的那个。 CMake options：这个是cmake运行时可以加入的命令行参数，比如我们可以-D来定义对应的变量控制对应的cmake行为，甚至于前面的Build type我们完全可以不写（当然这是CLion，这个空必须得被填充），然后使用-DCMAKE_BUILD_TYPE=Release，这个变量可以决定最终cmake生成的执行脚本是按照release的标准去运行的，又比如-DBUILD_SHARED_LIBS=ON，那么最终是会生成动态库而不是静态库，我上图中的 -DENABLE_TEST=ON 是内部的cmake有定义一个变量默认为OFF值，如果为ON时会加入测试代码为子项目。 现在cmake在CLion中的配置项已经讲完了，简单实践一下来体验之前讲的CLion到整个运行的流程，我这里就直接配图了： 通过cmake配置选项运行整个项目的CMakeList.txt。 生成makefile或其他底层脚本后再通过对应的工具去执行这个脚本。 我们先看一眼上一步cmake生成的文件（放出了两个不同的配置项产生的脚本，第一个使用的Generator为ninja，第二个使用的为gmake）： 如果想要继续执行这个脚本，应该在CLion中执行对应的源代码，CLion会自动识别入口点函数，然后给出可执行的按钮，如图： 点击执行后，不仅会直接对应的 makefile 或 build.ninja 还会顺便把这个程序运行到CLion内置的终端环境中。 运行编译好的程序：这一步已经在第二步一并执行了。 理解了这三步以及cmake的配置之后，我相信如果突然间CLion不出现执行程序的按钮，或者一个外部的项目我们无法跑起来，那么我们肯定是会有对应的排查思路了。比如没有执行程序的按钮，那可能对应的cmake配置项你还没设置好，如果外部项目跑不起来，你可以把那四个编译工具链中的某个换其他的环境试试？ ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:4:2","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"cmake的使用与实战 经过上述文字和图片讲解，我们很自然的想到，整个CLion运行C++代码其实就是在运行cmake和makefile(或build.ninja)，第二个过程我们参与不了，但是第一个cmake的编写过程我们却需要一直接触。 下面用CLion新建项目自动生成的cmake模板来简单对cmake语法热热身。 cmake_minimum_required(VERSION 3.22)project(untitled)set(CMAKE_CXX_STANDARD 17)add_executable(untitled main.cpp) cmake_minimum_required命令：规定了编译本项目的cmake工具至少需要3.22版本。 project命令：规定了本项目的项目名称，同时也根据这个传入的值生成了一堆变量，常用的如下： PROJECT_NAME：项目名称 PROJECT_BINARY_DIR：项目的二进制文件目录，即编译后的可执行文件和库文件的输出目录 PROJECT_SOURCE_DIR：项目的源文件目录，即包含CMakeLists.txt文件的目录 举个简单例子说明上述变量的作用： 比如一个测试的子项目中的CMakeList.txt，可能需要写下面的语句（先不管file命令），由于是作为直接的子项目，那么里面肯定不会存在project语句，所以PROJECT_SOURCE_DIR变量表示的仍然是整个项目的根目录，直接通过 ${} 的形式来使用它即可，这样就不需要关心相对或绝对路径了。 file(GLOB SONIC_TEST_FILES \"${PROJECT_SOURCE_DIR}/tests/*.h\" \"${PROJECT_SOURCE_DIR}/tests/*.cpp\" ) set命令：设置对应变量为对应的值，该变量存在，则修改该变量的值，如果不存在则会创建并初始化为对应的值，这里对set的使用是设置了 CMAKE_CXX_STANDARD 变量为17，这个变量可以控制最终编译采用的C++版本，这里是使用C++17。 add_executable命令：这是用于生成可执行程序的命令，第一个参数为该执行程序最终编译后生成的文件名，后面跟着的都是需要编译的源代码。 对于新手而言，其实压根不需要自己手写cmake，因为CLion会在你新建源文件的时候把相应源文件添加到add_excutable命令的后面。 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:5:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"常用的cmake变量（入门） 下面只列出了部分变量的作用，更多的变量请查看文档：https://cmake.org/cmake/help/latest/manual/cmake-variables.7.html PROJECT_NAME：项目名称 PROJECT_BINARY_DIR：项目的二进制文件目录，即编译后的可执行文件和库文件的输出目录 PROJECT_SOURCE_DIR：项目的源文件目录，即包含CMakeLists.txt文件的目录 CMAKE_BINARY_DIR：当前CMake运行的二进制文件目录，通常和PROJECT_BINARY_DIR是同一个目录 CMAKE_SOURCE_DIR：当前CMake运行的源文件目录，通常和PROJECT_SOURCE_DIR是同一个目录 CMAKE_C_STANDARD：指定C语言的标准版本 CMAKE_CXX_STANDARD：指定C++语言的标准版本 CMAKE_CXX_FLAGS：指定编译C++代码时使用的编译选项 CMAKE_C_FLAGS：指定编译C代码时使用的编译选项 CMAKE_EXE_LINKER_FLAGS：指定链接可执行文件时使用的链接选项 CMAKE_SYSTEM_NAME：指定当前操作系统名称（如Windows、Linux等） CMAKE_SYSTEM_PROCESSOR：指定当前处理器的类型（如x86、x86_64等） CMAKE_CXX_COMPILER_ID：指定了当前使用的C++编译器，同理可得C的编译器对应的名字。 对这些变量做一个简单的实践： 通过message打印出PROJECT_BINARY_DIR、PROJECT_SOURCE_DIR、CMAKE_BINARY_DIR、CMAKE_SOURCE_DIR来加以验证。 目录结构： .\r├── CMakeLists.txt\r├── main.cpp\r└── sub\r└── CMakeLists.txt\rcmake: main: cmake_minimum_required(VERSION 3.14)project(main)add_subdirectory(sub)message(STATUS \"main:${PROJECT_NAME}\\n pro-src:${PROJECT_SOURCE_DIR}\\n pro-bin:${PROJECT_BINARY_DIR}\\n cmake-src:${CMAKE_SOURCE_DIR}\\n cmake-bin:${CMAKE_BINARY_DIR}\")sub: project(sub)message(STATUS \"sub:${PROJECT_NAME}\\n pro-src:${PROJECT_SOURCE_DIR}\\n pro-bin:${PROJECT_BINARY_DIR}\\n cmake-src:${CMAKE_SOURCE_DIR}\\n cmake-bin:${CMAKE_BINARY_DIR}\")打印信息如下：我们发现CMake对应的变量没有变化，而Prject有了变量，因为我们在sub也使用了project命令。 通过变量检测环境执行不同的cmake代码： #判断当前的操作系统 if (CMAKE_SYSTEM_NAME MATCHES \"Linux\") target_link_libraries(my-logger PUBLIC fmt-header-only pthread) message(STATUS \"Now is Linux\")elseif (CMAKE_SYSTEM_NAME MATCHES \"Windows\") target_link_libraries(my-logger PUBLIC fmt-header-only ws2_32) message(STATUS \"Now is windows\")endif ()#判断当前使用的编译器 if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") # Do something for GCC elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\") # Do something for Intel C++ elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"Microsoft\") # Do something for Microsoft Visual C++ elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") # Do something for Clang endif()#判断当前的系统架构 if (CMAKE_SYSTEM_PROCESSOR MATCHES \"i.86|x86|x86_64|AMD64\") # Do something for x86 architecture elseif (CMAKE_SYSTEM_PROCESSOR MATCHES \"^(arm|aarch64)\") # Do something for ARM architecture elseif (CMAKE_SYSTEM_PROCESSOR MATCHES \"^(mips|mipsel|mips64)\") # Do something for MIPS architecture elseif (CMAKE_SYSTEM_PROCESSOR MATCHES \"^(powerpc|ppc64)\") # Do something for PowerPC architecture endif() 通过调整链接时的flag防止动态链接，因为如果你是使用Windows平台下的编译工具链，CLion有些时候最终链接并不是采用静态链接，导致你最终生成的可执行程序没法直接执行，这个时候你就需要使用下面的命令来强制静态链接了： set(CMAKE_EXE_LINKER_FLAGS \"-static\") ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:5:1","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"常用的cmake命令（入门） 下列只列出了部分命令，如果你以后有需要用到的其他命令，请前往官网进行查询：https://cmake.org/cmake/help/latest/manual/cmake-commands.7.html 我个人较为常用的命令： project：用于定义项目名称、版本号和语言。 add_executable：用于添加可执行文件。第一个参数很重要，被称为target，可以作为target_xxx命令的接收对象。 add_library：用于添加库文件，可以创建静态库或动态库。第一个参数很重要，被称为target，可以作为target_xxx命令的接收对象。简单使用如下 add_library(test_lib a.cc b.cc) #默认生成静态库 add_library(test_lib SHARED a.cc b.cc) #默认生成静态库 add_definitions：用于添加宏定义，注意该命令没有执行顺序的问题，只要改项目中用了该命令定义宏，那么所有的源代码都会被定义这个宏 add_definitions(-DFOO -DBAR ...) 。 add_subdirectory：用于添加子项目目录，如果有该条语句，就先会跑去执行子项目的cmake代码，这样会导致一些需要执行后立马生效的语句作用不到，比如include_directories和link_directories如果执行在这条语句后面，则他们添加的目录在子项目中无法生效。有些命令如target_include_directories和target_link_directories是根据目标target是否被链接使用来生效的，所以这些命令的作用范围与执行顺序无关，且恰好同一个cmake项目中产生的库文件是可以直接通过名称链接的，无论链接对象是在子目录还是父目录 target_link_libraries：用于将可执行文件或库文件链接到库文件或可执行文件。身为target_xxx的一员，很明显第二个参数也可以进行权限控制。 include_directories：用于指定头文件搜索路径，优点是简单直接，缺点是无法进行权限控制，一旦被执行后，后续的所有代码都能搜索到对应的文件路径。 target_include_directories：指定头文件搜索路径，并将搜索路径关联到一个target上，这里的target一般是指生成可执行程序命令里的target或者生成库文件的target，与上一个命令的不同点在于可以设置导出权限，比如现在我写了一个项目，这个项目引入了其他库，但是我不想让其他库的符号暴露出去（毕竟使用这个项目的人只关注这个项目的接口，不需要关注其他依赖的接口）可以通过PRIVATE将头文件搜索目录设置不导出的权限。 link_directories：与前面的include_directories命令类似，添加的是库的搜索路径。 target_link_directories：和前面的include版本一样的，只是改成了库路径。 if\\elseif\\endif ，在编程语言立马已经用烂了，现在主要是了解 if(condition) 中的条件到底如何判断的，以及内部都支持哪些操作，比如大于等于啥的，这方面直接看官方文档吧，非常好懂：https://cmake.org/cmake/help/latest/command/if.html aux_source_directory：这个指令简单实用，第一个参数传递一个文件目录，它会扫描这里面所有的源文件放到第二个参数定义的变量名中。注意第一个参数只能是文件夹。 aux_source_directory(${PROJECT_SOURCE_DIR} SRC) file：可以说是上面那个命令的增强版本，但如果熟悉这个命令的朋友肯定很快站出来反对，因为这个命令实在是太强大了，你如果翻一翻这个官方文档就会发现它具备几乎文件系统的所有功能，什么读写文件啊，什么从网上下载文件，本地上传文件之类的它都有，计算文件的相对路径，路径转化等等。但我们平时用到的最多的命令还是用来获取文件到变量里。比如file(GLOB FILES “文件路径表示1” “文件路径表示2” …) GLOB会产生一个由所有匹配globbing表达式的文件组成的列表，并将其保存到第二个参数定义的变量中。Globbing 表达式与正则表达式类似，但更简单，比如如果要实现前一个命令的功能可以这么写： file(GLOB SRC \"${PROJECT_SOURCE_DIR}/*.cc\")如果GLOB 换成GLOB_RECURSE ，那么上述命令将递归的搜寻其子目录的所有符合条件的文件，而不仅仅是一个层级。 execute_process：用于执行外部的命令，如下的示例代码是执行git clone命令，执行命令的工作目录在 ${CMAKE_BINARY_DIR}/deps/ execute_process(COMMAND git clone https://github.com/\u003cusername\u003e/\u003crepository\u003e.git WORKING_DIRECTORY ${CMAKE_BINARY_DIR}/deps/\u003crepository\u003e) message：打印出信息用于debug。 option：用于快速设置定义变量并赋值为对应的bool值，常被用于判断某些操作是否执行。 find_package：用于查找外界的package，其实就是查找外界对应的 \u003cpackage\u003eConfig.cmake 和 Find\u003cpackage\u003e.cmake 文件，这些文件里有外界包对应的变量信息以及库和头文件的各种路径信息。我们需要注意一些有关 find_package 命令查找 Config.cmake 路径的变量： CMAKE_PREFIX_PATH 变量是一个路径列表，CMake 会在这些路径中搜索包的 Config.cmake 文件。 \u003cPackage\u003e_DIR 变量是指向包的 Config.cmake 文件的路径。如果你手动设置了这个变量，那么 find_package 命令就可以找到包的信息。 同时他的一些常用参数如下： CONFIG ：显式指定find_package去查找 \u003cpackage\u003eConfig.cmake 文件，一般只要你在变量里面指定了 \u003cpackage\u003eConfig.cmake的路径，那么该参数填不填都没差别。我建议最好还是带上该参数比较好。 REQUIRED ：该参数表示如果没找到，那么直接产生cmake错误，退出cmake执行过程，如果没有REQUIRED，则即使没找到也不会终止编译。 PATHS ：这个参数的效果和前面的变量类似，也是指定查找的路径。 COMPONENTS ：用于指定查找的模块，模块分离在不同的文件中，需要使用哪个就指定哪个模块。典型的就是使用Qt时的cmake代码，比如 find_package(Qt5 COMPONENT Core Gui Widgets REQUIRED) 。 VERSION：可能有很多个不同版本的包，则需要通过该参数来指定，如：find_package(XXX VERSION 1.2.3)。 include：从文件或模块加载并运行 CMake 代码。我用这个命令实际上只是为了使用 FetchContent¶ 这个module的功能，该功能是从cmake3.11开始支持的，使用该module前需要通过include命令加载该模块，命令如下： include(FetchContent) FetchContent：这是一个模块功能，它用来从代码仓库中拉取代码，例如我要把最近写的日志库引入到当前的项目中使用（注意这中间不会有任何代理，所以拉取GitHub的仓库可能失败）： include(FetchContent)#引入功能模块 FetchContent_Declare( my-logger #项目名称 GIT_REPOSITORY https://github.com/ACking-you/my-logger.git #仓库地址 GIT_TAG v1.6.2 #仓库的版本tag GIT_SHALLOW TRUE #是否只拉取最新的记录 )FetchContent_MakeAvailable(my-logger)add_excutable(main ${SRC})#链接到程序进行使用 target_link_libraries(main my-logger)这样引入第三方库的好处显而易见，优点类似于包管理的效果了，但缺少了最关键的中心仓库来确保资源的有效和稳定。参考golang再做个proxy层级就好了。 同样可以拉取最新的googletest可以使用下列语句： FetchContent_Declare( googletest GIT_REPOSITORY https://github.com/google/googletest.git GIT_TAG release-1.12.1 GIT_SHALLOW TRUE )# For Windows: Prevent overriding the parent project's compiler/linker settings set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)FetchContent_MakeAvailable(googletest)target_link_libraries(main gtest_ma","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:5:2","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["开发环境配置"],"content":"ideaVim的使用与设置 关于vim的基本操作，这里就不细讲，网络上大把大把的教程，我这里只讲在CLion中怎么使用vim，怎么配置vim让它在CLion中更好用。 在使用ideaVim前，我们需要先下载 IdeaVim 这个拓展，ideaVim的链接：https://plugins.jetbrains.com/plugin/164-ideavim，下载好后，应该是默认启用了的。 一、配置ctrl+c和ctrl+v和ctrl+a防止这些你常用的快捷键被vim占用 二、配置.ideavimrc。 点击 Open ~/.ideavimrc 后，加入下面的配置（这是我个人使用的配置） let mapleader = \",\"\rlet g:mapleader = \",\"\rset timeoutlen=300\r\" general\rimap jk \u003cEsc\u003e\rimap kj \u003cEsc\u003e\rvnoremap q \u003cEsc\u003e\rnmap \u003cC-o\u003e :action Back\u003cCR\u003e\rnmap \u003cC-i\u003e :action Forward\u003cCR\u003e\rnnoremap \u003cLeader\u003er :\u003cC-u\u003eaction RenameElement\u003cCR\u003e\r\" Redo\rnnoremap U \u003cC-r\u003e\r\" code editing\rnnoremap == :\u003cC-u\u003eaction ReformatCode\u003cCR\u003e\rvnoremap == :\u003cC-u\u003eaction ReformatCode\u003cCR\u003e\rnnoremap cc :\u003cC-u\u003eaction CommentByLineComment\u003cCR\u003e\rvnoremap cc :\u003cC-u\u003eaction CommentByLineComment\u003cCR\u003e\r\" Run and debug\rnnoremap \\r :action RunClass\u003ccr\u003e\rnnoremap \\i :\u003cC-u\u003eaction OptimizeImports\u003cCR\u003e\rnnoremap \\R :action Run\u003ccr\u003e\rnnoremap [[ :action MethodUp\u003ccr\u003e\rnnoremap ]] :action MethodDown\u003ccr\u003e\r加入后点击右上角的小图标进行刷新即可。 上述配置文件实现了下面的功能： 按下 j+k 或者 k+j 将会退出插入模式。 在可视模式下按q退出。 ctrl+o可以实现指针回退到上一次的位置，ctrl+i前进到上次的位置。可以类比为VS李的前进和后退按钮（一般查看定义查看的比较深，可以使用该命令回退或前进）。 , + r 可以实现CLion中的重构变量名。 U取消撤销（由于vim中按u是撤销，而取消撤销需要ctrl+r，这让人很不习惯） = + =实现代码格式化。 c + c实现代码注释 [ + [跳转到前一个函数位置 ] + ]跳转到下一个函数位置 \\ + r 运行当前的可执行程序（如果当前代码段里面有运行按钮的话） \\ + R 运行当前可执行程序，这个是直接相当于按了右上角的三角按钮运行 \\ + i 优化导入的包 ","date":"2022-12-29","objectID":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/:6:0","tags":["CLion开发环境配置完全解析"],"title":"CLion开发环境配置完全解析","uri":"/posts/clion%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90/"},{"categories":["操作系统"],"content":"chatgpt教我内存对齐","date":"2022-12-19","objectID":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/","tags":["chatgpt教我内存对齐"],"title":"chatgpt教我内存对齐，对齐了但没完全对齐？","uri":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["操作系统"],"content":"内存对齐 ","date":"2022-12-19","objectID":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:0:0","tags":["chatgpt教我内存对齐"],"title":"chatgpt教我内存对齐，对齐了但没完全对齐？","uri":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["操作系统"],"content":"关于chatgpt的回答 我与chatgpt的对话如下： 我现在来描述与总结上述对话都干了啥以及我为什么要问这个。 我本来是在学习rapidjson源码里面的内存池实现，然后 RAPIDJSON_ALIGN 没有看懂，所以来问chatgpt。源码在：github.com/Tencent/rapidjson/blob/master/include/rapidjson/allocators.h 看了回答后结合自己的思考立马就懂了，然后出现一个新的疑问，内存对齐是怎么判断的？结果他给我的公式是没有什么问题，但最后给我算出来的结果却有比较大的问题。它举例说明的 8 字节不是以 4字节边界对齐的，这个很明显是错误的。 最后我问了下为什么要内存对齐？这个点说实话只要不是亲身写过一个内存池，真的就只会是字面上的理解。 ","date":"2022-12-19","objectID":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:0","tags":["chatgpt教我内存对齐"],"title":"chatgpt教我内存对齐，对齐了但没完全对齐？","uri":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["操作系统"],"content":"总结 得到两个公式的运用与理解： 判断值 v 是否按 x 内存对齐：(v \u0026 (x - 1)) == 0 ，这其实是很简单的道理，首先这个公式只能运用于2的n次方，其次这是一个很明显的利用位运算的取模操作，只能运用于 2 的 n 次方的原因在于 只有满足这种情况 x-1 后才能是连续完整的二进制填充位（比如 01000 -1 = 00111）。 得到值 v 按 x 向上对齐的结果：(v + (x-1)) \u0026 (~(x-1))，这里的x同样只能是2的n次方，这也是一个很简单的道理，我举个例子你就懂了。如果要让 123 按照 8 字节边界对齐，我们首先写出 123 对应的二进制 1111011 ，8为 1000，想要让123按8字节对齐，那么123就需要是8的倍数，由于8为2的3次方，所以只要某个数的二进制在第4位二进制之前没有1，则必然是 8 的倍数，原理很简单，由于8只有一个二进制位为1，而把这个二进制位左移则是*2，右移是/2，假设有个 1101000 的数，那么它分解后就是 8左移2位+8左移3位+8，所以必然是8的整数倍。现在回到上述公式 \u0026~(x-1) 的作用是把在 x 的二进制1右边的位置0，这样就保障了这个数必然是x的倍数，(v+(x-1)) 则保证了最终得到的数要大于等于原来的数，这就是所谓向上对齐。 chatgpt在编程或者说理科这种答案比较绝对的问题上也会出岔子，会出现给出的式子或代码逻辑没什么问题，但是它最终给出的判断可能是错误的，需要人为的进行思考。但这也让人在问问题的同时，自己也带入思考，而不是简单的copy，这点对自我学习的同学是很好的。 为什么要内存对齐，好处与坏处： 内存对齐是将内存地址向某个特定值进行对齐的过程。这个特定值称为对齐值。一般来说，内存对齐是为了提高内存访问效率。 在计算机中，内存访问的速度与内存地址有关。通常，计算机能够更快地访问某些内存地址，而对于其他内存地址，访问速度则会变慢。例如，对于许多计算机系统，当内存地址以 4 字节（纠正：需要看你电脑的默认字长，现在一般都是8字节）为单位对齐时，访问速度会更快。 因此，在设计内存布局时，通常会考虑内存对齐的因素。例如，如果你要存储一个结构体，可能会希望将结构体的首地址对齐到 4 字节边界，以便在访问结构体中的成员时能够提高访问效率。 内存对齐后，可以带来以下几点好处： 提高内存访问效率。如果内存地址以特定值对齐，那么访问内存的速度可能会更快。 减少内存碎片。如果内存地址不进行对齐，那么可能会产生许多小的内存块，这些内存块称为内存碎片。内存碎片会导致内存利用率降低，因为这些小的内存块可能无法被有效利用。如果内存地址进行对齐，则可以减少内存碎片的产生，从而提高内存利用率（其实就是如果你不对齐，你想申请多少就给你多少，由于内存一般都是以2的次方为倍数申请的，假设你申请一个奇数或者质数长度的，那么会导致很多内存没有被利用）。 内存对齐也可能带来一些潜在的问题，例如： 增加内存占用。如果内存地址进行对齐，则可能会增加内存的占用。例如，如果你要存储一个结构体，并且将结构体的首地址对齐到 4 字节边界，那么可能会增加 3 字节的内存占用。 增加代码复杂度。如果你要手动实现内存对齐，则可能会增加代码的复杂度。例如，你可能需要编写额外的代码来计算对齐后的内存地址。 因此，在设计内存布局时，需要权衡内存对齐带来的好处和问题，并在合理的情况下使用内存对齐。 个人总结：手动实现内存对齐，基本上只有在你需要写一个内存池的时候需要考虑到。但有些时候也必须意识到这个东西的存在，否则甚至会导致程序发生严重的内存错误。比如需要读取 char* 数据进行反序列化的时候，假设此时 char* 数据只存了一个 double 这个时候你可能会想到直接强转为double类型，但这样做其实是错误的，因为 char* 的地址可能没有按照对应的类型去对齐，所以可能产生不可预知的内存错误，这个时候最好的做法是把创建一个新的 double 变量，然后从 char* 所指的数据里初始化，重新开辟一片对应类型的内存空间，那么它的地址肯定是按照这个类型对齐的。 ","date":"2022-12-19","objectID":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:0","tags":["chatgpt教我内存对齐"],"title":"chatgpt教我内存对齐，对齐了但没完全对齐？","uri":"/posts/chatgp%E6%95%99%E6%88%91%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["数据结构——并查集"],"content":"关于并查集的一切","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"并查集初识 如果给你一些顶点，并且告诉你每个顶点的连接关系，你如何才能快速的找出两个顶点是否具有连通性呢？如「图 5. 连通性问题」，该图给出了顶点与顶点之间的连接关系，那么，我们如何让计算机快速定位 (0, 3) , (1, 5), (7, 8) 是否相连呢？此时我们就需要机智的「并查集」数据结构了。很多地方也会称「并查集」为算法，这也没问题。「并查集」的主要作用是用来解决网络中的连通性。这里的「网络」可以是计算机的网络，也可以是人际关系的网络等等。例如，你可以通过「并查集」来判定两个人是否来自同一个祖先。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:1:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"「并查集」常用术语 父节点：顶点的直接父亲节点。如「图5. 连通性问题」中，顶点 3 的父节点是 1；顶点 2 的父节点是 0；顶点 9 的父节点是自己本身 9。 根节点：没有父节点的节点，本身可以视为自己的父节点。如「图5. 连通性问题」中，顶点 3 和 2 的根节点都是 0；0 即是自己本身的父节点，也是自己的根节点；顶点 9 的根节点是自己本身 9。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:1:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"「并查集」基本思想 视频内容摘要： 如何在计算机中设计出「并查集」数据结构 「并查集」的 find 函数； 「并查集」的 union 函数。 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:1:2","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"「并查集」的两个实现方式 Quick Find 实现方式：它指的是实现「并查集」时，find 函数时间复杂度很低为 O(1)O(1)，但对应的 union 函数就需要承担更多的责任，它的时间复杂度为 O(N)O(N)。 Quick Union 实现方式：它指的是实现「并查集」时，相对于 Quick Find 的实现方式，我们通过降低 union 函数的职责来提高它的效率，但同时，我们也增加了 find 函数的职责。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:1:3","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"Quick Find 方式实现并查集 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:2:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"Quick Find工作原理： 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:2:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"代码实现与验证 #include \u003cbits/stdc++.h\u003eusing namespace std; class UnionFind{ private: int* root; int length; public: UnionFind(int size):length(size){ root = new int[size]; for(int i=0;i\u003clength;i++){ root[i] = i; } } int find(int x){ return root[x]; } void merge(int x,int y){ int rootX = find(x); int rootY = find(y); if(rootX!=rootY){ for(int i=0;i\u003clength;i++){ if(root[i]==rootY) root[i] = rootX; } } } bool isconnected(int x,int y){ return find(x)==find(y); } }; int main(){ UnionFind a(10); a.merge(1,2); a.merge(2,5); a.merge(5,6); a.merge(6,7); a.merge(3,8); a.merge(8,9); cout\u003c\u003ca.isconnected(1,5)\u003c\u003c' '\u003c\u003ca.isconnected(5,7)\u003c\u003c' '; cout\u003c\u003ca.isconnected(4,9)\u003c\u003c' '; a.merge(9,4); cout\u003c\u003ca.isconnected(4,9); } ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:2:2","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"时间复杂度 UnionFind 构造函数 find 函数 merge函数 isconnected 函数 时间复杂度 O(N) O(1) O(N) O(1) 注：N 为「图」中顶点的个数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:2:3","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"Quick Union 方式实现并查集 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:3:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"Quick Union的工作原理 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:3:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"为什么 Quick Union 比 Quick Find 更加高效？ 总体来说，Quick Union 是比 Quick Find 更加高效的。为什么呢？ 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:3:2","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"代码实现与验证 #include \u003cbits/stdc++.h\u003eusing namespace std; class UnionFind{ private: int* root; int length; public: UnionFind(int size):length(size){ root = new int[size]; for(int i=0;i\u003clength;i++){ root[i] = i; } } int find(int x){ while(x!=root[x]){ x = root[x]; } return x; } void merge(int x,int y){ int rootX = find(x); int rootY = find(y); if(rootX!=rootY){ root[rootY] = rootX; } } bool isconnected(int x,int y){ return find(x)==find(y); } }; int main(){ UnionFind a(10); a.merge(1,2); a.merge(2,5); a.merge(5,6); a.merge(6,7); a.merge(3,8); a.merge(8,9); cout\u003c\u003ca.isconnected(1,5)\u003c\u003c' '\u003c\u003ca.isconnected(5,7)\u003c\u003c' '; cout\u003c\u003ca.isconnected(4,9)\u003c\u003c' '; a.merge(9,4); cout\u003c\u003ca.isconnected(4,9); } ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:3:3","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"时间复杂度 UnionFind 构造函数 find 函数 merge函数 isconnected 函数 时间复杂度 O(N) O(H) O(H) O(H) 注：N 为「图」中顶点的个数，H 为「树」的高度。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:3:4","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"按秩合并优化并查集 小伙伴看到这里的时候，我们其实已经实现了 2 种「并查集」。但它们都有一个很大的缺点，这个缺点就是通过 merge 函数连接顶点之后，可能所有顶点连成一条线形成「图 5. 一条线的图」，这就是我们 find 函数在最坏的情况下的样子。那么我们有办法解决吗？ 当然，伟大的科学家已经给出了解决方案，就是按秩合并。这里的「秩」可以理解为「秩序」。之前我们在 merge 的时候，我们是随机选择 x 和 y 中的一个根节点/父节点作为另一个顶点的根节点。但是在「按秩合并」中，我们是按照「某种秩序」选择一个父节点。 这里的「秩」指的是每个顶点所处的高度。我们每次 merge 两个顶点的时候，选择根节点的时候不是随机的选择某个顶点的根节点，而是将「秩」大的那个根节点作为两个顶点的根节点，换句话说，我们将低的树合并到高的树之下，将高的树的根节点作为两个顶点的根节点。这样，我们就避免了所有的顶点连成一条线，这就是按秩合并优化的「并查集」。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:4:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"视频讲解 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:4:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"代码实现与验证 #include \u003cbits/stdc++.h\u003eusing namespace std; class UnionFind{ private: int* root; int* rank; int length; public: UnionFind(int size):length(size){ root = new int[size]; rank = new int[size]; for(int i=0;i\u003clength;i++){ root[i] = i; rank[i] = 1; } } int find(int x){ while(x!=root[x]){ x = root[x]; } return x; } void merge(int x,int y){ int rootX = find(x); int rootY = find(y); //高度小的树被高度大的合并，如果高度一致合并后高度增加 if(rootX!=rootY){ if(rank[rootX]\u003erank[rootY]){ root[rootY] = rootX; } else if(rank[rootX]\u003crank[rootY]){ root[rootX] = rootY; }else{ root[rootY] = rootX; rank[rootX]++; } } } bool isconnected(int x,int y){ return find(x)==find(y); } }; int main(){ UnionFind a(10); a.merge(1,2); a.merge(2,5); a.merge(5,6); a.merge(6,7); a.merge(3,8); a.merge(8,9); cout\u003c\u003ca.isconnected(1,5)\u003c\u003c' '\u003c\u003ca.isconnected(5,7)\u003c\u003c' '; cout\u003c\u003ca.isconnected(4,9)\u003c\u003c' '; a.merge(9,4); cout\u003c\u003ca.isconnected(4,9); } UnionFind 构造函数 find 函数 merge函数 isconnected 函数 时间复杂度 O(N) O(logN) O(logN) O(logN) 注：N 为「图」中顶点的个数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:4:2","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"路径压缩优化的并查集 从前面的「并查集」实现方式中，我们不难看出，要想找到一个元素的根节点，需要沿着它的父亲节点的足迹一直遍历下去，直到找到它的根节点为止。如果下次再查找同一个元素的根节点，我们还是要做相同的操作。那我们有没有什么办法将它升级优化下呢？ 答案是可以的！如果我们在找到根节点之后，将所有遍历过的元素的父节点都改成根节点，那么我们下次再查询到相同元素的时候，我们就仅仅只需要遍历两个元素就可以找到它的根节点了，这是非常高效的实现方式。那么问题来了，我们如何将所有遍历过的元素的父节点都改成根节点呢？这里就要拿出「递归」算法了。这种优化我们称之为「路径压缩」优化，它是对 find 函数的一种优化。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:5:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"视频讲解 视频链接 实际路径压缩应该还有一种迭代的方式，此视频未提到。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:5:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"代码实现(路径压缩+按秩合并) #include \u003cbits/stdc++.h\u003eusing namespace std; class UnionFind{ private: int* root; // 添加了 rank 数组来记录每个顶点的高度，也就是每个顶点的「秩」 int* rank; int length; public: UnionFind(int size):length(size){ root = new int[size]; rank = new int[size]; for(int i=0;i\u003clength;i++){ root[i] = i; rank[i] = 1; } } int find(int x){ //递归方式 if(x==root[x]) return x; return root[x]=find(root[x]); /*迭代方式 int cur = x; while(cur!=root[cur]){ root[cur] = root[root[cur]]; cur = root[cur]; } return cur;*/ } // 按秩合并优化的 merge 函数 void merge(int x,int y){ int rootX = find(x); int rootY = find(y); //高度小的树被高度大的合并，如果高度一致合并后高度增加 if(rootX!=rootY){ if(rank[rootX]\u003erank[rootY]){ root[rootY] = rootX; } else if(rank[rootX]\u003crank[rootY]){ root[rootX] = rootY; }else{ root[rootY] = rootX; rank[rootX]++; } } } bool isconnected(int x,int y){ return find(x)==find(y); } }; int main(){ UnionFind a(10); a.merge(1,2); a.merge(2,5); a.merge(5,6); a.merge(6,7); a.merge(3,8); a.merge(8,9); cout\u003c\u003ca.isconnected(1,5)\u003c\u003c' '\u003c\u003ca.isconnected(5,7)\u003c\u003c' '; cout\u003c\u003ca.isconnected(4,9)\u003c\u003c' '; a.merge(9,4); cout\u003c\u003ca.isconnected(4,9); } UnionFind 构造函数 find 函数 merge函数 isconnected 函数 时间复杂度 O(N) O(⍺(N)) O(⍺(N)) O(⍺(N)) 注：N 为「图」中顶点的个数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:5:2","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"综合运用例题 省份数量 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:6:0","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——并查集"],"content":"解题代码 这效率yyds！ class Solution { public: int findCircleNum(vector\u003cvector\u003cint\u003e\u003e\u0026 isConnected) { int n = isConnected.size(); root = new int[n]; rank = new int[n]; for(int i=0;i\u003cn;i++){ root[i] = i; rank[i] = 1; } cnt = n; for(int i=0;i\u003cn;i++){ for(int j=i+1;j\u003cn;j++){ if(isConnected[i][j]) merge(i,j); } } return cnt; } private: int* root; int* rank; int cnt; int find(int x){ if(root[x] == x) return x; return root[x] = find(root[x]); } void merge(int x,int y){ int rootX = find(x); int rootY = find(y); if(rootX!=rootY){ if(rank[rootX]\u003crank[rootY]) root[rootX] = rootY; else if(rank[rootX]\u003erank[rootY]) root[rootY] = rootX; else{ root[rootX] = rootY; rank[rootY]++; } //初始有多个集合，一旦合并一次就少一个集合 cnt--; } } }; ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/:6:1","tags":["关于并查集的一切"],"title":"关于并查集的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"关于最小生成树的一切","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"初识最小生成树 首先，小伙伴们可能要冒出第一个问题了。什么是生成树？生成树 指的是「无向图」中，具有该图的 全部顶点 且 边数最少 的连通子图。「图8. 生成树」中，所有粉色线条组成的一棵树[(A, B), (A, C), (A, D), (A, E)]，就是该无向图的其中一个生成树。其实[(A, E),(A, B), (B, C), (C, D)]也是该无向图的一个生成树。由此可见，一个「无向图」的生成树可以是多个。 那么再了解了什么是生成树后，小伙伴们可能又要冒出第二个问题了。什么是最小生成树。最小生成树指的是「加权无向图」中总权重最小的生成树。「图9. 最小生成树」中，所有绿色线条组成的一颗生成树[(A, E),(A, B), (B, C), (C, D)]，就是该加权无向图的其中一个最小生成树。其实[(A, E), (E, D), (A, B), (B, C)]也是该加权无向图的另一个最小生成树，由此可见，一个「加权无向图」的最小生成树可以是多个。 那么在该章节中，我们将学习下「生成最小生成树」的两种算法以及「切分定理」： 切分定理 Kruskal 算法 Prim 算法 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:1:0","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"切分定理 「切分」是什么呢？很多的定理都是以人的名字命名的，但是「切分」并不是一个人的名字。在「切分定理」中有两个基本概念，我们需要了解下： 切分：将「图」切成两个部分，称之为一个「切分」。「图 10. 切分图」就是一个「切分」，其中(B, A, E)为一个部分，(C, D)为另外一个部分。 横切边：如果一条边连接的两个顶点属于切分的两个部分，这个边称为「横切边」。在「图10. 切分图」中，(B, C), (A, C), (A, D), (E, D) 均为「横切边」。 再了解了切分定理的基础概念之后，我们就需要学习下「切分定理」了。切分定理是 Kruskal 算法和 Prim 算法的重要的理论支撑。那么什么是「切分定理」呢？根据 维基百科 的定义，「切分定理」指的是： 在一幅连通加权无向图中，给定任意的切分，如果有一条横切边的权值严格小于所有其他横切边，则这条边必然属于图的最小生成树中的一条边。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:2:0","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"切分定理的证明 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:2:1","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"Kruskal 算法(以边扩散) 「Kruskal 算法」是求解「加权无向图」的「最小生成树」的一种算法。 视频链接 时间复杂度: $O(E*logE)$ $E$ 表示边数。 空间复杂度： $O(V)$ $V$表示顶点数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:3:0","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"练习题–连接所有点的最小费用 视频讲解 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:3:1","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"解题代码 class Solution { // Kruskal Algorithm public int minCostConnectPoints(int[][] points) { if (points == null || points.length == 0) { return 0; } int size = points.length; PriorityQueue\u003cEdge\u003e pq = new PriorityQueue\u003cEdge\u003e((x, y) -\u003e x.cost - y.cost); UnionFind uf = new UnionFind(size); for (int i = 0; i \u003c size; i++) { for (int j = i+1; j \u003c size; j++) { int[] coordinate1 = points[i]; int[] coordinate2 = points[j]; // Calculate the distance between two coordinates. int cost = Math.abs(coordinate1[0] - coordinate2[0]) + Math.abs(coordinate1[1] - coordinate2[1]); Edge edge = new Edge(i, j, cost); pq.add(edge); } } int result = 0; int count = size - 1; while ( pq.size() \u003e 0 \u0026\u0026 count \u003e 0 ) { Edge e = pq.poll(); if ( !uf.connected(e.point1, e.point2)) { uf.union(e.point1, e.point2); result += e.cost; count--; } } return result; } class Edge { int point1; int point2; int cost; Edge(int point1, int point2, int cost) { this.point1 = point1; this.point2 = point2; this.cost = cost; } } class UnionFind { int root[]; int rank[]; public UnionFind(int size) { root = new int[size]; rank = new int[size]; for (int i = 0; i \u003c size; i++) { root[i] = i; rank[i] = 1; } } public int find(int x) { if (x == root[x]) { return x; } return root[x] = find(root[x]); } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX != rootY) { if (rank[rootX] \u003e rank[rootY]) { root[rootY] = rootX; } else if (rank[rootX] \u003c rank[rootY]) { root[rootX] = rootY; } else { root[rootY] = rootX; rank[rootX] += 1; } } } public boolean connected(int x, int y) { return find(x) == find(y); } } } ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:3:2","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"Prim算法(以顶点扩散) 「Prim 算法」是求解「加权无向图」的「最小生成树」的另一种算法。 视频链接 算法证明 视频链接 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:0","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"「Kruskal 算法」和 「Prim 算法」区别 在「Kruskal 算法」中，我们通过增加边数来扩大「最小生成树」； 在「Prim 算法」中，我们通过增加顶点来扩大「最小生成树」。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:1","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"时间复杂度 普通二叉堆：$O(ElogV)O(ElogV)$。 斐波那契堆：$O(E+VlogV)O(E+VlogV)$。 $V$ 表示顶点数，$E$ 表示边数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:2","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"空间复杂度 $O(V)$。 $V$表示顶点数。 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:3","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"练习题–连接所有点的最小费用 题目上面有图 视频讲解 ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:4","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["数据结构——最小生成树"],"content":"解题代码 class Solution { // Prim Algorithm public int minCostConnectPoints(int[][] points) { if (points == null || points.length == 0) { return 0; } int size = points.length; PriorityQueue\u003cEdge\u003e pq = new PriorityQueue\u003cEdge\u003e((x, y) -\u003e x.cost - y.cost); boolean[] visited = new boolean[size]; int result = 0; int count = size - 1; // Add all edges from points[0] vertexs for (int j = 1; j \u003c size; j++) { // Calculate the distance between two coordinates. int[] coordinate1 = points[0]; int[] coordinate2 = points[j]; int cost = Math.abs(coordinate1[0] - coordinate2[0]) + Math.abs(coordinate1[1] - coordinate2[1]); Edge edge = new Edge(0, j, cost); pq.add(edge); } visited[0] = true; while (pq.size() \u003e 0 \u0026\u0026 count \u003e 0) { Edge e = pq.poll(); int point1 = e.point1; int point2 = e.point2; int cost = e.cost; if ( !visited[point2] ) { result += cost; visited[point2] = true; for (int j = 0; j \u003c size; j++ ) { if ( !visited[j] ) { int distance = Math.abs(points[point2][0] - points[j][0]) + Math.abs(points[point2][1] - points[j][1]); pq.add(new Edge(point2, j, distance)); } } count--; } } return result; } class Edge { int point1; int point2; int cost; Edge(int point1, int point2, int cost) { this.point1 = point1; this.point2 = point2; this.cost = cost; } } } ","date":"2022-12-19","objectID":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/:4:5","tags":["关于最小生成树的一切"],"title":"关于最小生成树的一切","uri":"/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E4%B8%80%E5%88%87/"},{"categories":["Linux命令"],"content":"zip、gzip、bzip2、tar有何联系？","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"归类 我们把这几个命令归类为几种能力，一个是解压缩能力一个是打拆包能力。 我这里打包的意思是不使用压缩算法对文件进行压缩，只是简单的把多个文件归档为一个文件。而拆包则是它的逆过程。 压缩则是大家熟知的将文件以某种压缩算法对内容进行缩减，解压则是它的逆过程。 ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:1:0","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"zip与unzip命令 zip命令用于将文件压缩为 .zip 格式的文件，之所以将他定义为可打包拆包是因为他可以支持多文件的压缩，而其他的 gzip 和 bzip2 只能对单文件进行压缩，也就是说如果要压缩多文件，需要先经过打包过程变成单文件再压缩，.zip 格式的文件默认将打包和压缩放在一起了，而 zip 命令也不支持单独的打包再压缩。 ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:1:1","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"tar命令 不加入 -z 或 -j 参数，加入 -c 表示对文件或目录进行归档。 所以tar命令是支持单独的归档功能的，具体的压缩算法可以通过参数指定可以是 gzip 也可以是 bzip2 或其他。 ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:1:2","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"使用方式 ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:0","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"zip 命令格式 zip [-r] [压缩后文件名] [文件或目录] 命令描述 zip命令用来对文件进行打包操作。zip是个使用广泛的压缩程序，文件经它压缩后会另外产生具有“.zip”扩展名的压缩文件。 选项 -r: 递归处理，将指定目录下的所有文件和子目录一并处理，用于压缩目录 -x：压缩时排除符合条件的文件 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ zip a.zip a.txt #压缩文件 adding: a.txt (stored 0%) ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc a.txt a.zip ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ touch abc/tmp.txt ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ zip -r abc.zip abc/ -x abc/tmp.txt #压缩文件夹并忽略某些文件 adding: abc/ (stored 0%) ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:1","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"unzip 命令格式 unzip [选项] 文件名 命令描述 unzip命令用于解压缩由zip命令压缩的“.zip”压缩包。 选项 -n：解压缩时不要覆盖原有的文件； -o：不必先询问用户，unzip执行后覆盖原有的文件； -d \u003c目录\u003e：指定文件解压缩后所要存储的目录(不指定目录，默认解压到当前目录) 可配合tar命令对 tar.gz 文件进行解压缩和拆包 tar -zxvf 文件名 示例 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ unzip abc.zip -d ~ #解压到指定目录 Archive: abc.zip creating: /home/ljb/abc/ ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:2","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"gzip 命令格式 gzip [文件] 命令描述 经 gzip 压缩过后，其名称后面会多处 .gz 扩展名。 注意：gzip 只能压缩文件，不能压缩文件夹，压缩后原文件会被删除。 gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip压缩常常用在http的网络请求中。 示例 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ touch b.txt ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc abc.zip a.txt a.zip b.txt ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ gzip b.txt #压缩后生产b.txt.gz，原本的文件不见了 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc abc.zip a.txt a.zip b.txt.gz ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:3","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"gunzip 命令格式 gunzip [文件] 命令描述 gunzip命令用来解压缩 xxx.gz 文件。 示例 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc abc.zip a.txt a.zip b.txt.gz ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ gunzip b.txt.gz ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls #解压缩成功，得到b.txt文件 abc abc.zip a.txt a.zip b.txt ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:4","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"bzip2 命令格式 bzip2 [选项] [文件] 命令描述 bzip2命令用于压缩 .bz2 格式的压缩包，是gzip的升级版本，可以保留原文件。 bzip2的压缩比比较高，可用于压缩较大文件。 bzip2也是只对文件进行压缩，如果相对目录进行压缩的话，可以用其他命令打包成一个文件(如tar)。 选项 -k（keep）：保留原文件（不删除原文件） 示例 # bzip2 压缩文件 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ touch c.txt ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ bzip2 -k c.txt ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc abc.zip a.txt a.zip b.txt c.txt c.txt.bz2 # 配合使用tar 命令，完成打包压缩（后面会讲tar命令的使用） ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ mkdir music ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ tar -jcvf music.tar.bz2 music/ ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls abc abc.zip a.txt a.zip b.txt c.txt c.txt.bz2 music music.tar.bz2 ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:2:5","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"bunzip2 命令格式 bunzip2 [选项] [文件] 命令描述 解压缩 .bz2 格式的压缩文件； gunzip 的升级版，可以使用-k保留原文件； 可以配合使用tar命令，完成解压缩解包： tar -jxvf 文件名 选项 -k（keep）：保留原文件（不删除原文件） 示例 #删除只剩下c.txt.bz2 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ rm -rf !(c.txt.bz2) ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls c.txt.bz2 #解压bz2文件 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ bunzip2 c.txt.bz2 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls c.txt ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:3:0","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["Linux命令"],"content":"tar 命令格式 tar [选项] [压缩后文件名] [目录] 命令描述 打包目录，将目录打包成一个文件，同时可以压缩，可以自由选择压缩算法。 使用tar命令时，如果想要打包并压缩一个目录，可以有两种方法进行： 先利用 tar 命令打包目录为一个文件，然后使用 gzip 或 bzip2 压缩。 直接利用 tar 命令打包并压缩 (简单方便，推荐使用)。 解压时也有两种方式： 先使用 gunzip 或 bunzip2 解压缩，再使用 tar 解包. 直接利用tar命令解压缩并解包 (简单方便，推荐使用) 选项 -c：打包 -x：拆包 -v：显示详细信息 -f：指定文件名 -z：表示使用 gzip 进行解压缩，压缩后的拓展名为 .tar.gz，这个拓展名在下载Linux相关安装包时很常见。 -j：表示使用 bzip2 进行解压缩，压缩后拓展名为 .tar.bz2。 一般来说，-vf 参数肯定会带上的，根据打包/拆包选择-c或-x，根据使用的压缩算法选择 -z 或 -j。 示例 # 1、2为打包压缩的两种方法 # 1. 使用 tar -cvf 打包，然后使用gzip压缩 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ mkdir movie ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$# tar -cvf movie.tar movie/ movie/ ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie.tar ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ gzip movie.tar ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie.tar.gz # 2. 使用 tar -zcvf 打包并压缩 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ tar -zcvf movie2.tar.gz movie movie/ ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie2.tar.gz movie.tar.gz # 3、4为解压缩并解包的两种方法 # 3. 先使用gunzip解压缩，然后 tar -xvf解包 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie2.tar.gz movie.tar.gz ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ rm -rf movie ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie2.tar.gz movie.tar.gz ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ gunzip movie.tar.gz ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie2.tar.gz movie.tar ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ tar -xvf movie.tar movie/ ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie2.tar.gz movie.tar # 4. 使用tar -zxvf 解压缩并解包 ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie2.tar.gz movie.tar ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ rm -rf movie ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ tar -zxvf movie2.tar.gz movie/ ljb@Wangzhe0cnmdUTF-8:/mnt/d/linux_shell$ ls movie movie2.tar.gz movie.tar ","date":"2022-12-04","objectID":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/:3:1","tags":["zip、gzip、bzip2、tar有何联系？"],"title":"zip、gzip、bzip2、tar有何联系？","uri":"/posts/zipgzipbzip2tar%E6%9C%89%E4%BD%95%E8%81%94%E7%B3%BB/"},{"categories":["性能对比测试"],"content":"比较AVL树和红黑树的性能差异","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["性能对比测试"],"content":"缘起 最近在复习数据结构，顺便把以前自己写的博客简单的看了一遍，然后发现了一篇手写AVL树的博客 徒手写的AVL竟然比STL中的红黑树效率更高？✨，看了下代码，风格确实不忍直视，尤其比较草率的测试方式。所以我决定重新对 AVL 和 stl的红黑树进行测试对比，顺便也复习下左旋、右旋、寻找结点的前驱后继，方便为手写红黑树打基础。 ","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/:1:0","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["性能对比测试"],"content":"性能测试 ","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/:2:0","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["性能对比测试"],"content":"测试代码 #include \u003ciostream\u003e#include\u003cvector\u003e#include\u003cset\u003e#include \"AVLTree.h\"#include \"SimpleBenchTool4cpp/runtime_assert.hpp\"#include \"SimpleBenchTool4cpp/timer.hpp\" //测试规模 const int testNum = 1000000; //验证内容是否一致 void validSequence(AVLTree\u0026 avl, std::set\u003cint\u003e\u0026 rb) { auto avlIter = avl.begin(); auto rbIter = rb.begin(); while (avlIter != avl.end()) { assert(*avlIter == *rbIter); ++avlIter; ++rbIter; } assert(rbIter == rb.end()); } //产生随机序列 std::vector\u003cint\u003e genRandomNum() { srand(time(NULL)); std::vector\u003cint\u003e ret; for (int i = 0; i \u003c testNum; i++) { ret.push_back(rand() % testNum); } return ret; } //测试插入操作 void testAVLTreeInsert(AVLTree\u0026 avl, std::set\u003cint\u003e\u0026 rb, std::vector\u003cint\u003e\u0026 src) { { Timer t; for (auto n : src) { avl.insert(n); } } { Timer t; for (auto n : src) { rb.insert(n); } } validSequence(avl, rb); } //测试遍历操作 void testAVLTreeIter(AVLTree\u0026 avl, std::set\u003cint\u003e\u0026 rb) { int n1, n2; { Timer t; for (int num : avl) { n1 = num; } } { Timer t; for (int num : rb) { n2 = num; } } assert(n1 == n2); } //测试查找操作 void testAVLTreeFind(AVLTree\u0026 avl, std::set\u003cint\u003e\u0026 rb, std::vector\u003cint\u003e\u0026 src) { assert(avl.size() == rb.size()); { Timer t; for (auto\u0026\u0026 n : src) { assert(avl.find(n)); } } { Timer t; for (auto\u0026\u0026 n : src) { assert(rb.find(n) != rb.end()); } } } //测试删除操作 void testAVLErase(AVLTree\u0026 avl, std::set\u003cint\u003e\u0026 rb, std::vector\u003cint\u003e\u0026 src) { { Timer t; for (int n : src) { avl.remove(n); } } { Timer t; for (int n : src) { rb.erase(n); } } } int main() { AVLTree avl; std::set\u003cint\u003e rb; auto randomTestNum = genRandomNum(); std::cout \u003c\u003c \"-----------------insert--------------\" \u003c\u003c std::endl; testAVLTreeInsert(avl, rb, randomTestNum); std::cout \u003c\u003c \"-----------------iter--------------\" \u003c\u003c std::endl; testAVLTreeIter(avl, rb); std::cout \u003c\u003c \"-----------------find--------------\" \u003c\u003c std::endl; testAVLTreeFind(avl, rb, randomTestNum); std::cout \u003c\u003c \"-----------------erase--------------\" \u003c\u003c std::endl; testAVLErase(avl, rb, randomTestNum); } ","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/:2:1","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["性能对比测试"],"content":"结果分析 ","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/:2:2","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["性能对比测试"],"content":"手写AVL树获得了什么 ","date":"2022-11-30","objectID":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/:3:0","tags":["比较AVL树和红黑树的性能差异"],"title":"比较AVL树和红黑树的性能差异","uri":"/posts/%E6%AF%94%E8%BE%83avl%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%AE%E5%BC%82/"},{"categories":["C++元模板编程"],"content":"C++编译期反射——以AOP为例","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"编译期反射实践 自古以来，C++就一直缺少一个编程语言的重要特性——反射，但如果熟悉C++元模板编程的同学，就知道以C++的风格，肯定是不会在标准库中添加运行时的反射支持的，从最新的C++版本演进来看，倒是编译期反射可能得到更好的支持。C++11 -\u003e C++14 -\u003e C++17 -\u003e C++20… 不断让元模板编程变得更简单，更规范。 本次的编译期反射实践，代码要求的最低C++版本为14，因为用到了 make_shared、decay_t。 本次实践的完整代码仓库：MyUtil/tree/master/aop ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:0:0","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"获取类的方法 ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:1:0","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"判断类是否具有某方法 我们如何判断某个类是否具有某个方法呢？ 要想在编译期间实现这样一个判断，我们的思路可以是这样：写两个模板，如果这个类型具有这个方法，就匹配到返回 std::true_type() 的模板，如果不具备则匹配到返回 std::false_type() 的模板，最后通过 std::is_same 能够判断匹配结果，也就是实现了在编译期间判断类是否有这个方法。 上述过程，利用 SFINAE 的原理可以轻松实现，如果不了解 SFINAE 以及对应的 enable_if 的运用，可以看看这篇文章：C++模板进阶指南：SFINAE。 我们现在就开始动手实现上述代码，假设我们需要判断一个类型是否有 before() 方法。 template \u003ctypename T, typename... Args\u003e struct has_member_before { private: template \u003ctypename U\u003e static auto Check(int) -\u003e decltype( std::declval\u003cU\u003e().before(std::declval\u003cArgs\u003e()...),std::true_type() //1 ); template \u003ctypename U\u003e static std::false_type Check(...); //2 public: enum { value = std::is_same\u003cdecltype(Check\u003cT\u003e(0)), std::true_type\u003e::value //3 }; }; 先讲下上述代码定义后如何使用吧，比如现在有个 Student 类型，我们来判断是否具有 before 成员函数，则只需要写下下面的代码： has_member_before\u003cStudent,int\u003e::value //判断Student类是否有Student::before(int)方法 上面的代码重点有三段，已经作为标记1、2、3： 代码1处，利用了 std::declval 在编译期创建类型U的实例，并调用其 before 方法，这是在元模板中判断一个类型是否具有某个方法的常有手段，因为 SFINAE 的存在，即便该处替换出错，编译器会去继续寻找下一个替换是否能够正确，直到所有的替换都出错。 很明显这里是一定会替换成功的，因为代码2有一个包容性很强的重载，这个重载的参数不能和代码1处的重载参数一致，否则会算作重复定义，当然如果你使用 std::enable_if 对参数一致的模板参数进行唯一性的限制，那么重复定义的错误也可以避免。但是写成 C 的可变参数是最快的解决方式。 代码1处，有个逗号表达式的细节，如果成功被代码1处替换，那么返回值类型将会是 decltype() 中的表达式类型，也就是逗号表达式最后的结果 std::true_type。 代码3是利用enum类型在编译期得到具体的常量值。具体是通过调用 Check\u003cT\u003e(0) 获取该函数的返回值类型，这期间模板的匹配替换就会牵扯到前面的代码1、2。所以一旦模板被实例化，那么该class是否具有该方法的信息也就清楚了。 最后我们可以把该段代码提取为宏作为通用代码： #define HAS_MEMBER(member) \\ template \u003ctypename T, typename... Args\u003e struct has_member_##member { \\ private: \\ template \u003ctypename U\u003e \\ static auto Check(int) \\ -\u003e decltype(std::declval\u003cU\u003e().member(std::declval\u003cArgs\u003e()...), \\ std::true_type()); \\ template \u003ctypename U\u003e static std::false_type Check(...); \\ \\ public: \\ enum { \\ value = std::is_same\u003cdecltype(Check\u003cT\u003e(0)), std::true_type\u003e::value \\ }; \\ }; 如果想要生成判断是否有before或者其他方法的代码，则只需要调用这个宏。 HAS_MEMBER(before) //生成判断是否有before的代码\rHAS_MEMBER(after) //生成判断是否有after的代码\r","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:1:1","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"将类方法转为function保存 直接上代码，再逐一讲解： 以下代码是将该类的before和after方法包装成一个function，并返回一个pair。完整代码：reflect_util.hpp template \u003ctypename T, typename... Args\u003e typename std::enable_if\u003c //1 has_member_before\u003cT, Args...\u003e::value \u0026\u0026 has_member_after\u003cT, Args...\u003e::value, std::pair\u003cstd::function\u003cvoid(Args...)\u003e, std::function\u003cvoid(Args...)\u003e\u003e\u003e::type GetMemberFunc() { auto fun = std::make_shared\u003cstd::decay_t\u003cT\u003e\u003e(); //2 return std::make_pair( //3 [self = fun](Args \u0026\u0026...args) { self-\u003ebefore(std::forward\u003cArgs\u003e(args)...); }, [self = fun](Args \u0026\u0026...args) { self-\u003eafter(std::forward\u003cArgs\u003e(args)...); }); } 在代码段1中，通过 enable_if 确保在该类型有before和after方法，同时也可以保证写其他版本的时候不会出现重复定义的错误。enable_if 第一个参数是需要满足的条件，第二个参数是enable_if内部的type类型，默认为void。 代码段2中，创建一个T类型的实例，并用shread_ptr管理，原因在于before方法和after方法需要共用内存，而这两个方法都要被提取为单独的function，要保证内存安全，故需要使用智能指针。其中 std::decay_t\u003cT\u003e 效果等同于 std::decay\u003cT\u003e::type，作用是消除T类型的const修饰和引用修饰。因为make_shared\u003c\u003e中的模板参数不能为引用类型。 代码段3中，利用lamda表达式将fun拷贝一份到其中命名为self，最后返回pair即可。 当前写的功能是不完整的，需要多几个模板的重载来实现只有before方法、以及只有after方法的情况。写法和上述代码一致，只不过 enable_if 中的条件稍作改变即可。前面也提到过enable_if千万不能丢，否则会报重复定义的错误，当然如果你是C++17的版本，可以直接使用 if constexpr 来实现更为简洁的代码而无需单独写三个函数。 如下： #define ST_ASSERT \\ static_assert( \\ has_member_before\u003cT, Args...\u003e::value || \\ has_member_after\u003cT, Args...\u003e::value, \\ \"class need T::before(args...) or T::after(args...) member function!\"); template \u003ctypename T, typename... Args\u003e std::pair\u003cstd::function\u003cvoid(Args...)\u003e, std::function\u003cvoid(Args...)\u003e\u003e GetMemberFunc() { ST_ASSERT // 确保至少before after有其一 auto fun = std::make_shared\u003cstd::decay_t\u003cT\u003e\u003e(); if constexpr (has_member_before\u003cT, Args...\u003e::value \u0026\u0026 has_member_after\u003cT, Args...\u003e::value) { // 有before和after return std::make_pair( [self = fun](Args \u0026\u0026...args) { self-\u003ebefore(std::forward\u003cArgs\u003e(args)...); }, [self = fun](Args \u0026\u0026...args) { self-\u003eafter(std::forward\u003cArgs\u003e(args)...); }); } else if constexpr (has_member_before\u003cT, Args...\u003e::value \u0026\u0026 !has_member_after\u003cT, Args...\u003e::value) { // 有before return std::make_pair( [self = fun](Args \u0026\u0026...args) { self-\u003ebefore(std::forward\u003cArgs\u003e(args)...); }, nullptr); } else { // 只有after return std::make_pair(nullptr, [self = fun](Args \u0026\u0026...args) { self-\u003eafter(std::forward\u003cArgs\u003e(args)...); }); } } 下面我简单解释下代码： ST_ASSERT宏的作用是，通过static_assert在编译期抛出提示，T类型必须有before或after两个方法之一。 通过该类型拥有的情况不同，给出不同的返回值。 很明显去除了enable_if后，我们代码清爽了许多。 ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:1:2","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"AOP的实现 关于AOP，大家可以去搜一搜，这里就不过多赘述。我的简单理解就是一个事件回调，可以嵌入到业务的执行前后，把这个事件的概念换成一个切面，把业务代码看作一个横向坐标轴上的面，那么AOP就是在这个面的前后添加其他切面来实现常用的业务复用。比如用户的身份验证，可以在业务之前添加身份验证的切面，比如需要测试该业务的性能，那么可以在业务切面的前后添加开始计时和终止计时的逻辑。 ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:2:0","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"Invoke调用实现AOP 根据上述对AOP的描述，我们要切入的代码主要是前和后两个逻辑，故每个要切入的类可以规定他必须定义Before或者After方法。然后通过可变参模板递归实现任意个参数的切面调用。 可以把整个切面调用过程看作一个洋葱圈层，比如添加s1类型的before和after作为切片，s2类型的before和after作为切片，s3类型的before作为切片。把业务代码逻辑作为foo函数。 则他们的调用过程如下： s1-\u003ebefore =\u003e s2-\u003ebefore =\u003e s3-\u003ebefore =\u003e foo业务逻辑 =\u003e s1-\u003eafter =\u003e s2-\u003eafter。 如果稍微学过点数据结果，这个调用就能想到前中后序遍历上去了。 代码实现如下（C++11需要使用eable_if来实现，代码量很多，所以这里就直接用C++17的 if constexpr 来实现了）： /*以下是截取的一个类的两个方法*/ // 递归的尽头 template \u003ctypename T\u003e void Invoke(Args \u0026\u0026...args, T \u0026\u0026aspect) { ST_ASSERT if constexpr (has_member_Before\u003cT, Args...\u003e::value \u0026\u0026 has_member_After\u003cT, Args...\u003e::value) { aspect.Before(std::forward\u003cArgs\u003e(args)...); // 核心逻辑之前的切面逻辑 m_func(std::forward\u003cArgs\u003e(args)...); // 核心逻辑 aspect.After(std::forward\u003cArgs\u003e(args)...); // 核心逻辑之后的切面逻辑 } else if constexpr (has_member_Before\u003cT, Args...\u003e::value \u0026\u0026 !has_member_After\u003cT, Args...\u003e::value) { aspect.Before(std::forward\u003cArgs\u003e(args)...); // 核心逻辑之前的切面逻辑 m_func(std::forward\u003cArgs\u003e(args)...); // 核心逻辑 } else { m_func(std::forward\u003cArgs\u003e(args)...); // 核心逻辑 aspect.After(std::forward\u003cArgs\u003e(args)...); // 核心逻辑之后的切面逻辑 } } // 变参模板递归 template \u003ctypename T, typename... Tail\u003e void Invoke(Args \u0026\u0026...args, T \u0026\u0026headAspect, Tail \u0026\u0026...tailAspect) { ST_ASSERT if constexpr (has_member_Before\u003cT, Args...\u003e::value \u0026\u0026 has_member_After\u003cT, Args...\u003e::value) { headAspect.Before(std::forward\u003cArgs\u003e(args)...); Invoke(std::forward\u003cArgs\u003e(args)..., std::forward\u003cTail\u003e(tailAspect)...); headAspect.After(std::forward\u003cArgs\u003e(args)...); } else if constexpr (has_member_Before\u003cT, Args...\u003e::value \u0026\u0026 !has_member_After\u003cT, Args...\u003e::value) { headAspect.Before(std::forward\u003cArgs\u003e(args)...); Invoke(std::forward\u003cArgs\u003e(args)..., std::forward\u003cTail\u003e(tailAspect)...); } else { Invoke(std::forward\u003cArgs\u003e(args)..., std::forward\u003cTail\u003e(tailAspect)...); headAspect.After(std::forward\u003cArgs\u003e(args)...); // 核心逻辑之后的切面逻辑 } } 上述完整代码：aspect.hpp 上述代码是根据C++变参模板实现的通用性操作，可以同时添加多个切片 ，他们都是Aspect类的两个方法，具体实现逻辑就是：通过之前得到的编译期常量( has_member_Before\u003cT,Args...\u003e::value )判断 T 是否具有Before或者After方法，分三种情况： 同时又Before和After：利用中序进行递归。 只有Before：利用前序进行递归。 只有After：利用后序进行递归。 为了简化调用过程，继续封装如下： 最后记得定义一个终止模板递归的最终形态。 template \u003ctypename T\u003e using identity_t = T; // AOP的辅助函数，简化调用 template \u003ctypename... AP, typename... Args, typename Func\u003e void Invoke(Func \u0026\u0026f, Args \u0026\u0026...args) { Aspect\u003cFunc, Args...\u003e asp(std::forward\u003cFunc\u003e(f)); asp.Invoke(std::forward\u003cArgs\u003e(args)..., identity_t\u003cAP\u003e()...); } 最终如果像最开始讲的要拓展s1、s2、s3的方法上去，那么简单的使用如下代码即可： Invoke\u003cs1,s2,s3\u003e(\u0026foo,args); //s1,s2,s3为拓展逻辑，foo为业务逻辑 ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:2:1","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["C++元模板编程"],"content":"统一转function存储并实现AOP调用顺序 统一转function存储 将任意类的before和after方法集体装箱为function的关键代码逻辑如下，完整代码请看： void Get() {} //空的func，用于结束模板的递归实例化 template \u003ctypename T, typename... Tail\u003e void Get(T \u0026\u0026head, Tail \u0026\u0026...tails) { ST_ASSERT auto \u0026\u0026p = details::GetMemberFunc\u003cT, Args...\u003e(); m_output.push_back(p); Get(tails...); } 由于所有的获取before和after的逻辑在前面获取类的方法已经讲到，所以单个类型直接调用 GetMemberFunc 函数即可得出结果，并放入vector中，最后通过模板实例化的递归将所有的类型都装箱。 具体的使用方式也很简单，如下代码： #include\"reflect_util.hpp\"using func_t = reflect::MemberFunc\u003cint\u003e::func_t; using func_pair_t = reflect::MemberFunc\u003cint\u003e::func_pair_t; struct LoginAspect { void before(int i) { cout \u003c\u003c \"Login start \" \u003c\u003c i \u003c\u003c endl; } void after(int i) { cout \u003c\u003c \"after start \" \u003c\u003c i \u003c\u003c endl; } }; int main(){ vector\u003cfunc_pair_t\u003e out; // 获取before和after方法，并通过function进行包装 reflect::MemberFunc\u003cint\u003e(out).Get( TimeElapsedAspect{}, LoggingAspect{}, LoginAspect{} ); //将三个类型的before和after方法分离成function后以pair的形式存储在out中 for(auto\u0026\u0026 p : out){ if(p.first){//如果before存在则调用 p.first(0); } if(p.second){//如果after存在则调用 p.second(1); } } } AOP的调用顺序实现 // 根据AOP的顺序存入out数组 void AspectOrder(vector\u003cfunc_t\u003e \u0026out, vector\u003cfunc_pair_t\u003e \u0026v, const func_t \u0026func, int index) { if (v.size() \u003c= index) { out.push_back(func); return; } if (v[index].first) { out.push_back(v[index].first); } AspectOrder(out, v, func, index + 1); if (v[index].second) { out.push_back(v[index].second); } } 完整测试代码：test_aspect.cc ","date":"2022-11-25","objectID":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/:2:2","tags":["C++编译期反射——以AOP为例"],"title":"C++编译期反射——以AOP为例","uri":"/posts/c++%E7%BC%96%E8%AF%91%E6%9C%9F%E5%8F%8D%E5%B0%84%E4%BB%A5aop%E4%B8%BA%E4%BE%8B/"},{"categories":["golang相关"],"content":"go语言业务代码一键逆向生成","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"go_project_quickstart ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:0:0","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"快速开始 ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:1:0","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"要求 Go 1.18 及以上版本 ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:1:1","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"安装 1.下载并安装 gin： go get -u github.com/ACking-you/quickstart_project\r2.将 gin 引入到代码中： import \"github.com/ACking-you/quickstart_project\"\r3.一键根据数据库快速逆向生成所有业务代码： import ( \"github.com/ACking-you/quickstart_project\" \"github.com/ACking-you/quickstart_project/util\" ) func autoQuickStart() { config := quickstart.DefaultConfig(\"项目名称\", \"root\", \"123\", \"127.0.0.1\", 3306, \"数据库名称\"). //打印出生成结果 EnableDebug(true). //改变基本路径（默认为项目根目录） BasePath(\"./example\") err := quickstart.Run(config) if err != nil { panic(err) } } func main() { autoQuickStart() } 上述代码，只更改了默认配置项中的两项，其他配置项的更改和作用请翻看源代码：./config 上述调用会一键生成 model、dao、service、vo、to、controller层的所有模板代码，且无法做到对每层代码生成的精确控制，如果本身项目已有model层的结构体，那么可以利用我提供的 dao_convertor 、service_convertor、controller_convertor 对整个代码生成做细化处理。 上述接口具体如何使用请点击以下文档进行查看： dao_convertor/example service_convertor/example controller_convertor/example ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:1:2","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"项目实现 ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:2:0","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"架构设计 对应的项目文件如下： ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:2:1","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["golang相关"],"content":"代码逻辑 上述架构设计阐述了，如何将四个单独的模块生成解耦，且同时数据也能产生关联，下面将详细介绍这一块。 统一的调用逻辑 如果细心的使用者会发现，所有模块的接口调用都是统一的形式。 创建对应模块的 Config ，通过链式调用进行配置，或者直接使用默认配置。 将 config 传入，new出新的 convertor 对象。 调用 Run 方法。当然，除了 model_convertor 不需要反射机制的接口外，其余的层级都提供了 AutoMigrate 方法供使用者选择基于哪个信息生成代码。 代码复用 一、反射信息生成函数复用 最开始，在写 dao_convertor 的时候并未意识到解耦的重要性，但写到后面发现都要用到这个解析类型元信息的功能，而且代码十分重复，但是又不能直接抽离，因为此段逻辑与每个层级的代码产生了一定的耦合。 耦合体现在：每个层级的 tag 解析过程和需要的数据结构都是不一样的，而之前的操作则是将这部分代码嵌入到了元信息的解析中。 如何解耦：将耦合的代码通过函数回调的统一参数接口形式，代码便不再耦合了，成功实现复用。具体实现在 UpdateFromStruct 二、文件保存动作的复用 这个行为在所有的层级都需要用到，在解析完信息并拼装生成好代码后，最后的动作就是要保存到文件了。 我这里对文件保存动作的复用分为三个级别： code SaveAction：将一个go文件分为三个组成部分，本文件包名、需要导入的包名、具体的代码内容。将上述三个信息以及保存路径传入到此函数后，会自动根据保存的路径是文件还是文件夹决定是分文件保存还是单文件保存，文件名取自 common_info 里各个模型数据的名字 + 当前的包名(如user_dao.go)。 saveHelper.singleFileSave：无法被外界调用，由SaveAction选择性调用，单文件保存，计算某些必要信息后，继续调用 SaveFile。 saveHelper.multiFileSave：无法被外界调用，由SaveAction选择性调用，多文件保存，计算某些必要信息后，继续调用 SaveFile。 SaveFile：最简单且底层的封装，只需要传递文件名和文件内容，负责文件创建保存，由于所有的文件保存工作最终都会经过它，所以在此添加的任何操作将会作用于所有文件(比如可以在文件创建后利用gofmt格式化)。 对于保存的内容复杂且多的情况，直接准备好数据调用 SaveAction 是最好的选择，而对于只有单个文件，且数据相对固定，能很快得出文件内容的，则可以直接提前调用 SaveFile(如dao层的init.go)。 三、有意思的小工具轮子 name_util：实现了各种代码风格的转化如snackcase-\u003ePascalCase sscanf：一个根据格式化串和资源串来填充后续字符串变量的函数，比如 Sscanf(\"hello( you)world)\",\"$($)$\",\u0026s1,\u0026s2,\u0026s3) //s1:hello s2: you s3:world 写个这玩意是在使用fmt.Sscanf的时候被恶心到了，fmt版本的有分隔符的限制，所以不得不自己造个轮子了，这个主要用在tag的解析上面。 str_util：暂时只写了一个StrHandleByChain，主要用于方便链式调用来操作字符串的替换等操作，实现很简单，如下： type StrHandleByChain struct { Str string } func (s *StrHandleByChain) ReplaceAll(old, new string) *StrHandleByChain { s.Str = strings.ReplaceAll(s.Str, old, new) return s } 但是能简化我的代码效果如下： strings.ReplaceAll(strings.ReplaceAll(content,\"old\",\"new\"),\"old\",\"new\") =\u003e content.ReplaceAll(\"old\",\"new\").ReplaceAll(\"old\",\"new\") 很明显可读性变高了，而且这还只是套了两层的结果。。。 ","date":"2022-11-18","objectID":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/:2:2","tags":["go语言业务代码一键逆向生成"],"title":"go语言业务代码一键逆向生成","uri":"/posts/go%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%80%E9%94%AE%E9%80%86%E5%90%91%E7%94%9F%E6%88%90/"},{"categories":["操作系统"],"content":"[CS原理]多级页表到底如何节约内存？","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"前言 在学习计算机组成原理时，书中谈到，“使用多级页表可以压缩页表占用的内存”，在了解了多级页表的原理后，恐怕对这句话还是理解不了：把页表换成多级页表了就能节约内存了？不是还是得映射所有的虚拟地址空间么？ 比如做个简单的数学计算，假如虚拟地址空间为32位（即4GB）、每个页面映射4KB以及每条页表项占4B，则进程需要1M个页表项（4GB / 4KB = 1M），即页表（每个进程都有一个页表）占用4MB（1M * 4B = 4MB）的内存空间。而假如我们使用二级页表，还是上述条件，但一级页表映射4MB、二级页表映射4KB，则需要1K个一级页表项（4GB / 4MB = 1K）、每个一级页表项对应1K个二级页表项（4MB / 4KB = 1K），这样页表占用4.004MB（1K * 4B + 1K * 1K * 4B = 4.004MB）的内存空间。多级页表的内存空间占用反而变大了？ 其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的局部性原理么？ ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:1:0","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"如何节约内存 我们分两方面来谈这个问题：第一，二级页表可以不存在；第二，二级页表可以不在主存。 ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:2:0","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"二级页表可以不存在 我们反过来想，每个进程都有4GB的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到4GB，何必去映射不可能用到的空间呢？ 也就是说，一级页表覆盖了整个4GB虚拟地址空间，但如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表。做个简单的计算，假设只有20%的一级页表项被用到了，那么页表占用的内存空间就只有0.804MB（1K * 4B + 0.2 * 1K * 1K * 4B = 0.804MB），对比单级页表的4M是不是一个巨大的节约？ 那么为什么不分级的页表就做不到这样节约内存呢？我们从页表的性质来看，保存在主存中的页表承担的职责是将虚拟地址翻译成物理地址；假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有1M个页表项来映射，而二级页表则最少只需要1K个页表项（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。 ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:2:1","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"二级页表可以不在主存 其实这就像是把页表当成了页面。回顾一下请求分页存储管理，当需要用到某个页面时，将此页面从磁盘调入到内存；当内存中页面满了时，将内存中的页面调出到磁盘，这是利用到了程序运行的局部性原理。我们可以很自然发现，虚拟内存地址存在着局部性，那么负责映射虚拟内存地址的页表项当然也存在着局部性了！这样我们再来看二级页表，根据局部性原理，1024个第二级页表中，只会有很少的一部分在某一时刻正在使用，我们岂不是可以把二级页表都放在磁盘中，在需要时才调入到内存？我们考虑极端情况，只有一级页表在内存中，二级页表仅有一个在内存中，其余全在磁盘中（虽然这样效率非常低），则此时页表占用了8KB（1K * 4B + 1 * 1K * 4B = 8KB），对比上一步的0.804MB，占用空间又缩小了好多倍！ ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:2:2","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"总结 我们把二级页表再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。 回头想想，这么大幅度地解决内存空间，我们失去了什么呢？计算机的很多问题无外乎就是时间换空间和空间换时间了，而多级页表就是典型的时间换空间的例子了，动态创建二级页表、调入和调出二级页表都是需要花费额外时间的，远没有不分级的页表来的直接；而我们也仅仅是利用局部性原理让这个额外时间开销降得比较低了而已。 ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:3:0","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["操作系统"],"content":"参考 Bryant R, David Richard O H. 深入理解计算机系统[M]. 机械工业出版社, 2016. ","date":"2022-10-07","objectID":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/:4:0","tags":["[CS原理]多级页表到底如何节约内存？"],"title":"[CS原理]多级页表到底如何节约内存？","uri":"/posts/cs%E5%8E%9F%E7%90%86%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98/"},{"categories":["算法——滑动窗口"],"content":"LCP68.美观的花束——sliding_window","date":"2022-10-07","objectID":"/posts/lcp68.%E7%BE%8E%E8%A7%82%E7%9A%84%E8%8A%B1%E6%9D%9Fsliding_window/","tags":["LCP68.美观的花束——sliding_window"],"title":"LCP68.美观的花束——sliding_window","uri":"/posts/lcp68.%E7%BE%8E%E8%A7%82%E7%9A%84%E8%8A%B1%E6%9D%9Fsliding_window/"},{"categories":["算法——滑动窗口"],"content":"class Solution { public: const int mod = 1e9+7; int beautifulBouquet(vector\u003cint\u003e\u0026 flowers, int cnt) { int n = flowers.size(); int ret = 0; int left = 0,right = 0; unordered_map\u003cint,int\u003e count; int option = 1; while(right \u003c= n){ if(option == 1){ //right指针向右边滑动，更新窗口上限 if(right == n){ option = 2; continue ; } int p = ++count[flowers[right++]]; if(p \u003e cnt){ option = 2; } }else{ //left指针滑动开始收缩窗口，同时不断更新答案 int span = right -1 -left; if( right == n\u0026\u0026 count[flowers.back()] \u003c= cnt){ //2 span = right - left; } ret = (ret + span) % mod; int p = --count[flowers[left++]]; if(p == cnt){ option = 1; } } if(left == right) break ; } return ret; } }; 题目链接 这道题首先由简单的枚举来过渡：比如1232，cnt=1。 我们普通的做法可以这样枚举：从左到右枚举首元素：以1开头有1、12、123，以2开头有2、23，以3开头有3、32，以2开头有2. 很明显这样枚举是可行的，但是需要O(n^2)的时间复杂度，外层for枚举开头，里层寻找终点（连续不包含cnt个重复数字的最长序列）。 比如我最开始就是这样写的：很自然的就超时了 class Solution { public: const int mod = 1e9+7; int beautifulBouquet(vector\u003cint\u003e\u0026 flowers, int cnt) { int n = flowers.size(); int ret = 0; for(int i=0;i\u003cn;i++){ int ma = 0; unordered_map\u003cint,int\u003e mmp; int j=i; for(;j\u003cn;j++){ int p = ++mmp[flowers[j]]; ma = max(ma,p); if(ma \u003e cnt){ break; } } ret = (ret+(j-i))%mod; } return ret; } }; 我们在枚举里层循环的时候，发现有多次重复计算，比如1开头的尾部是3，2开头的尾部还是3，那么该怎么优化呢？直接做缓存处理也是不可行的，因为无法确定后续遍历的右边界都一致。 那么如何优化呢？很快我们可以想到建立一个窗口来滑动的思想，我们的问题在于无法确认当前遍历到的头部的右边界（重复数字）是否还是前一个数确认的边界，滑动窗口可以通过哈希表记录简单的解决这一问题。 我们先把窗口向右扩张(right++、++count[x])，也就是随便先找到一个右边界（这个边界内重复的数字不超过 cnt 个）。 不断的收缩窗口并更新答案(left--,--count[x])，如果在收缩过程中 count[x] 被减小到正好等于 cnt ，那就说明之后 left 经过元素的有边界发生了改变，此时需要再次向右扩大窗口找到新的边界。 1和2的过程不断循环重复，直到 left 遍历并更新完了所有的元素，即 left == right 时，跳出循环。 小细节：有两种情况需要被分清楚否则会出错，1.right往后扩散发现没有重复元素（即此时没有了右边界） 2.右边界的位置就是最后一个元素的位置，即right的位置在最后一个元素的下标位置. 在我写的代码中，由于left和right始终指向还未扫描过的位置，所以上述两种情况right的值会相同都为flowers数组的大小n。但这两种情况计算答案则不同，第二种情况会少一种情况。所以有了上述题解代码的 2 进行判断和处理。 ","date":"2022-10-07","objectID":"/posts/lcp68.%E7%BE%8E%E8%A7%82%E7%9A%84%E8%8A%B1%E6%9D%9Fsliding_window/:0:0","tags":["LCP68.美观的花束——sliding_window"],"title":"LCP68.美观的花束——sliding_window","uri":"/posts/lcp68.%E7%BE%8E%E8%A7%82%E7%9A%84%E8%8A%B1%E6%9D%9Fsliding_window/"},{"categories":["个人项目"],"content":"C++高可用日志库实现","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"使用教程 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:0:0","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"如何添加到项目中使用 输入以下命令得到项目文件 git clone https://github.com/ACking-you/my-logger.git 项目目录如下： dependencies：整个项目的外部依赖，以源码形式存在（比如fmt库）。 lib：整个项目已经打包好的库文件，我打包的是动态库，如有静态库需要请自行编译。库的名称为logger和fmt。 其余均为项目关键源码，就Logger.cpp和Logger.h两个。main.cc是项目运行测试代码。 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:1:0","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"CMake配置 由于本人对 cmake 的 install 指令了解的并不深入，所以本库并不支持最后的 make install ，为了防止大伙编译库的长时间等待，我已经编译好了动态库版本，所以直接拿去用即可（当然不排除环境的不兼容，所以可能还是需要手动编译）。 如果你clone本项目和你的项目为同级目录，请在你的cmake文件中添加下面的代码，然后再链接这两个库便可得到本库的所有支持。 include_directories(../my-logger)#具体为本项目的根目录，可用相对路径也可用绝对。此处是为了方便搜索头文件 link_directories(../my-logger/lib)#路径说明同上，该路径为链接库的目录 .... target_link_libraries(${CMAKE_PROJECT_NAME} logger fmt)#添加对应的链接库 注意：如果你不需要使用fmt库中的高级功能，那么你可以不链接fmt库，如果需要使用其高级功能进行打印（比如支持打印std::map等标准库容器），那么请链接fmt库。 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:1:1","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"库的编译 下面为logger库的编译命令： mkdir build-logger cd build-logger cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON make 输入上面的命令后，会在 build-logger 文件夹中生成对应的动态库（不定义第二个变量时默认编译静态库）。建议把这个生成的动态库移动到这个项目 lib 目录中，方便在使用时只需添加之前的cmake代码。 下面为fmt库的编译命令（由于该库较大，编译时间可能较久，我的电脑用了将近十分钟）： mkdir build-fmt cd build-fmt cmake ../dependencies/fmt/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON 同样会在 build-fmt 目录生成对应的动态库，建议同样放入该项目lib目录中，注意 fmt 库的动态库是由多个文件构成，这多个文件都需要被放在一起。 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:1:2","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"如何使用库 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:2:0","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"如何配置 如下，为整个日志的所有配置项目，当然你也可以不进行配置，也有对应的默认值 #include\"Logger.h\" int main(){ Config::Set({ .print_flag = LstdFlags | lblog::LthreadId, //设置打印的内容，有日期、时间、文件名（长、短）、行号、线程id这些选项可选 （默认为LstdFlags包含Ldata | Ltime | Lshortname | Lline .output_prefix = \"my\", //设置输出日志的前缀名，默认为空 .output_file = \"./log.txt\", //设置输出日志的文件，默认为空 .is_console = true //设置是否输出到控制台，默认为true }); } 注意：如果需要调用此配置函数进行配置，需要在所有打印日志宏调用之前，且同一个进程请不要使用两次配置（因为整个配置是一个单例，配置的更改在同一个进程中会是非线程安全的行为）。 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:2:1","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"精确控制输出等级 我们的库共提供以下五种宏来进行日志的打印： debug、info、warn、error、fatal 使用debug进行日志输出的，在release模式下将不再进行输出。 我们同样也可以通过手动定义对应的宏控制日志输出等级。 如下： #define LOG_LIMIT_WARN #include \"Logger.h\" int main(){ debug(\"hhh\"); //无效 info(\"hhh\"); //无效 warn(\"hhh\"); //有效 error(\"hhh\"); //有效 fatal(\"hhh\"); //有效 } 同理也可以通过 LOG_LIMIT_ERROR 控制至少是error等级才输出日志。 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:2:2","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"日志输出介绍 前面介绍了基本的日志的控制，接下来介绍打印输出的功能，以debug和info为例： 本日志的最终输出使用的是C++第三方库 fmt ，虽然在C++20中它已经入标准库了，但是距离我们真正的使用还需要一段时间，下面是简单输出方式，更多的输出方式可以查看fmt官方文档： fmt官方文档 #include\"Logger.h\" int main(){ debug(\"hello {}\",\"world\"); info(\"world {}\",\"hello\"); } 注意：{} 是支持所有的C++标准库容器的，包括vector和string等等容器的直接打印，如 debug(\"{}\",vector\u003cint\u003e{1,3,32,432,432}); 是可行的。但不要忘了链接 fmt 库。 上面的默认输出效果如下：（控制台内是带颜色的，文件就没有颜色了） 整体架构 ","date":"2022-09-05","objectID":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/:2:3","tags":["C++高易用日志库实现"],"title":"C++高易用日志库实现","uri":"/posts/c++%E9%AB%98%E6%98%93%E7%94%A8%E6%97%A5%E5%BF%97%E5%BA%93%E5%AE%9E%E7%8E%B0/"},{"categories":["个人项目"],"content":"驾考软件实现文档","date":"2022-09-05","objectID":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/","tags":["驾考软件实现文档"],"title":"驾考软件实现文档","uri":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"整体架构 ","date":"2022-09-05","objectID":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/:0:0","tags":["驾考软件实现文档"],"title":"驾考软件实现文档","uri":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"1.软件后台架构 采用标准的三层架构： Models层用于底层获取数据和对应的序列化过程 Service层用于包装数据提供接口 Controller层用于提供良好易用的接口 后台采用Python语言的fastapi框架编写，数据采用request库请求再用beautifulsoup库解析得到（网络爬虫获得）。 ","date":"2022-09-05","objectID":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/:0:1","tags":["驾考软件实现文档"],"title":"驾考软件实现文档","uri":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/"},{"categories":["个人项目"],"content":"2.软件前台架构 如下图： 整个前台采用跨平台方案flutter框架进行编写 界面如下： 数据流传递 从Api网络调用层级请求得到对应的Future数据 。 传递Future到Service层级。 Service根据UI的需求返回对应的 Future\u003cQuestionResponse\u003e ，且封装默认数据（用于不存在的时的数据）。 在UI层的 MyHomePage 主界面点击按钮后，通过await同步得到Future里的 QuestionResponse 数据并将该数据用于构建一个新的 QuestionPage 界面。 QuestionPage 中通过封装好各个方法通过对应的动作来操作数据生成新的界面。比如题目通过更新 _index 下标再 setState 来实现重建，答案则通过 showAnswer 这个bool标志来判断是否需要显示… 由于每次答案都是得到的Future数据，所以可以在界面中使用 FutureBuilder 来实现数据的展示。 坑点： 注意Future数据不能在底层进行await同步，否则界面层进行调用时数据会得不到，所以网络数据最好是直接传毒Future或者Stream。 注意每个异常的处理，否则它将会导致处理数据的底层完全崩溃（异常不被处理将不再往下走，UI界面表现正常，但是数据无），Flutter很多地方都很容易发生异常，比如类型的强制转化不成功将导致异常（如int转Int64），或者空安全将导致异常（采用?则表示断言会告诉你出现错误的地方，而如果采用!则表示一定不为空，则直接抛出异常很难察觉发生在哪）。 ","date":"2022-09-05","objectID":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/:0:2","tags":["驾考软件实现文档"],"title":"驾考软件实现文档","uri":"/posts/%E9%A9%BE%E8%80%83%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%96%87%E6%A1%A3/"},{"categories":["C++多线程"],"content":"细粒度锁线程安全队列实现","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"包含同步语义的简单实现 template \u003ctypename T\u003e class ThreadSafeQueue { public: void Push(T new_value) { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); m_queue.push(std::move(new_value)); m_cond.notify_one(); // 1 } void WaitAndPop(T \u0026value) // 2 { std::unique_lock\u003cstd::mutex\u003e lk(m_mtx); m_cond.wait(lk, [this] { return !m_queue.empty(); }); value = std::move(m_queue.front()); m_queue.pop(); } std::shared_ptr\u003cT\u003e WaitAndPop() // 3 { std::unique_lock\u003cstd::mutex\u003e lk(m_mtx); m_cond.wait(lk, [this] { return !m_queue.empty(); }); // 4 std::shared_ptr\u003cT\u003e res( std::make_shared\u003cT\u003e(std::move(m_queue.front()))); m_queue.pop(); return res; } bool TryPop(T \u0026value) { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); if (m_queue.empty()) return false; value = std::move(m_queue.front()); m_queue.pop(); return true; } std::shared_ptr\u003cT\u003e TryPop() { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); if (m_queue.empty()) return std::shared_ptr\u003cT\u003e(); // 5 std::shared_ptr\u003cT\u003e res( std::make_shared\u003cT\u003e(std::move(m_queue.front()))); m_queue.pop(); return res; } bool Empty() const { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); return m_queue.empty(); } private: mutable mutex m_mtx; queue\u003cT\u003e m_queue; condition_variable m_cond; }; 这个版本是最为简单的实现版本，直接用的stl库中的队列来实现，所有成员函数公用一把锁来实现线程安全，需要注意的点有以下几点： 条件变量产生的虚假唤醒，你可以通过手动while循环来避免，也可以通过在wait后面加上谓词条件（lamda表达式） 锁需要设置为mutable，保证const版本的成员函数可用 但这个实现有非常大的隐患和不足！ ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:1:0","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"隐患 如果在调用WaitAndPop函数时发生了异常，由于可能有其他的线程也在调用WaitAndPop发生等待，而由于每次notify一个线程，一旦构造 std::shared_ptr的过程中发生异常，那么其他的线程将会陷入永久的等待！ 解决方法： 由于异常发生在内存的申请过程中，我们如果把 std::queue 中直接存入 shared_ptr 那么就不会有这个问题。 改写后的代码如下： template \u003ctypename T\u003e class ThreadSafeQueue { public: void Push(T new_value) { auto data = std::make_shared(std::move(new_value)); std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); m_queue.push(data); m_cond.notify_one(); // 1 } void WaitAndPop(T \u0026value) // 2 { std::unique_lock\u003cstd::mutex\u003e lk(m_mtx); m_cond.wait(lk, [this] { return !m_queue.empty(); }); value = std::move(*m_queue.front()); m_queue.pop(); } std::shared_ptr\u003cT\u003e WaitAndPop() // 3 { std::unique_lock\u003cstd::mutex\u003e lk(m_mtx); m_cond.wait(lk, [this] { return !m_queue.empty(); }); // 4 std::shared_ptr\u003cT\u003e res = m_queue.front(); m_queue.pop(); return res; } bool TryPop(T \u0026value) { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); if (m_queue.empty()) return false; value = std::move(*m_queue.front()); m_queue.pop(); return true; } std::shared_ptr\u003cT\u003e TryPop() { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); if (m_queue.empty()) return std::shared_ptr\u003cT\u003e(); // 5 std::shared_ptr\u003cT\u003e res = m_queue.front(); m_queue.pop(); return res; } bool Empty() const { std::lock_guard\u003cstd::mutex\u003e lk(m_mtx); return m_queue.empty(); } private: mutable mutex m_mtx; queue\u003cstd::shared_ptr\u003cT\u003e\u003e m_queue; condition_variable m_cond; }; 这个版本的代码不仅是预防了异常安全，同样性能也得到了很好的优化，Push 过程的内存申请过程可以放到临界区以外，提高了并发度。 ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:1:1","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"设计细粒度锁队列提高并发 前面的简单版本，有个非常明显的不足，几乎没有任何并发的性能，因为所有的成员函数都必须加锁，临界区非常的大，这哪里是并发，这都强行变成了同步执行，那这样肯定不行啊，我们找找原因。 这个原因很简单，由于我们是通过stl内部的queue封装所实现的，我们的任何的成员函数操作实现都必须访问到这个共享变量，一旦变量被共享，要实现线程安全那就必须加锁同步，这便是原因所在了。 这就是我们现在要做的事情，把锁的粒度减少，实际就是把变量的共享和操作细分。 ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:2:0","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"细粒度锁队列实现 实现简单队列 在这之前我们先自己实现一个简单的队列，如下： template\u003ctypename T\u003e class Queue { private: struct node { T data_; std::unique_ptr \u003cnode\u003e next_; node(T data) : data_(std::move(data)) {} }; std::unique_ptr \u003cnode\u003e m_head; node *m_tail{}; public: Queue() = default; Queue(const Queue \u0026other) = delete; Queue \u0026operator=(const Queue \u0026other) = delete; std::shared_ptr \u003cT\u003e TryPop() { if (!m_head) { return nullptr; } auto ret = std::make_shared\u003cT\u003e(std::move(m_head-\u003edata_)); auto oldHead = std::move(m_head); m_head = std::move(oldHead-\u003enext_); //这里把next资源进行转移，防止oldHead析构后导致整个链表析构 return ret; } void Push(T new_value) { auto p = std::make_unique\u003cnode\u003e(new_value); auto *new_tail = p.get(); if (m_tail) {//如果队列不为空 m_tail-\u003enext_ = std::move(p); } else {//队列为空则需要特殊处理 m_head = std::move(p); } m_tail = new_tail; } }; next指针为啥不用原始指针？嗯，其实应该要用原始指针的，这里偷个懒，为了不写delete语句，用的unique_ptr，在使用这个独占指针的时候记得要转移所有权，否则会出现连环析构的现象！ 由于使用了unique_ptr管理next_指针，那么析构的时候会自动完成，但是会有个问题，如果队列中的数据量大的话，整个函数栈会爆掉，我亲自测试了下，大概存入的数据量达到1e4级别就会爆栈。。。但是没关系，我们将他用作并发编程中的队列时，用于生产消费的队列里的空闲任务一般也不会到达这个量级，当然有空的话也可以改进然后优化。 分析并发设计 我们再来简单分析下这个内存共享的情况pop操作需要用到head，push操作需要用到head和tail。但是有个严重的问题：除了这两个内存被共享外，由于未采用空头节点，两个成员函数内用 next_ 指针访问到的内存都可能发生共享（对应 m_tail-\u003enext_ 与 oldHead-\u003enext_）。这样的话很难在保证细粒度的情况下实现线程安全了。。。这样下去的实现还不如之前的。 通过分离数据实现并发 前面的隐患已经分析清楚了，如何解决它？你可以使用预分配一个虚拟节点(无数据)，确保这个节点永远在队列的最后，用来分离头尾指针能访问的节点”的办法，走出这个困境。这样通过 pop 和 push 操作通过 next_ 指针访问到的数据就永远不可能是同一个数据了。 代码如下： template\u003ctypename T\u003e class Queue { private: struct node { std::shared_ptr\u003cT\u003e data_; std::unique_ptr \u003cnode\u003e next_; }; std::unique_ptr \u003cnode\u003e m_head; node *m_tail; public: Queue():m_head(new node),m_tail(m_head.get()){}; //初始化空节点 Queue(const Queue \u0026other) = delete; Queue \u0026operator=(const Queue \u0026other) = delete; std::shared_ptr \u003cT\u003e TryPop() { if (m_head.get() == m_tail) { return nullptr; } auto ret = m_head-\u003edata_; auto oldHead = std::move(m_head); m_head = std::move(oldHead-\u003enext_); return ret; } void Push(T new_value) { auto data = std::make_shared\u003cT\u003e(std::move(new_value)); auto p = std::make_unique\u003cnode\u003e(new_value); //新的空节点 m_tail-\u003edata_ = data; //开始移动补充最后的空节点 auto* new_tail = p.get(); m_tail-\u003enext_ = std::move(p); m_tail = new_tail; } }; 现在两个操作共享的内存就只有 m_head 和 m_tail 了，而且在 Push 操作中只使用到了共享内存 m_tail，那么接下来的并发安全实现可以开始细粒度化了，我们用两个互斥锁来实现它。一个互斥锁用于锁住访问m_head的行为，一个用于锁住访问m_tail的行为，具体到代码可以因使用时间的长短对临界区进行进一步缩小。 具体代码如下： template\u003ctypename T\u003e class ThreadSafeQueue { struct node { std::shared_ptr \u003cT\u003e data; std::unique_ptr \u003cnode\u003e next; }; std::mutex m_headMtx; std::unique_ptr \u003cnode\u003e m_head; std::mutex m_tailMtx; node *m_tail; public: ThreadSafeQueue() : m_head(new node), m_tail(m_head.get()) {} ThreadSafeQueue(const ThreadSafeQueue \u0026other) = delete; ThreadSafeQueue \u0026operator=(const ThreadSafeQueue \u0026other) = delete; std::shared_ptr \u003cT\u003e TryPop() { std::unique_ptr \u003cnode\u003e old_head = pop_head(); return old_head ? old_head-\u003edata : std::shared_ptr\u003cT\u003e(); } void Push(T new_value) { std::shared_ptr \u003cT\u003e new_data( std::make_shared\u003cT\u003e(std::move(new_value))); std::unique_ptr \u003cnode\u003e p(new node); node *const new_tail = p.get(); //开始锁临界区 std::lock_guard \u003cstd::mutex\u003e tail_lock(m_tailMtx); m_tail-\u003edata = new_data; m_tail-\u003enext = std::move(p); m_tail = new_tail; } private: node *get_tail() { std::lock_guard \u003cstd::mutex\u003e tail_lock(m_tailMtx); return m_tail; } std::unique_ptr \u003cnode\u003e pop_head() { //这里head一定要先被锁 std::lock_guard \u003cstd::mutex\u003e head_lock(m_headMtx); if (m_head.get() == get_tail()) { return nullptr; } std::unique_ptr \u003cnode\u003e old_head = std::move(m_head); m_head = std::move(old_head-\u003enext); return old_head; } }; 注意： 当get_tail()调用前，请确保 m_headMtx 已经上锁，这一步也是很重要的哦。如果不这样，调用pop_head()时，就无法确保 get_tail 得到的数据在使用的时候为最新，如下代码，如果进入head_lock临界区后，old_tail被其他线程改了，那么整个操作就不对了。 std::unique_ptr\u003cnode\u003e pop_head() // 这是个有缺陷的实现 { node* const old_tail=get_tail(); // ① 在m_headMtx范围外获取旧尾节点的值 std::lock_guard\u003cstd::mutex\u003e head_lock(head_mutex); if(head.get()==old_tail) // ② { return nullptr; } std::unique_ptr\u003cnode\u003e old_head=std::move(head); head=std::move(old_head-\u003enext); // ③ return old_head; } 再来看看异常安全是否有保证，","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:2:1","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"添加条件变量实现同步等待 现在已经实现了细粒度锁的线程安全队列，不过只有TryPop()可以并发访问(且只有一个重载存在)。那么方便的同步的WaitAndPop()呢？ Push实现 向队列中添加新节点是相当简单的——下面的实现与上面的代码差不多。 template\u003ctypename T\u003e void ThreadSafe\u003cT\u003e::Push(T new_value) { auto new_data = std::make_shared\u003cT\u003e(std::move(new_value)); std::unique_ptr\u003cnode\u003e p(new node); {//生产临界区 std::lock_guard\u003cstd::mutex\u003e tail_lock(tail_mutex); tail-\u003edata=new_data; auto* new_tail=p.get(); tail-\u003enext=std::move(p); tail=new_tail; } data_cond.notify_one(); } WaitAndPop实现 template\u003ctypename T\u003e class ThreadSafeQueue { private: node* get_tail() { std::lock_guard\u003cstd::mutex\u003e tail_lock(tail_mutex); return tail; } std::unique_ptr\u003cnode\u003e pop_head() // 1 { std::unique_ptr\u003cnode\u003e old_head=std::move(head); head=std::move(old_head-\u003enext); return old_head; } std::unique_lock\u003cstd::mutex\u003e wait_for_data() // 2 { std::unique_lock\u003cstd::mutex\u003e head_lock(head_mutex); data_cond.wait(head_lock,[\u0026]{return head.get()!=get_tail();}); return std::move(head_lock); // 3 } std::unique_ptr\u003cnode\u003e wait_pop_head() { std::unique_lock\u003cstd::mutex\u003e head_lock(wait_for_data()); // 4 return pop_head(); } std::unique_ptr\u003cnode\u003e wait_pop_head(T\u0026 value) { std::unique_lock\u003cstd::mutex\u003e head_lock(wait_for_data()); // 5 value=std::move(*head-\u003edata); return pop_head(); } public: std::shared_ptr\u003cT\u003e WaitAndPop() { std::unique_ptr\u003cnode\u003e const old_head=wait_pop_head(); return old_head-\u003edata; } void WaitAndPop(T\u0026 value) { auto _ = wait_pop_head(value); } }; 可能大家看到代码好像有点多，实际上都只是为了代码的重用，例如pop_head()①和wait_for_data()②，这些函数分别是删除头结点和等待队列中有数据弹出的。wait_for_data()特别值得关注，因为其不仅等待使用lambda函数对条件变量进行等待，而且它还会将锁的实例返回给调用者③。这就确保了wait_pop_head的线程安全。pop_head()是对TryPop()代码的复用。 TryPop和Empty实现 template\u003ctypename T\u003e class ThreadSafeQueue { private: std::unique_ptr\u003cnode\u003e try_pop_head() { std::lock_guard\u003cstd::mutex\u003e head_lock(head_mutex); if(head.get()==get_tail()) { return std::unique_ptr\u003cnode\u003e(); } return pop_head(); } std::unique_ptr\u003cnode\u003e try_pop_head(T\u0026 value) { std::lock_guard\u003cstd::mutex\u003e head_lock(head_mutex); if(head.get()==get_tail()) { return std::unique_ptr\u003cnode\u003e(); } value=std::move(*head-\u003edata); return pop_head(); } public: std::shared_ptr\u003cT\u003e TryPop() { std::unique_ptr\u003cnode\u003e old_head=try_pop_head(); return old_head?old_head-\u003edata:std::shared_ptr\u003cT\u003e(); } bool TryPop(T\u0026 value) { std::unique_ptr\u003cnode\u003e const old_head=try_pop_head(value); return old_head; } void Empty() { std::lock_guard\u003cstd::mutex\u003e head_lock(head_mutex); return (head.get()==get_tail()); } }; ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:2:2","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"简单测试 一个生产者，三个消费者，数据量15000 次数 v1版本 v3版本 v3原始指针版 第一次 7.31ms 8.51 8.53 第二次 6.61ms 9.26 7.25 第三次 7.60ms 8.90 7.95 一个生产者，三个同时消费生产，一个消费，数据量1500000 次数 v1版本 v3版本 v3原始指针版 第一次 397.90 爆栈 399.68 第二次 398.0 爆栈 362.06 第三次 319.28 爆栈 355.62 我的测试仅限于少量线程，而且任务负担也不重，故测出来的结果竟然是直接封装标准库的队列性能最好（都是在release模式下），我猜大概率是标准库的内存分配器优于我这个简单的内存管理，再加上我测试的线程数量非常少，细粒度的锁并未体现出它的优势。。。 ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:3:0","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"完整代码 代码仓库：thread_safe_queue ","date":"2022-08-20","objectID":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/:4:0","tags":["细粒度锁线程安全队列实现"],"title":"细粒度锁线程安全队列实现","uri":"/posts/%E7%BB%86%E7%B2%92%E5%BA%A6%E9%94%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/"},{"categories":["C++多线程"],"content":"async、packaged_task、promise、future的区别与使用","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"使用方法 想要更详尽的介绍可以看看这本书，这几个函数牵扯到的内容是并发操作的同步，对应《C++并发编程实战》的第四章，这本书很难啃，有很多地方我也是断断续续看了几遍才有体会。 首先我们来看看让人迷惑的地方：std::aync、std::package_task、std::promise 这几个调用都能获得 std::future，那到底有什么区别呢？下面我们挨个来看看基本使用方法，再进行一个小总结。 ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:1:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"std::async 前言 async 这个词在其他语言中想必也不陌生，比如 js 里有await和async关键字用于执行异步任务等等，C++中的这个词表示一个函数，实际上也是用于异步执行。但是与js那个实现原理有很大的不同，简单的说就是js那个更加的上层，而C++这个偏底层。导致的区别就是js那两个关键字配合起来只为达到并发的效果，而其中的实现细节是非常复杂的，比如js实际上始终只用到了一个线程来实现了并发，这点类似于协程，具体到内部js是通过事件循环队列来实现的，所以你始终无法确切的理解到它的底层工作。 而C++的async与之相比，就好像一个原始人，需要你传递参数来确定它的工作机制，但和C++标准库的其他api相比，它却是非常高级的存在。 使用 在此之前先简单了解下C++的async使用方式，一个简单的代码如下： #include \u003cfuture\u003e#include \u003ciostream\u003eint find_the_answer_to_ltuae(); void do_other_stuff(); int main() { std::future\u003cint\u003e the_answer=std::async(find_the_answer_to_ltuae); do_other_stuff(); std::cout\u003c\u003c\"The answer is \"\u003c\u003cthe_answer.get()\u003c\u003cstd::endl; } 这个函数会返回一个future用于将异步的任务和主线程同步。也就是当你调用future的get方法时，它会等待异步任务完成，并得到对应的返回值。 关于 std::future 也有三个重要的api：wait、wait_for、get，前面两个不返回结果，只等待任务，wait_for可以设置等待的时间限制。 但是请注意，这个任务不一定是按照你想的那样异步执行的，它可能在你调用get方法的时候才执行，这个取决于你传递的第一个参数。 std::launch::async：表示会开启一个线程去执行任务。 std::launch::deferred：表示延迟调用，只有在外界需要得到结果的时候才调用。 不传参数或者两者相与：表示由C++底层去调度，可能是async也可能是defferred。 实际上async函数在这几个C++的api里是最高级的，它同时拥有了异步与同步的能力。 ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:2:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"std::packaged_task 这个api与线程是没有任何关系的，它只是负责把普通的函数或者仿函数包装成方便异步转同步的任务。 比如我们开启一个线程去执行任务，我们又想对外界保有同步获取任务结果的能力，这个时候就需要用 std::package_task 来对现有的任务进行封装了。 如下代码： 以抖音用户的一次点赞行为对应的数据库底层需要调用的api举例子： plusVideoLike:增加对应视频的点赞数目，返回值为自定义的error类型 plusUserLikeList:增加用户喜欢的视频，返回值为自定义的error类型 很明显，这个两个过程互相并不影响，可以使用并发操作进行。 UserService::addLike(int uid,int vid){ auto task1 = std::package_task\u003cerror(int)\u003e{plusVideoLike}; auto task2 = std::package_task\u003cerror(int,int)\u003e{plusUserLikeList}; auto f1 = task1.get_future(); auto f2 = task2.get_future(); std::thread t1([\u0026](){ task1(vid); //video点赞+1 }); std::thread t2([\u0026](){ task2(uid,vid); //用户点赞视频+1 }); //得到返回值的同时完成同步操作 if(f1.get() == 某错误){ } if(f2.get()== 某错误{ } } ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:3:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"std::promise 从上面看下来，我们其实不难发现，async和packaged_task好像有某种联系？似乎async是封装了packaged_task的方便操作的api，没错大概就是这样。同样promise也是packaged_task中不可缺少的一环。 std::promise算是C++中提供的方便线程间通信的纽带之一，它的行为有些类似于管道，熟悉Go的朋友，你可以把这个类似于无缓冲的管道，通过promise我们存入数据，通过它的future我们读取数据，这里就类似于管道的读写了，只不过不能通过一个类型进行双向读写，这一点还是没有Golang的方便啊。 线程间通信我们一般都是通过共享内存，而共享内存写起来就很麻烦，为了线程安全以及事件完成的通知还要手动去封装一大堆的东西（用条件变量、信号量之类的去封装），我们大部分场景并不需要这么高的控制粒度，所以标准库帮你封装了一个promise和对应的future进行线程间的通信也是一个选择，当然如果能再进化成Golang中的channl模样，那就更好了（似乎C++20有协程又有管道😁）。 说了这么多，来一个简单的例子吧。 我们再切换到一个场景：实现一个聊天程序的后台服务器，我们举其中较为契合的地方，比如一个聊天室的消息收发，当聊天室里的一个人发送消息后，需要将该消息对聊天室里的其他人进行广播，这个广播的过程，我们可以用并发来进行，但是最好建议不要把每个任务都开启一个新线程去执行，这样服务器迟早崩溃，最好的做法是使用线程池或者直接上协程无所畏惧。这里的执行成功或者失败需要传递到外界，这时我们可以使用promise当然你也可以用package_task将任务进行封装，只不过没有promise灵活（但大部分时候比promise好用，比如这里我还是用package_task好些） 我们把该类命名为ChatRoom类，方法如下： broadcast(int userId,Message msg);用于对消息进行广播 房间内有非常多的用户对应User类，而user有对应的接收消息的方法： receive(Message msg); 接收对应的消息 broadcast(int userId,Message msg){ vector\u003cfuture\u003cerror\u003e\u003e futures; for(auto\u0026\u0026 user:Users){ if(user.id != userId){ thread_pool.submit([\u0026]{ package_task\u003cbool(Message)\u003etask{std::bind(User::recive,\u0026user)}; futures.push_back(task.get_future()); task(msg); }); } } bool f; do{ //主线程等待完成并进行对应的错误处理，当全都完成任务执行完成 f = true; for(auto\u0026\u0026 f:futures){ if(f.wait_for(std::chrono::seconds(1) == std::future_status::ready){ ... //对应的错误处理 continue; } f = false; } }while(f); } 其实这里的线程池返回一个 future 是最好，否则同步操作将不那么优雅。 ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"自顶向下 看完上面的内容，相信你对这些线程同步的api应该有了一定的了解，我们会发现这几个api的应用场景都非常相似，实际上是一层套一层的往上进行封装起来得到对应的功能的。 有了promise，我们可以封装出package_task，有了package_task我们可以封装出async，这都是一环套一环，使用的复杂度不断降低的同时，线程同步的灵活性也在不断降低。当我们需要自己封装一套工具的时候，大概率是用的promise和package_task比较多，而我们只管简单使用时，则最好时使用已经封装好的上层api。比如我们如果要自己实现一个返回future的线程池，那么封装packaged_task是最好的选择。 现在我们自顶向下，来简单看看如何实现（以下代码非原创，如有雷同，那就是我抄的😥）。 ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"用packaged_task实现async std::future\u003cint\u003e my_async(function\u003cint(int i)\u003e task, int i) { std::packaged_task\u003cint(int)\u003e package{task}; std::future\u003cint\u003e f = package.get_future(); std::thread t(std::move(package), i); t.detach(); return f; } int main() { auto task = [](int i) { std::this_thread::sleep_for(std::chrono::seconds(5)); return i+100; }; std::future\u003cint\u003e f = my_async(task, 5); std::cout \u003c\u003c f.get() \u003c\u003c std::endl; return 0; } ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:1","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"使用promise实现packaged_task template \u003ctypename\u003e class my_task; template \u003ctypename R, typename ...Args\u003e class my_task\u003cR(Args...)\u003e { std::function\u003cR(Args...)\u003e fn; std::promise\u003cR\u003e pr; // the promise of the result public: template \u003ctypename ...Ts\u003e explicit my_task(Ts \u0026\u0026... ts) : fn(std::forward\u003cTs\u003e(ts)...) { } template \u003ctypename ...Ts\u003e void operator()(Ts \u0026\u0026... ts) { pr.set_value(fn(std::forward\u003cTs\u003e(ts)...)); // fulfill the promise } std::future\u003cR\u003e get_future() { return pr.get_future(); } // disable copy, default move }; ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:2","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["C++多线程"],"content":"总结 如果需要封装自己的api方便使用，那么packaged_task和promise可能是你最常用的。 如果想要开箱即用，那么直接async也挺好。 ","date":"2022-08-17","objectID":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/:6:0","tags":["async、packaged_task、promise、future的区别与使用"],"title":"async、packaged_task、promise、future的区别与使用","uri":"/posts/asyncpackage_task%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["面试相关"],"content":"C++面试一条龙","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"面试题的学习（八股文 cpp面试题1 cpp面试题2 面向企业刷题的刷题网站 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:1:0","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"计算机系统和计算机网络视频学习 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:2:0","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"计算机系统 准备面试用 深入学习用 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:2:1","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"计算机网络 准备面试用 深入学习用 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:2:2","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"项目准备 项目我的建议是看书+动手敲+解决问题 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:3:0","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"书籍推荐 入门+浅浅实战 深入实战 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:3:1","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["面试相关"],"content":"视频推荐 零基础相关视频学习、 这个视频基本上可以平替第一本书，但是第一本书里面的系统调用讲解的更为丰富和全面，这个视频讲解的是用到最多的。 视频链接 muduo网络库相关 这两个视频都是围绕第二本书的库进行讲解 muduo网络库源码解析 这个视频不全 muduo网络库从0知识解析 ","date":"2022-08-05","objectID":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/:3:2","tags":["C++面试一条龙"],"title":"C++面试面试一条龙","uri":"/posts/c++%E9%9D%A2%E8%AF%95%E4%B8%80%E6%9D%A1%E8%B7%AF/"},{"categories":["个人轮子计划"],"content":"JSON解析器实现","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":" 代码开源仓库：cpp造轮子项目–实现json解析器 ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:0:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"JSON格式介绍 JSON(JavaScript Object Notation)，是一种序列化的格式，最大的优点在于可读性极强，以及可直接嵌入到js代码中，所以广泛运用于web数据的收发。 JSON格式有以下基本类型： null类型：值为null，表示为空 bool类型：值为true和false number类型：值为int、double（即整数或小数 string类型：形如 “abc” 以及以下复合类型： list类型（也称array类型） [\"abc\",3.2,323,\"sdaf\"] dict类型（也称object类型） { \"id\":32, \"name\":\"hhh\" } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:1:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"解析json字符串 整套解析流程如下： ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:2:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"创建JObject类 我们需要把json的类型对应到计算机语言的类型。 由于json的数据在我们看来都是字符串，那么有如下对应关系： “null\"对应我们构造的null类型 “true”,“false\"对应内部的bool类型即可 number类型数据对应int、double类型 string类型数据对应string即可 list类型对应C++中的vector dict类型对应C++中的map或unordered_map 我们在计算机语言中，需要构造一个对象类型，用于将以上类型全部涵盖。 在C++中我们通过std::variant来进行，还需要一个枚举tag来表示当前对象内存储的数据类型。 当然如果做的更绝的话，可以通过一个void* + 申请堆内存来解决，然后再强转为对应类型来操作。 对应的代码如下：（中间的类方法就暂时省略了 enum TYPE { T_NULL, T_BOOL, T_INT, T_DOUBLE, T_STR, T_LIST, T_DICT }; using null_t = string; using int_t = int32_t; using bool_t = bool; using double_t = double; using str_t = string; using list_t = vector\u003cJObject\u003e; using dict_t = map\u003cstring, JObject\u003e; class JObject { public: using value_t = variant\u003cbool_t, int_t, double_t, str_t, list_t, dict_t\u003e; ... private: TYPE m_type; value_t m_value; }; ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:2:1","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"创建Parser类 我们有了JObject，可以把所有的JSON数据接收起来，现在要做的就是扫描JSON字符串，对其中的数据进行读取处理，然后转化为JObject。 关键代码如下： JObject Parser::parse() { char token = get_next_token(); if (token == 'n') { return parse_null(); } if (token == 't' || token == 'f') { return parse_bool(); } if (token == '-' || std::isdigit(token)) { return parse_number(); } if (token == '\\\"') { return parse_string(); } if (token == '[') { return parse_list(); } if (token == '{') { return parse_dict(); } throw std::logic_error(\"unexpected character in parse json\"); } 以上就是整个字符串的解析过程，每次通过get_next_token这个方法得到整个字符串的下一个token，根据token决定解析对应的数据类型。 get_next_token方法 跳过空白符号，以及跳过注释（标准的JSON格式不支持注释，我这里硬加的，为了vscode的JSON格式配置文件解析 char Parser::get_next_token() { while (std::isspace(m_str[m_idx])) m_idx++; if (m_idx \u003e= m_str.size()) throw std::logic_error(\"unexpected character in parse json\"); //如果是注释，记得跳过 skip_comment(); return m_str[m_idx]; } parse_null和parse_bool 由于这两个很简单，就放在一起了。 parse_null JObject Parser::parse_null() { if (m_str.compare(m_idx, 4, \"null\") == 0) { m_idx += 4; return {}; } throw std::logic_error(\"parse null error\"); } parse_bool bool Parser::parse_bool() { if (m_str.compare(m_idx, 4, \"true\") == 0) { m_idx += 4; return \"true\"; } if (m_str.compare(m_idx, 5, \"false\") == 0) { m_idx += 5; return \"false\"; } throw std::logic_error(\"parse bool error\"); } parse_number JObject Parser::parse_number() { auto pos = m_idx; //integer part if (m_str[m_idx] == '-') { m_idx++; } if (isdigit(m_str[m_idx])) while (isdigit(m_str[m_idx])) m_idx++; else { throw std::logic_error(\"invalid character in number\"); } if (m_str[m_idx] != '.') { return (int) strtol(m_str.c_str() + pos, nullptr, 10); } //decimal part if (m_str[m_idx] == '.') { m_idx++; if (!std::isdigit(m_str[m_idx])) { throw std::logic_error(\"at least one digit required in parse float part!\"); } while (std::isdigit(m_str[m_idx])) m_idx++; } return strtof64(m_str.c_str() + pos, nullptr); } parse_list JObject Parser::parse_list() { JObject arr((list_t()));//得到list类型的JObject m_idx++; char ch = get_next_token(); if (ch == ']') { m_idx++; return arr; } while (true) { arr.push_back(parse()); ch = get_next_token(); if (ch == ']') { m_idx++; break; } if (ch != ',') //如果不是逗号 { throw std::logic_error(\"expected ',' in parse list\"); } //跳过逗号 m_idx++; } return arr; } parse_dict JObject Parser::parse_dict() { JObject dict((dict_t()));//得到dict类型的JObject m_idx++; char ch = get_next_token(); if (ch == '}') { m_idx++; return dict; } while (true) { //解析key string key = std::move(parse().Value\u003cstring\u003e()); ch = get_next_token(); if (ch != ':') { throw std::logic_error(\"expected ':' in parse dict\"); } m_idx++; //解析value dict[key] = parse(); ch = get_next_token(); if (ch == '}') { m_idx++; break; //解析完毕 } if (ch != ',')//没有结束，此时必须为逗号 { throw std::logic_error(\"expected ',' in parse dict\"); } //跳过逗号 m_idx++; } return dict; } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:2:2","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"完善JObject类 很明显，我们需要为JObject类提供一个方法，此方法可以让调用者直接访问到std::variant里面对应的数据，并且我们也需要提供一个方法能让JObject快速初始化为对应的类型。 ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:3:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"初始化接口 添加好下面这些方法后，外界可通过调用方法把JObject的内部状态改变。 void Null() { m_type = T_NULL; m_value = \"null\"; } void Int(int_t value) { m_value = value; m_type = T_INT; } void Bool(bool_t value) { m_value = value; m_type = T_BOOL; } void Double(double_t value) { m_type = T_DOUBLE; m_value = value; } void Str(string_view value) { m_value = string(value); m_type = T_STR; } void List(list_t value) { m_value = std::move(value); m_type = T_LIST; } void Dict(dict_t value) { m_value = std::move(value); m_type = T_DICT; } 为了方便平时的赋值时候的隐式转化，我们应该再添加对应的构造函数，如下：（隐式转化在C++里有个坑，只能为类提供一种方向的隐式转化，比如提供了int把转为JObject的隐式转化后，就不能再提供把JObject转为int的隐式转化了，这两种必须要有一个是explicit，否则报错 JObject()//默认为null类型 { m_type = T_NULL; m_value = \"null\"; } JObject(int_t value) { Int(value); } JObject(bool_t value) { Bool(value); } JObject(double_t value) { Double(value); } JObject(str_t const \u0026value) { Str(value); } JObject(list_t value) { List(std::move(value)); } JObject(dict_t value) { Dict(std::move(value)); } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:3:1","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"请求值接口 设计思路：本人不喜欢一大堆的get/set，那样真的是麻烦。我通过提供一个Value方法，该方法为泛型，其内部有调用value()方法得到对应的数据指针，而Value方法则负责将指针强转，其内部也实现了强大的错误处理，防止处理指针的意外宕机。。。 value方法如下，用于调用get_if得到对应的数据指针，关于怎么获取std::variant的数据，有get和get_if两种方式，get得到的是对象的引用，如果获取不到，则抛出异常，get_if获取对象的指针，如果获取不到则返回nullptr。我选择使用get_if的原因是，这个异常的处理可以由你自己来设定提示，而不是对着底层的get提示而摸不着头脑。 void *JObject::value() { switch (m_type) { case T_NULL: return get_if\u003cstr_t\u003e(\u0026m_value); case T_BOOL: return get_if\u003cbool_t\u003e(\u0026m_value); case T_INT: return get_if\u003cint_t\u003e(\u0026m_value); case T_DOUBLE: return get_if\u003cdouble_t\u003e(\u0026m_value); case T_LIST: return get_if\u003clist_t\u003e(\u0026m_value); case T_DICT: return get_if\u003cdict_t\u003e(\u0026m_value); case T_STR: return std::get_if\u003cstr_t\u003e(\u0026m_value); default: return nullptr; } } Value方法： #define THROW_GET_ERROR(erron) throw std::logic_error(\"type error in get \"#erron\" value!\") template\u003cclass V\u003e V \u0026Value() { //添加安全检查 if constexpr(IS_TYPE(V, str_t)) { if (m_type != T_STR) THROW_GET_ERROR(string); } else if constexpr(IS_TYPE(V, bool_t)) { if (m_type != T_BOOL) THROW_GET_ERROR(BOOL); } else if constexpr(IS_TYPE(V, int_t)) { if (m_type != T_INT) THROW_GET_ERROR(INT); } else if constexpr(IS_TYPE(V, double_t)) { if (m_type != T_DOUBLE) THROW_GET_ERROR(DOUBLE); } else if constexpr(IS_TYPE(V, list_t)) { if (m_type != T_LIST) THROW_GET_ERROR(LIST); } else if constexpr(IS_TYPE(V, dict_t)) { if (m_type != T_DICT) THROW_GET_ERROR(DICT); } void *v = value(); if (v == nullptr) throw std::logic_error(\"unknown type in JObject::Value()\"); return *((V *) v); //强转即可 } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:3:2","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"重载方法让对象更好用 当JObject为dict类型时，我们可以直接用下标运算符进行key-value的赋值（得益于隐式转化和运算符重载 JObject \u0026operator[](string const \u0026key) { if (m_type == T_DICT) { auto \u0026dict = Value\u003cdict_t\u003e(); return dict[key]; } throw std::logic_error(\"not dict type! JObject::opertor[]()\"); } 同样如果为list对象，我们也准备了push_back等方法 void push_back(JObject item) { if (m_type == T_LIST) { auto \u0026list = Value\u003clist_t\u003e(); list.push_back(std::move(item)); return; } throw std::logic_error(\"not a list type! JObjcct::push_back()\"); } 当然，为了让类更好用，你也可以重载很多其他方法，但是注意别忘了类型的安全检查！ ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:3:3","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"完善Parser类 我们之前是完成了整个字符串到JObject的解析过程，但是每次都需要创建一个Parser类，然后调用方法，这样的过程未免有些繁琐，我们可以对外提供FromString的静态方法，然后充分利用一个对象便可完成整个解析过程。 如下： JObject Parser::FromString(string_view content) { static Parser instance; instance.init(content); return instance.parse(); } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:4:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"完成序列化和反序列化过程 我们前面所做的工作其实已经把这个过程相当于完成了，当然还差一个把JObject变为JSON字符串的方法，现在添加上就是，也不是很难，按照相似的逻辑反推一遍就行。如下给JObject添加一个to_string方法： string JObject::to_string() { void *value = this-\u003evalue(); std::ostringstream os; switch (m_type) { case T_NULL: os \u003c\u003c \"null\"; break; case T_BOOL: if (GET_VALUE(bool)) os \u003c\u003c \"true\"; else os \u003c\u003c \"false\"; break; case T_INT: os \u003c\u003c GET_VALUE(int); break; case T_DOUBLE: os \u003c\u003c GET_VALUE(double); break; case T_STR: os \u003c\u003c '\\\"' \u003c\u003c GET_VALUE(string) \u003c\u003c '\\\"'; break; case T_LIST: { list_t \u0026list = GET_VALUE(list_t); os \u003c\u003c '['; for (auto i = 0; i \u003c list.size(); i++) { if (i != list.size() - 1) { os \u003c\u003c ((list[i]).to_string()); os \u003c\u003c ','; } else os \u003c\u003c ((list[i]).to_string()); } os \u003c\u003c ']'; break; } case T_DICT: { dict_t \u0026dict = GET_VALUE(dict_t); os \u003c\u003c '{'; for (auto it = dict.begin(); it != dict.end(); ++it) { if (it != dict.begin()) //为了保证最后的json格式正确 os \u003c\u003c ','; os \u003c\u003c '\\\"' \u003c\u003c it-\u003efirst \u003c\u003c \"\\\":\" \u003c\u003c it-\u003esecond.to_string(); } os \u003c\u003c '}'; break; } default: return \"\"; } return os.str(); } 有关JObject的方法也都补充的差不多了，那么我们现在要考虑的是如何通过JObject这个中间对象将我们自定义的任何一个类给序列化和反序列化？ 如图： 所有的序列化和反序列化的过程都依托于JObject进行。而Parser这个类在中间作为一个方便使用的对外接口。 ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:5:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"序列化接口设计 在Parser类中添加一个ToJSON的静态方法，用来对任意类型进行序列化，这个方法使用模板。 代码如下： template\u003cclass T\u003e static string ToJSON(T const \u0026src) { //如果是基本类型 if constexpr(IS_TYPE(T, int_t)) { JObject object(src); return object.to_string(); } else if constexpr(IS_TYPE(T, bool_t)) { JObject object(src); return object.to_string(); } else if constexpr(IS_TYPE(T, double_t)) { JObject object(src); return object.to_string(); } else if constexpr(IS_TYPE(T, str_t)) { JObject object(src); return object.to_string(); } //如果是自定义类型调用方法完成dict的赋值，然后to_string即可 json::JObject obj((json::dict_t())); src.FUNC_TO_NAME(obj); return obj.to_string(); } 如上代码如果是基本类型则直接初始化JObject调用to_string方法，如果是自定义类型，则需要在该类型中嵌入一个方法，这个方法名字我们用FUNC_TO_NAME这个宏代替。 也就是说，如果是自定义的类型，那么肯定就是对应JSON数据的dict类型，所以只需要你对该类型定义对应的方法，该方法需要将值传递给JObject，为了简化这个过程我们用宏来替代。 宏定义如下： #define FUNC_TO_NAME _to_json #define FUNC_FROM_NAME _from_json #define START_TO_JSON void FUNC_TO_NAME(json::JObject \u0026 obj) const{ #define to(key) obj[key] //push一个自定义类型的成员 #define to_struct(key, struct_member) json::JObject tmp((json::dict_t())); struct_member.FUNC_TO_NAME(tmp); obj[key] = tmp #define END_TO_JSON } #define START_FROM_JSON void FUNC_FROM_NAME(json::JObject\u0026 obj) { #define from(key, type) obj[key].Value\u003ctype\u003e() #define from_struct(key, struct_member) struct_member.FUNC_FROM_NAME(obj[key]) #define END_FROM_JSON } 基本使用如下： struct Base { int pp; string qq; START_FROM_JSON //生成反序列化相关的方法 pp = from(\"pp\", int); qq = from(\"qq\", string); END_FROM_JSON START_TO_JSON //生成序列化相关的代码 to(\"pp\") = pp; to(\"qq\") = qq; END_TO_JSON }; struct Mytest { int id; std::string name; Base q; START_TO_JSON //序列化相关代码 to_struct(\"base\", q); to(\"id\") = id; to(\"name\") = name; END_TO_JSON START_FROM_JSON //反序列化相关代码 id = from(\"id\", int); name = from(\"name\", string); from_struct(\"base\", q); END_FROM_JSON }; 实际上上面代码等效于下面的代码，以Base类为例： struct Base { int pp; string qq; void _from_json(json::JObject\u0026 obj){ //反序列化 pp = obj.Value\u003cint\u003e(); qq = obj.Value\u003cstring\u003e(); } void _to_json(json::JObject\u0026 obj){//序列化代码 obj[\"pp\"] = pp; obj[\"qq\"] = qq; } }; ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:5:1","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"简单使用 #include\u003ciostream\u003e#include \"JObject.h\"#include \"Parser.h\"#include \u003cfstream\u003e#include \"../benchmark/Timer.hpp\" using namespace json; struct Base { int pp; string qq; START_FROM_JSON pp = from(\"pp\", int); qq = from(\"qq\", string); END_FROM_JSON START_TO_JSON to(\"pp\") = pp; to(\"qq\") = qq; END_TO_JSON }; struct Mytest { int id; std::string name; Base q; START_TO_JSON to_struct(\"base\", q); to(\"id\") = id; to(\"name\") = name; END_TO_JSON START_FROM_JSON id = from(\"id\", int); name = from(\"name\", string); from_struct(\"base\", q); END_FROM_JSON }; void test_class_serialization() { Mytest test{.id=32, .name=\"fda\"}; auto item = Parser::FromJson\u003cMytest\u003e(R\"({\"base\":{\"pp\":0,\"qq\":\"\"},\"id\":32,\"name\":\"fda\"} )\"); //serialization std::cout \u003c\u003c Parser::ToJSON(item); //deserialization } void test_string_parser() { { Timer t; std::ifstream fin(\"../../test_source/test.json\"); std::string text((std::istreambuf_iterator\u003cchar\u003e(fin)), std::istreambuf_iterator\u003cchar\u003e()); json::Parser p; p.init(text); auto q = p.parse(); std::ofstream fout(\"../../test_source/test_out.json\"); fout \u003c\u003c q.to_string(); } } int main() { test_class_serialization(); test_string_parser(); } ","date":"2022-07-27","objectID":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/:6:0","tags":["JSON解析器实现"],"title":"JSON解析器实现","uri":"/posts/json%E8%A7%A3%E6%9E%90%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"Logger日志库的实现","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":" 代码仓库：https://github.com/ACking-you/MyUtil/tree/master/my-logger ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:0:0","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"日志的重要性以及概述 一个理想的日志库，至少需要满足以下三点： 支持高度自定义。 拓展性好。 使用方便，且有安全性保证。 当然要是能在输出的时候出现彩色那就再好不过了🥳 ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:1:0","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"具体实现 ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:2:0","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"以模块拆分 ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:2:1","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"代码逻辑 日志输出逻辑（省去了标志位的判断 日志状态的初始化逻辑 ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:3:0","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["个人轮子计划"],"content":"后期拓展展望 支持分布式日志存储 ","date":"2022-07-24","objectID":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/:4:0","tags":["Logger日志库的实现"],"title":"Logger日志库的实现","uri":"/posts/logger%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"categories":["Linux网络编程"],"content":"select、poll、epoll浅析","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":" 整篇博客的完整示例代码在：github ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:0:0","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"select ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:1:0","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"介绍与使用 一、介绍： select系统调用的目的是：在一段指定时间内，监听用户感兴趣的文件描述符上的可读、可写和异常事件。poll和select应该被归类为这样的系统 调用，它们可以阻塞地同时探测一组支持非阻塞的IO设备，直至某一个设备触发了事件或者超过了指定的等待时间——也就是说它们的职责不是做IO，而是帮助 调用者寻找当前就绪的设备。 原理图： 二、使用 需要的系统调用API如下： #include \u003csys/time.h\u003e#include \u003csys/types.h\u003e#include \u003cunistd.h\u003eint select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 一般经过以下三个过程： 定义bitmap结构，就是定义fd_set类型变量，内部实现是一个int数组，整个数组的bit位长 FD_SETSIZE，这个宏默认为1024。 根据文件描述符大小设置bitmap，该步骤调用FD_SET宏进行设置。 调用select函数，并传入bitmap，select内部会根据bitmap上的标记进行轮询，一旦有文件描述符触发事件就将其重新标记到用户态的bitmap里，select除了第一个参数外（第一个参数传入fd的最大值+1），后面连续三个传入的指针参数分别代表：监听读取事件的bitmap、监听写入事件的bitmap、监听异常事件的bitmap，最后一个参数表示延迟时间。 根据bitmap得到已经准备好的文件描述符，并对其执行相应的操作。（Reactor模型 一个文件描述符是否准备就绪，有以下判断： 在网络编程中， 下列情况下socket可读： a) socket内核接收缓冲区的字节数大于或等于其低水位标记SO_RCVLOWAT； b) socket通信的对方关闭连接，此时该socket可读，但是一旦读该socket，会立即返回0（可以用这个方法判断client端是否断开连接）； c) 监听socket上有新的连接请求； d) socket上有未处理的错误。 下列情况下socket可写： a) socket内核发送缓冲区的可用字节数大于或等于其低水位标记SO_SNDLOWAT； b) socket的读端关闭，此时该socket可写，一旦对该socket进行操作，该进程会收到SIGPIPE信号； c) socket使用connect连接成功之后； d) socket上有未处理的错误。 关键代码如下： //1.定义bitmap结构。 fd_set，是一个bitmap，大小为1024位，是一个长度为1024/32的int数组 fd_set rset; char buffer[MAXBUF]; while (1) { FD_ZERO(\u0026rset); //重置为0 for (int i = 0; i \u003c 5; ++i) { FD_SET(fds[i],\u0026rset); //2.根据文件描述符标记bitmap } puts(\"round again\\n\"); select(max+1,\u0026rset,NULL,NULL,NULL); //3.调用select进行轮询 for(auto \u0026fd:fds){ if(FD_ISSET(fd,\u0026rset)){ //4.获取已经准备好的描述符进行相应操作 memset(buffer, 0, MAXBUF); read(fd, buffer, MAXBUF); puts(buffer); } } } ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:1:1","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"优缺点 从以下三个角度来评析： 支持的最大连接数：与 FD_SETSIZE 宏的大小有关，一般为1024。 内核态到用户态的拷贝消耗：非常高，每次select调用都会重新copy一次。 内核态扫描的数据结构：线性扫描，FD剧增后会造成很大的效率问题 我们发现上面所说的都是它的各方面特性，同时也体现出了它的缺点。 缺点：支持的最大文件描述符不够大、每次select调用需要进行大量拷贝且bitmap每次都需要重新set值、内部逻辑是线性扫描不适合大量描述符的情况。 同样在某些条件下，这种简单模型反而会成为优点。 优点：如果是连接数特别少的情况下，线性扫描反而可能是最优的选择。 ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:1:2","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"poll ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:2:0","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"介绍与使用 一、介绍 poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。 二、使用 经过以下过程： 创建pollfd结构，并设置好fd和events。 调用poll函数进行轮询，第一个参数传入需要轮询的pollfd数组，第二个参数传入数组长度，第三个是超时时间。 遍历pollfd数组检测就绪fd并对其进行相应的处理。 代码如下： //1.创建pollfd结构，并设置好要管理的文件描述符以及对应的事件 pollfd pollfds[5]; for(auto\u0026 pollfd: pollfds){ pollfd.fd = accept(server_fd,(sockaddr*)\u0026client,\u0026socklen); if(pollfd.fd\u003c0) ERR_EXIT(\"fd accept error\"); pollfd.events = POLLIN; //检测读取事件 } while (1){ puts(\"round again\"); //2.开始进行poll轮询 if(poll(pollfds,5,5000)\u003c0) //通过把需要监听的fd拷贝到内核态，如果有事件可读，则设置revents ERR_EXIT(\"poll error\"); //3.遍历pollfd数组检测已经就绪的事件，并执行对应的操作 for(auto \u0026 pollfd:pollfds){ if(pollfd.revents\u0026POLLIN){ pollfd.revents = 0; //重新设置 char buffer[1024]{}; int len; if((len= read(pollfd.fd,buffer,1024))\u003c0) ERR_EXIT(\"pollfd read error\"); if(write(pollfd.fd,buffer,len)\u003c0){ ERR_EXIT(\"pollfd write error\"); } } } } ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:2:1","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"优缺点 三个角度剖析 支持的最大连接数：非常多（具体根据系统调度和创建的结构来。 内核态到用户态的拷贝消耗：非常高，每次poll调用都会将整个数组重新从用户态到内核态copy一次，且只支持水平触发。 内核态扫描的数据结构：线性扫描，FD剧增后会造成很大的效率问题。 优点：解决了select因为采取bitmap的连接数限制，且利用的是事件与fd绑定的结构体，用起来会更顺手。 缺点：仍然没有解决关键的效率问题，同时每次还是通过循环所有结构体来判断事件是否产生的方式来进行，所以本质上和select没有什么不同。 ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:2:2","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["Linux网络编程"],"content":"epoll 前面的讲解都只是为epoll铺路。 具体而言epoll解决了前面的IO复用模型的很多问题。 特点： IO事件和监视的fd只需要添加一次。epoll_ctl 轮询操作是直接将已经就绪的fd赋值到用户态的数组，返回就绪的长度，也就是不需要再遍历所有的fd来判断是否就绪。epoll_wait 底层采用红黑树进行轮询，即使有大量fd需要检测，效率也不会太差。 支持水平触发和边缘触发。 技术点： 数据结构优化：底层采取红黑树+就绪队列。 零拷贝优化：使用mmap进行内存映射实现内存共享。 多种触发形式优化：支持EPOLLLT和EPOLLET两种触发模式，也就是支持水平触发和边缘触发。 关于水平触发与边缘触发： 水平触发是只要符合触发条件，就会一直触发可读信号，而边缘触发仅仅在状态转变的时候触发一次。 这个和数字电路里面的概念类似：你可以把当前是否符合触发条件想象成一个张图，这张图上有两个状态符合触发条件和不符合触发条件，水平触发是只要状态符合就触发，边缘触发是状态发生改变就触发，而在IO事件里这里的触发状态指的是前面select中讲的socket可读可写等状态。 使用： 调用epoll_create创建结构并返回描述符。（需要传入一个参数，Linux 2.6.8后，传入的参数只要大于0即可，在此之前，表示最大的文件描述符 调用epoll_ctl加入需要监听的文件描述符以及对应事件（epoll_event结构体）。（需要传入四个参数：1.需要操作的epoll结构的描述符。2.需要进行的操作。3.需要操作的文件描述符。4.需要加入的结构体。 调用epoll_wait等待事件的到来，并返回就绪的文件描述符数量。（需要传入四个参数：1.需要操作的epoll结构的描述符。2.用于存放就绪的文件描述符的数组。3.数组的最大长度。4.超时时间。）返回值：就绪的文件描述符数量。 代码如下： //1.在内核中创建结构并返回描述符 epoll_create主要是将内核态的数据结构创建出来并初始化，然后再将它加入进程文件表，得到文件描述符 auto epfd = epoll_create(1); //Linux 2.6.8后，传入的参数只要大于0即可，在此之前，表示最大的文件描述符 //2.调用epoll_ctl加入需要监听的文件描述符以及对应事件 epoll_event evs[5]; for(int i=0;i\u003c5;i++){ sockaddr_in client{}; socklen_t socklen; evs[i].data.fd = accept(server_fd,(sockaddr*)\u0026client,\u0026socklen); evs[i].events = EPOLLIN; //需要读取的事件 if(epoll_ctl(epfd,EPOLL_CTL_ADD,evs[i].data.fd,\u0026evs[i])\u003c0) //添加或删除内核数据结构中指定的文件描述符对应的结点 ERR_EXIT(\"epoll_ctl\"); } while(1){ puts(\"round again\"); //3.调用epoll_wait等待事件的到来，并返回就绪的文件描述符数量 //查询就绪队列，并将已经ok的文件描述符，从左到右写入数组，返回写入的长度，返回-1表示轮询超时 auto nfds = epoll_wait(epfd,evs,5,10000); //查询就绪队列，然后把就绪到位的文件描述符和对应的情况写入用户空间中的数组 //遍历数组，执行最后的动作 for(int i=0;i\u003cnfds;i++){ char buffer[1024]{}; int len{}; if((len = read(evs[i].data.fd,buffer,1024))\u003c0){ ERR_LOG(\"read error\"); }else{ if(write(evs[i].data.fd,buffer,len)\u003c0){ ERR_LOG(\"write error\"); } } } } 对epoll实现原理更为具体的描述有以下链接： epoll 原理是如何实现的？ 图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！ ","date":"2022-06-22","objectID":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/:3:0","tags":["select、poll、epoll浅析"],"title":"select、poll、epoll浅析","uri":"/posts/selectpollepoll%E6%B5%85%E6%9E%90/"},{"categories":["面试相关"],"content":"C++面试题1","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"1、 在main执行之前和之后执行的代码可能是什么？ main函数执行之前，主要就是初始化系统相关资源： 设置栈指针 初始化静态static变量和global全局变量，即.data段的内容 将未初始化部分的全局变量赋初值：数值型short，int，long等为0，bool为FALSE，指针为NULL等等，即.bss段的内容 全局对象初始化，在main之前调用构造函数，这是可能会执行前的一些代码 将main函数的参数argc，argv等传递给main函数，然后才真正运行main函数 __attribute__((constructor))这是gnu C里面的东西，可修饰函数，控制它在main函数之前执行，它不跨平台。 main函数执行之后： 全局对象的析构函数会在main函数之后执行； 可以用 atexit 注册一个函数，它会在main 之后执行;（跨平台，是标准库里的东西 __attribute__((destructor))同上面的construct，只不过是在main函数执行后。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:1:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"2、结构体内存对齐问题？ 结构体内成员按照声明顺序存储，第一个成员地址和整个结构体地址相同。 未特殊说明时，按结构体中size最大的成员对齐（若有double成员，按8字节对齐。） c++11以后引入两个关键字 alignas (opens new window)与 alignof (opens new window)。其中alignof可以计算出类型的对齐方式，alignas可以指定结构体的对齐方式。 但是alignas在某些情况下是不能使用的，具体见下面的例子: // alignas 生效的情况 struct Info { uint8_t a; uint16_t b; uint8_t c; }; std::cout \u003c\u003c sizeof(Info) \u003c\u003c std::endl; // 6 2 + 2 + 2 std::cout \u003c\u003c alignof(Info) \u003c\u003c std::endl; // 2 struct alignas(4) Info2 { uint8_t a; uint16_t b; uint8_t c; }; std::cout \u003c\u003c sizeof(Info2) \u003c\u003c std::endl; // 8 4 + 4 std::cout \u003c\u003c alignof(Info2) \u003c\u003c std::endl; // 4 alignas将内存对齐调整为4个字节。所以sizeof(Info2)的值变为了8。 // alignas 失效的情况 struct Info { uint8_t a; uint32_t b; uint8_t c; }; std::cout \u003c\u003c sizeof(Info) \u003c\u003c std::endl; // 12 4 + 4 + 4 std::cout \u003c\u003c alignof(Info) \u003c\u003c std::endl; // 4 struct alignas(2) Info2 { uint8_t a; uint32_t b; uint8_t c; }; std::cout \u003c\u003c sizeof(Info2) \u003c\u003c std::endl; // 12 4 + 4 + 4 std::cout \u003c\u003c alignof(Info2) \u003c\u003c std::endl; // 4 若alignas小于自然对齐的最小单位，则被忽略。 如果想使用单字节对齐的方式，使用alignas是无效的。应该使用#pragma pack(push,1)或者使用__attribute__((packed))。 #if defined(__GNUC__) || defined(__GNUG__) #define ONEBYTE_ALIGN __attribute__((packed)) #elif defined(_MSC_VER) #define ONEBYTE_ALIGN #pragma pack(push,1) #endif struct Info { uint8_t a; uint32_t b; uint8_t c; } ONEBYTE_ALIGN; #if defined(__GNUC__) || defined(__GNUG__) #undef ONEBYTE_ALIGN #elif defined(_MSC_VER) #pragma pack(pop) #undef ONEBYTE_ALIGN #endif std::cout \u003c\u003c sizeof(Info) \u003c\u003c std::endl; // 6 1 + 4 + 1 std::cout \u003c\u003c alignof(Info) \u003c\u003c std::endl; // 6 确定结构体中每个元素大小可以通过下面这种方法: #if defined(__GNUC__) || defined(__GNUG__) #define ONEBYTE_ALIGN __attribute__((packed)) #elif defined(_MSC_VER) #define ONEBYTE_ALIGN #pragma pack(push,1) #endif /** * 0 1 3 6 8 9 15 * +-+---+-----+---+-+-------------+ * | | | | | | | * |a| b | c | d |e| pad | * | | | | | | | * +-+---+-----+---+-+-------------+ */ struct Info { uint16_t a : 1; uint16_t b : 2; uint16_t c : 3; uint16_t d : 2; uint16_t e : 1; uint16_t pad : 7; } ONEBYTE_ALIGN; #if defined(__GNUC__) || defined(__GNUG__) #undef ONEBYTE_ALIGN #elif defined(_MSC_VER) #pragma pack(pop) #undef ONEBYTE_ALIGN #endif std::cout \u003c\u003c sizeof(Info) \u003c\u003c std::endl; // 2 std::cout \u003c\u003c alignof(Info) \u003c\u003c std::endl; // 1 这种处理方式是alignas处理不了的。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:2:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"3、指针和引用的区别 指针是一个变量，存储的是一个地址，引用跟原来的变量实质上是同一个东西，是原变量的别名 指针可以有多级，引用只有一级 指针可以为空，引用不能为NULL且在定义时必须初始化 指针在初始化后可以改变指向，而引用在初始化之后不可再改变 sizeof指针得到的是本指针的大小，sizeof引用得到的是引用所指向变量的大小 当把指针作为参数进行传递时，也是将实参的一个拷贝传递给形参，两者指向的地址相同，但不是同一个变量，在函数中改变这个变量的指向不影响实参，而引用却可以。 引用本质是一个指针，同样会占4字节内存；指针是具体变量，需要占用存储空间（，具体情况还要具体分析）。 引用在声明时必须初始化为另一变量，一旦出现必须为typename refname \u0026varname形式；指针声明和定义可以分开，可以先只声明指针变量而不初始化，等用到时再指向具体变量。 引用一旦初始化之后就不可以再改变（变量可以被引用为多次，但引用只能作为一个变量引用）；指针变量可以重新指向别的变量。 不存在指向空值的引用，必须有具体实体；但是存在指向空值的指针。 参考代码： void test(int *p) { int a=1; p=\u0026a; cout\u003c\u003cp\u003c\u003c\" \"\u003c\u003c*p\u003c\u003cendl; } int main(void) { int *p=NULL; test(p); if(p==NULL) cout\u003c\u003c\"指针p为NULL\"\u003c\u003cendl; return 0; } //运行结果为： //0x22ff44 1 //指针p为NULL void testPTR(int* p) { int a = 12; p = \u0026a; } void testREFF(int\u0026 p) { int a = 12; p = a; } void main() { int a = 10; int* b = \u0026a; testPTR(b);//改变指针指向，但是没改变指针的所指的内容 cout \u003c\u003c a \u003c\u003c endl;// 10 cout \u003c\u003c *b \u003c\u003c endl;// 10 a = 10; testREFF(a); cout \u003c\u003c a \u003c\u003c endl;//12 } 在编译器看来, int a = 10; int \u0026b = a; 等价于 int * const b = \u0026a; 而 b = 20; 等价于 *b = 20; 自动转换为指针和自动解引用. ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:3:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"4、在传递函数参数时，什么时候该使用指针，什么时候该使用引用呢？ 需要返回函数内局部变量的内存的时候用指针。使用指针传参需要开辟内存，用完要记得释放指针，不然会内存泄漏。而返回局部变量的引用是没有意义的 对栈空间大小比较敏感（比如递归）的时候使用引用。使用引用传递不需要创建临时变量，开销要更小 类对象作为参数传递的时候使用引用，这是C++类对象传递的标准方式 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:4:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"5、堆和栈的区别 申请方式不同。 栈由系统自动分配。 堆是自己申请和释放的。 申请大小限制不同。 栈顶和栈底是之前预设好的，栈是向栈底扩展，大小固定，可以通过ulimit -a查看，由ulimit -s修改。 堆向高地址扩展，是不连续的内存区域，大小可以灵活调整。 申请效率不同。 栈由系统分配，速度快，不会有碎片。 堆由程序员分配，速度慢，且会有碎片。 栈空间默认是4M, 堆区一般是 1G - 4G 堆 栈 管理方式 堆中资源由程序员控制（容易产生memory leak） 栈资源由编译器自动管理，无需手工控制 内存管理机制 系统有一个记录空闲内存地址的链表，当系统收到程序申请时，遍历该链表，寻找第一个空间大于申请空间的堆结点，删 除空闲结点链表中的该结点，并将该结点空间分配给程序（大多数系统会在这块内存空间首地址记录本次分配的大小，这样delete才能正确释放本内存空间，另外系统会将多余的部分重新放入空闲链表中） 只要栈的剩余空间大于所申请空间，系统为程序提供内存，否则报异常提示栈溢出。（这一块理解一下链表和队列的区别，不连续空间和连续空间的区别，应该就比较好理解这两种机制的区别了） 空间大小 堆是不连续的内存区域（因为系统是用链表来存储空闲内存地址，自然不是连续的），堆大小受限于计算机系统中有效的虚拟内存（32bit 系统理论上是4G），所以堆的空间比较灵活，比较大 栈是一块连续的内存区域，大小是操作系统预定好的，windows下栈大小是2M（也有是1M，在 编译时确定，VC中可设置） 碎片问题 对于堆，频繁的new/delete会造成大量碎片，使程序效率降低 对于栈，它是有点类似于数据结构上的一个先进后出的栈，进出一一对应，不会产生碎片。（看到这里我突然明白了为什么面试官在问我堆和栈的区别之前先问了我栈和队列的区别） 生长方向 堆向上，向高地址方向增长。 栈向下，向低地址方向增长。 分配方式 堆都是动态分配（没有静态分配的堆） 栈有静态分配和动态分配，静态分配由编译器完成（如局部变量分配），动态分配由alloca函数分配，但栈的动态分配的资源由编译器进行释放，无需程序员实现。 分配效率 堆由C/C++函数库提供，机制很复杂。所以堆的效率比栈低很多。 栈是其系统提供的数据结构，计算机在底层对栈提供支持，分配专门 寄存器存放栈地址，栈操作有专门指令。 形象的比喻 栈就像我们去饭馆里吃饭，只管点菜（发出申请）、付钱、和吃（使用），吃饱了就走，不必理会切菜、洗菜等准备工作和洗碗、刷锅等扫尾工作，他的好处是快捷，但是自由度小。 堆就象是自己动手做喜欢吃的菜肴，比较麻烦，但是比较符合自己的口味，而且自由度大。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:5:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"6、你觉得堆快一点还是栈快一点？ 毫无疑问是栈快一点。 因为操作系统会在底层对栈提供支持，会分配专门的寄存器存放栈的地址，栈的入栈出栈操作也十分简单，并且有专门的指令执行，所以栈的效率比较高也比较快。 而堆的操作是由C/C++函数库提供的，在分配堆内存的时候需要一定的算法寻找合适大小的内存。并且获取堆的内容需要两次访问，第一次访问指针，第二次根据指针保存的地址访问内存，因此堆比较慢。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:6:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"7、区别以下指针类型？ int *p[10] int (*p)[10] int *p(int) int (*p)(int) int *p[10]表示指针数组，强调数组概念，是一个数组变量，数组大小为10，数组内每个元素都是指向int类型的指针变量。 int (*p)[10]表示数组指针，强调是指针，只有一个变量，是指针类型，不过指向的是一个int类型的数组，这个数组大小是10。 int *p(int)是函数声明，函数名是p，参数是int类型的，返回值是int *类型的。 int (*p)(int)是函数指针，强调是指针，该指针指向的函数具有int类型参数，并且返回值是int类型的。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:7:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"8、new / delete 与 malloc / free的异同 相同点 都可用于内存的动态申请和释放 不同点 前者是C++运算符，后者是C/C++语言标准库函数 new自动计算要分配的空间大小，malloc需要手工计算 new是类型安全的，malloc不是。例如： int *p = new float[2]; //编译错误 int *p = (int*)malloc(2 * sizeof(double));//编译无错误 new调用名为operator new的标准库函数分配足够空间并调用相关对象的构造函数，delete对指针所指对象运行适当的析构函数；然后通过调用名为operator delete的标准库函数释放该对象所用内存。后者均没有相关调用 后者需要库文件支持，前者不用 new是封装了malloc，直接free不会报错，但是这只是释放内存，而不会析构对象 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:8:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"9、new和delete是如何实现的？ new的实现过程是：首先调用名为operator new的标准库函数，分配足够大的原始为类型化的内存，以保存指定类型的一个对象；接下来运行该类型的一个构造函数，用指定初始化构造对象；最后返回指向新分配并构造后的的对象的指针 delete的实现过程：对指针指向的对象运行适当的析构函数；然后通过调用名为operator delete的标准库函数释放该对象所用内存 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:9:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"10、malloc和new的区别？ malloc和free是标准库函数，支持覆盖；new和delete是运算符，不重载。 malloc仅仅分配内存空间，free仅仅回收空间，不具备调用构造函数和析构函数功能，用malloc分配空间存储类的对象存在风险；new和delete除了分配回收功能外，还会调用构造函数和析构函数。 malloc和free返回的是void类型指针（必须进行类型转换），new和delete返回的是具体类型指针。 update1:感谢微信好友“猿六学算法”指出错误，已修正！ ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:10:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"11、既然有了malloc/free，C++中为什么还需要new/delete呢？直接用malloc/free不好吗？ malloc/free和new/delete都是用来申请内存和回收内存的。 在对非基本数据类型的对象使用的时候，对象创建的时候还需要执行构造函数，销毁的时候要执行析构函数。而malloc/free是库函数，是已经编译的代码，所以不能把构造函数和析构函数的功能强加给malloc/free，所以new/delete是必不可少的。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:11:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"12、被free回收的内存是立即返还给操作系统吗？ 不是的，被free回收的内存会首先被ptmalloc使用双链表保存起来，当用户下一次申请内存的时候，会尝试从这些内存中寻找合适的返回。这样就避免了频繁的系统调用，占用过多的系统资源。同时ptmalloc也会尝试对小块内存进行合并，避免过多的内存碎片。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:12:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"13、宏定义和函数有何区别？ 宏在预处理阶段完成替换，之后被替换的文本参与编译，相当于直接插入了代码，运行时不存在函数调用，执行起来更快；函数调用在运行时需要跳转到具体调用函数。 宏定义属于在结构中插入代码，没有返回值；函数调用具有返回值。 宏定义参数没有类型，不进行类型检查；函数参数具有类型，需要检查类型。 宏定义不要在最后加分号。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:13:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"14、宏定义和typedef区别？ 宏主要用于定义常量及书写复杂的内容；typedef主要用于定义类型别名。 宏替换发生在编译阶段之前，属于文本插入替换；typedef是编译的一部分。 宏不检查类型；typedef会检查数据类型。 宏不是语句，不在在最后加分号；typedef是语句，要加分号标识结束。 注意对指针的操作，typedef char * p_char和#define p_char char *区别巨大。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:14:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"15、变量声明和定义区别？ 声明仅仅是把变量的声明的位置及类型提供给编译器，并不分配内存空间；定义要在定义的地方为其分配存储空间。 相同变量可以在多处声明（外部变量extern），但只能在一处定义。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:15:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"16、strlen和sizeof区别？ sizeof是运算符，并不是函数，结果在编译时得到而非运行中获得；strlen是字符处理的库函数。 sizeof参数可以是任何数据的类型或者数据（sizeof参数不退化）；strlen的参数只能是字符指针且结尾是'\\0’的字符串。 因为sizeof值在编译时确定，所以不能用来得到动态分配（运行时分配）存储空间的大小。 int main(int argc, char const *argv[]){ const char* str = \"name\"; sizeof(str); // 取的是指针str的长度，是8 strlen(str); // 取的是这个字符串的长度，不包含结尾的 \\0。大小是4 return 0; } ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:16:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"16.2、（补充题）一个指针占多少字节？ 在16题中有提到sizeof（str）的值为8，是在64位的编译环境下的，指针的占用大小为8字节； 而在32位环境下，指针占用大小为4字节。 一个指针占内存的大小跟编译环境有关，而与机器的位数无关。 还有疑问的，可以自行打开Visual Studio编译器自己实验一番。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:16:1","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"17、常量指针和指针常量区别？ 指针常量是一个指针，读成常量的指针，指向一个只读变量，也就是后面所指明的int const 和 const int，都是一个常量，可以写作int const *p或const int *p。 常量指针是一个不能给改变指向的指针。指针是个常量，必须初始化，一旦初始化完成，它的值（也就是存放在指针中的地址）就不能在改变了，即不能中途改变指向，如int *const p。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:17:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"18、a和\u0026a有什么区别？ 假设数组int a[10]; int (*p)[10] = \u0026a;其中： a是数组名，是数组首元素地址，+1表示地址值加上一个int类型的大小，如果a的值是0x00000001，加1操作后变为0x00000005。*(a + 1) = a[1]。 \u0026a是数组的指针，其类型为int (*)[10]（就是前面提到的数组指针），其加1时，系统会认为是数组首地址加上整个数组的偏移（10个int型变量），值为数组a尾元素后一个元素的地址。 若(int *)p ，此时输出 *p时，其值为a[0]的值，因为被转为int *类型，解引用时按照int类型大小来读取。 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:18:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"19、C++和Python的区别 包括但不限于： Python是一种脚本语言，是解释执行的，而C++是编译语言，是需要编译后在特定平台运行的。python可以很方便的跨平台，但是效率没有C++高。 Python使用缩进来区分不同的代码块，C++使用花括号来区分 C++中需要事先定义变量的类型，而Python不需要，Python的基本数据类型只有数字，布尔值，字符串，列表，元组等等 Python的库函数比C++的多，调用起来很方便 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:19:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"20、C++和C语言的区别 C++中new和delete是对内存分配的运算符，取代了C中的malloc和free。 标准C++中的字符串类取代了标准C函数库头文件中的字符数组处理函数（C中没有字符串类型）。 C++中用来做控制态输入输出的iostream类库替代了标准C中的stdio函数库。 C++中的try/catch/throw异常处理机制取代了标准C中的setjmp()和longjmp()函数。 在C++中，允许有相同的函数名，不过它们的参数类型不能完全相同，这样这些函数就可以相互区别开来。而这在C语言中是不允许的。也就是C++可以重载，C语言不允许。 C++语言中，允许变量定义语句在程序中的任何地方，只要在是使用它之前就可以；而C语言中，必须要在函数开头部分。而且C++不允许重复定义变量，C语言也是做不到这一点的 在C++中，除了值和指针之外，新增了引用。引用型变量是其他变量的一个别名，我们可以认为他们只是名字不相同，其他都是相同的。 C++相对与C增加了一些关键字，如：bool、using、dynamic_cast、namespace等等 ","date":"2022-06-20","objectID":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/:20:0","tags":["C++面试题1"],"title":"C++面试题1","uri":"/posts/c++%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%981/"},{"categories":["面试相关"],"content":"面试八股——计算机网络1","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"1、OSI 的七层模型分别是？各自的功能是什么？ ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:1:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"简要概括 物理层：底层数据传输，如网线；网卡标准。 数据链路层：定义数据的基本格式，如何传输，如何标识；如网卡MAC地址。 网络层：定义IP编址，定义路由功能；如不同设备的数据转发。 传输层：端到端传输数据的基本功能；如 TCP、UDP。 会话层：控制应用程序之间会话能力；如不同软件数据分发给不同软件。 表示层：数据格式标识，基本压缩加密功能。 应用层：各种应用软件，包括 Web 应用。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:1:1","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"说明 在四层，既传输层数据被称作段（Segments）； 三层网络层数据被称做包（Packages）； 二层数据链路层时数据被称为帧（Frames）； 一层物理层时数据被称为比特流（Bits）。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:1:2","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"总结 网络七层模型是一个标准，而非实现。 网络四层模型是一个实现的应用模型。 网络四层模型由七层模型简化合并而来。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:1:3","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"2、说一下一次完整的HTTP请求过程包括哪些内容？ ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:2:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"第一种回答 建立起客户机和服务器连接。 建立连接后，客户机发送一个请求给服务器。 服务器收到请求给予响应信息。 客户端浏览器将返回的内容解析并呈现，断开连接。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:2:1","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"第二种回答 域名解析 –\u003e 发起TCP的3次握手 –\u003e 建立TCP连接后发起http请求 –\u003e 服务器响应http请求，浏览器得到html代码 –\u003e 浏览器解析html代码，并请求html代码中的资源（如js、css、图片等） –\u003e 浏览器对页面进行渲染呈现给用户。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:2:2","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"3、你知道DNS是什么？ 官方解释：DNS（Domain Name System，域名系统），因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。 通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。 通俗的讲，我们更习惯于记住一个网站的名字，比如www.baidu.com,而不是记住它的ip地址，比如：167.23.10.2。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:3:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"4、DNS的工作原理？ 将主机域名转换为ip地址，属于应用层协议，使用UDP传输。（DNS应用层协议，以前有个考官问过） 总结： 浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。 一、主机向本地域名服务器的查询一般都是采用递归查询。 二、本地域名服务器向根域名服务器的查询的迭代查询。 过程如下： 当用户输入域名时，浏览器先检查自己的缓存中是否 这个域名映射的ip地址，有解析结束。 若没命中，则检查操作系统缓存（如Windows的hosts）中有没有解析过的结果，有解析结束。 若无命中，则请求本地域名服务器解析（ LDNS）。 若LDNS没有命中就直接跳到根域名服务器请求解析。根域名服务器返回给LDNS一个 主域名服务器地址。 此时LDNS再发送请求给上一步返回的gTLD（ 通用顶级域）， 接受请求的gTLD查找并返回这个域名对应的Name Server的地址 Name Server根据映射关系表找到目标ip，返回给LDNS 。 LDNS缓存这个域名和对应的ip， 把解析的结果返回给用户，用户根据TTL值缓存到本地系统缓存中，域名解析过程至此结束。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:4:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"5、为什么域名解析用UDP协议？ 因为UDP快啊！UDP的DNS协议只要一个请求、一个应答就好了。 而使用基于TCP的DNS协议要三次握手、发送数据以及应答、四次挥手，但是UDP协议传输内容不能超过512字节。 不过客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:5:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"6、为什么区域传送用TCP协议？ 因为TCP协议可靠性好啊！ 你要从主DNS上复制内容啊，你用不可靠的UDP？ 因为TCP协议传输的内容大啊，你用最大只能传512字节的UDP协议？万一同步的数据大于512字节，你怎么办？所以用TCP协议比较好！ ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:6:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"7、HTTP长连接和短连接的区别 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:7:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"8、什么是TCP粘包/拆包？发生的原因？ 一个完整的业务可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这个就是TCP的拆包和粘包问题。 原因 应用程序写入数据的字节大小大于套接字发送缓冲区的大小. 进行MSS大小的TCP分段。( MSS=TCP报文段长度-TCP首部长度) 以太网的payload大于MTU进行IP分片。（ MTU指：一种通信协议的某一层上面所能通过的最大数据包大小。） 解决方案 消息定长。 在包尾部增加回车或者空格符等特殊字符进行分割 将消息分为消息头和消息尾。 使用其它复杂的协议，如RTMP协议等。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:8:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"9、为什么服务器会缓存这一项功能?如何实现的？ 原因 缓解服务器压力； 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。并且缓存服务器在地理位置上也有可能比源服务器来得近，例如浏览器缓存。 实现方法 让代理服务器进行缓存； 让客户端浏览器进行缓存。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:9:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"10、HTTP请求方法你知道多少？ 客户端发送的 请求报文 第一行为请求行，包含了方法字段。 根据 HTTP 标准，HTTP 请求可以使用多种请求方法。 HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。 HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。 序 号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:10:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"11、GET 和 POST 的区别，你知道哪些？ get是获取数据，post是修改数据 get把请求的数据放在url上， 以?分割URL和传输数据，参数之间以\u0026相连，所以get不太安全。而post把数据放在HTTP的包体内（requrest body） get提交的数据最大是2k（ 限制实际上取决于浏览器）， post理论上没有限制。 GET产生一个TCP数据包，浏览器会把http header和data一并发送出去，服务器响应200(返回数据); POST产生两个TCP数据包，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。 GET请求会被浏览器主动缓存，而POST不会，除非手动设置。 本质区别：GET是幂等的，而POST不是幂等的 这里的幂等性：幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一URL的多个请求应该返回同样的结果。 正因为它们有这样的区别，所以不应该且不能用get请求做数据的增删改这些有副作用的操作。因为get请求是幂等的，在网络不好的隧道中会尝试重试。如果用get请求增数据，会有重复操作的风险，而这种重复操作可能会导致副作用（浏览器和操作系统并不知道你会用get请求去做增操作）。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:11:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"12、一个TCP连接可以对应几个HTTP请求？ 一般来讲http协议是建立连接发送完消息后就会关闭连接，但如果维持TCP长连接，那么肯定是可以发送多个HTTP请求的。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:12:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"13、一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？ HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。 在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的。在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行。 那么在 HTTP/1.1 时代，浏览器是如何提高页面加载效率的呢？主要有下面两点： 维持和服务器已经建立的 TCP 连接，在同一连接上顺序处理多个请求。 和服务器建立多个 TCP 连接。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:13:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"14、浏览器对同一 Host 建立 TCP 连接到的数量有没有限制？ 假设我们还处在 HTTP/1.1 时代，那个时候没有多路传输，当浏览器拿到一个有几十张图片的网页该怎么办呢？肯定不能只开一个 TCP 连接顺序下载，那样用户肯定等的很难受，但是如果每个图片都开一个 TCP 连接发 HTTP 请求，那电脑或者服务器都可能受不了，要是有 1000 张图片的话总不能开 1000 个TCP 连接吧，你的电脑同意 NAT 也不一定会同意。 有。Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。 如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2，如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。 如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于浏览器设置，这些连接会在空闲的时候被浏览器用来发送新的请求，如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。 update1：微信好友“卷轴”提出勘误“连接到”-》“连接到的” ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:14:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"15、在浏览器中输入url地址后显示主页的过程? 根据域名，进行DNS域名解析； 拿到解析的IP地址，建立TCP连接； 向IP地址，发送HTTP请求； 服务器处理请求； 返回响应结果； 关闭TCP连接； 浏览器解析HTML； 浏览器布局渲染； ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:15:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"16、在浏览器地址栏输入一个URL后回车，背后会进行哪些技术步骤？ ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:16:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"第一种回答 1、查浏览器缓存，看看有没有已经缓存好的，如果没有 2 、检查本机host文件， 3、调用API，Linux下Scoket函数 gethostbyname 4、向DNS服务器发送DNS请求，查询本地DNS服务器，这其中用的是UDP的协议 5、如果在一个子网内采用ARP地址解析协议进行ARP查询如果不在一个子网那就需要对默认网关进行DNS查询，如果还找不到会一直向上找根DNS服务器，直到最终拿到IP地址（全球400多个根DNS服务器，由13个不同的组织管理） 6、这个时候我们就有了服务器的IP地址 以及默认的端口号了，http默认是80 https是 443 端口号，会，首先尝试http然后调用Socket建立TCP连接， 7、经过三次握手成功建立连接后，开始传送数据，如果正是http协议的话，就返回就完事了， 8、如果不是http协议，服务器会返回一个5开头的的重定向消息，告诉我们用的是https，那就是说IP没变，但是端口号从80变成443了，好了，再四次挥手，完事， 9、再来一遍，这次除了上述的端口号从80变成443之外，还会采用SSL的加密技术来保证传输数据的安全性，保证数据传输过程中不被修改或者替换之类的， 10、这次依然是三次握手，沟通好双方使用的认证算法，加密和检验算法，在此过程中也会检验对方的CA安全证书。 11、确认无误后，开始通信，然后服务器就会返回你所要访问的网址的一些数据，在此过程中会将界面进行渲染，牵涉到ajax技术之类的，直到最后我们看到色彩斑斓的网页 update1:为微信好友”卷轴“提出勘误”缺少5“-》现已加上 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:16:1","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"第二种回答 浏览器检查域名是否在缓存当中（要查看 Chrome 当中的缓存， 打开 chrome://net-internals/#dns）。 如果缓存中没有，就去调用 gethostbyname 库函数（操作系统不同函数也不同）进行查询。 如果 gethostbyname 没有这个域名的缓存记录，也没有在hosts 里找到，它将会向 DNS 服务器发送一条 DNS 查询请求。DNS 服务器是由网络通信栈提供的，通常是本地路由器或者 ISP 的缓存 DNS 服务器。 查询本地 DNS 服务器 如果 DNS 服务器和我们的主机在同一个子网内，系统会按照下面的 ARP 过程对 DNS 服务器进行 ARP查询 如果 DNS 服务器和我们的主机在不同的子网，系统会按照下面的 ARP 过程对默认网关进行查询 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:16:2","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"17、谈谈DNS解析过程，具体一点 请求一旦发起，若是chrome浏览器，先在浏览器找之前有没有缓存过的域名所对应的ip地址，有的话，直接跳过dns解析了，若是没有，就会找硬盘的hosts文件，看看有没有，有的话，直接找到hosts文件里面的ip 如果本地的hosts文件没有能得到对应的ip地址，浏览器会发出一个dns请求到本地dns服务器，本地dns服务器一般都是你的网络接入服务器商提供，比如中国电信，中国移动等。 查询你输入的网址的DNS请求到达本地DNS服务器之后，本地DNS服务器会首先查询它的缓存记录，如果缓存中有此条记录，就可以直接返回结果，此过程是递归的方式进行查询。如果没有，本地DNS服务器还要向DNS根服务器进行查询。 本地DNS服务器继续向域服务器发出请求，在这个例子中，请求的对象是.com域服务器。.com域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址。 最后，本地DNS服务器向域名的解析服务器发出请求，这时就能收到一个域名和IP地址对应关系，本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:17:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"18、DNS负载均衡是什么策略？ 当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会崩掉。处理办法就是用DNS负载均衡技术，它的原理是在DNS服务器中为同一个主机名配置多个IP地址,在应答DNS查询时,DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果,将客户端的访问引导到不同的机器上去,使得不同的客户端访问不同的服务器,从而达到负载均衡的目的｡例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:18:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"19、HTTPS和HTTP的区别 1、HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全， HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。 2、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:19:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["面试相关"],"content":"20、什么是SSL/TLS ？ SSL代表安全套接字层。它是一种用于加密和验证应用程序（如浏览器）和Web服务器之间发送的数据的协议。 身份验证 ， 加密Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。 SSL/TLS协议作用：认证用户和服务，加密数据，维护数据的完整性的应用层协议加密和解密需要两个不同的密钥，故被称为非对称加密；加密和解密都使用同一个密钥的对称加密：优点在于加密、解密效率通常比较高 ，HTTPS 是基于非对称加密的， 公钥是公开的。 关于对称加密和非对称加密，可以自己再具体搜索看看。 ","date":"2022-06-20","objectID":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/:20:0","tags":["面试八股——计算机网络1"],"title":"面试八股——计算机网络1","uri":"/posts/%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C1/"},{"categories":["JavaWeb笔记"],"content":"redis的介绍与使用","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"视频讲解链接 Redis数据库 灵魂拷问：不是学了MySQL吗，存数据也能存了啊，又学一个数据库干嘛？ 在前面我们学习了MySQL数据库，它是一种传统的关系型数据库，我们可以使用MySQL来更好地管理和组织我们的数据，虽然在小型Web应用下，只需要一个MySQL+Mybatis自带的缓存系统就可以胜任大部分的数据存储工作。但是MySQL的缺点也很明显，它的数据始终是存储在硬盘上的，对于我们的用户信息这种不需要经常发生修改的内容，使用MySQL存储确实可以，但是如果是快速更新或是频繁使用的数据，比如微博热搜、双十一秒杀，这些数据不仅要求服务器需要提供更高的响应速度，而且还需要面对短时间内上百万甚至上千万次访问，而MySQL的磁盘IO读写性能完全不能满足上面的需求，能够满足上述需求的只有内存，因为速度远高于磁盘IO。 因此，我们需要寻找一种更好的解决方案，来存储上述这类特殊数据，弥补MySQL的不足，以应对大数据时代的重重考验。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:0:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"NoSQL概论 NoSQL全称是Not Only SQL（不仅仅是SQL）它是一种非关系型数据库，相比传统SQL关系型数据库，它： 不保证关系数据的ACID特性 并不遵循SQL标准 消除数据之间关联性 乍一看，这玩意不比MySQL垃圾？我们再来看看它的优势： 远超传统关系型数据库的性能 非常易于扩展 数据模型更加灵活 高可用 这样，NoSQL的优势一下就出来了，这不就是我们正要寻找的高并发海量数据的解决方案吗！ NoSQL数据库分为以下几种： **键值存储数据库：**所有的数据都是以键值方式存储的，类似于我们之前学过的HashMap，使用起来非常简单方便，性能也非常高。 **列存储数据库：**这部分数据库通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列。 **文档型数据库：**它是以一种特定的文档格式存储数据，比如JSON格式，在处理网页等复杂数据时，文档型数据库比传统键值数据库的查询效率更高。 **图形数据库：**利用类似于图的数据结构存储数据，结合图相关算法实现高速访问。 其中我们要学习的Redis数据库，就是一个开源的键值存储数据库，所有的数据全部存放在内存中，它的性能大大高于磁盘IO，并且它也可以支持数据持久化，他还支持横向扩展、主从复制等。 实际生产中，我们一般会配合使用Redis和MySQL以发挥它们各自的优势，取长补短。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:1:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"Redis安装和部署 我们这里还是使用Windows安装Redis服务器，但是官方指定是安装到Linux服务器上，我们后面学习了Linux之后，再来安装到Linux服务器上。由于官方并没有提供Windows版本的安装包，我们需要另外寻找： 官网地址：https://redis.io GitHub Windows版本维护地址：https://github.com/tporadowski/redis/releases ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:2:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"基本操作 在我们之前使用MySQL时，我们需要先在数据库中创建一张表，并定义好表的每个字段内容，最后再通过insert语句向表中添加数据，而Redis并不具有MySQL那样的严格的表结构，Redis是一个键值数据库，因此，可以像Map一样的操作方式，通过键值对向Redis数据库中添加数据（操作起来类似于向一个HashMap中存放数据） 在Redis下，数据库是由一个整数索引标识，而不是由一个数据库名称。 默认情况下，我们连接Redis数据库之后，会使用0号数据库，我们可以通过Redis配置文件中的参数来修改数据库总数，默认为16个。 我们可以通过select语句进行切换： select序号;","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:3:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"数据操作 我们来看看，如何向Redis数据库中添加数据： set\u003ckey\u003e\u003cvalue\u003e-- 一次性多个 mset[\u003ckey\u003e\u003cvalue\u003e]...所有存入的数据默认会以字符串的形式保存，键值具有一定的命名规范，以方便我们可以快速定位我们的数据属于哪一个部分，比如用户的数据： -- 使用冒号来进行板块分割，比如下面表示用户XXX的信息中的name属性，值为lbw setuser:info:用户ID:namelbw我们可以通过键值获取存入的值： get\u003ckey\u003e你以为Redis就仅仅只是存取个数据吗？它还支持数据的过期时间设定： set\u003ckey\u003e\u003cvalue\u003eEX秒set\u003ckey\u003e\u003cvalue\u003ePX毫秒当数据到达指定时间时，会被自动删除。我们也可以单独为其他的键值对设置过期时间： expire\u003ckey\u003e秒通过下面的命令来查询某个键值对的过期时间还剩多少： ttl\u003ckey\u003e-- 毫秒显示 pttl\u003ckey\u003e-- 转换为永久 persist\u003ckey\u003e那么当我们想直接删除这个数据时呢？直接使用： del\u003ckey\u003e...删除命令可以同时拼接多个键值一起删除。 当我们想要查看数据库中所有的键值时： keys*也可以查询某个键是否存在： exists\u003ckey\u003e...还可以随机拿一个键： randomkey我们可以将一个数据库中的内容移动到另一个数据库中： move\u003ckey\u003e数据库序号修改一个键为另一个键： rename\u003ckey\u003e\u003c新的名称\u003e-- 下面这个会检查新的名称是否已经存在 renamex\u003ckey\u003e\u003c新的名称\u003e如果存放的数据是一个数字，我们还可以对其进行自增自减操作： -- 等价于a = a + 1 incr\u003ckey\u003e-- 等价于a = a + b incrby\u003ckey\u003eb-- 等价于a = a - 1 decr\u003ckey\u003e最后就是查看值的数据类型： type\u003ckey\u003eRedis数据库也支持多种数据类型，但是它更偏向于我们在Java中认识的那些数据类型。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:3:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"数据类型介绍 一个键值对除了存储一个String类型的值以外，还支持多种常用的数据类型。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"Hash 这种类型本质上就是一个HashMap，也就是嵌套了一个HashMap罢了，在Java中就像这样： #Redis默认存String类似于这样： Map\u003cString, String\u003e hash = new HashMap\u003c\u003e(); #Redis存Hash类型的数据类似于这样： Map\u003cString, Map\u003cString, String\u003e\u003e hash = new HashMap\u003c\u003e(); 它比较适合存储类这样的数据，由于值本身又是一个Map，因此我们可以在此Map中放入类的各种属性和值，以实现一个Hash数据类型存储一个类的数据。 我们可以像这样来添加一个Hash类型的数据： hset\u003ckey\u003e[\u003c字段\u003e\u003c值\u003e]...我们可以直接获取： hget\u003ckey\u003e\u003c字段\u003e-- 如果想要一次性获取所有的字段和值 hgetall\u003ckey\u003e同样的，我们也可以判断某个字段是否存在： hexists\u003ckey\u003e\u003c字段\u003e删除Hash中的某个字段： hdel\u003ckey\u003e我们发现，在操作一个Hash时，实际上就是我们普通操作命令前面添加一个h，这样就能以同样的方式去操作Hash里面存放的键值对了，这里就不一一列出所有的操作了。我们来看看几个比较特殊的。 我们现在想要知道Hash中一共存了多少个键值对： hlen\u003ckey\u003e我们也可以一次性获取所有字段的值： hvals\u003ckey\u003e唯一需要注意的是，Hash中只能存放字符串值，不允许出现嵌套的的情况。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"List 我们接着来看List类型，实际上这个猜都知道，它就是一个列表，而列表中存放一系列的字符串，它支持随机访问，支持双端操作，就像我们使用Java中的LinkedList一样。 我们可以直接向一个已存在或是不存在的List中添加数据，如果不存在，会自动创建： -- 向列表头部添加元素 lpush\u003ckey\u003e\u003celement\u003e...-- 向列表尾部添加元素 rpush\u003ckey\u003e\u003celement\u003e...-- 在指定元素前面/后面插入元素 linsert\u003ckey\u003ebefore/after\u003c指定元素\u003e\u003celement\u003e同样的，获取元素也非常简单： -- 根据下标获取元素 lindex\u003ckey\u003e\u003c下标\u003e-- 获取并移除头部元素 lpop\u003ckey\u003e-- 获取并移除尾部元素 rpop\u003ckey\u003e-- 获取指定范围内的 lrange\u003ckey\u003estartstop注意下标可以使用负数来表示从后到前数的数字（Python：搁这儿抄呢是吧）: -- 获取列表a中的全部元素 lrangea0-1没想到吧，push和pop还能连着用呢： -- 从前一个数组的最后取一个数出来放到另一个数组的头部，并返回元素 rpoplpush当前数组目标数组它还支持阻塞操作，类似于生产者和消费者，比如我们想要等待列表中有了数据后再进行pop操作： -- 如果列表中没有元素，那么就等待，如果指定时间（秒）内被添加了数据，那么就执行pop操作，如果超时就作废，支持同时等待多个列表，只要其中一个列表有元素了，那么就能执行 blpop\u003ckey\u003e...timeout","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:2","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"Set和SortedSet Set集合其实就像Java中的HashSet一样（我们在JavaSE中已经讲解过了，HashSet本质上就是利用了一个HashMap，但是Value都是固定对象，仅仅是Key不同）它不允许出现重复元素，不支持随机访问，但是能够利用Hash表提供极高的查找效率。 向Set中添加一个或多个值： sadd\u003ckey\u003e\u003cvalue\u003e...查看Set集合中有多少个值： scard\u003ckey\u003e判断集合中是否包含： -- 是否包含指定值 sismember\u003ckey\u003e\u003cvalue\u003e-- 列出所有值 smembers\u003ckey\u003e集合之间的运算： -- 集合之间的差集 sdiff\u003ckey1\u003e\u003ckey2\u003e-- 集合之间的交集 sinter\u003ckey1\u003e\u003ckey2\u003e-- 求并集 sunion\u003ckey1\u003e\u003ckey2\u003e-- 将集合之间的差集存到目标集合中 sdiffstore目标\u003ckey1\u003e\u003ckey2\u003e-- 同上 sinterstore目标\u003ckey1\u003e\u003ckey2\u003e-- 同上 sunionstore目标\u003ckey1\u003e\u003ckey2\u003e移动指定值到另一个集合中： smove\u003ckey\u003e目标value移除操作： -- 随机移除一个幸运儿 spop\u003ckey\u003e-- 移除指定 srem\u003ckey\u003e\u003cvalue\u003e...那么如果我们要求Set集合中的数据按照我们指定的顺序进行排列怎么办呢？这时就可以使用SortedSet，它支持我们为每个值设定一个分数，分数的大小决定了值的位置，所以它是有序的。 我们可以添加一个带分数的值： zadd\u003ckey\u003e[\u003cvalue\u003e\u003cscore\u003e]...同样的： -- 查询有多少个值 zcard\u003ckey\u003e-- 移除 zrem\u003ckey\u003e\u003cvalue\u003e...-- 获取区间内的所有 zrange\u003ckey\u003estartstop由于所有的值都有一个分数，我们也可以根据分数段来获取： -- 通过分数段查看 zrangebyscore\u003ckey\u003estartstop[withscores][limit]-- 统计分数段内的数量 zcount\u003ckey\u003estartstop-- 根据分数获取指定值的排名 zrank\u003ckey\u003e\u003cvalue\u003ehttps://www.jianshu.com/p/32b9fe8c20e1 有关Bitmap、HyperLogLog和Geospatial等数据类型，这里暂时不做介绍，感兴趣可以自行了解。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:4:3","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"持久化 我们知道，Redis数据库中的数据都是存放在内存中，虽然很高效，但是这样存在一个非常严重的问题，如果突然停电，那我们的数据不就全部丢失了吗？它不像硬盘上的数据，断电依然能够保存。 这个时候我们就需要持久化，我们需要将我们的数据备份到硬盘上，防止断电或是机器故障导致的数据丢失。 持久化的实现方式有两种方案：一种是直接保存当前已经存储的数据，相当于复制内存中的数据到硬盘上，需要恢复数据时直接读取即可；还有一种就是保存我们存放数据的所有过程，需要恢复数据时，只需要将整个过程完整地重演一遍就能保证与之前数据库中的内容一致。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"RDB RDB就是我们所说的第一种解决方案，那么如何将数据保存到本地呢？我们可以使用命令： save-- 注意上面这个命令是直接保存，会占用一定的时间，也可以单独开一个子进程后台执行保存 bgsave执行后，会在服务端目录下生成一个dump.rdb文件，而这个文件中就保存了内存中存放的数据，当服务器重启后，会自动加载里面的内容到对应数据库中。保存后我们可以关闭服务器： shutdown重启后可以看到数据依然存在。 虽然这种方式非常方便，但是由于会完整复制所有的数据，如果数据库中的数据量比较大，那么复制一次可能就需要花费大量的时间，所以我们可以每隔一段时间自动进行保存；还有就是，如果我们基本上都是在进行读操作，而没有进行写操作，实际上只需要偶尔保存一次即可，因为数据几乎没有怎么变化，可能两次保存的都是一样的数据。 我们可以在配置文件中设置自动保存，并设定在一段时间内写入多少数据时，执行一次保存操作： save 300 10 # 300秒（5分钟）内有10个写入\rsave 60 10000 # 60秒（1分钟）内有10000个写入\r配置的save使用的都是bgsave后台执行。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"AOF 虽然RDB能够很好地解决数据持久化问题，但是它的缺点也很明显：每次都需要去完整地保存整个数据库中的数据，同时后台保存过程中也会产生额外的内存开销，最严重的是它并不是实时保存的，如果在自动保存触发之前服务器崩溃，那么依然会导致少量数据的丢失。 而AOF就是另一种方式，它会以日志的形式将我们每次执行的命令都进行保存，服务器重启时会将所有命令依次执行，通过这种重演的方式将数据恢复，这样就能很好解决实时性存储问题。 但是，我们多久写一次日志呢？我们可以自己配置保存策略，有三种策略： always：每次执行写操作都会保存一次 everysec：每秒保存一次（默认配置），这样就算丢失数据也只会丢一秒以内的数据 no：看系统心情保存 可以在配置文件中配置： #注意得改成也是appendonlyyes#appendfsyncalwaysappendfsynceverysec#appendfsyncno重启服务器后，可以看到服务器目录下多了一个appendonly.aof文件，存储的就是我们执行的命令。 AOF的缺点也很明显，每次服务器启动都需要进行过程重演，相比RDB更加耗费时间，并且随着我们的操作变多，不断累计，可能到最后我们的aof文件会变得无比巨大，我们需要一个改进方案来优化这些问题。 Redis有一个AOF重写机制进行优化，比如我们执行了这样的语句： lpush test 666\rlpush test 777\rlpush test 888\r实际上用一条语句也可以实现： lpush test 666 777 888\r正是如此，只要我们能够保证最终的重演结果和原有语句的结果一致，无论语句如何修改都可以，所以我们可以通过这种方式将多条语句进行压缩。 我们可以输入命令来手动执行重写操作： bgrewriteaof或是在配置文件中配置自动重写： # 百分比计算，这里不多介绍\rauto-aof-rewrite-percentage 100\r# 当达到这个大小时，触发自动重写\rauto-aof-rewrite-min-size 64mb\r至此，我们就完成了两种持久化方案的介绍，最后我们再来进行一下总结： AOF： 优点：存储速度快、消耗资源少、支持实时存储 缺点：加载速度慢、数据体积大 RDB： 优点：加载速度快、数据体积小 缺点：存储速度慢大量消耗资源、会发生数据丢失 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:5:2","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"事务和锁机制 和MySQL一样，在Redis中也有事务机制，当我们需要保证多条命令一次性完整执行而中途不受到其他命令干扰时，就可以使用事务机制。 我们可以使用命令来直接开启事务： multi当我们输入完所有要执行的命令时，可以使用命令来立即执行事务： exec我们也可以中途取消事务： discard实际上整个事务是创建了一个命令队列，它不像MySQL那种在事务中也能单独得到结果，而是我们提前将所有的命令装在队列中，但是并不会执行，而是等我们提交事务的时候再统一执行。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:6:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"锁 又提到锁了，实际上这个概念对我们来说已经不算是陌生了。实际上在Redis中也会出现多个命令同时竞争同一个数据的情况，比如现在有两条命令同时执行，他们都要去修改a的值，那么这个时候就只能动用锁机制来保证同一时间只能有一个命令操作。 虽然Redis中也有锁机制，但是它是一种乐观锁，不同于MySQL，我们在MySQL中认识的锁是悲观锁，那么什么是乐观锁什么是悲观锁呢？ 悲观锁：时刻认为别人会来抢占资源，禁止一切外来访问，直到释放锁，具有强烈的排他性质。 乐观锁：并不认为会有人来抢占资源，所以会直接对数据进行操作，在操作时再去验证是否有其他人抢占资源。 Redis中可以使用watch来监视一个目标，如果执行事务之前被监视目标发生了修改，则取消本次事务： watch我们可以开两个客户端进行测试。 取消监视可以使用： unwatch至此，Redis的基础内容就讲解完毕了，在之后的SpringCloud阶段，我们还会去讲解集群相关的知识，包括主从复制、哨兵模式等。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:6:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"使用Java与Redis交互 既然了解了如何通过命令窗口操作Redis数据库，那么我们如何使用Java来操作呢？ 这里我们需要使用到Jedis框架，它能够实现Java与Redis数据库的交互，依赖： \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eredis.clients\u003c/groupId\u003e \u003cartifactId\u003ejedis\u003c/artifactId\u003e \u003cversion\u003e4.0.0\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:7:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"基本操作 我们来看看如何连接Redis数据库，非常简单，只需要创建一个对象即可： public static void main(String[] args) { //创建Jedis对象 Jedis jedis = new Jedis(\"localhost\", 6379); //使用之后关闭连接 jedis.close(); } 通过Jedis对象，我们就可以直接调用命令的同名方法来执行Redis命令了，比如： public static void main(String[] args) { //直接使用try-with-resouse，省去close try(Jedis jedis = new Jedis(\"192.168.10.3\", 6379)){ jedis.set(\"test\", \"lbwnb\"); //等同于 set test lbwnb 命令 System.out.println(jedis.get(\"test\")); //等同于 get test 命令 } } Hash类型的数据也是这样： public static void main(String[] args) { try(Jedis jedis = new Jedis(\"192.168.10.3\", 6379)){ jedis.hset(\"hhh\", \"name\", \"sxc\"); //等同于 hset hhh name sxc jedis.hset(\"hhh\", \"sex\", \"19\"); //等同于 hset hhh age 19 jedis.hgetAll(\"hhh\").forEach((k, v) -\u003e System.out.println(k+\": \"+v)); } } 我们接着来看看列表操作： public static void main(String[] args) { try(Jedis jedis = new Jedis(\"192.168.10.3\", 6379)){ jedis.lpush(\"mylist\", \"111\", \"222\", \"333\"); //等同于 lpush mylist 111 222 333 命令 jedis.lrange(\"mylist\", 0, -1) .forEach(System.out::println); //等同于 lrange mylist 0 -1 } } 实际上我们只需要按照对应的操作去调用同名方法即可，所有的类型封装Jedis已经帮助我们完成了。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:7:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"SpringBoot整合Redis 我们接着来看如何在SpringBoot项目中整合Redis操作框架，只需要一个starter即可，但是它底层没有用Jedis，而是Lettuce： \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e starter提供的默认配置会去连接本地的Redis服务器，并使用0号数据库，当然你也可以手动进行修改： spring:redis:#Redis服务器地址host:192.168.10.3#端口port:6379#使用几号数据库database:0starter已经给我们提供了两个默认的模板类： @Configuration( proxyBeanMethods = false ) @ConditionalOnClass({RedisOperations.class}) @EnableConfigurationProperties({RedisProperties.class}) @Import({LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class}) public class RedisAutoConfiguration { public RedisAutoConfiguration() { } @Bean @ConditionalOnMissingBean( name = {\"redisTemplate\"} ) @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public RedisTemplate\u003cObject, Object\u003e redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate\u003cObject, Object\u003e template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) { return new StringRedisTemplate(redisConnectionFactory); } } 那么如何去使用这两个模板类呢？我们可以直接注入StringRedisTemplate来使用模板： @SpringBootTest class SpringBootTestApplicationTests { @Autowired StringRedisTemplate template; @Test void contextLoads() { ValueOperations\u003cString, String\u003e operations = template.opsForValue(); operations.set(\"c\", \"xxxxx\"); //设置值 System.out.println(operations.get(\"c\")); //获取值 template.delete(\"c\"); //删除键 System.out.println(template.hasKey(\"c\")); //判断是否包含键 } } 实际上所有的值的操作都被封装到了ValueOperations对象中，而普通的键操作直接通过模板对象就可以使用了，大致使用方式其实和Jedis一致。 我们接着来看看事务操作，由于Spring没有专门的Redis事务管理器，所以只能借用JDBC提供的，只不过无所谓，正常情况下反正我们也要用到这玩意： \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-jdbc\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003c/dependency\u003e @Service public class RedisService { @Resource StringRedisTemplate template; @PostConstruct public void init(){ template.setEnableTransactionSupport(true); //需要开启事务 } @Transactional //需要添加此注解 public void test(){ template.multi(); template.opsForValue().set(\"d\", \"xxxxx\"); template.exec(); } } 我们还可以为RedisTemplate对象配置一个Serializer来实现对象的JSON存储： @Test void contextLoad2() { //注意Student需要实现序列化接口才能存入Redis template.opsForValue().set(\"student\", new Student()); System.out.println(template.opsForValue().get(\"student\")); } ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:7:2","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"使用Redis做缓存 我们可以轻松地使用Redis来实现一些框架的缓存和其他存储。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:8:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"Mybatis二级缓存 还记得我们在学习Mybatis讲解的缓存机制吗，我们当时介绍了二级缓存，它是Mapper级别的缓存，能够作用与所有会话。但是当时我们提出了一个问题，由于Mybatis的默认二级缓存只能是单机的，如果存在多台服务器访问同一个数据库，实际上二级缓存只会在各自的服务器上生效，但是我们希望的是多台服务器都能使用同一个二级缓存，这样就不会造成过多的资源浪费。 我们可以将Redis作为Mybatis的二级缓存，这样就能实现多台服务器使用同一个二级缓存，因为它们只需要连接同一个Redis服务器即可，所有的缓存数据全部存储在Redis服务器上。我们需要手动实现Mybatis提供的Cache接口，这里我们简单编写一下： //实现Mybatis的Cache接口 public class RedisMybatisCache implements Cache { private final String id; private static RedisTemplate\u003cObject, Object\u003e template; //注意构造方法必须带一个String类型的参数接收id public RedisMybatisCache(String id){ this.id = id; } //初始化时通过配置类将RedisTemplate给过来 public static void setTemplate(RedisTemplate\u003cObject, Object\u003e template) { RedisMybatisCache.template = template; } @Override public String getId() { return id; } @Override public void putObject(Object o, Object o1) { //这里直接向Redis数据库中丢数据即可，o就是Key，o1就是Value，60秒为过期时间 template.opsForValue().set(o, o1, 60, TimeUnit.SECONDS); } @Override public Object getObject(Object o) { //这里根据Key直接从Redis数据库中获取值即可 return template.opsForValue().get(o); } @Override public Object removeObject(Object o) { //根据Key删除 return template.delete(o); } @Override public void clear() { //由于template中没封装清除操作，只能通过connection来执行 template.execute((RedisCallback\u003cVoid\u003e) connection -\u003e { //通过connection对象执行清空操作 connection.flushDb(); return null; }); } @Override public int getSize() { //这里也是使用connection对象来获取当前的Key数量 return template.execute(RedisServerCommands::dbSize).intValue(); } } 缓存类编写完成后，我们接着来编写配置类： @Configuration public class MainConfiguration { @Resource RedisTemplate\u003cObject, Object\u003e template; @PostConstruct public void init(){ //把RedisTemplate给到RedisMybatisCache RedisMybatisCache.setTemplate(template); } } 最后我们在Mapper上启用此缓存即可： //只需要修改缓存实现类implementation为我们的RedisMybatisCache即可 @CacheNamespace(implementation = RedisMybatisCache.class) @Mapper public interface MainMapper { @Select(\"select name from student where sid = 1\") String getSid(); } 最后我们提供一个测试用例来查看当前的二级缓存是否生效： @SpringBootTest class SpringBootTestApplicationTests { @Resource MainMapper mapper; @Test void contextLoads() { System.out.println(mapper.getSid()); System.out.println(mapper.getSid()); System.out.println(mapper.getSid()); } } 手动使用客户端查看Redis数据库，可以看到已经有一条Mybatis生成的缓存数据了。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:8:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"Token持久化存储 我们之前使用SpringSecurity时，remember-me的Token是支持持久化存储的，而我们当时是存储在数据库中，那么Token信息能否存储在缓存中呢，当然也是可以的，我们可以手动实现一个： //实现PersistentTokenRepository接口 @Component public class RedisTokenRepository implements PersistentTokenRepository { //Key名称前缀，用于区分 private final static String REMEMBER_ME_KEY = \"spring:security:rememberMe:\"; @Resource RedisTemplate\u003cObject, Object\u003e template; @Override public void createNewToken(PersistentRememberMeToken token) { //这里要放两个，一个存seriesId-\u003eToken，一个存username-\u003eseriesId，因为删除时是通过username删除 template.opsForValue().set(REMEMBER_ME_KEY+\"username:\"+token.getUsername(), token.getSeries()); template.expire(REMEMBER_ME_KEY+\"username:\"+token.getUsername(), 1, TimeUnit.DAYS); this.setToken(token); } //先获取，然后修改创建一个新的，再放入 @Override public void updateToken(String series, String tokenValue, Date lastUsed) { PersistentRememberMeToken token = this.getToken(series); if(token != null) this.setToken(new PersistentRememberMeToken(token.getUsername(), series, tokenValue, lastUsed)); } @Override public PersistentRememberMeToken getTokenForSeries(String seriesId) { return this.getToken(seriesId); } //通过username找seriesId直接删除这两个 @Override public void removeUserTokens(String username) { String series = (String) template.opsForValue().get(REMEMBER_ME_KEY+\"username:\"+username); template.delete(REMEMBER_ME_KEY+series); template.delete(REMEMBER_ME_KEY+\"username:\"+username); } //由于PersistentRememberMeToken没实现序列化接口，这里只能用Hash来存储了，所以单独编写一个set和get操作 private PersistentRememberMeToken getToken(String series){ Map\u003cObject, Object\u003e map = template.opsForHash().entries(REMEMBER_ME_KEY+series); if(map.isEmpty()) return null; return new PersistentRememberMeToken( (String) map.get(\"username\"), (String) map.get(\"series\"), (String) map.get(\"tokenValue\"), new Date(Long.parseLong((String) map.get(\"date\")))); } private void setToken(PersistentRememberMeToken token){ Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); map.put(\"username\", token.getUsername()); map.put(\"series\", token.getSeries()); map.put(\"tokenValue\", token.getTokenValue()); map.put(\"date\", \"\"+token.getDate().getTime()); template.opsForHash().putAll(REMEMBER_ME_KEY+token.getSeries(), map); template.expire(REMEMBER_ME_KEY+token.getSeries(), 1, TimeUnit.DAYS); } } 接着把验证Service实现了： @Service public class AuthService implements UserDetailsService { @Resource UserMapper mapper; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException { Account account = mapper.getAccountByUsername(username); if(account == null) throw new UsernameNotFoundException(\"\"); return User .withUsername(username) .password(account.getPassword()) .roles(account.getRole()) .build(); } } Mapper也安排上： @Data public class Account implements Serializable { int id; String username; String password; String role; } @CacheNamespace(implementation = MybatisRedisCache.class) @Mapper public interface UserMapper { @Select(\"select * from users where username = #{username}\") Account getAccountByUsername(String username); } 最后配置文件配一波： @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .and() .rememberMe() .tokenRepository(repository); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth .userDetailsService(service) .passwordEncoder(new BCryptPasswordEncoder()); } OK，启动服务器验证一下吧。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:8:2","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"三大缓存问题 **注意：**这部分内容作为选学内容。 虽然我们可以利用缓存来大幅度提升我们程序的数据获取效率，但是使用缓存也存在着一些潜在的问题。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:9:0","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"缓存穿透 当我们去查询一个一定不存在的数据，比如Mybatis在缓存是未命中的情况下需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 这显然是很浪费资源的，我们希望的是，如果这个数据不存在，为什么缓存这一层不直接返回空呢，这时就不必再去查数据库了，但是也有一个问题，缓存不去查数据库怎么知道数据库里面到底有没有这个数据呢？ 这时我们就可以使用布隆过滤器来进行判断。什么是布隆过滤器？（当然不是打辅助的那个布隆，只不过也挺像，辅助布隆也是挡子弹的） 使用布隆过滤器，能够告诉你某样东西一定不存在或是某样东西可能存在。 布隆过滤器本质是一个存放二进制位的bit数组，如果我们要添加一个值到布隆过滤器中，我们需要使用N个不同的哈希函数来生成N个哈希值，并对每个生成的哈希值指向的bit位置1，如上图所示，一共添加了三个值abc。 接着我们给一个d，那么这时就可以进行判断，如果说d计算的N个哈希值的位置上都是1，那么就说明d可能存在；这时候又来了个e，计算后我们发现有一个位置上的值是0，这时就可以直接断定e一定不存在。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:9:1","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"缓存击穿 某个 Key 属于热点数据，访问非常频繁，同一时间很多人都在访问，在这个Key失效的瞬间，大量的请求到来，这时发现缓存中没有数据，就全都直接请求数据库，相当于击穿了缓存屏障，直接攻击整个系统核心。 这种情况下，最好的解决办法就是不让Key那么快过期，如果一个Key处于高频访问，那么可以适当地延长过期时间。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:9:2","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["JavaWeb笔记"],"content":"缓存雪崩 当你的Redis服务器炸了或是大量的Key在同一时间过期，这时相当于缓存直接GG了，那么如果这时又有很多的请求来访问不同的数据，同一时间内缓存服务器就得向数据库大量发起请求来重新建立缓存，很容易把数据库也搞GG。 解决这种问题最好的办法就是设置高可用，也就是搭建Redis集群，当然也可以采取一些服务熔断降级机制，这些内容我们会在SpringCloud阶段再进行探讨。 ","date":"2022-06-11","objectID":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/:9:3","tags":["redis的介绍与使用"],"title":"redis的介绍与使用","uri":"/posts/redis%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"categories":["个人项目"],"content":"[Java课设]Swing用分层思想浅写个管理系统课设","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":" 本为Java实验课设，为了不浪费时间，故用良好的构思，写出了这个管理系统。 项目仓库地址：已开源GitHub ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:0:0","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"项目需求 基本要求： 社团管理：具体完成社团的新增、修改、查询等功能； 人员管理：针对某一指定的社团完成其学员（学生编号、姓名、年龄、所学专业、兴趣爱好等）进行新增、修改、查询、删除等功能； 给定某一学员，查找其所参加的全部社团，并将相关社团信息进行显示输出; 必须要用面向对象设计思想编程实现 高级要求： 界面友好 实现对社团人数按大小排序的功能； 在新增学员过程中，实现给定一学生编号，若该学生编号在其他社团中存在，则将该学生的信息自动进行显示，若不存在，则需要录入该学生的所有详细数据信息； ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:1:0","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"项目实现 本项目采取前后端分离的思想，Swing仅仅负责前端页面的展示，数据由后端访问数据库提供。 代码结构如下： ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:2:0","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"后端实现 数据库设计 一共有四张表： users：用于存储用户的账户和密码 clubs：存储所有社团信息 club_members：存储每个成员的基本信息 club_member_relations：存储社团和社团成员的多对多关系 架构分层 架构分层如下： models：负责直接和数据库打交道初步的增删改查操作。 service：真正提供给前端界面的接口，进一步封装增删改查，对上层参数校验，或者将models层零散的数据再包装。 接口实现情况 所有接口如下： 关键设计模式：单例模式。 DAO类均为单例访问模式。 Club相关 public static void AddClub(Club club) throws RuntimeException; public static void AddClubList(List\u003cClub\u003e clubList) throws RuntimeException; public static void UpdateClub(Club club) throws RuntimeException; public static void DeleteClub(Club club) throws RuntimeException; public static List\u003cClub\u003e QueryClubsByName(String name) throws RuntimeException; public static List\u003cClub\u003e QueryClubByMemberId(int memberId) throws RuntimeException; public static List\u003cClub\u003e QueryAllClub(); //根据order值返回按memberCount排序的结果：true顺序，false逆序 public static List\u003cClub\u003e QueryAllClubByOrder(boolean order); ClubMember相关 public static void AddClubMember(int clubId, ClubMember member) throws RuntimeException; //把一堆人加入到一个社团中 public static void AddClubMemberList(int clubId, List\u003cClubMember\u003e clubMemberList) throws RuntimeException; public static void UpdateClubMember(ClubMember member) throws RuntimeException; public static List\u003cClubMember\u003e QueryClubMemberByName(int clubId, String name) throws RuntimeException; public static void DeleteClubMemberById(int clubId, int id) throws RuntimeException; public static List\u003cClubMember\u003e QueryClubMembersByClubId(int clubId) throws RuntimeException; ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:2:1","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"前端实现 架构分层情况 核心设计 所有的界面均采用单例模式进行操作，这样界面之间的交互就不存在任何问题，想要显示任意一个窗口，只需要如下代码，由于用的是一个单例，则每次获取单例时我们应该把它的状态先调整为0，也就是需要单独写一共reset成员函数来进行调整，具体用法如下： //此处演示为MainMenu窗口，任意窗口都是通过这种模式进行窗口之间的切换和显示，而不是会产生很多问题的new MainMenu.getInstance().reset(); 界面美化设计和特色 用定时器不断切换精美背景图片。 重写JLable实现自定义超链接。 Excel导入导出功能。 查询的排序算法，支持按照社团人数升序或者降序排序。 所有操作添加了快捷键。 还有一共看似不起眼，但我认为却非常重要的，那就是代码的健壮性，由于后端的分层逻辑使得数据之间的增删改查稳定性极佳。 最终界面效果 ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:2:2","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"项目运行 项目运行环境 JDK版本11，只用到了Java1.8的特性，故只需要JDK版本\u003e=1.8。 mybatis：版本3.5.7，用于快速实现和数据库和模型之间的映射。 JUnit：版本4.13.2，用于测试各个模块的功能。 lombok：用于快速生成getter和setter poi：版本3.11，用于导入导出Excel表格 界面的运行流程 界面中所有对数据的操作，都是直接调用后端接口实现。 ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:2:3","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["个人项目"],"content":"项目代码仓库 仓库链接 ","date":"2022-06-05","objectID":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/:2:4","tags":["[Java课设]Swing用分层思想浅写个管理系统课设"],"title":"[Java课设]Swing用分层思想浅写个管理系统课设","uri":"/posts/java%E8%AF%BE%E8%AE%BEswing%E7%94%A8%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%E6%B5%85%E5%86%99%E4%B8%AA%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AF%BE%E8%AE%BE/"},{"categories":["青训营笔记"],"content":"消息队列","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"消息队列的应用场景 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:0","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"场景一：解耦 如上所述，如果记录用户行为的这个请求过程，和后台服务的记录过程直接耦合，将会产生很严重的后果，如果后台服务宕机或者直接删库跑路等等状况，那么前端的请求也不会得到正确的响应，这时前端的请求服务将会一直阻塞，这会严重影响用户体验。 此时如果我们在记录存储的前面引入消息队列，那么每次前端发送的请求只要到了消息队列就能正常返回了，这会大大提高响应的速度，后续的存储过程就不需要再关心了，这就是解耦的应用。 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:1","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"场景二：削峰 具体到生活中的业务就是，如果有一个抢购秒杀活动，这个时候，肯定会导致在某个时间段流量达到很大的峰值，而我们最终只需要寥寥10个名额甚至更少，那么我们该如何解决？ 这个时候可以利用到消息队列，可以在后端正式处理之前加上一个消息队列，便可达到以下效果： 可以控制活动的人数 可以缓解短时间内高流量压垮应用 用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面 后台只需消费队列中的内容即可 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:2","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"场景三：异步 如上图所示，这是一个串行的过程，很明显可以优化为并行过程，但如果还是像图中的直接将发起订单后的三个过程并行，那最终还是得等30s才能有结果！那么如何解决呢？ 这个时候可以加个消息队列，前端和消息队列接触后，直接返回，然后后台负责去消费这个消息队列即可（实际上这个过程更像是解耦的过程 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:3","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"场景四：日志处理 同样，如果是通过消息队列作为中间的过程来传递日志，那么不用担心真正后台记录日志的服务器宕机且后台日志丢失等严重问题。 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:4","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"消费队列的定义 其中的高并发和高吞吐都很清楚，而这里的高可用，指的就是不会随便发生异常行为而导致服务不可用！ ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:1:5","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"业界的消息队列 业界常用消息队列对比 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:2:0","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"Kafka 以上为Kafka的使用流程。 创建集群。 需要在这个集群中创建一个Topic，并且设置好分片数量。 引入对应语言的SDK，配置好集群和Topic等参数，初始化一个生产者，调用Send方法，将你的Hello World发送出去。 引入对应语言的SDK，配置好集群和Topic等参数，初始化一个消费者，调用Poll方法，你将收到你刚刚发送的Hello World。 基本名词 Cluster Topic：Kakfa中的逻辑队列，可以理解成每一个不同的业务场景就是一个不同的topic，对于这个业务来说，所有的数据都存储在这个topic中。 Cluster：Kafka的物理集群，每个集群中可以新建多个不同的topic。 Producer：顾名思义，也就是消息的生产端，负责将业务消息发送到Topic当中。 Consumer：消息的消费端，负责消费已经发送到topic中的消息。 Partition：通常topic会有多个分片，不同分片直接消息是可以并发来处理的，这样提高单个Topic的吞吐。 Offset 对于每一个Partition来说，每一条消息都有一个唯一的Offset，消息在partition内的相对位置信息，并且严格递增。 Replica Replica：分片的副本，分布在不同的机器上，可用来容灾，Leader对外服务，Follower异步去拉取leader的数据进行一个同步，如果leader挂掉了，可以将Follower提升成leader再堆外进行服务 ISR：意思是同步中的副本，对于Follower来说，始终和leader是有一定差距的，但当这个差距比较小的时候，我们就可以将这个follower副本加入到ISR中，不在ISR中的副本是不允许提升成Leader的。 实际架构过程分析 Broker 注意看上图，每个broker代表一个节点，实际上对应一台机器，所有的Broker组成了一个集群，我们不要被每个框框束缚住，只需关注一共有哪些Topic和它对应的Partition即可。 整个图表示，图中整个集群，包含了4个Broker机器节点，集群有两个Topic，分别是Topic1和Topic2，Topic1有两个分片，Topic2有1个分片，每个分片都是三副本的状态。这里中间有一个Broker同时也扮演了Controller的角色，Controller是整个集群的大脑，负责对副本和Broker进行分配。 Zookeeper 而在集群的基础上，还有一个模块是ZooKeeper，这个模块其实是存储了集群的元数据信息，比如副本的分配信息等等，Controller计算好的方案都会放到这个地方。 Kafka高吞吐和稳定的秘诀 Producer 通过批量发送减少IO次数。 为了防止批量发送的数据包过大，使用压缩算法进行压缩。 Broker 采用以下三点进行优化： graph TB a[顺序写] b[消息索引] c[零拷贝] d[优化方式] d--\u003ea d--\u003eb d--\u003ec 先来看看消息队列的的文件结构： 在每一个Broker，都分布着不同Topic的不同分片。 顺序写 顺序写：每条消息都是紧凑排列的顺序写入。 消息索引 由于文件名和第一条存储数据的索引相同，故可先通过二分查找找到小于offset的最大索引位置，然后再遍历这个文件内的索引记录，便可找到目标消息。 同理，也可通过时间戳二分查找。 零拷贝 通过操作系统的API，直接实现内核空间直接发送到网络。 Consumer 对于一个Consumer Group来说，多个分片可以并发的消费，这样可以大大提高消费的效率，但需要解决的问题是，Consumer和Partition的分配问题，也就是对于每一个Partition来讲，该由哪一个Consumer来消费的问题。对于这个问题，我们一般有两种解决方法，手动分配和自动分配。 手动分配 第一，手动分配，也就是Kafka中所说的Low Level消费方式进行消费，这种分配方式的一个好处就是启动比较快，因为对于每一个Consumer来说，启动的时候就已经知道了自己应该去消费哪个消费方式，就好比图中的Consumer Group1来说，Consumer1去消费Partition1,2,3 Consumer2，去消费456， Consumer3去消费78。这些Consumer再启动的时候就已经知道分配方案了，但这样这种方式的缺点又是什么呢，想象一下，如果我们的Consumer3挂掉了，我们的7,8分片是不是就停止消费了。又或者，如果我们新增了一台Consumer4，那是不是又需要停掉整个集群，重新修改配置再上线，保证Consumer4也可以消费数据，其实上面两个问题，有时候对于线上业务来说是致命的。 自动分配 所以Kafka也提供了自动分配的方式，这里也叫做High Level的消费方式，简单的来说，就是在我们的Broker集群中，对于不同的Consumer Group来讲，都会选取一台Broker当做Coordinator，而Coordinator的作用就是帮助Consumer Group进行分片的分配，也叫做分片的rebalance，使用这种方式，如果ConsumerGroup中有发生宕机，或者有新的Consumer加入，整个partition和Consumer都会重新进行分配来达到一个稳定的消费状态。 Kafka存在的问题 运维成本高 举个例子来说，如果我们对一个机器进行重启 首先，我们会关闭一个Broker，此时如果该Broker上存在副本的Leader，那么该副本将发生leader切换，切换到其他节点上面并且在ISR中的Follower副本，可以看到图中是切换到了第二个Broker上面 而此时，因为数据在不断的写入，对于刚刚关闭重启的Broker来说，和新Leader之间一定会存在数据的滞后，此时这个Broker会追赶数据，重新加入到ISR当中 当数据追赶完成之后，我们需要回切leader，这一步叫做prefer leader，这一步的目的是为了避免，在一个集群长期运行后，所有的leader都分布在少数节点上，导致数据的不均衡 通过上面的一个流程分析，我们可以发现对于一个Broker的重启来说，需要进行数据复制，所以时间成本会比较大，比如一个节点重启需要10分钟，一个集群有1000个节点，如果该集群需要重启升级，则需要10000分钟，那差不多就是一个星期，这样的时间成本是非常大的。 你可能会说，可以不可以并发多台重启呀，问的好，不可以。为什么呢，在一个两副本的集群中，重启了两台机器，对某一分片来讲，可能两个分片都在这台机器上面（可能这几个机器包含所有分片，则会导致该集群处于不可用的状态。这是更不能接受的。 负载不均衡场景解决方案复杂 这个场景当中，同一个Topic有4个分片，两副本，可以看到，对于分片1来说，数据量是明显比其他分片要大的，当我们机器IO达到瓶颈的时候，可能就需要把第一台Broker上面的Partition3迁移到其他负载小的Broker上面，但我们的数据复制又会引起Broker1的IO升高，所以问题就变成了，我为了去解决IO升高，但解决问题的过程又会带来更高的IO，所以就需要权衡IO设计出一个极其复杂的负载均衡策略。 没有自己的缓存 Kafka没有自己的缓存，在进行数据读取的时候，只有Page Cache可以用，所以不是很灵活。 Controller、Coordinator、Broker处于同一进程 Kafka的Controller和Coordinator都是和Broker部署在一起的，Broker因为承载大量IO的原因，会导致Controller和Coordinator的性能下降，如果到一定程度，可能会影响整个集群的可用性。 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:2:1","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"BMQ 架构模型（解决Kafka存在的问题 解决运维成本问题 实际上对于所有节点变更的操作，都仅仅只是集群元数据的变化，通常情况下都能秒级完成，而真正的数据已经移到下层分布式文件存储去了，所以运维操作不需要额外关心数据复制所带来的时间成本。 分布式系统的具体文件写入操作： 通过前面的介绍，我们知道了，同一个副本是由多个segment组成，我们来看看BMQ对于单个文件写入的机制是怎么样的，首先客户端写入前会选择一定数量的DataNode，这个数量是副本数，然后将一个文件写入到这三个节点上，切换到下一个segment之后，又会重新选择三个节点进行写入。这样一来，对于单个副本的所有segment来讲，会随机的分配到分布式文件系统的整个集群中。 解决负载均衡问题 对于Kafka分片数据的写入，是通过先在Leader上面写好文件，然后同步到Follower上，所以对于同一个副本的所有Segment都在同一台机器上面。就会存在之前我们所说到的单分片过大导致负载不均衡的问题，但在BMQ集群中，因为对于单个副本来讲，是随机分配到不同的节点上面的（分布式存储，因此不会存在Kafka的负载不均问题。 其实对于写入的逻辑来说，我们还有一个状态机的机制，用来保证不会出现同一个分片在两个Broker上同时启动的情况，另外也能够保证一个分片的正常运行。首先，Controller做好分片的分配之后，如果在该Broker分配到了Broker，首先会start这个分片，然后进入Recover状态，这个状态主要有两个目的获取分片写入权利，也就是说，对于hdfs来讲，只会允许我一个分片进行写入，只有拿到这个权利的分片我才能写入，第二一个目的是如果上次分片是异常中断的，没有进行save checkpoint，这里会重新进行一次save checkpoint，然后就进入了正常的写流程状态，创建文件，写入数据，到一定大小之后又开始建立新的文件进行写入。 读写流程 文件写入流程 数据校验：CRC , 参数是否合法 校验完成后，会把数据放入Buffer中 通过一个异步的Write Thread线程将数据最终写入到底层的存储系统当中。 这里有一个地方需要注意一下，就是对于业务的写入来说，可以配置返回方式，可以在写完缓存之后直接返回，另外我也可以数据真正写入存储系统后再返回，对于这两个来说前者损失了数据的可靠性，带来了吞吐性能的优势，因为只写入内存是比较快的，但如果在下一次flush前发生宕机了，这个时候数据就有可能丢失了，后者的话，因为数据已经写入了存储系统，这个时候也不需要担心数据丢失，相应的来说吞吐就会小一些 我们再来看看Thread的具体逻辑，首先会将Buffer中的数据取出来，调用底层写入逻辑，在一定的时间周期上去flush，flush完成后开始建立Index，也就是offset和timestamp对于消息具体位置的映射关系 Index建立好以后，会save一次checkpoint，也就表示，checkpoint后的数据是可以被消费的，我们想一下，如果没有checkpoint的情况下会发生什么问题，如果flush完成之后宕机，index还没有建立，这个数据是不应该被消费的 最后当文件到达一定大小之后，需要建立一个新的segment文件来写入。 文件获取流程 首先Consumer发送一个Fetch Request，然后会有一个Wait流程，那么他的左右是什么呢，想象一个Topic，如果一直没有数据写入，那么，此时consumer就会一直发送Fetch Request，如果Consumer数量过多，BMQ的server端是扛不住这个请求的，因此，我们设置了一个等待机制，如果没有fetch到指定大小的数据，那么proxy会等待一定的时间，再返回给用户侧，这样也就降低了fetch请求的IO次数，经过我们的wait流程后，我们会到我们的Cache里面去找到是否有存在我们想要的数据，如果有直接返回，如果没有，再开始去存储系统当中寻找，首先会Open这个文件，然后通过Index找到数据所在的具体位置，从这个位置开始读取数据。 高级特性 泳道 Databus 在直接的消息队列的客户端操作之中又封装了一层，客户端代码可用得到简化，可缓解集群压力。 Mirror 主要用于解决跨区域（不同国家）的读写问题。 Index 通过索引构建类似于数据库的表结构。 Parquet Parquet 是一种支持嵌套结构的列式存储格式 非常适用于 OLAP 场景，按列存储和扫描 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:2:2","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"RocketMQ 基本概念 Broker节点有Master和Slave的概念 NameServer为集群提供轻量级服务发现和路由。 根据我们刚刚的介绍，可以看到Producer，Consumer，Broker这三个部分，Kafka和RocketMQ是一样的，而Kafka中的Partition概念在这里叫做ConsumerQueue。 底层原理 存储模型 接下来我们来看看RocketMQ消息的存储模型，对于一个Broker来说所有的消息的会append到一个CommitLog上面，然后按照不同的Queue，重新Dispatch到不同的Consumer中，这样Consumer就可以按照Queue进行拉取消费，但需要注意的是，这里的ConsumerQueue所存储的并不是真实的数据，真实的数据其实只存在CommitLog中，这里存的仅仅是这个Queue所有消息在CommitLog上面的位置，相当于是这个Queue的一个密集索引。 高级特性 事务消息 先看一下我们最开始说的这个场景，正常情况下，这个下单的流程应该是这个样子，首先我保证库存足够能够顺利-1，这个时候再消息队列让我其他系统来处理，比如订单系统和商家系统，但这里有个比较重要的点，我库存服务和消息队列必须要是在同一个事务内的，大家还记不记得事务的基本特性是什么。ACID（原子性、一致性、隔离性、持久性），这里库存记录和往消息队列里面发的消息这两个事情，是需要有事务保证的，这样不至于发生，库存已经-1了，但我的订单没有增加，或者商家也没有收到通知要发货。因此RocketMQ提供事务消息来保证类似的场景。 ACID: 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务前后数据的完整性必须保持一致。 隔离性（Isolation） 事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。 延迟队列 执行原理： 重试和死信队列 对于处理失败的情况下，用充实和死信队列的方式处理。 特性说明 死信消息具有以下特性： 不会再被消费者正常消费。 有效期与正常消息相同，均为3天，3天后会被自动删除。因此，请在死信消息产生后的3天内及时处理。 死信队列具有以下特性： 一个死信队列对应一个Group ID， 而不是对应单个消费者实例。 如果一个Group ID未产生死信消息，消息队列RocketMQ版不会为其创建相应的死信队列。 一个死信队列包含了对应Group ID产生的所有死信消息，不论该消息属于哪个Topic。 消息队列RocketMQ版控制台提供对死信消息的查询、导出和重发的功能。 ","date":"2022-06-04","objectID":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/:2:3","tags":["消息队列"],"title":"消息队列","uri":"/posts/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"categories":["青训营笔记"],"content":"jwt","date":"2022-05-17","objectID":"/posts/jwt/","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"这是我参与「第三届青训营 -后端场」笔记创作活动的的第四篇笔记。 由于最近14周忙的焦头烂额，所以一直没有时间写笔记，而且也很久没有认真看青训营的课了（期末考试，所有专业课都放在了14周考试，考完这个周，后面就没有重要的考试了（6级英语和蓝桥杯国赛除外😭 我为什么突然将JWT鉴权，因为抖音项目里面的前后端身份识别就是通过传递JWT token进行身份验证的传递。 JSON Web Token（缩写 JWT）是目前最流行的跨域认证解决方案，本文介绍它的原理和用法。 ","date":"2022-05-17","objectID":"/posts/jwt/:0:0","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"用户认证 在服务端和客户端之间通信的过程中，服务端需要有效的识别出客户端的身份，才能进行对应的通信，毕竟大部分时候客户端和服务器并不是长连接，一般都是socket都是连接后收发一个包就断开了，所以这种情景下，每次请求肯定是需要重新经过认证过程的。 互联网服务离不开用户认证。一般流程是下面这样。 用户向服务器发送用户名和密码。 服务器验证通过后，在当前对话（session）里面保存相关数据，比如用户角色、登录时间等等。 服务器向用户返回一个 session_id，写入用户的 Cookie。 用户随后的每一次请求，都会通过 Cookie，将 session_id 传回服务器。 服务器收到 session_id，找到前期保存的数据，由此得知用户的身份。 这种模式的问题在于，扩展性不好。单机当然没有问题，如果是服务器集群，或者是跨域的服务导向架构，就要求 session 数据共享，每台服务器都能够读取 session。 举例来说，A 网站和 B 网站是同一家公司的关联服务。现在要求，用户只要在其中一个网站登录，再访问另一个网站就会自动登录，请问怎么实现？ 一种解决方案是 session 数据持久化，写入数据库或别的持久层。各种服务收到请求后，都向持久层请求数据。这种方案的优点是架构清晰，缺点是工程量比较大。另外，持久层万一挂了，就会单点失败。 另一种方案是服务器索性不保存 session 数据了，所有数据都保存在客户端，每次请求都发回服务器。JWT 就是这种方案的一个代表。 ","date":"2022-05-17","objectID":"/posts/jwt/:1:0","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"JWT到底是什么 ","date":"2022-05-17","objectID":"/posts/jwt/:2:0","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"JWT原理 为什么叫做JWT，从缩写中又json，就不难猜出，它的数据格式肯定和json格式有关联。 JWT 的原理是，服务器认证以后，生成一个 JSON 对象，发回给用户，就像下面这样。 { \"姓名\": \"张三\", \"角色\": \"管理员\", \"到期时间\": \"2022年5月27日0点0分\" } 之后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名（详见后文）。 服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。 ","date":"2022-05-17","objectID":"/posts/jwt/:2:1","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"JWT数据结构 虽然如上文所说，发送的JWT是一个json对象是没错，但实际发送的并不是直接的明文，而应该是由 . 分割的三个字符串。如下图： 以上的三个部分的字符串，分别被称为以下三个组成部分： Header（头部） Payload（负载） Signature（签名） 即 Header.Paload.Signature 下面依次介绍这三个部分。 Header Header 部分是一个 JSON 对象，描述 JWT 的元数据，通常是下面的样子。 { \"alg\": \"HS256\", \"typ\": \"JWT\" } 上面代码中，alg属性表示签名的算法（algorithm），默认是 HMAC SHA256（写成 HS256）；typ属性表示这个令牌（token）的类型（type），JWT 令牌统一写为JWT。 Payload Payload 部分也是一个 JSON 对象，用来存放实际需要传递的数据。JWT 规定了7个官方字段，供选用。 iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 除了官方字段，你还可以在这个部分定义私有字段，下面就是一个例子。 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } 注意：JWT 默认是不加密的（即便后面的签名也只是为了防止数据篡改），任何人都可以读到，所以不要把秘密信息放在这个部分。 Signature Signature 部分是对前两部分的签名，防止数据篡改。 首先，需要指定一个密钥（secret）。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256），按照下面的公式产生签名。 HMACSHA256(\rbase64UrlEncode(header) + \".\" +\rbase64UrlEncode(payload),\rsecret)\r算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用\"点\"（.）分隔，就可以返回给用户。 ","date":"2022-05-17","objectID":"/posts/jwt/:2:2","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"JWT实战 根据前面的描述，我们对JWT的数据结构有了一定的了解。 现在转换到我们的Go语言中具体的使用它！ 首先go get一下我们处理JWT的包。 go get github.com/dgrijalva/jwt-go ","date":"2022-05-17","objectID":"/posts/jwt/:3:0","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"签发JWT的token 根据前面的讲解，我们已经了解到JWT的数据结构，它就是一个json文件，包含三个部分。 那么我们现在需要做的就是填写这三部分，最后通过base64UrlEncode编码得到字符串即为最终的token。 Header部分 这个部分包含两个字段，这两个字段都不用我们填，会由你选择的签名算法自动填写好。 Payload部分 我们查看 jwt.StandardClaims 类型的字段，可以发现它包含如下的类型： 这正好就是Payload部分默认填写的几个字段，我们自定义一个结构体，将该结构体类型嵌入。如下，我定义了Claims类型： type Claims struct { UserId int64 jwt.StandardClaims } 然后我们把这些字段填充完整，来用它得到token。 claims := \u0026Claims{ UserId: user.UserInfoId, StandardClaims: jwt.StandardClaims{ ExpiresAt: expirationTime.Unix(), IssuedAt: time.Now().Unix(), Issuer: \"douyin_pro_131\", Subject: \"L_B__\", }} token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) jwt.NewWithClaims() 将返回一个 Token 对象。 看看源码，我们发现它仅仅构造了Token对象并返回，里面包含了你已经确认的签名方法回调。 Signature部分 最后通过得到的 Token 对象调用它的 SignedString 方法并传入产生签名需要的密钥(自定义byte[]类型的数据)，将会自动完成token的整个编码过程：包括对 Signature 字段的填写，以及将上述三个部分json数据通过base64url进行最后的编码返回最终的token字符串。 完整源码 ","date":"2022-05-17","objectID":"/posts/jwt/:3:1","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"解析JWT的token 解析过程就非常的简单了，这相当于一个反序列化的过程，和json的解析过程类似，jwt提供的函数需要传入对象的指针，然后在内部帮你完整反序列化操作绑定到这个对象，唯一的不一样就是我们还需要传入一个回调，这个回调返回密钥以便最终的签名判断防止数据篡改。 代码如下： 注意：JWT解析过程中，它是会判断token是否过期了，如果过期，那么就会返回err。 最后我们服务端得到claims后，便可得到其中包含的用户信息了，然后对该用户执行对应的操作，比如得到该用户喜欢的视频列表。 以上所有代码均出自我写的抖音项目里的 jwt.go 目录中。 源码链接 ","date":"2022-05-17","objectID":"/posts/jwt/:3:2","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["青训营笔记"],"content":"JWT的几个特点总结 （1）JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。 （2）JWT 不加密的情况下，不能将秘密数据写入 JWT。 （3）JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。 （4）JWT 的最大缺点是，由于服务器不保存 session 状态，因此无法在使用过程中废止某个 token，或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。 （5）JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT 的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。 （6）为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。 参考链接： https://jwt.io/introduction/ learn how to use jwt? ","date":"2022-05-17","objectID":"/posts/jwt/:4:0","tags":["用户鉴权是什么？jwt是什么？"],"title":"【项目必会】用户鉴权是什么？jwt是什么？ | 青训营笔记","uri":"/posts/jwt/"},{"categories":["JavaWeb笔记"],"content":"一文进入JavaWeb后端","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"前端基础 整个前端基础详细介绍看下面这个链接，这里直接讲里面的实践。 前端基础 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:1:0","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"css实践编写一个漂亮的登录界面 html文件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003elbwnb\u003c/title\u003e \u003clink rel=\"icon\" href=\"icon.png\" type=\"image/x-icon\"\u003e \u003clink rel=\"stylesheet\" href=\"style.css\"\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e登录系统\u003c/h1\u003e \u003cform\u003e \u003chr\u003e \u003cdiv id=\"username\"\u003e \u003clabel\u003e \u003cinput type=\"text\" placeholder=\"用户名\" oninput=\"\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv id=\"password\"\u003e \u003clabel\u003e \u003cinput type=\"password\" placeholder=\"密码\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003cbutton\u003e登录\u003c/button\u003e \u003c/div\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e css文件 body{ margin: 0; text-align: center; } h1:hover { color: rgb(45, 150, 189); } input{ border: none; background: #e0e0e0; line-height: 48px; border-radius: 20px; padding: 0 20px 0 20px; width: 250px; font-size: 15px; margin-top: 20px; outline: 0; } input:focus{ box-shadow: 0 2px 10px gray; } button { margin-top: 60px; background: #4174e3; border: 0; border-radius: 20px; height: 50px; width: 200px; color: white; font-size: 18px; box-shadow: 0 2px 10px #63b9f6; } button:focus{ outline: 0; background: #3b4f72; } ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:1:1","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"前端数据交互和动态的关键 JavaScript事件 事件就是各种动作会触发的信号，这个信号可以添加一个回调给他来调用，在每次触发的时候会调用对应的回调。 事件有很多种类。 常用的有： onclick：点击事件 oninput：内容输入事件 onsubmit：内容提交事件 如何为事件添加一个回调呢？ 直接在属性值里面添加js代码，如下：（注意代码里不要用双引号 \u003cinput type=\"password\" oninput=\"console.log('正在输入文本')\" 引入js文件，然后直接能调用js里的函数。如下： \u003cscript type=\"text/javascript\" src=\"test.js\"\u003e\u003c/script\u003e 然后如下进行使用： \u003cinput type=\"password\" placeholder=\"密码\" oninput=\"f()\"\u003e js文件如下： function f(){ console.log('正在输入文本') } Document Object Model(DOM) 当网页被加载时，浏览器会创建页面的文档对象模型（Document Object Model），它将整个页面的所有元素全部映射为JS对象，这样我们就可以在JS中操纵页面中的元素。 这个对象的名字就叫做document。 比如我现在想要读取页面中某个输入框中的内容，那么我们就需要从DOM中获取此输入框元素的对象： document.getElementById(\"pwd\").value //通过控件的id获取 DOM实践 我们通过DOM实现这样一个功能：我们结合事件，来进行密码长度的校验，密码长度小于6则不合法，不合法的密码，会让密码框边框变红。 输入框变红的css样式： .illegal-pwd{ border: red 1px solid !important; box-shadow: 0 0 5px red; } 对应的js代码，此函数接受一个参数（元素本身的对象）检测输入的长度是否大于6，否则就将当前元素的class属性设定为css指定的class： function checkIllegal(e) { if(e.value.length \u003c 6) { e.setAttribute(\"class\", \"illegal-pwd\") }else { e.removeAttribute(\"class\") } } html最终使用：注意参数传this表示这个组件 \u003cinput type=\"password\" placeholder=\"密码\" oninput=\"checkIllegal(this)\"\u003e 完成以上修改后，会自动检查密码是否合法了。 既然oninput本身也是一个属性，那么实际上我们可以动态进行修改： document.getElementById(\"pwd\").oninput = () =\u003e console.info(\"???\") 那么，我们前面提及的window对象又是什么东西呢？ 实际上Window对象范围更加广阔，它甚至直接代表了整个窗口，当然也包含我们的Document对象，我们一般通过Window对象来弹出提示框之类的东西。 window.document可以得到当前的DOM。 js发送XHR请求 通过使用XMLHttpRequest对象，来向服务器发送一个HTTP请求，下面是一个最简单的请求格式： let xhr = new XMLHttpRequest(); xhr.open('GET', 'https://www.baidu.com'); xhr.send(); 上面的例子中，我们向服务器发起了一次网络请求，但是我们请求的是百度的服务器，并且此请求的方法为GET请求。 我们现在将其绑定到一个按钮上作为事件触发： function http() { let xhr = new XMLHttpRequest(); xhr.open('GET', 'https://www.baidu.com'); xhr.send(); } \u003cinput id=\"button\" type=\"button\" onclick=\"http()\"\u003e 我们可以在网络中查看我们发起的HTTP请求并且查看请求的响应结果，比如上面的请求，会返回百度这个页面的全部HTML代码。 实际上，我们的浏览器在我们输入网址后，也会向对应网站的服务器发起一次HTTP的GET请求。 在浏览器得到页面响应后，会加载当前页面，如果当前页面还引用了其他资源文件，那么会继续向服务器发起请求，直到页面中所有的资源文件全部加载完成后，才会停止。 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:1:2","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"JavaWeb后端 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:2:0","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"HTTP协议简介 Web通信使用的上层协议都是HTTP协议，更准确一点应该是HTTPS，但是HTTPS仅仅只是对HTTP协议的通信过程进行加密，而报文的内容是完全一致的。 需要注意两点： http不是长连接，请求完得到结果后，会立马断开连接。若要保持长连接，应该使用websocket。 http是应用层协议，流量的传输过程和他是无关的，他只负责规定双方如何建立聊天，他采取的底层传输协层协议是TCP协议。 如果对底层的TCP/IP协议发送流量的过程感兴趣，可以看下我之前的博客：TCP协议详解 http请求的报文 请求头报文 响应头报文 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:2:1","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"搭建Tomcat服务器 bin目录：Tomcat服务器的可执行二进制文件等。 conf目录：配置文件。 lib目录：Tomcat服务端运行的一些依赖，不用关心。 logs目录：所有的日志信息都在这里。 temp目录：存放运行时产生的一些临时文件，不用关心。 work目录：工作目录，Tomcat会将jsp文件转换为java文件（我们后面会讲到，这里暂时不提及） webapp目录：所有的Web项目都在这里，每个文件夹都是一个Web应用程序： 启动服务器：进入bin目录运行startup.bat即可。 结束服务器：运行shutdown.bat 登录后台GUI管理 打开conf目录下的tomcat-users.xml文件，添加以下代码用来创建管理GUI的用户名和密码（添加到最后\u003c/tomcat-users\u003e之前。 \u003crole rolename=\"manager-gui\"/\u003e \u003cuser username=\"root\" password=\"123\" roles=\"manager-gui\"/\u003e 开启服务器，然后用浏览器输入URL：http://localhost:8080/manager，输入上述的用户名和密码便可进入后台。 后台如下： 将之前的前端页面部署到tomcat服务器 到webapps目录下创建一个文件夹，我这里创建了一个名为test的文件夹。 进入test文件夹，把需要加载的前端页面的文件copy到里面。 经过以上步骤，打开本地浏览器，输入http://localhost:8080/test便可完成访问。 注意整个主页面的html的命名格式，应该名为index.html。 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:2:2","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"使用Maven创建Web项目 虽然我们已经可以在Tomcat上部署我们的前端页面了，但是依然只是一个静态页面（每次访问都是同样的样子），那么如何向服务器请求一个动态的页面呢（比如显示我们访问当前页面的时间）这时就需要我们编写一个Web应用程序来实现了，我们需要在用户向服务器发起页面请求时，进行一些处理，再将结果发送给用户的浏览器。 **注意：**这里需要使用终极版IDEA，如果你的还是社区版，就很难受了。 我们打开IDEA，新建一个项目，选择Java Enterprise（社区版没有此选项！）项目名称随便，项目模板选择Web应用程序，然后我们需要配置Web应用程序服务器，将我们的Tomcat服务器集成到IDEA中。配置很简单，首先点击新建，然后设置Tomcat主目录即可，配置完成后，点击下一步即可，依赖项使用默认即可，然后点击完成，之后IDEA会自动帮助我们创建Maven项目。 创建完成后，直接点击右上角即可运行此项目了，但是我们发现，有一个Servlet页面不生效。 需要注意的是，Tomcat10以上的版本比较新，Servlet API包名发生了一些变化，因此我们需要修改一下依赖： \u003cdependency\u003e \u003cgroupId\u003ejakarta.servlet\u003c/groupId\u003e \u003cartifactId\u003ejakarta.servlet-api\u003c/artifactId\u003e \u003cversion\u003e5.0.0\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e 注意包名全部从javax改为jakarta，我们需要手动修改一下。 现在我们手动点击运行，可以发现网页已经可以跑起来了。 为什么整个项目能够通过tomcat跑起来？ 实际上因为tomcat支持Servlet的标准，然后可以调用Servlet实现动态页面加载。 当然也可以静态加载网站。我们可以通过maven把项目打包成war包，然后把target里面的war后缀的文件复制到tomcat的webApps目录下，然后重启启动tomcat服务器，你会发现他会被解压为一个文件夹，然后这就是一个可以被服务的新项目了。 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:2:3","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"Servlet类介绍 ","date":"2022-05-13","objectID":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/:2:4","tags":["一文进入JavaWeb后端"],"title":"一文进入JavaWeb后端","uri":"/posts/%E4%B8%80%E6%96%87%E8%BF%9B%E5%85%A5javaweb/"},{"categories":["JavaWeb笔记"],"content":"前端基础","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"前端基础 本章节会讲解前端基础内容（如果已经学习过，可以直接跳到下一个大章节了）那么什么是前端，什么又是后端呢？ 前端：我们网站的页面，包括网站的样式、图片、视频等一切用户可见的内容都是前端的内容。 后端：处理网站的所有数据来源，比如我们之前从数据库中查询数据，而我们查询的数据经过处理最终会被展示到前端，而用于处理前端数据的工作就是由后端来完成的。 相当于，前端仅仅是一层皮，它直接决定了整个网站的美观程度，我们可以自由地编排页面的布局，甚至可以编写好看的特效；而灵魂则是后端，如何处理用户的交互、如何处理数据查询是后端的职责所在，我们前面学习的都是后端内容，而Java也是一门专注于后端开发的语言。 对于前端开发我们需要学习一些新的内容，只有了解了它们，我们才能编写出美观的页面。 本教程并不会过多地去讲解前端知识，我们只会提及一些必要的内容，我们主要学习的是JavaWeb，更倾向于后端开发，学习前端的目的只是为了让同学们了解前后端的交互方式，在进行后端开发时思路能够更加清晰，有关前端的完整内容学习，可以浏览其他前端知识教程。 我们在最开始讲解网络编程时，提到了浏览器访问服务器，实际上浏览器访问服务器就是一种B/S结构，而我们使用Java代码编写的客户端连接服务器就是一种C/S结构。 Web开发还要从HTML开始讲起，这个语言非常简单，很好学习，看完视频如果你觉得前端简单自己更喜欢一些，建议马上转前端吧，还来得及，工资还比后端高，不像后端那么枯燥乏味。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:0:0","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"HTML页面 我们前面学习了XML语言，它是一种标记语言，我们需要以成对标签的格式进行填写，但是它是专用于保存数据，而不是展示数据，而HTML恰恰相反，它专用于展示数据，由于我们前面已经学习过XML语言了，HTML语言和XML很相似，所以我们学习起来会很快。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:1:0","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"第一个HTML页面 我们前面知道，通过浏览器可以直接浏览XML文件，而浏览器一般是用于浏览HTML文件的，以HTML语言编写的内容，会被浏览器识别为一个页面，并根据我们编写的内容，将对应的组件添加到浏览器窗口中。 我们一般使用Chrome、Safari、Microsoft Edge等浏览器进行测试，IE浏览器已经彻底淘汰了！ 比如我们可以创建一个Html文件来看看浏览器会如何识别，使用IDEA也能编写HTML页面，我们在IDEA中新建一个Web模块，进入之后我们发现，项目中没有任何内容，我们右键新建一个HTML文件，选择HTML5文件，并命名为index，创建后出现： \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003c/body\u003e \u003c/html\u003e 我们发现，它和XML基本长得一样，并且还自带了一些标签，那么现在我们通过浏览器来浏览这个HTML文件（这里推荐使用内置预览，不然还得来回切换窗口） 我们发现现在什么东西都没有，但是在浏览器的标签位置显示了网页的名称为Title，并且显示了一个IDEA的图标作为网页图标。 现在我们稍微进行一些修改： \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003elbw的直播间\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e 现在全体起立 \u003c/body\u003e \u003c/html\u003e 再次打开浏览器，我们发现页面中出现了我们输入的文本内容，并且标题也改为了我们自定义的标题。 我们可以在设置-\u003e工具-\u003eWeb浏览器和预览中将重新加载页面规则改为变更时，这样我们使用内置浏览器或是外部浏览器，可以自动更新我们编写的内容。 我们还可以在页面中添加一个图片，随便将一张图片放到html文件的同级目录下，命名为image.xxx，其中xxx是后缀名称，不要修改，我们在body节点中添加以下内容： \u003cimg width=\"300\" src=\"image.xxx\" alt=\"剑光如我，斩尽牛杂\"\u003e \u003c!-- 注意xxx替换成对应的后缀名称 --\u003e 我们发现，我们的页面中居然能够显示我们添加的图片内容。因此，我们只需要编写对应的标签，浏览器就能够自动识别为对应的组件，并将其展示到我们的浏览器窗口中。 我们再来看看插入一个B站的视频，很简单，只需要到对应的视频下方，找到分享，我们看到有一个嵌入代码： \u003ciframe src=\"//player.bilibili.com/player.html?aid=333231998\u0026bvid=BV1rA411g7q8\u0026cid=346917516\u0026page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" width=\"800\" height=\"500\"\u003e \u003c/iframe\u003e 每一个页面都是通过这些标签来编写的，几乎所有的网站都是使用HTML编写页面。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:1:1","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"HTML语法规范 一个HTML文件中一般分为两个部分： 头部：一般包含页面的标题、页面的图标、还有页面的一些设置，也可以在这里导入css、js等内容。 主体：整个页面所有需要显示的内容全部在主体编写。 我们首先来看头部，我们之前使用的HTML文件中头部包含了这些内容： \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003elbw的直播间\u003c/title\u003e 首先meta标签用于定义页面的一些元信息，这里使用它来定义了一个字符集（编码格式），一般是UTF-8，下面的title标签就是页面的标题，会显示在浏览器的上方。我们现在来给页面设置一个图标，图标一般可以在字节跳动的IconPark网站找到：https://iconpark.oceanengine.com/home，选择一个自己喜欢的图标下载即可。 将图标放入到项目目录中，并命名为icon.png，在HTML头部添加以下内容： \u003clink rel=\"icon\" href=\"icon.png\" type=\"image/x-icon\" /\u003e link标签用于关联当前HTML页面与其他资源的关系，关系通过rel属性指定，这里使用的是icon表示这个文件是当前页面图标。 现在访问此页面，我们发现页面的图标已经变成我们指定的图标样式了。 现在我们再来看主体，我们可以在主体内部编写该页面要展示的所有内容，比如我们之前就用到了img标签来展示一个图片，其中每一个标签都称为一个元素： \u003cimg width=\"300\" src=\"image.xxx\" alt=\"当图片加载失败时，显示的文本\"\u003e 我们发现，这个标签只存在一个，并没有成对出现，HTML中有些标签是单标签，也就是说只有这一个，还有一些标签是双标签，必须成对出现，HTML中，也不允许交叉嵌套，但是出现交叉嵌套时，浏览器并不会提示错误，而是仍旧尝试去解析这些内容，甚至会帮助我们进行一定程度的修复，比如： \u003cbody\u003e \u003ciframe src=\"//player.bilibili.com/player.html?aid=333231998\u0026bvid=BV1rA411g7q8\u0026cid=346917516\u0026page=1\" width=\"800\" height=\"500\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"\u003e \u003c/body\u003e \u003c/iframe\u003e 很明显上面的代码已经出现交叉嵌套的情况了，但是依然能够在浏览器中正确地显示。 在主体中，我们一般使用div标签来分割页面： \u003cbody\u003e \u003cdiv\u003e我是第一块\u003c/div\u003e \u003cdiv\u003e我是第二块\u003c/div\u003e \u003c/body\u003e 通过使用div标签，我们将整个页面按行划分，而高度就是内部元素的高度，那么如果只希望按元素划分，也就是说元素占多大就划分多大的空间，那么我们就可以使用span标签来划分： \u003cbody\u003e \u003cdiv\u003e \u003cspan\u003e我是第一块第一个部分\u003c/span\u003e \u003cspan\u003e我是第一块第二个部分\u003c/span\u003e \u003c/div\u003e \u003cdiv\u003e我是第二块\u003c/div\u003e \u003c/body\u003e 我们也可以使用p段落标签，它一般用于文章分段： \u003cbody\u003e \u003cp\u003e 你看这个彬彬啊，才喝几罐就醉了，真的太逊了。 这个彬彬就是逊呀！ 听你这么说，你很勇哦？ 开玩笑，我超勇的，超会喝的啦。 超会喝，很勇嘛。身材不错哦，蛮结实的嘛。 \u003c/p\u003e \u003cp\u003e 哎，杰哥，你干嘛啊。都几岁了，还那么害羞！我看你，完全是不懂哦！ 懂，懂什么啊？ 你想懂？我房里有一些好康的。 好康，是新游戏哦！ 什么新游戏，比游戏还刺激！ \u003c/p\u003e \u003cp\u003e 杰哥，这是什么啊？ 哎呦，你脸红啦！来，让我看看。 不要啦！！ 让我看看嘛。 不要啦，杰哥，你干嘛啊！ 让我看看你法语正不正常啊！ \u003c/p\u003e \u003c/body\u003e 那么如果遇到特殊字符该怎么办呢？和XML一样，我们可以使用转义字符： **注意：**多个连续的空格字符只能被识别为一个，如果需要连续多个必须使用转义字符，同时也不会识别换行，换行只会变成一个空格，需要换行必须使用br标签。 通过了解了HTML的一些基础语法，我们现在就知道一个页面大致是如何编写了。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:1:2","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"HTML常用标签 前面我们已经了解了HTML的基本语法规范，那么现在我们就来看看，有哪些常用的标签吧，首先是换行和分割线： br 换行 hr 分割线 \u003cbody\u003e \u003cdiv\u003e 我是一段文字\u003cbr\u003e我是第二段文字 \u003c/div\u003e \u003chr\u003e \u003cdiv\u003e我是底部文字\u003c/div\u003e \u003c/body\u003e 标题一般用h1到h6表示，我们来看看效果： \u003cbody\u003e \u003ch1\u003e一级标题\u003c/h1\u003e \u003ch2\u003e二级标题\u003c/h2\u003e \u003ch3\u003e三级标题\u003c/h3\u003e \u003ch4\u003e四级标题\u003c/h4\u003e \u003ch5\u003e五级标题\u003c/h5\u003e \u003ch6\u003e六级标题\u003c/h6\u003e \u003cp\u003e我是正文内容，真不错。\u003c/p\u003e \u003c/body\u003e 现在我们来看看超链接，我们可以添加一个链接用于指向其他网站： \u003ca href=\"https://www.bilibili.com\"\u003e点击访问小破站\u003c/a\u003e 我们也可以指定页面上的一个锚点进行滚动： \u003cbody\u003e \u003ca href=\"#test\"\u003e跳转锚点\u003c/a\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cdiv id=\"test\"\u003e我是锚点\u003c/div\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003cimg src=\"image.jpeg\" width=\"500\"\u003e \u003c/body\u003e 每个元素都可以有一个id属性，我们只需要给元素添加一个id属性，就使用a标签可以跳转到一个指定锚点。 我们接着来看看列表元素，这是一个无需列表，其中每一个li表示一个列表项： \u003cul\u003e \u003cli\u003e一号选项\u003c/li\u003e \u003cli\u003e二号选项\u003c/li\u003e \u003cli\u003e三号选项\u003c/li\u003e \u003cli\u003e四号选项\u003c/li\u003e \u003cli\u003e五号选项\u003c/li\u003e \u003c/ul\u003e 我们也可以使用ol来显示一个有序列表： \u003col\u003e \u003cli\u003e一号选项\u003c/li\u003e \u003cli\u003e二号选项\u003c/li\u003e \u003cli\u003e三号选项\u003c/li\u003e \u003cli\u003e四号选项\u003c/li\u003e \u003cli\u003e五号选项\u003c/li\u003e \u003c/ol\u003e 表格也是很重要的一种元素，但是它编写起来相对有一点麻烦： \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e学号\u003c/th\u003e \u003cth\u003e姓名\u003c/th\u003e \u003cth\u003e性别\u003c/th\u003e \u003cth\u003e年级\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003e0001\u003c/td\u003e \u003ctd\u003e小明\u003c/td\u003e \u003ctd\u003e男\u003c/td\u003e \u003ctd\u003e2019\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e0002\u003c/td\u003e \u003ctd\u003e小红\u003c/td\u003e \u003ctd\u003e女\u003c/td\u003e \u003ctd\u003e2020\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 虽然这样生成了一个表格，但是这个表格并没有分割线，并且格式也不符合我们想要的样式，那么如何才能修改这些基础属性的样式呢，我们就需要聊聊CSS了。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:1:3","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"HTML表单 表单就像其名字一样，用户在页面中填写了对应的内容，点击按钮就可以提交到后台，比如登陆界面，就可以使用表单来实现： 一个网页中最重要的当属输入框和按钮了，那么我们来看看如何创建一个输入框和按钮： \u003clabel\u003e 我是输入框 \u003cinput type=\"text\"\u003e \u003c/label\u003e 对于一个输入框，我们一般会将其包括在一个lable标签中，它和span效果一样，但是我们点击前面文字也能快速获取输入框焦点。 \u003cbody\u003e \u003cdiv\u003e登陆我们的网站\u003c/div\u003e \u003chr\u003e \u003cdiv\u003e \u003clabel\u003e 账号： \u003cinput type=\"text\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003clabel\u003e 密码： \u003cinput type=\"password\"\u003e \u003c/label\u003e \u003c/div\u003e \u003c/body\u003e 输入框可以有很多类型，我们来试试看password，现在输入内容就不会直接展示原文了。 创建一个按钮有以下几种方式，在学习JavaWeb时，我们更推荐第二种方式，我们后面进行登陆操作需要配合表单使用： \u003cbutton\u003e登陆\u003c/button\u003e \u003cinput type=\"submit\" value=\"登陆\"\u003e \u003cinput type=\"button\" value=\"登陆\"\u003e 现在我们就可以写一个大致的登陆页面了： \u003cbody\u003e \u003ch1\u003e登陆我们的网站\u003c/h1\u003e \u003cform\u003e \u003cdiv\u003e \u003clabel\u003e 账号： \u003cinput type=\"text\" placeholder=\"Username...\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003clabel\u003e 密码： \u003cinput type=\"password\" placeholder=\"Password...\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cbr\u003e \u003ca href=\"https://www.baidu.com\"\u003e忘记密码\u003c/a\u003e \u003cbr\u003e \u003cbr\u003e \u003cdiv\u003e \u003cinput type=\"submit\" value=\"登陆\"\u003e \u003c/div\u003e \u003c/form\u003e \u003c/body\u003e 表单一般使用form标签将其囊括，但是现在我们还用不到表单提交，因此之后我们再来讲解表单的提交。 input只能实现单行文本，那么如何实现多行文本呢？ \u003clabel\u003e 这是我们的文本框\u003cbr\u003e \u003ctextarea placeholder=\"文本内容...\" cols=\"10\" rows=\"10\"\u003e\u003c/textarea\u003e \u003c/label\u003e 我们还可以指定默认的行数和列数，拖动左下角可以自定义文本框的大小。 我们还可以在页面中添加勾选框： \u003clabel\u003e \u003cinput type=\"checkbox\"\u003e 我同意本网站的隐私政策 \u003c/label\u003e 上面演示的是一个多选框，那么我们来看看单选框： \u003clabel\u003e \u003cinput type=\"radio\" name=\"role\"\u003e 学生 \u003c/label\u003e \u003clabel\u003e \u003cinput type=\"radio\" name=\"role\"\u003e 教师 \u003c/label\u003e 这里需要使用name属性进行分组，同一个组内的选项只能选择一个。 我们也可以添加列表让用户进行选择，创建一个下拉列表： \u003clabel\u003e 登陆身份： \u003cselect\u003e \u003coption\u003e学生\u003c/option\u003e \u003coption\u003e教师\u003c/option\u003e \u003c/select\u003e \u003c/label\u003e 默认选取的是第一个选项，我们可以通过selected属性来决定默认使用的是哪个选项。 当然，HTML的元素远不止我们所提到的这些，有关更多HTML元素的内容，可以自行了解。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:1:4","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"CSS样式 之前我们编写的页面非常基础，我们只能通过一些很基本的属性来排列我们的页面元素，那么如何实现更高度的自定义呢，我们就需要用到CSS来自定义样式，首先我们创建一个名为style.css的文件。 首先在我们HTML文件的头部添加： \u003clink href=\"style.css\" rel=\"stylesheet\"\u003e 我们在CSS文件中添加以下内容： body { text-align: center; } 我们发现，网页的内容全部变为居中显示了，这正是css在生效，相当于我们现在给页面添加了自定义的样式规则。 当然，我们也可以选择不使用CSS，而是直接对某个元素添加样式： \u003cbody style=\"text-align: center;\"\u003e ... 这样的效果其实是等同于上面的css文件的，相当于我们直接把样式定义在指定元素上。 也可以在头部直接定义样式，而不是使用外部文件： \u003cstyle\u003e body { text-align: center; } \u003c/style\u003e 使用以上三种方式都可以自定义页面的样式，我们推荐使用还是第一种，不然我们的代码会很繁杂。 样式的属性是非常多的，我们不可能一个一个全部讲完，视频中用到什么再来讲解什么，如果同学们感兴趣，可以自行下去了解。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:2:0","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"CSS选择器 我们首先来了解一下选择器，那么什么是选择器呢？我们想要自定义一个元素的样式，那么我们肯定要去选择某个元素，只有先找到要自定义的元素，我们才能开始编写样式。 我们上面的例子中使用的就是标签名选择器，它可以快速选择页面中所有指定的的标签，比如我们之前使用的就是body标签，那么就相当于页面中所有的body元素全都使用此样式，那么我们现在来试试看选择页面中所有的input标签： input { width: 200px; } 我们发现，页面中所有的input元素宽度全部被设定为了200个像素（px是单位大小，代表像素，除了px还有em和rem，他们是根据当前元素字体大小决定的相对大小，一般用于适配各种大小的浏览器窗口，这里暂时不用） 样式编写完成后，如果只有一个属性，可以不带;若多个属性则每个属性后面都需要添加一个; 因此，一个标签选择器的格式为： 标签名称 { 属性名称: 属性值 } 我们还可以设定输入框的字体大小、行高等： input { width: 200px; font-size: 20px; line-height: 40px; } 我们现在可以通过选择器快速地去设置某个元素样式了，那么如何实现只设置某个元素的样式呢，现在我们来看看，id选择器，我们之前已经讲解过了，每个元素都可以有一个id属性，我们可以将其当做一个跳转的锚点使用，而现在，我们可以使用css来进行定位： 我们先为元素添加id属性： \u003ch1 id=\"title\"\u003e登陆我们的网站\u003c/h1\u003e 现在使用CSS选择我们的元素，并设定一个属性，选择某个id需要在前面加上一个#： #title { color: red; } 虽然id选择器已经可以很方便的指定某个元素，但是如果我们希望n个但不是元素都被选择，id选择器就无法实现了，因为每个元素的id是唯一的，不允许出现重复id的元素，因此接着我们来讲解一下类选择器。 每个元素都可以有一个class属性，表示当前元素属于某个类（注意这里的类和我们Java中的类概念完全不同）一个元素可以属于很多个类，一个类也可以被很多个元素使用： \u003cform\u003e \u003cdiv \u003e \u003clabel class=\"test\"\u003e 账号： \u003cinput type=\"text\" placeholder=\"Username...\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003clabel class=\"test\"\u003e 密码： \u003cinput type=\"password\" placeholder=\"Password...\"\u003e \u003c/label\u003e \u003c/div\u003e \u003c/form\u003e 上面的例子中，两个label元素都使用了test类（类名称是我们自定义的），现在我们在css文件中编写以下内容来以类进行选择： .test{ color: blue; } 我们发现，两个标签的文本内容都变为了蓝色，因此使用类选择器，能够对所有为此类的元素添加样式。注意在进行类选择时，我们需要在类名前面加上.来表示。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:2:1","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"组合选择器和优先级问题 我们也可以让多个选择器，共用一个css样式： .test, #title { color: red; } 只需要并排写即可，注意中间需要添加一个英文的逗号用于分割，我们也可以使用*来一次性选择所有的元素： * { color: red; } 我们还可以选择位于某个元素内的某个元素： div label { color: red; } 这样的话，就会选择所有位于div元素中的label元素。 当然，我们这里只介绍了一些常用的选择器，有关详细的CSS选择器可以查阅：https://www.runoob.com/cssref/css-selectors.html 我们接着来看一下选择器的优先级： 我们根据上面的信息，来测试一下，首先编写一下HTML文件： \u003cbody\u003e \u003cdiv class=\"test\" id=\"simple\" style=\"color: blue\"\u003e我是测试文本内容\u003c/div\u003e \u003c/body\u003e 现在我们来编写一下css文件： .test { color: yellow; } #simple { color: red; } * { color: palegreen; } 那么现在我们可以看到，实际上生效的是我们直接编写在标签内部的内联属性，那么现在我们依次进行移除，来看看它们的优先级。 那么如果我们希望某个属性无视任何的优先级，我们可以在属性后面添加!important标记，表示此属性是一个重要属性，它的优先级会被置为最高。 **思考：**那要是我每个选择器的这个属性后面都加一个!important会怎么样？ ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:2:2","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"自定义边距 我们来看看，如何使用css控制一个div板块的样式，首先编写以下代码，相当于一个div嵌套了一个div元素： \u003cdiv id=\"outer\"\u003e \u003cdiv id=\"inner\"\u003e \u003c/div\u003e \u003c/div\u003e 现在编写一下自定义的css样式，我们将div设定为固定大小，并且背景颜色添加为绿色： #outer { background: palegreen; width: 300px; height: 300px; } 我们发现左侧快速预览页面存在空隙，这是因为浏览器给我们添加了一个边距属性，我们只需要覆盖此属性并将其设定为0即可： body { margin: 0; } 现在我们给内部嵌套的div也设定一个大小，并将颜色设定为橙色： #inner { background: darkorange; width: 100px; height: 100px; } 现在我们发现内部的div元素位于右上角，我们还可以以百分比的形式来指定大小： #inner { background: darkorange; width: 100%; height: 100%; } 百分比会依照当前可用大小来进行分配，比如当前位于一个div内部，并且外部div元素是固定大小300px，因此100%就相当于使用了外部的全部大小，也是300px，现在内部元素完全将外部元素覆盖了，整个元素现在呈现为橙色。 我们可以为一个元素设定边距，边距分为外边距和内边距，外部元素内边距决定了内部元素与外部元素之间的间隔，我们来修改一下css样式： #outer { background: palegreen; width: 300px; height: 300px; padding: 10px; } 我们发现，内部的div元素小了一圈，这是因为外部div元素设定了内边距，上下左右都被设定为10px大小。 而我们发现，实际上我们在一开始也是将body的外边距设定为了0，整个页面跟浏览器窗口直接间隔0px的宽度。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:2:3","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"编写一个漂亮的登陆界面 现在我们就来尝试编写一个漂亮的登陆界面吧！ ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:2:4","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript语言 也称为js，是我们整个前端基础的重点内容，只有了解了JavaScript语言，我们才能了解前端如何与后端交互。 JavaScript与Java没有毛关系，仅仅只是名字中包含了Java而已，跟Java比起来，它更像Python，它是一门解释型语言，不需要进行编译，它甚至可以直接在浏览器的命令窗口中运行。 它相当于是前端静态页面的一个补充，它可以让一个普通的页面在后台执行一些程序，比如我们点击一个按钮，我们可能希望执行某些操作，比如下载文件、页面跳转、页面弹窗、进行登陆等，都可以使用JavaScript来帮助我们实现。 我们来看看一个简单的JavaScript程序： const arr = [0, 2, 1, 5, 9, 3, 4, 6, 7, 8] for (let i = 0; i \u003c arr.length; i++) { for (let j = 0; j \u003c arr.length - 1; j++) { if(arr[j] \u003e arr[j+1]){ const tmp = arr[j] arr[j] = arr[j+1] arr[j+1] = tmp } } } window.alert(arr) 这段代码实际上就是实现了一个冒泡排序算法，我们可以直接在页面的头部中引用此js文件，浏览器会在加载时自动执行js文件中编写的内容： \u003cscript src=\"test.js\"\u003e\u003c/script\u003e 我们发现JS的语法和Java非常相似，但是它还是和Java存在一些不同之处，而且存在很多阴间语法，那么我们来看看JS的语法。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:0","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript基本语法 在js中，定义变量和Java中有一些不同，定义一个变量可以使用let关键字或是var关键字，IDEA推荐我们使用let关键字，因为var存在一定的设计缺陷（这里就不做讲解了，之后一律使用let关键字进行变量声明）： let a = 10; a++; window.alert(a) 上面的结果中，我们得到了a的结果是11，也就是说自增和自减运算在JS中也是支持的，并且JS每一句结尾可以不用加分号。 js并不是Java那样的强类型语言（任意变量的类型一定是明确的），它是一门弱类型语言，变量的类型并不会在一开始确定，因此我们在定义变量时无需指定变量的确切类型，而是在运行时动态解析类型： let a = 10; a = \"HelloWorld！\" console.info(a) 我们发现，变量a已经被赋值为数字类型，但是我们依然在后续能将其赋值一个字符串，它的类型是随时可变的。 很多人说，这种变态的类型机制是JS的一大缺陷。 世界上只有两种语言：一种是很多人骂的，一种是没人用的。 我们接着来看看，JS中存在的基本数据类型： Number：数字类型（包括小数和整数） String：字符串类型（可以使用单引号或是双引号） Boolean：布尔类型（与Java一致） 还包括一些特殊值： undefined：未定义 - 变量声明但不赋值默认为undefined null：空值 - 等同于Java中的null NaN：非数字 - 值不是合法数字，比如： window.alert(100/'xx') 我们可以使用typeof关键字来查看当前变量值的类型： let a = 10; console.info(typeof a) a = 'Hello World' console.info(typeof a) ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:1","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript逻辑运算和流程控制 我们接着来看看js中的关系运算符，包括如下8个关系运算符：大于（\u003e）,小于（\u003c）,小于等于（\u003c=）,大于等于（\u003e=）,相等（==），不等（!=），全等（===），不全等（!==） 其实关系运算符大致和Java中的使用方法一致，不过它还可以进行字符串比较，有点像C++的语法： console.info(666 \u003e 777) console.info('aa' \u003e 'ab') 那么，相等和全等有什么区别呢？ console.info('10' == 10) console.info('10' === 10) 我们发现，在Java中，若运算符两边是不同的基本数据类型，会直接得到false，而JS中却不像这样，我们发现字符串的10居然等于数字10，而使用全等判断才是我们希望的结果。 ==的比较规则是：当操作数类型一样时，比较的规则和恒等运算符一样，都相等才相等，如果两个操作数是字符串，则进行字符串的比较，如果里面有一个操作数不是字符串，那两个操作数通过Number()方法进行转换，转成数字进行比较。 因此，我们上面进行的判断实际上是运算符两边都进行了数字转换的结果进行比较，自然也就得到了true，而全等判断才是我们在Java中认识的相等判断。 我们接着来看逻辑运算，JS中包括\u0026\u0026、||、\u0026、|、?:等，我们先来看看位运算符： console.info(4 \u0026 7) console.info(4 | 7) 实际上和Java中是一样的，那么我再来看看逻辑运算： console.info(true || false) 对于boolean变量的判断，是与Java一致的，但是JS也可以使用非Boolen类型变量进行判断： console.info(!0) console.info(!1) 和C/C++语言一样，0代表false，非0代表true，那么字符串呢？ console.info(!\"a\") console.info(!\"\") 我们发现，空串为false，非空串为true，我们再来看看： console.info(true || 7) console.info(7 || true) 我们发现，前者得到的结果为true，而后者得到的结果却是是7，真是滑天下之大稽，什么鬼玩意，实际上是因为，默认非0都是true，而后者又是先判断的7，因此会直接得到7而不是被转换为true 那么我们再来看看几个特殊值默认代表什么： console.info(!undefined) console.info(!null) console.info(!NaN) 最后来使用一下三元运算符，实际上和Java中是一样的： let a = true ? \"xx\" : 20 console.info(a) 得益于JS的动态类型，emmm，三元运算符不一定需要固定的返回值类型。 JS的分支结构，实际上和Java是一样的，也是使用if-else语句来进行： if(\"lbwnb\"){ //非空串为true console.info(\"!!!\") } else { console.info(\"???\") } 同理，多分支语句也能实现： if(\"\"){ console.info(\"!!!\") } else if(-666){ console.info(\"???\") } else { console.info(\"O.O\") } 当然，多分支语句也可以使用switch来完成： let a = \"a\" switch (a){ case \"a\": console.info(\"1\") break case \"b\": console.info(\"2\") break case \"c\": console.info(\"3\") break default: console.info(\"4\") } 接着我们来看看循环结构，其实循环结构也和Java相差不大： let i = 10 while(i--){ console.info(\"100\") } for (let i = 0; i \u003c 10; i++) { console.info(\"??\") } ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:2","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript函数定义 JS中的方法和Java中的方法定义不太一样，JS中一般称其为函数，我们来看看定义一个函数的格式是什么： function f() { console.info(\"有一个人前来买瓜\") } 定义一个函数，需要在前面加上function关键字表示这是一个函数，后面跟上函数名称和()，其中可以包含参数，在{}中编写函数代码。我们只需要直接使用函数名+()就能调用函数： f(); 我们接着来看一下，如何给函数添加形式参数以及返回值： function f(a) { console.info(\"得到的实参为：\"+a) return 666 } f(\"aa\"); 由于JS是动态类型，因此我们不必指明参数a的类型，同时也不必指明返回值的类型，一个函数可能返回不同类型的结果，因此直接编写return语句即可。同理，我们可以在调用函数时，不传参，那么默认会使用undefined： function f(a) { console.info(\"得到的实参为：\"+a) return 666 } f(); 那么如果我们希望不传参的时候使用我们自定义的默认值呢？ function f(a = \"6666\") { console.info(\"得到的实参为：\"+a) return 666 } f(); 我们可以直接在形参后面指定默认值。 函数本身也是一种类型，他可以被变量接收，所有函数类型的变量，也可以直接被调用： function f(a = \"6666\") { console.info(\"得到的实参为：\"+a) return 666 } let k = f; k(); 我们也可以直接将匿名函数赋值给变量： let f = function (str) { console.info(\"实参为：\"+str) } 既然函数是一种类型，那么函数也能作为一个参数进行传递： function f(test) { test(); } f(function () { console.info(\"这是一个匿名函数\") }) 对于所有的匿名函数，可以像Java的匿名接口实现一样编写lambda表达式： function f(test) { test(); } f(() =\u003e { console.info(\"可以，不跟你多bb\") }) function f(test) { test(\"这个是回调参数\"); } f(param =\u003e { console.info(\"接受到回调参数：\"+param) }) ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:3","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript数组和对象 JS中的数组定义与Java不同，它更像是Python中的列表，数组中的每个元素并不需要时同样的类型： let arr = [1, \"lbwnb\", false, undefined, NaN] 我们可以直接使用下标来访问： let arr = [1, \"lbwnb\", false, undefined, NaN] console.info(arr[1]) 我们一开始编写的排序算法，也是使用了数组。 数组还可以动态扩容，如果我们尝试访问超出数组长度的元素，并不会出现错误，而是得到undefined，同样的，我们也可以直接往超出数组长度的地方设置元素： let arr = [1, \"lbwnb\", false, undefined, NaN] arr[5] = \"???\" console.info(arr) 也可以使用push和pop来实现栈操作： let arr = [1, \"lbwnb\", false, undefined, NaN] arr.push(\"bbb\") console.info(arr.pop()) console.info(arr) 数组还包括一些其他的方法，这里就不一一列出了： let arr = [1, \"lbwnb\", false, undefined, NaN] arr.fill(1) console.info(arr.map(o =\u003e { return 'xxx'+o })) 我们接着来看对象，JS中也能定义对象，但是这里的对象有点颠覆我们的认知： let obj = new Object() let obj = {} 以上两种写法都能够创建一个对象，但是更推荐使用下面的一种。 JS中的对象也是非常随意的，我们可以动态为其添加属性： let obj = {} obj.name = \"伞兵一号\" console.info(obj) 同理，我们也可以给对象动态添加一个函数： let obj = {} obj.f = function (){ console.info(\"我是对象内部的函数\") } obj.f() 我们可以在函数内使用this关键字来指定对象内的属性： let name = \"我是外部变量\" let obj = {} obj.name = \"我是内部变量\" obj.f = function (){ console.info(\"name属性为：\"+this.name) } obj.f() **注意：**如果使用lambda表达式，那么this并不会指向对象。 除了动态添加属性，我们也可以在一开始的时候指定对象内部的成员： let obj = { name: \"我是内部的变量\", f: function (){ console.info(\"name属性为：\"+this.name) } } obj.f() 注意如果有多行属性，需要在属性定义后添加一个,进行分割！ ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:4","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"JavaScript事件 当我们点击一个页面中的按钮之后，我们希望之后能够进行登陆操作，或是执行一些JS代码来实现某些功能，那么这个时候，就需要用到事件。 事件相当于一个通知，我们可以提前设定好事件发生时需要执行的内容，当事件发生时，就会执行我们预先设定好的JS代码。 事件有很多种类型，其中常用的有： onclick：点击事件 oninput：内容输入事件 onsubmit：内容提交事件 那么如何为事件添加一个动作呢？ \u003cinput type=\"password\" oninput=\"console.info('正在输入文本')\"\u003e 我们可以直接为一个元素添加对应事件的属性，比如oninput事件，我们可以直接在事件的值中编写js代码，但是注意，只能使用单引号，因为双引号用于囊括整个值。 我们也可以单独编写一个函数，当事件发生时直接调用我们的函数： function f() { window.alert(\"你输入了一个字符\") } \u003cinput type=\"password\" oninput=\"oninput()\"\u003e 仅仅了解了事件，还不足以实现高度自定义，我们接着来看DOM。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:5","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"Document对象 当网页被加载时，浏览器会创建页面的文档对象模型（Document Object Model），它将整个页面的所有元素全部映射为JS对象，这样我们就可以在JS中操纵页面中的元素。 比如我现在想要读取页面中某个输入框中的内容，那么我们就需要从DOM中获取此输入框元素的对象： document.getElementById(\"pwd\").value 通过document对象就能够快速获取当前页面中对应的元素，并且我们也可以快速获取元素中的一些属性。 比如现在我们可以结合事件，来进行密码长度的校验，密码长度小于6则不合法，不合法的密码，会让密码框边框变红，那么首先我们先来编写一个css样式： .illegal-pwd{ border: red 1px solid !important; box-shadow: 0 0 5px red; } 接着我们来编写一下js代码，定义一个函数，此函数接受一个参数（元素本身的对象）检测输入的长度是否大于6，否则就将当前元素的class属性设定为css指定的class： function checkIllegal(e) { if(e.value.length \u003c 6) { e.setAttribute(\"class\", \"illegal-pwd\") }else { e.removeAttribute(\"class\") } } 最后我们将此函数绑定到oninput事件即可，注意传入了一个this，这里的this代表的是输入框元素本身： \u003cinput id=\"pwd\" oninput=\"checkIllegal(this)\" type=\"password\"\u003e 现在我们在输入的时候，会自动检查密码是否合法。 既然oninput本身也是一个属性，那么实际上我们可以动态进行修改： document.getElementById(\"pwd\").oninput = () =\u003e console.info(\"???\") 那么，我们前面提及的window对象又是什么东西呢？ 实际上Window对象范围更加广阔，它甚至直接代表了整个窗口，当然也包含我们的Document对象，我们一般通过Window对象来弹出提示框之类的东西。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:6","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"发送XHR请求 JS的大致内容我们已经全部学习完成了，那么如何使用JS与后端进行交互呢？ 我们知道，如果我们需要提交表单，那么我们就需要将表单的信息全部发送给我们的服务器，那么，如何发送给服务器呢？ 通过使用XMLHttpRequest对象，来向服务器发送一个HTTP请求，下面是一个最简单的请求格式： let xhr = new XMLHttpRequest(); xhr.open('GET', 'https://www.baidu.com'); xhr.send(); 上面的例子中，我们向服务器发起了一次网络请求，但是我们请求的是百度的服务器，并且此请求的方法为GET请求。 我们现在将其绑定到一个按钮上作为事件触发： function http() { let xhr = new XMLHttpRequest(); xhr.open('GET', 'https://www.baidu.com'); xhr.send(); } \u003cinput id=\"button\" type=\"button\" onclick=\"http()\"\u003e 我们可以在网络中查看我们发起的HTTP请求并且查看请求的响应结果，比如上面的请求，会返回百度这个页面的全部HTML代码。 实际上，我们的浏览器在我们输入网址后，也会向对应网站的服务器发起一次HTTP的GET请求。 在浏览器得到页面响应后，会加载当前页面，如果当前页面还引用了其他资源文件，那么会继续向服务器发起请求，直到页面中所有的资源文件全部加载完成后，才会停止。 ","date":"2022-05-13","objectID":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/:3:7","tags":["前端基础"],"title":"前端基础","uri":"/posts/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80/"},{"categories":["青训营笔记"],"content":"Go语言编码规范和性能调优","date":"2022-05-11","objectID":"/posts/glang3/","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"编码规范 ","date":"2022-05-11","objectID":"/posts/glang3/:1:0","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"注释 Good code has lots of comments,bad code requires lots of comments. 不必要的注释 场景一 如上图所示，第一个Open函数应该解释代码作用，而第二个函数这样的作用解释则毫无必要，因为它的函数名就已经解释了。 场景二 第一个函数的逻辑较为复杂，很多情况没法看懂，需要注释，而第二个则完全没必要。 需要的注释 公共符号始终要注释 这里的公共符号包括全局可见的函数和变量，而方法则不包含在内。 小结 代码是最好的注释。 注释应提供代码未表达出的上下文信息。 变量命名 简洁 缩略词都大写，比如HTTP不要Http 变量定义的位置距离使用的地方越远，命名需要越详细，特别是全局变量，有时需要注释 函数内 例如for循环时： for i:=0; i\u003csize;i++{ //good ... } for index:=0;index\u003csize;index++{//bad ... } 而作为函数参数时： //good func (c *Client) send(req *Request, deadline time.Time) //bad func (c *Client) send(req *Request, t time.Time) 由于第一个变量仅在for循环这个作用域内，它的作用也很清晰，如果命名的更详细，反而会影响阅读。 第二个变量作为函数的参数，作用域很大，且需要作为提供给使用者的名字，这个需要带有详细信息描述。 包内 函数名应该不携带报名的上下文信息。 例如在http包中有个Serve方法和ServeHTTP方法这两个命名，我们应该选择Serve去命名而不是ServeHTTP。因为我们使用的时候会携带包名。类比于C++的命名空间，Java的包名。 包命名 只有小写字母组成（不包含下划线等字母 简短并包含一定的信息 不要和标准库的包冲突，标准库的很多包名喜欢用复数，我们应该避免使用复数形式为包名，比如strings是标准库的一个包名。 ","date":"2022-05-11","objectID":"/posts/glang3/:1:1","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"控制流程 这里我用之前在其他地方学到的两个优化代码的方式来讲。 嵌套条件校验链 嵌套两层以上的if if a\u003e10 { if b\u003e10 { if c\u003e10 { ... } } } 优化如下： for { if !(a\u003e10) { break } if !(b\u003e10) { break } if !(c\u003e10) { break } ... } 互斥条件表驱动 比如有以下并列的if嵌套逻辑： func CalculateByCmd(cmd string,a,b int)(int,error){ if strings.EqualFold(cmd,\"add\"){ return a+b,nil } if strings.EqualFold(cmd,\"sub\"){ return a-b,nil } if strings.EqualFold(cmd,\"mul\"){ return a*b,nil } return 0,errors.New(\"cmd not exist\") } 这段代码是根据给出的字符串命令，得出对应的计算结果。但是我们发现，这个代码的可读性虽然还行，但由于最终的计算和这个函数的耦合性太强，实现功能拓展有点拖后腿。 我们通过表驱动做出以下优化： var mapCalculate = map[string]func(a,b int) int{ \"add\": func(a, b int) int { return a+b }, \"sub\": func(a, b int) int { return a-b }, \"mul\": func(a, b int) int { return a*b }, } func CalculateByCmd(cmd string,a,b int)(int,error){ if v,ok := mapCalculate[cmd];ok{ return v(a,b),nil } return 0,errors.New(\"cmd not exist\") } ","date":"2022-05-11","objectID":"/posts/glang3/:1:2","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"错误处理 error相关的函数 errors.New() return errors.New(\"需要的错误信息描述\") errors.Is() 用于判断错误断言，不同于简单的==，它能够判断错误链中是否包含它。 errors.As() 从错误链中提取想要的错误。 panic和recover和defer 这几样东西，语法就那样，真要理解原理可以看看下面这些视频链接。 老版本go derfer实现 新版本go defer实现 panic和recover ","date":"2022-05-11","objectID":"/posts/glang3/:1:3","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"性能调优 ","date":"2022-05-11","objectID":"/posts/glang3/:2:0","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"benchmark测试 这个benchmark，之前在（二）里面讲了如何去使用，这里直接贴图看如何看懂测试结果。 ","date":"2022-05-11","objectID":"/posts/glang3/:2:1","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"slice的阴暗面 对于之前使用C++的同学，这里slice的预分配应该不用多讲。主要就是学会避坑。 最大坑点 更新slice后持有的底层数组相同。 具体而言：有时我们只是想要底层数组的一小部分，结果因为简单切个片，然后就和他共用了同一片底层数组。go的垃圾回收机制在某种程度上和C++智能指针(引用计数)很相似，如果此时大的数组实际上已经没用了，而有用的只有小数组，而它们是共用同一片底层数组，此时这整个底层数组的空间会得不到释放，因为引用计数不为零！算是意外延长了生命周期。 更新slice后持有的底层数组不同。 具体而言：如果一个map映射的值是slice类型，那么我们每次更新这个slice里的元素时，我们还得更新map映射的这个slice，这是为了防止底层数组发生了变化。所以一般slice作为值最好是使用slice指针。 ","date":"2022-05-11","objectID":"/posts/glang3/:2:2","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"关于string可变与不可变的优化 不可变string的优化 go语言和Java等等语言都把string设置为了不可变，我觉得这样设置是非常合理的，毕竟字符串在使用过程中，多数情况下都是传递，而且可以利用不可变做很多优化，比如内存池之类的。 string不可变的缺陷 一旦string不可变，就意味着，每次得到一个新的字符串就需要申请一片新的内存，那么这样的话，多次字符串拼接的过程中将会有严重的性能损失！ 如下，每次 += 右边的值都会引起内存的分配。 s := \"aa\" for i:=0;i\u003csize;i++{ s += \"ccc\" } 解决方案 使用strings.Builder。 var s strings.Builder for i:=0;i\u003csize;i++{ s.WriteString(\"cc\") } s.String() 使用bytes.Builder。 var s bytes.Buffer for i:=0;i\u003c10;i++{ s.WriteString(\"cc\") } s.String() 第一种解决方案比第二种要快。 因为在最后的String()阶段bytes包的处理方式是再进行一次切片处理。 而strings包则是直接指针强转。 可变string的优化 可变string的代表便是C++。 C++的string是可变的，而且因为实现了重载=号实现的深拷贝，导致很多情况下string的使用是需要进行拷贝的，但为了解决这个问题C++可以使用 const\u0026 来引用字符串以防止拷贝，但这就有一个问题：由于string是可变的万一我在使用的过程中string被外界改变了怎么办？这是可变string需要面临的最大问题，所以可变string一般都是会实现深拷贝，但使用起来大多数人还是会通过引用传递，毕竟拷贝太耗时了！但这时使用起来就得遵守编码规范唯唯诺诺了。 回到正题，即便是使用了 const\u0026 进行字符串的传递，从实践来看，它至少有以下几方面问题： 字符串字面值、字符数组、字符串指针的传递仍要数据拷贝 这三类低级数据类型与string类型不同，传入时，编译器需要做隐式转换，即需要拷贝这些数据生成string临时对象。const string\u0026指向的实际上是这个临时对象。通常字符串字面值较小，性能损耗可以忽略不计；但字符串指针和字符数组某些情况下可能会比较大（比如读取文件的内容），此时会引起频繁的内存分配和数据拷贝，会严重影响程序的性能。 substr O(n)复杂度 这是一个特别常用的函数，好在std::string提供了这个函数，美中不足的是其每次都返回一个新生成的子串，很容易引起性能热点。实际上我们本意并不是要改变原字符串，为什么不在原字符串基础上返回呢？ 说说可变字符串类型的好处，内存的开辟问题不会太多，因为虽然在拷贝，但基本上都是复用的同一片内存。 解决方案 方案一：SSO优化 SSO，全称为：小字符串优化。 这个优化简单粗暴，就是根据字符串的长度来决定内存的开辟情况。 比如字符串长度如果小于128字节，那么内存就开辟在栈上面，众所周知，栈内存开辟比堆内存开辟的代价小很多！ 方案二：string_view（C++17引入） 通过提供一个新的类型，这个类型和不可变的字符串的类型类似，它是不可变的，只能看，不然怎么叫view🤭 每次string赋值给它，代价都很小，不是直接拷贝字符串，而是指针的赋值而已。 而且string_view重写了substr，这个方法返回string_view而且你会发现通过它再构建string性能会比string调用substr快很多。 string_view虽然解决了拷贝问题，但是依旧没有解决C++的内存安全问题，string_view内部是原始指针，不会意外延长生命周期，所以要非常注意它所观察的字符串内存是否被释放了，如果被释放string_view将失效，将会产生严重的内存安全问题。 关于string_view，可以看看我的这篇博客string_view。 ","date":"2022-05-11","objectID":"/posts/glang3/:2:3","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"空结构体的使用 空结构体，不占内存，仅作为占位符，所以可以作为map实现set的理想工具。 ","date":"2022-05-11","objectID":"/posts/glang3/:2:4","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"atomic包 用atomic保护变量的并发安全，用sync.Mutex保护一段代码逻辑的并发安全。 对于非数值变量，可以使用atomic.Value来承载一个空接口。 ","date":"2022-05-11","objectID":"/posts/glang3/:2:5","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"性能调优实战 ","date":"2022-05-11","objectID":"/posts/glang3/:3:0","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"pprof工具的使用 这个东西暂时用不来，感觉暂时这个阶段也很难用上。用上了再研究（主要是感觉很多东西都还完全不会分析，只会工具操作很麻木的感觉 pprof ","date":"2022-05-11","objectID":"/posts/glang3/:3:1","tags":["Go语言编码规范和性能调优"],"title":"Go语言编码规范和性能调优","uri":"/posts/glang3/"},{"categories":["青训营笔记"],"content":"这是我参与「第三届青训营 -后端场」笔记创作活动的的第二篇笔记。","date":"2022-05-08","objectID":"/posts/golang-note2/","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"这是我参与「第三届青训营 -后端场」笔记创作活动的的第二篇笔记。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:0:0","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"并发和Goroutine ","date":"2022-05-08","objectID":"/posts/golang-note2/:1:0","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"并发和并行的区别 并发可能更多的是精确到语言的逻辑，也就是直接的多线程，或者多进程。 而并行则是一种表述程序运行的方式，就如同异步和同步的描述。 并发程序不一定是并行的，这个看操作系统的调度。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:1:1","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"线程与协程的区别 线程：是比进程更小粒度的运行单位，存在于内核态，需要操作系统来调度，内存消耗是MB级别。 协程：是比线程更小的粒度，通过m:n的比例在一个线程中再细分出来的单位，存在于用户态，用户可以自由调度，内存消耗是KB级别。 协程对比线程的优势： 存在于用户态，可操作性强，调度可由自己控制。 更轻量，所需资源更少。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:1:2","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"Goroutine go语言的go关键字跑的就是协程，我们称为goroutine。 关于协程背后更多的故事，可以看这个视频 go协程实现原理 ，我们这里只讲简单使用。 用法 简单用法如下： package main import ( \"fmt\" \"sync\" ) func hello(i int) { println(\"hello world : \" + fmt.Sprint(i)) } func main() { //go的风格来说一般都喜欢运行一个闭包 go func(j int) { hello(j) }(i) } 并发的通信 并发程序之间的通信，一般都是通过共享内存的形式实现通信，临界区一般需要加锁保护。 而go语言采取的是通过通信来实现共享内存，这个过程是反过来的，但用起来更为直观。 Channel 通过内置函数 make 可以得到两种类型的 channel 。 注意：channel是类似于引用的一个类型，如果直接通过var声明定义是没法初始化得到内部内存的，故记得通过make创建channel。还有就是记得不用的时候关闭。 channel的使用 channel的简单使用如下： func main() { var src chan int src = make(chan int)//不带缓冲 dest := make(chan int, 3)//带缓冲 go func() { defer close(src) for i := 0; i \u003c 10; i++ { src \u003c- i//生产 } }() go func() { defer close(dest) for i := range src {//消费者1 dest \u003c- i * i } }() for i := range dest {//消费者2 println(i) } } 使用带缓冲channel的好处 在一个生产者消费者模型中，生产者的生产效率远高于消费者，那么可以使用带缓冲的channel，防止生产者因为等待消费者消费过程而产生阻塞。反之对消费者来说也是受用的。 并发安全 互斥锁 go语言并没有对加锁机制的弃用，标准库里面仍然有sync.Mutex。 以下为简单加锁实现并发安全： package main import ( \"fmt\" \"sync\" \"time\" ) var( x int mut sync.Mutex ) func AddWithLock() { mut.Lock() for i:=0;i\u003c2000;i++ { x++ } mut.Unlock() } func AddWithoutLock() { for i:=0;i\u003c2000;i++ { x++ } } func main() { //开五个协程的锁版本，再打印最终结果 for i := 0; i \u003c 5; i++ { go AddWithoutLock() } //等待上面的协程执行结束 time.Sleep(time.Second) fmt.Println(x) //有锁版本 x = 0 for i:=0;i\u003c5;i++{ go AddWithLock() } time.Sleep(time.Second) fmt.Println(x) } 计数器 WaitGroup，通过Add(a)计时器+a，通过Done()计数器-1，通过Wait()阻塞直到计数器为0。这个东西我觉得有些类似于操作系统的信号量。 以下为实例： package main import ( \"fmt\" \"sync\" ) func hello(){ fmt.Println(\"hello\") } func main() { var wg sync.WaitGroup wg.Add(5) for i := 0; i \u003c 5; i++ { go func() { defer wg.Done() hello() }() } wg.Wait() } ","date":"2022-05-08","objectID":"/posts/golang-note2/:1:3","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"依赖管理 Go依赖管理的演进： graph LR\ra[GOPATH]\rb[Go Vendor]\rc[Go Module]\ra--\u003eb--\u003ec\r","date":"2022-05-08","objectID":"/posts/golang-note2/:2:0","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"GOPATH go语言有一个内置的全局环境变量GOPATH，指定了GOPATH文件夹后，他会在这个文件夹内创建以下三个文件夹： |——bin：项目编译的二进制文件 |——pkg：项目编译的中间产物，加速编译 |——src：项目源码 项目直接依赖src下的代码，go get命令下载的软件包都会在src目录下。 GOPATH弊端 当我们对某个依赖进行升级后，则项目A依赖的版本可能无法实现兼容，这就是GOPATH无法解决的多版本控制问题。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:2:1","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"Go Vendor 为了解决多版本控制问题，go又增加了Go Vendor的方式来管理依赖。 使用govendor init 在项目根目录会生成vendor文件夹，其中存放了当前项目依赖的副本。在Vendor机制下，如果当前项目存在Vendor目录，会优先使用该目录下的依赖，如果依赖不存在，会从GOPATH中寻找；这样解决了更新GOPATH依赖源码后之前的版本不兼容的问题。 Go Vendor弊端 弊端很明显，无法解决依赖的依赖。 同样还是无法解决依赖的冲突。 归根到底vendor不能很清晰的标识依赖的版本概念。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:2:2","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"Go Module （最终解决方案 特点： 通过 go.mod 管理依赖包版本。 通过 go get/mod 工具管理依赖包。 最终目标：定义版本规则和管理项目的依赖关系。 依赖管理三要素 配置文件，描述依赖 （对应go.mod） 中心仓库管理依赖库 （GoProxy） 本地工具 go get/mod 配置文件 每个依赖单元用模块路径+版本来唯一标示。 版本规则 gopath和govendor都是源码副本方式依赖，没有版本规则概念，而gomod为了放方便管理则定义了版本规则。 对于语义化版本有如下规则： MAJOR：表示是不兼容的 API，所以即使是同一个库，MAJOR 版本不同也会被认为是不同的模块。 MINOR：通常是新增函数或功能，向后（向下）兼容。 PATCH：修复 bug。 杂项 版本号后面添加 //indirect 表示间接依赖。 选择题 选择1.4，因为它向后兼容。 中心仓库管理依赖库 依赖的分发 如果直接向代码托管平台进行依赖的请求，很快会发现有以下这些问题： 无法保证构建的稳定性（可能代码仓库的所有者更改删除了包版本 无法保证可用性 增加了平台压力 为了很好的解决以上依赖分发的问题，go采用Proxy进行代理分发。 Go Proxy 是一个服务站点，它会缓源站中的软件内容，缓存的软件版本不会改变，并且在源站软件删除之后依然可用。 较为神奇的地方 Go语言通过设置环境变量GOPROXY来设置具体的服务站点。可以通过逗号设置多个Proxy站点，最后如果这几个都没有找到，那么会通过direct进行回源，也就是回到本来的请求站点，而不是代理站。有意思的是，当你此时从源站下载好依赖后，你之前走过的Proxy站点也会将这个缓存下来。 有趣的实践 通过go mod init创建一个项目，写好后提交到GitHub仓库里，然后通过go get对你的代码进行请求，注意最后回源的direct要加上，否则肯定get不到，最后你会发现你的Proxy站上，也有了你的代码🥳 你会发现这样的过程，让go语言的代码仓库非常的繁荣，各种库都可以go get得到！ 本地工具 go get命令 go mod命令 ","date":"2022-05-08","objectID":"/posts/golang-note2/:2:3","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"测试 ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:0","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"为什么要测试？ 测试是避免事故发生的最后一道关口！ ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:1","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"测试类型 回归测试：是指修改了旧代码后，重新测试以确认修改没有引入新的错误或导致其他代码产生错误。 集成测试：集成测试的目的是在集成这些不同的软件模块时揭示它们之间交互中的缺陷。 单元测试：单元测试测试开发阶段，开发者对单独的函数、模块做功能验证。 层级从上至下，测试成本逐渐减低，而测试覆盖率确逐步上升，所以单元测试的覆盖率一定程度上决定这代码的质量。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:2","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"单元测试 go单测的规则 go单测实例 写了一个json解析的单测 json.go package attention import ( \"bytes\" \"encoding/json\" \"fmt\" ) func NumUnmarshal() { jsonStr := `{\"id\":1,\"name\":\"Jerry\"}` var res map[string]interface{} _ = json.Unmarshal([]byte(jsonStr), \u0026res) fmt.Printf(\"%T\\n\", res[\"id\"]) i := res[\"id\"].(int64) fmt.Println(i) } func NumDecode() { jsonStr := `{\"id\":1,\"name\":\"Jerry\"}` var res map[string]interface{} decoder := json.NewDecoder(bytes.NewReader([]byte(jsonStr))) decoder.UseNumber() _ = decoder.Decode(\u0026res) i, _ := res[\"id\"].(json.Number).Int64() fmt.Println(i) } json_test.go package attention import \"testing\" func TestNumUnmarshal(t *testing.T) { NumUnmarshal() } func TestNumDecode(t *testing.T) { NumDecode() } 测试结果：通过 go test 会执行这个软件包里面所有的测试。如果需要执行特定的测试在后面跟上这个测试的go文件名以及对应的测试文件名。 单元测试框架 go语言常见的测试框架有testfy。在go mod文件里面的require部分填上以下代码便可通过go mod download进行下载。 github.com/stretchr/testify v1.7.1 或者直接 go get这个包也行。 这个包里包含测试常用的断言。 基础用法如下，更多用法请去查看官方文档。 衡量单元测试的标准 代码覆盖率 需要在测试时展示代码覆盖率可以通过添加–cover命令行参数。 下面是我的一次带代码覆盖率的单元测试结果： 我们可以看到百分比的覆盖率，也就是本次测试经过的代码块占比。 被测试到的代码都变成了绿色。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:3","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"打桩测试 在打桩测试前，我们先了解单侧的稳定性和幂等性。 稳定：稳定是指相互隔离，能在任何时间，任何环境，运行测试。 幂等：幂等是指每一次测试运行都应该产生与之前一样的结果。 如果在有外部依赖的情况下进行单测，换一个测试环境，那么这个外部依赖信息可能会发生变化，比如需要打开某个文件，如果你把这个给别人测试，那么在他本地的文件路径肯定就不一致。这就完全没法符合稳定和幂等两个条件。 如下代码： 那么我们如何解决这样的问题呢？ 我们通过打桩来解决这个问题。 所谓打桩就是通过你指定的行为来对原本的行为替换，到计算机语言里面来讲就是通过你定义的桩函数把原本的函数进行替换，这就是打桩。 那打桩有什么用呢？ 隔离：将测试任务从产品项目中分离出来，使之能够独立编译、链接，并独立运行。 补齐：用桩来代替未实现的代码，例如，函数A调用了函数B，而函数B由其他程序员编写，且未实现，那么，可以用桩来代替函数B，使函数A能够运行并测试。 控制：控制是指在测试时，人为设定相关代码的行为，使之符合测试需求。 go语言的打桩实现原理： 在运行时通过通过 Go 的 unsafe 包，将内存中函数的地址替换为运行时函数的地址。 将待打桩函数或方法的实现跳转到。 打桩更改后的测试： ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:4","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"基准测试（Benchmark） 很多时候我们需要清楚代码的运行效率，这个时候，我们就需要对代码进行基准测试了。 基准测试需要遵循以下语法规定： go语言中的基准测试也是基于单元测试，所以还是需要遵循 *_test.go 的命名规则。 用于基准测试的函数名必须以Benchmark开头。 函数的入参需要是 *testing.B 。 具体例子 代码分析 负载均衡中随机选择执行服务器。 server_select.go package benchmark import ( \"github.com/bytedance/gopkg/lang/fastrand\" \"math/rand\" ) var ServerIndex [10]int // InitServerIndex 初始化服务器的描述符 func InitServerIndex() { for i:=0;i\u003c10;i++{ ServerIndex[i] = i+100 } } // RandSelect 随机选择一个服务器 func RandSelect() int { return ServerIndex[rand.Intn(10)] } // FastRandSelect 用外部的fast包 func FastRandSelect()int{ return ServerIndex[fastrand.Intn(10)] } server_select_test.go package benchmark import \"testing\" func BenchmarkSelect(b *testing.B){ InitServerIndex() b.ResetTimer() for i:=0;i\u003cb.N;i++{ RandSelect() } } func BenchmarkSelectParallel(b *testing.B) { InitServerIndex() b.ResetTimer() b.RunParallel(func(pb *testing.PB) { for pb.Next(){ FastRandSelect() } }) } 我们对Benchmark的代码进行以下讲解： 对一个测试用例的默认测试时间是 1 秒，当测试用例函数返回时还不到 1 秒，那么 testing.B 中的 N 值将按 1、2、5、10、20、50……递增，并以递增后的值重新进行用例函数测试。 Resttimer重置计时器，我们在reset之前做了init或其他的准备操作，这些操作不应该作为基准测试的范围。 runparallel是多协程并发测试。 代码效率分析 我们发现，多线程的测试反而效率更慢了！ 主要原因是rand为了保证全局的随机性和并发安全，持有了一把全局锁。 这里贴了字节实现的较为快速的随机数实现库：fastrand 安装这个库也很简单，下面一行命令即可： go get github.com/bytedance/gopkg/lang/fastrand 优化代码 通过把 rand 替换为 fastrand 后，重新测试结果如下： 我们发现多线程的效率与之前的效率相比，提升了百倍！ fastrand主要的实现思路是牺牲了一定的数列一致性，在大多数场景是适用的，同学在后面遇到随机的场景可以尝试用一下。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:3:5","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"项目实战 ","date":"2022-05-08","objectID":"/posts/golang-note2/:4:0","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"需求描述 展示话题（标题，文字描述）和回帖列表 暂不考虑前端页面实现，仅实现一个本地的web服务 话题和回帖数据用文件存储 用户浏览 实例图 ","date":"2022-05-08","objectID":"/posts/golang-note2/:4:1","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"项目分层结构 数据层：Repository 数据Model，封装外部数据的增删改查，并将数据初步反序列化，且需要直接与底层的数据存储形式打交道，比如存储形式是文件，还是数据库，还是微服务等等。 逻辑层：Service 业务Entity，这里会利用数据层得到封装好的数据再次封装得到更贴近客户端请求的数据，同样也需要写好增删改查，但这里的增删改查并不会与真正的外部数据打交道，也就是说Service层不关心底层数据的存储形式，只关心核心业务输出。 视图层：Controller 视图View，处理和外部的交互逻辑，也就是说，这个层级也是依赖于上一个层级的数据，它负责真正和客户端交互的过程，只关心返回什么样的数据给客户端，而前面两个层级都是为这个层级做的铺垫。 ","date":"2022-05-08","objectID":"/posts/golang-note2/:4:2","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"代码实现 代码实现可以到TraningCamp查看lesson2源码（温馨提示github域名后加上1s可以有意想不到的源码阅读体验哦 Repository层实现 主要实现底层存储数据序列化到具体的结构体上，以及对应的增删改查。 一般经过以下过程： graph LR\ra(初始化)\rb(底层存储的交互)\ra--\u003eb\r 初始化：主要是对数据的准备，或者时数据库的连接的初始化。 底层存储的交互：如果数据库，那么就是对数据库发起请求得到对应的Model，如果是文件存储，那么数据应该已经初始化到内存，直接进行取值即可。 数据映射 由于本次的存储实现采取的是文件存储，故需要每次一次性把文件读取好并完成数据的反序列化。这里用到的map进行映射数据方便查询。 如果是数据库，这时应该通过一些orm框架直接进行数据的增删改查映射，但在此之前还是得连接数据库（初始化过程 具体源码实现（我多加了一个记录最后一个Id的，方便完成id的不重复生成 数据的增删改查 topic.go 实现对话题的增删改查，这里用到了一个结构体+方法的方式去实现，且用sync.Once实现单例，我觉得好处在于： 防止重名。 方便记忆，方便调用时进行对应的语法补全（比如想要对Topic进行操作，只需要想到TopicDao这个即可补全后续的操作 post.go 和前面的实现类似，这里我完成了homework，添加了AddPost方法以及对应的将数据插入到文件的方法，由于可能出现多个客户端同时发起post请求，这时我们需要对数据进行并发安全的保护，这里我使用的Mutex加锁的方式。 Service层实现 主要是对Repository层的Modle进行进一步的封装成更上层需要的Entity。 一般经过以下流程： graph LR\ra(参数校验)\rb(准备数据)\rc(组装实体)\ra--\u003eb--\u003ec\r 参数校验：由于是和上层通信的层，上层调用得到数据时，首先**需要传入对应的参数，那么我们需要对这个参数进行校验，**不同的方法需要的参数是不同的，需要进行的校验也是不同的，比如本项目查询的方法和插入的方法，需要的参数就不同，所以对应的也是走的这三个流程。 准备数据：在正式组装得到整个实体之前，我们应该先进行数据的准备，也就是需要把零件得到，当然，不一次性组装好的原因，我认为更重要的是这样可以减少代码的耦合，这样一来准备每个数据的过程可以独立开，且可以进行针对性的优化，或者进行局部的修改，也不会直接对组装代码造成影响。 组装实体：把准备好的数据返回即可。 为了实现上述过程，我们建立一个结构体，保存准备的数据，且把整个组装实体的过程流程化。 结构体如下： // PageInfo 一个页面的信息包括，topic和它上面的post言论 type PageInfo struct { Topic *repository.Topic PostList []*repository.Post } // QueryPageInfoFlow 为了防止高耦合度的构造PageInfo，可以构造如下结构体实现流式处理 type QueryPageInfoFlow struct { topicId int64 pageInfo *PageInfo topic *repository.Topic posts []*repository.Post } 整个组装过程： // Do 整个组装过程 func (q *QueryPageInfoFlow) Do() (*PageInfo, error) { //对id进行合法性验证 if err := q.checkNum(); err != nil { return nil, err } //准备好生成PageInfo的数据 if err := q.prepareInfo(); err != nil { return nil, err } //打包最终的PageInfo if err := q.packPageInfo(); err != nil { return nil, err } return q.pageInfo, nil } 参数校验 由于这个查询过程暂时只需要校验这一个参数 func (q *QueryPageInfoFlow) checkNum() error { if q.topicId \u003c= 0 { return errors.New(\"topic must larger than 0\") } return nil } 准备数据 由于两个数据的查询毫无关联，可以通过并行处理。 graph LR\ra[话题信息]\rb[回帖信息]\rc[查询]\rd[结束]\rc--\u003ea\rc--\u003eb\ra--\u003ed\rb--\u003ed\r//这两个过程，由于是毫无关联的，可以用go协程进行并发处理 func (q *QueryPageInfoFlow) prepareInfo() error { var wg sync.WaitGroup wg.Add(2) //获取Topic go func() { defer wg.Done() q.topic = repository.NewTopicDao().QueryTopicFromId(q.topicId) }() //获取Posts go func() { defer wg.Done() q.posts = repository.NewPostDao().QueryPostsFromParentId(q.topicId) }() wg.Wait() return nil } 组装实体 //更新最终的PageInfo func (q *QueryPageInfoFlow) packPageInfo() error { q.pageInfo = \u0026PageInfo{ Topic: q.topic, PostList: q.posts, } return nil } 这样的话实现整个QueryPageInfo函数就只需要调用这个结构体的方法即可。 如下： func QueryPageInfo(id int64) (*PageInfo, error) { return NewQueryPageInfoFlow(id).Do() } Controller层实现 这个层级是真正对客户端发来的请求进行直接响应的层级，直接与客户端交互。 一般经过以下过程： graph LR\ra[参数解析]\rb[构造数据]\rc[返回数据]\ra--\u003eb--\u003ec\r 参数解析：由于对接的数据直接是上层收到的信息，所以大概率是纯字符串，所以需要先对参数进行解析。 构造数据：也就是构造响应的数据，一般来说除了直接的数据外，还需要提供一个错误码和错误信息给前端。 返回数据：根据不同情况构造的不同数据直接返回即可。 具体代码 // PageData 最终发送给客户端的json数据对应的结构体，我们需要错误码，以及对应错误码对应的消息，最后再是数据(用空接口实现泛型 type PageData struct { Code int64 `json:\"code\"` Msg string `json:\"msg\"` Data interface{} `json:\"data\"` } // QueryPageINfo 真正和客户端进行交互的函数，需要注意客户端发来的流量都是字符串形式 func QueryPageINfo(topicIdStr string) *PageData { pageId, err := strconv.Atoi(topicIdStr) if err != nil { return \u0026PageData{Code: 1, Msg: err.Error(), Data: nil} } pageInfo, err := service.QueryPageInfo(int64(pageId)) if err != nil { return \u0026PageData{Code: 2, Msg: err.Error(), Data: nil} } return \u0026PageData{Code: 0, Msg: \"success\", Data: pageInfo} } homework部分 作业内容与思考 课后实战： 支持发布帖子。 本地Id生成保证不重复。 Append文件，更新索引，注意并发安全问题。 我发现一个特点，这种分Controller、Service、Repository层的情况， 当你上层调用查询接口的时候，数据是自下往上的，也就是数据是从下往上依次封装。 而如果是实现添加操作接口的时候，数据是自上往下的，则数据是从上往下依次封装。 具体实现 思路： Id生成唯一性，是用的一个lastIndexId保存整个post中最大的id，之后每次添加post都继续增加这个lastIndexId来得到新的id。 并发安全问题，用到Mutex加锁临界区即可。 Repository层 AddPost提供是提供给Service层的接口。 需要实现把数据添加到map里以及append到文件中（对应fileDataInsertPost函数） func (d *PostDao) AddPost(post *Post) error { //加锁保证同时请求的并发安全 lock := sync.Mutex{} lock.Lock() posts, ok := postIndexMap[post.ParentId] if !ok { return errors.New(\"post invalid,not exist parent id\") } /","date":"2022-05-08","objectID":"/posts/golang-note2/:4:3","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"实测结果 服务端代码 server.go package main import ( \"github.com/ACking-you/TraningCamp/lesson2/homework/controller\" \"github.com/ACking-you/TraningCamp/lesson2/homework/repository\" \"gopkg.in/gin-gonic/gin.v1\" \"os\" \"strings\" ) //最后再通过gin框架搭建服务器 func main() { //准备数据 if err := Init(\"./lesson2/homework/data/\"); err != nil { os.Exit(-1) } //注册路由 r := gin.Default() r.GET(\"me:id\", func(c *gin.Context) { topicId := c.Param(\"id\") topicId = strings.TrimLeft(topicId, \":,\") println(topicId) data := controller.QueryPageINfo(topicId) c.JSONP(200, data) }) r.POST(\"/post/do\", func(c *gin.Context) { uid, _ := c.GetPostForm(\"uid\") println(uid) topicId, _ := c.GetPostForm(\"topic_id\") println(topicId) content, _ := c.GetPostForm(\"content\") println(content) data := controller.PublishPost(uid, topicId, content) c.JSON(200, data) }) err := r.Run() if err != nil { return } } func Init(filepath string) error { err := repository.Init(filepath) if err != nil { return err } return nil } 请求结果 使用的是goland里面的http请求工具进行的。 GET请求测试（成功） 请求报文如下： GET http://localhost:8080/me:1 Accept: application/json 返回报文如下： HTTP/1.1 200 OK Content-Type: application/json; charset=utf-8 Date: Mon, 09 May 2022 05:17:28 GMT Content-Length: 426 { \"code\": 0, \"msg\": \"success\", \"data\": { \"Topic\": { \"id\": 1, \"title\": \"青训营来啦!\", \"content\": \"小姐姐，快到碗里来~\", \"create_time\": 1650437625 }, \"PostList\": [ { \"id\": 1, \"parent_id\": 1, \"content\": \"小姐姐快来1\", \"create_time\": 1650437616, \"user_id\": 1 }, { \"id\": 2, \"parent_id\": 1, \"content\": \"小姐姐快来2\", \"create_time\": 1650437617, \"user_id\": 2 }, { \"id\": 3, \"parent_id\": 1, \"content\": \"小姐姐快来3\", \"create_time\": 1650437618, \"user_id\": 13 } ] } } Response code: 200 (OK); Time: 174ms; Content length: 368 bytes POST请求测试（成功） 请求报文： POST http://localhost:8080/post/do Content-Type: application/x-www-form-urlencoded uid=2\u0026topic_id=1\u0026content=测试内容嗨嗨嗨嗨 返回报文： HTTP/1.1 200 OK Content-Type: application/json; charset=utf-8 Date: Mon, 09 May 2022 05:22:38 GMT Content-Length: 47 { \"code\": 0, \"msg\": \"success\", \"data\": { \"post_id\": 5 } } Response code: 200 (OK); Time: 103ms; Content length: 47 bytes 再看看文件里面的内容是否添加： 成功！ ","date":"2022-05-08","objectID":"/posts/golang-note2/:4:4","tags":["Go工程实践"],"title":"Go语言工程实践（二） | 青训营笔记","uri":"/posts/golang-note2/"},{"categories":["青训营笔记"],"content":"这是我参与「第三届青训营 -后端场」笔记创作活动的的第一篇笔记。","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"这是我参与「第三届青训营 -后端场」笔记创作活动的的第一篇笔记。 ","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:0:0","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"语法速览 ","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:1:0","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"基础语法 基础语法有几点需要注意： 第一：类型 有值类型，有指针，指针只能作为引用的替代品，无法指针直接运算。 go语言有值类型，可以直接像下面这样定义变量： type Student struct { name string sid string } func main(){ var student = Student{name: \"John\", sid: \"1001\"} //student为值类型 var student = \u0026Student{name: \"John\", sid: \"1001\"} //student为指针类型（注意由于go有垃圾回收机制，所以这里会自动为我们开辟堆内存 student := new(Student) //也可通过内置的new()函数直接开辟堆内存，而不立马初始化，得到一个指针 } go语言的切片 同样切片类型也有上述两种获得内存的定义方式，也可通过内置的make函数对内部的cap和len进行初始的控制。 nums := make([]int,2,10)//得到一个底层数组长度为2，cap为10的初始切片 nums1 := nums2[0:3] //第二种切片方式 第二：内置库部分 json库的使用 通过在字段后面跟着的字符串进行序列化的定义，后面跟着的称为域标签。 package main import ( \"encoding/json\" \"fmt\" ) type Student struct { Name string `json:\"name\"` Sid string `json:\"sid\"` } func main() { s := Student{Name: \"jonh\" ,Sid: \"10323\"} //序列化 p ,err := json.Marshal(s) if err!=nil { panic(err) } fmt.Println(string(p)) //反序列化 err = json.Unmarshal(p,\u0026s) if err!=nil { panic(err) } fmt.Println(s) } 官方对域标签有以下说明： // Field appears in JSON as key \"myName\". Field int `json:\"myName\"` // Field appears in JSON as key \"myName\" and // the field is omitted from the object if its value is empty, // as defined above. Field int `json:\"myName,omitempty\"` // Field appears in JSON as key \"Field\" (the default), but // the field is skipped if empty. // Note the leading comma. Field int `json:\",omitempty\"` // Field is ignored by this package. Field int `json:\"-\"` // Field appears in JSON as key \"-\". Field int `json:\"-,\"` 时间库的使用 时间的获取 获取当前时间： package main import ( \"fmt\" \"time\" ) func main() { now := time.Now() //获取当前时间 fmt.Printf(\"current time:%v\\n\", now) year := now.Year() //年 month := now.Month() //月 day := now.Day() //日 hour := now.Hour() //小时 minute := now.Minute() //分钟 second := now.Second() //秒 fmt.Printf(\"%d-%02d-%02d %02d:%02d:%02d\\n\", year, month, day, hour, minute, second) } 获取时间戳 package main import ( \"fmt\" \"time\" ) func main() { now := time.Now() //获取当前时间 timestamp1 := now.Unix() //时间戳 timestamp2 := now.UnixNano() //纳秒时间戳 fmt.Printf(\"现在的时间戳：%v\\n\", timestamp1) fmt.Printf(\"现在的纳秒时间戳：%v\\n\", timestamp2) } 时间戳与时间的转换 package main import ( \"fmt\" \"time\" ) func main() { now := time.Now() //获取当前时间 timestamp := now.Unix() //时间戳 timeObj := time.Unix(timestamp, 0) //将时间戳转为时间格式 fmt.Println(timeObj) year := timeObj.Year() //年 month := timeObj.Month() //月 day := timeObj.Day() //日 hour := timeObj.Hour() //小时 minute := timeObj.Minute() //分钟 second := timeObj.Second() //秒 fmt.Printf(\"%d-%02d-%02d %02d:%02d:%02d\\n\", year, month, day, hour, minute, second) } 获取星期几 package main import ( \"fmt\" \"time\" ) func main() { t := time.Now() fmt.Println(t.Weekday().String()) } 时间的操作 （1）Add(during)函数实现某个时间 + 时间间隔 package main import ( \"fmt\" \"time\" ) func main() { now := time.Now() later := now.Add(time.Hour) // 当前时间加1小时后的时间 fmt.Println(later) } （2）Sub(Time)获取时间差值 返回一个时间段 t - u 的值。如果结果超出了 Duration 可以表示的最大值或最小值，将返回最大值或最小值，要获取时间点 t - d（d 为 Duration），可以使用 t.Add(-d)。 （3）Equal(Time)判断时间是否相同 （4）Before 和 After某个时间是否在他之前或之后 定时任务 使用 time.Tick(时间间隔) 可以设置定时器，定时器的本质上是一个通道（channel） package main import ( \"fmt\" \"time\" ) func main() { ticker := time.Tick(time.Second) //定义一个1秒间隔的定时器 for i := range ticker { fmt.Println(i) //每秒都会执行的任务 } } 解析字符串格式的时间 Parse 函数可以解析一个格式化的时间字符串并返回它代表的时间。 func Parse(layout, value string) (Time, error) 与 Parse 函数类似的还有 ParseInLocation 函数。 func ParseInLocation(layout, value string, loc *Location) (Time, error) ParseInLocation 与 Parse 函数类似，但有两个重要的不同之处： 第一，当缺少时区信息时，Parse 将时间解释为 UTC 时间，而 ParseInLocation 将返回值的 Location 设置为 loc； 第二，当时间字符串提供了时区偏移量信息时，Parse 会尝试去匹配本地时区，而 ParseInLocation 会去匹配 loc。 示例代码如下： package main import ( \"fmt\" \"time\" ) func main() { var layout string = \"2006-01-02 15:04:05\" var timeStr string = \"2019-12-12 15:22:12\" timeObj1, _ := time.Parse(layout, timeStr) fmt.Println(timeObj1) timeObj2, _ := time.ParseInLocation(layout, timeStr, time.Local) fmt.Println(timeObj2) } 字符串和数字互转 字符串与数字互转的想关库函数全在一个包内：strconv包 一图胜千言： os相关信息 os包里面封装了很多和操作系统相关的内容。 如下： package main import ( \"fmt\" \"os\" \"os/exec\" ) func main() { fmt.Println(os.Args) //打印命令行参数 fmt.Println(os.Getenv(\"PATH\")) //打印环","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:1:1","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"实战项目 ","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:2:0","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"猜谜游戏（pass，过于简单） ","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:2:1","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"在线词典 想要实现在线词典，首先就得用到别人的翻译引擎 第一步：抓包得得到数据进行分析 以彩云词典为例： 从网页调试工具里面查看随时收发的网络数据包，挨个查看它们的response，如果里面的json数据出现翻译结果，那么说明这个包就是返回的翻译结果！ 那么我们只需要让go语言来做同样的两件事： 发起请求。 解析返回的json内容。 只要做好了这两件事，那么就很快得到了一个单词的翻译了。 第二步：利用工具生成代码 在此之前我们需要清楚有两个神器般存在的网站： curlconverter 把curl请求直接转为go的请求代码。 oktools JSON转Golang Struct 那么我们先肯定是要得到请求的代码，然后稍作更改，解析body后得出想要的结果。 curl请求直接转为go的请求代码 如下图进到刚才我们捕捉到的目标包，然后复制cURL，到网站进行解析得到最终代码。 package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"strings\" ) func main() { client := \u0026http.Client{} var data = strings.NewReader(`{\"trans_type\":\"en2zh\",\"source\":\"hello\"}`) req, err := http.NewRequest(\"POST\", \"https://api.interpreter.caiyunai.com/v1/dict\", data) if err != nil { log.Fatal(err) } req.Header.Set(\"Connection\", \"keep-alive\") req.Header.Set(\"sec-ch-ua\", `\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"99\", \"Google Chrome\";v=\"99\"`) req.Header.Set(\"sec-ch-ua-mobile\", \"?0\") req.Header.Set(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\") req.Header.Set(\"app-name\", \"xy\") req.Header.Set(\"Content-Type\", \"application/json;charset=UTF-8\") req.Header.Set(\"Accept\", \"application/json, text/plain, */*\") req.Header.Set(\"os-type\", \"web\") req.Header.Set(\"X-Authorization\", \"token:qgemv4jr1y38jyq6vhvi\") req.Header.Set(\"sec-ch-ua-platform\", `\"Windows\"`) req.Header.Set(\"Origin\", \"https://fanyi.caiyunapp.com\") req.Header.Set(\"Sec-Fetch-Site\", \"cross-site\") req.Header.Set(\"Sec-Fetch-Mode\", \"cors\") req.Header.Set(\"Sec-Fetch-Dest\", \"empty\") req.Header.Set(\"Referer\", \"https://fanyi.caiyunapp.com/\") req.Header.Set(\"Accept-Language\", \"zh-CN,zh;q=0.9\") resp, err := client.Do(req) if err != nil { log.Fatal(err) } defer resp.Body.Close() bodyText, err := ioutil.ReadAll(resp.Body) if err != nil { log.Fatal(err) } fmt.Printf(\"%s\\n\", bodyText) } 观察代码的改变我们只需对source部分的内容进行更改，即可得到对应的翻译结果。 JSON转Golang Struct 得到翻译结果，body内容后，我们需要把body内容解析为本地的sturct才能正常使用（当然你头铁的话可以直接找对应的字符串即可，也不需要反序列化。 type AutoGenerated struct { Rc int `json:\"rc\"` Wiki struct { KnownInLaguages int `json:\"known_in_laguages\"` Description struct { Source string `json:\"source\"` Target interface{} `json:\"target\"` } `json:\"description\"` ID string `json:\"id\"` Item struct { Source string `json:\"source\"` Target string `json:\"target\"` } `json:\"item\"` ImageURL string `json:\"image_url\"` IsSubject string `json:\"is_subject\"` Sitelink string `json:\"sitelink\"` } `json:\"wiki\"` Dictionary struct { Prons struct { EnUs string `json:\"en-us\"` En string `json:\"en\"` } `json:\"prons\"` Explanations []string `json:\"explanations\"` Synonym []string `json:\"synonym\"` Antonym []interface{} `json:\"antonym\"` WqxExample [][]string `json:\"wqx_example\"` Entry string `json:\"entry\"` Type string `json:\"type\"` Related []interface{} `json:\"related\"` Source string `json:\"source\"` } `json:\"dictionary\"` } 第三步：更改代码实现功能 通过前面生成的代码已经能够实现请求和接收响应，且可以直接把响应内容反序列化为结构体，那么接下来，只需要把想要的部分遍历打印即可。 最终代码如下： package src import ( \"bytes\" \"encoding/json\" \"fmt\" \"io/ioutil\" \"log\" \"net/http\" ) func QueryCaiyun(word string) { client := \u0026http.Client{} request := DictRequestCaiyun{TransType: \"en2zh\", Source: word} buf, err := json.Marshal(request) if err != nil { log.Fatal(err) } var data = bytes.NewReader(buf) req, err := http.NewRequest(\"POST\", \"https://api.interpreter.caiyunai.com/v1/dict\", data) if err != nil { log.Fatal(err) } req.Header.Set(\"Connection\", \"keep-alive\") req.Header.Set(\"sec-ch-ua\", `\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"99\", \"Google Chrome\";v=\"99\"`) req.Header.Set(\"sec-ch-ua-mobile\", \"?0\") req.Header.Set(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\") req.Header.Set(\"app-name\", \"xy\") req.Header.Set(\"Content-Type\", \"application/json;charset=UTF-8\") req.Header.Set(\"Accept\", \"application/json, text/plain, */*\") req.Header.Set(\"os-type\", \"web\") req.Header.Set(\"X-Authorization\", \"token:qgemv4jr1y38jyq6vhvi\") req.Header.Set(\"sec-ch-ua-platform\", `\"Windows\"`) req.Header.Set(\"Origin\", \"https://fanyi.caiy","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:2:2","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["青训营笔记"],"content":"SOCKS5代理服务器 SOCKS5简单介绍 SOCKS5是代理协议，在使用TCP/IP协议通信的前端机器和服务器之间发挥中介作用，使内部网络的前端机器能够访问互联网的服务器，使通信更加安全。SOCKS5服务器将前端发送的请求转发给真正的目标服务器，模拟前端行为。此处，前端与SOCKS5之间也是通过TCP/IP协议进行通信的，前端向SOCKS5服务器发送请求发送给SOCKS5服务器，然后SOCKS5服务器将请求发送给真正的服务器。SOCKS5服务器在将通讯请求发送给真正服务器的过程中，对于请求数据包本身不加任何改变（明文传输）。SOCKS5服务器在收到真实服务器响应后，也原样转发到前端。 它的用途是， 比如某些企业的内网为了确保安全性，有很严格的防火墙策略，但是带来的副作用就是访问某些资源会很麻烦。 socks5 相当于在防火墙开了个口子，让授权的用户可以通过单个端口去访问内部的所有资源。实际上很多翻墙软件，最终暴露的也是一个 socks5 协议的端口。 SOCKS5代理原理 正常浏览器访问一个网站，如果不经过代理服务器的话，就是先和对方的网站建立 TCP 连接，然后三次握手，握手完之后发起 HTTP 请求，然后服务返回 HTTP 响应。如果设置代理服务器之后，流程会变得复杂一些。 首先是浏览器和 socks5 代理建立 TCP 连接，代理再和真正的服务器建立 TCP 连接。这里可以分成四个阶段，握手阶段、认证阶段、请求阶段、 relay 阶段。 握手阶段：浏览器会向 socks5 代理发送请求，包的内容包括一个协议的版本号，还有支持的认证的种类，socks5 服务器会选中一个认证方式，返回给浏览器。如果返回的是 00 的话就代表不需要认证，返回其他类型的话会开始认证流程，这里我们就不对认证流程进行概述了。（本次课程跳过认证阶段） 请求阶段：认证通过之后浏览器会对 socks5 服务器发起请求。主要信息包括 版本号，请求的类型，一般主要是 connection 请求，就代表代理服务器要和某个域名或者某个 IP 地址某个端口建立 TCP 连接。代理服务器收到响应之后，会真正和后端服务器建立连接，然后返回一个响应。 relay 阶段：此时浏览器会发送 正常发送请求，然后代理服务器接收到请求之后，会直接把请求转换到真正的服务器上。然后如果真正的服务器以后返回响应的话，那么也会把请求转发到浏览器这边。然后实际上 代理服务器并不关心流量的细节，可以是 HTTP流量，也可以是其它 TCP 流量。 这个就是 socks5 协议的工作原理。 graph LR a[握手阶段] b[认证阶段] c[请求阶段] d[转发relay阶段] a--\u003eb--\u003ec--\u003ed 具体实现 v1-简单echo服务器 package main import ( \"bufio\" \"fmt\" \"log\" \"net\" ) func main() { server, err := net.Listen(\"tcp\", \"0.0.0.0:1080\") if err != nil { panic(err) } for { client, err := server.Accept() if err != nil { log.Printf(\"Accept failed %v\", err) continue } fmt.Printf(\"连接成功! clent:%v \\n\", client.RemoteAddr()) go process(client) } } func process(conn net.Conn) { defer func() { conn.Close() fmt.Printf(\"连接断开! clent:%v \\n\", conn.RemoteAddr()) }() //用缓冲流进行一次包装，减少底层IO次数，让读取效率更高效 reader := bufio.NewReader(conn) for { b, err := reader.ReadByte() if err != nil { break } _, err = conn.Write([]byte{b}) if err != nil { break } } } 客户端验证： 没必要再写一个客户端，这时完全可以netcat工具进行tcp连接测试。 如下： v2-实现SOCKS5的握手阶段 实现SOCKS5之前我们需要清楚SOCKS5的握手阶段的请求和返回是怎么样的，如下面的图表所示： VER NMETHODS METHODS 1byte 1byte 1 to 255 byte 协议版本信息socks5为0x05 支持认证的方法数量值为0x00则表示无需认证 NMETHODS的值为多少METHODS就有多少个字节 package main //auth 阶段 import ( \"bufio\" \"fmt\" \"io\" \"log\" \"net\" ) const( socks5Ver = 0x05 cmdBind = 0x01 atypIPV4 = 0x01 atypeHOST = 0x03 atypeIPV6 = 0x04 ) func main() { server, err := net.Listen(\"tcp\", \"0.0.0.0:1080\") if err != nil { panic(err) } for { client, err := server.Accept() if err != nil { log.Printf(\"Accept failed %v\", err) continue } fmt.Printf(\"连接成功! clent:%v \\n\", client.RemoteAddr()) go process(client) } } func process(conn net.Conn) { defer func() { conn.Close() fmt.Printf(\"连接断开! clent:%v \\n\", conn.RemoteAddr()) }() reader := bufio.NewReader(conn) err := auth(reader,conn) if err!=nil{ log.Printf(\"client %v auth failed:%v\",conn.RemoteAddr(),err) } log.Println(\"auth success\") } func auth(reader *bufio.Reader, conn net.Conn) (err error) { //协议版本 ver,err := reader.ReadByte() if err != nil{ return fmt.Errorf(\"read ver failed:%w\",err) } if ver != socks5Ver{ return fmt.Errorf(\"not supported ver:%v\",ver) } //支持的方法数量 methodSize,err := reader.ReadByte() if err!=nil{ return fmt.Errorf(\"read methodSize failed:%w\",err) } //方法值 method := make([]byte,methodSize) _,err = io.ReadFull(reader,method) if err!=nil{ return fmt.Errorf(\"read method failed %w\",err) } log.Println(\"ver\",ver,\"method\",method) //返回的内容表示SOCKS5通信，且无需认证 _,err = conn.Write([]byte{socks5Ver,0x00}) if err !=nil{ return fmt.Errorf(\"write failed:%w\",err) } return nil } v3-实现SOCKS5的请求阶段 同样来看看此时的消息协议： 客户端请求： VER CMD RSV ATYP DST.ADDR DST.PORT 1byte 1byte 1byte 1byte Variable 2byte 协议版本0x05为SOCKS5 代表请求类型0x01表示CONNECT请求 保留字段（不理会） 目标地址类型（IPV4/IPV6/域名） 地址值，根据不同地址类型，长度不同 需要访问的服务器的端口号 服务端响应： VER REP RSV ATYP BIND.ADDR BIND.PORT 1byte 1byte 1byte 1byte Variable 2byte 协议版本0x05为SOCKS5 代表响应。成功就返回0 保留字段（不理会） 地址类型（IPV4/IPV6/域名） 地址值（这里暂时不需要 端口号（这里暂时不需要 这一过程的代码： 对当前的实现进行测试： 进行如下curl命令： curl --socks5 localhost:1080 -v http://www.qq.com 此时请求会失败，但我们已经能看到正常打印出来的ip和端口号 v4-实现SOCKS5的转发阶段（最终完全版本 最后的转发过程，由于不需要对流量进行任何的处理，所以没有上层协议，直接再Write操作完后把流量进行转发即可。 对于两个连接流量的转发，标准库里有有一些好用的函数库。 通过net.Dial建立tcp连接。 dest, err := net.Dial(\"tcp\", fmt","date":"2022-05-07","objectID":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/:2:3","tags":["Go语言上手"],"title":"Go语言上手（一） | 青训营笔记","uri":"/posts/%E4%B8%80go%E8%AF%AD%E8%A8%80%E4%B8%8A%E6%89%8B-%E9%9D%92%E8%AE%AD%E8%90%A5%E7%AC%94%E8%AE%B0/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"实现实时qq好友搜索框","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"项目背景 想要用纯C++实现一个QQ，包括客户端的ui和通信，以及服务端的数据收发通信。 客户端：使用C++的Qt框架实现UI（正在进行），使用自己手写的网络框架进行通信（还未开始），以及手写的bencode编码进本地序列化（已经完成）。 服务端：使用自己造好的网络框架进行数据的收发和通信（还未开始）。 本篇进度：已经实现了登录界面和基本的主界面，本次是想拓展一个实时的搜索框，结果碰到了坑，然后在此写下记录。 目前UI实现情况如下图： 登录界面： 主界面： ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:1:0","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"实时QQ好友搜索框实现思路 实现目标：当用户在搜索框输入名字的时候，会实时将当前列表内的包含该名字的好友给列举出来。 实现方式：通过连接信号 QLineEdit::textChanged 来对应用户按钮的存储控件中的槽函数。 ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:2:0","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"具体思路 由于我是通过 QListWidget 存下多个自己重写的 QPushButton 来实现好友列表，故想要进行列表中的删除和增加就是对 QListWidget 增加或删除 QListWidgetItem 。由于 QListWidget 只提供了 QListWidget::addItem 方法进行添加，而这个方法并不能保证在具体哪一行插入，只会在列表的尾部进行添加，所以我通过这样一个方法实现实时的列表搜索项：清空再重新创建符合条件的 QPushButton。 ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:2:1","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"实践 ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:3:0","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"实践一：通过保存一份拷贝(扑街bug) 根据这个思路，我很快想到，需要保存好搜索前的所有列表项，然后再通过用户输入的文字筛选，将符合条件的列表项添加进去。 这样很快就出现了问题：由于清空列表会导致列表项里面的所有内存都被析构！所以如果你去按照这样一个思路去实现，我们需要在每次调用 addUserBtn 添加用户按钮的时候存下一份拷贝，然后你把列表清空后内存析构也一直能用这一份拷贝再生成新的拷贝按钮来添加到列表中去，这样问题应该就解决了。 然后我马不停蹄的马上写好了代码，跑起来准备测试，立马出现新的问题：每次输入时确实会显示对应的用户按钮是没错，但图片都没了，甚至很多按钮是空的？？？ 经过注意排查，发现是我写拷贝构造函数的时候把很多 QString 成员转了右值用了移动构造，所以每次发送拷贝时，实际上资源已经转移了。。。 改好这个bug，又继续测试，然后果不其然，又出现了新的问题：用户输入的时候会有符合条件的项被创建没错，且都有对应的数据，但是随着继续输入创建的对象是越来越多了？？？ 又经过仔细排查，发现是因为每次 addUserBtn 的时候都会把对应的数据添加一份拷贝到缓存，但是如果是用户在输入框立马输入信息进行查询时，调用了这个add函数，那么就会不断的添加重复的信息到缓存中去！！！ 此时很多人可能会想着用 set 替代 list 做缓存，但是我就此发现了我这个实现思路就很有问题： 一、每次重新添加控件需要把整个信息全拷贝一遍，如果数据多了会是一个很耗时的工作！ 二、我发现了罪恶之源：QListWidget 把控件的生命周期牢牢把握！！！要不是每次删除控件都得把立马的东西析构才能让这个控件不显示，才不会出现这等问题！也才不会需要重复不断的创建对象！ ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:3:1","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"实践二：通过前后端分离+智能指针完美解决 这次推倒重来，前面的试探也并非是在做无用功，至少通过前面的实践发现，既然我们没法把 Button 的生命周期拿在手里（每次还是需要重新new出新的Button），但我们至少把 Button 里数据的生命周期拿在手里啊，我们利用数据和界面分离的思想，将数据以一个智能指针的方式保存在 Button 内部，这样一来按钮被析构，数据也不一定就会被析构，只有当智能指针的引用计数为0的时候才被析构。 所以回到前面的实践一，这次我们还是以一个缓存存下原本的数据，但是这次我们存储的不是 Button 对象，而是 Button 对象中的智能指针数据，所以只要缓存还存在，那么 Button 里面的数据就还是可以复用的！所以每次 new 出新的 Button 去显示的代价是很小的 ，因为指针的copy几乎是没有代价的。。。 而且如果用set存储指针的话，直接通过判断地址就能清楚是否是相同的数据了！这样的存储性能会比直接存下数据要高很多。 像这样把数据和界面分离开的思路，我觉得就和web端的前后端分离的思路是一模一样，所以我愿意称之为桌面端的前后端分离。 ","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:3:2","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["(bug日记)C++实现QQ——UI篇"],"content":"附录 最后我贴上我具体实现的代码： 能看懂的应该会有收获，看不懂以后总会看得懂😁 UserButton.h class UserButton :public QPushButton{ Q_OBJECT private: UserButton(QWidget*parent); public: //按钮的类型 enum class Type{ Meg_Btn, User_Btn }; //存储按钮数据的类型 struct ButtonInfo{ int _unread = 0; int _hide = 0; int _uid = 0; State _state = State::OffLine; Type _type = Type::Meg_Btn; QString _iconPath; QString _name; QString _text; static std::shared_ptr\u003cButtonInfo\u003e getDefaultButtonInfo(QString const\u0026name,QString const\u0026text,QString const\u0026iconPath); }; using shared_info = std::shared_ptr\u003cButtonInfo\u003e; //根据外界数据创建按钮 static UserButton* fromButtonInfo(shared_info const\u0026 info,QWidget* parent=nullptr); /** * 持久化成功返回存储的路径，否则返回空optional * @param imageSrc * @param name * @return */ static std::optional\u003cQString\u003e StorageIcon(QByteArray const\u0026imageSrc,QString const\u0026 name); ~UserButton() override; protected: void paintEvent(QPaintEvent *) override; public: //画出Message按钮 void PainMessageBtn(); //画出User按钮 void PainUserBtn(); QRect adjustSize(QString\u0026dest); //一大堆getter和setter QString getName(); void setName(QString name); int getNumUnread() const; void setNumUnread(int numUnread); State getMState() const; void setMState(State mState); const QString \u0026getMIconPath() const; void setMIconPath(const QString \u0026mIconPath); Type getMType() const; void setMType(Type mType); void setHideMsgStyle(bool isHide); int getMisHide(); int getUid() const; void setUid(int uid); void setContent(QString const\u0026 text); QString getContent(); shared_info getSharedInfo(); public slots: //用于外界实时更新消息框的显示内容的槽函数 void updateContent(QString const\u0026 text); private: //数据 shared_info m_info_; }; UserButton.cpp // // Created by Alone on 2022-4-18. // #include \"UserButton.h\"#include \u003cQFile\u003e#include \u003cQByteArray\u003e#include \u003cutility\u003e#include \u003cQDir\u003e#include \u003cconfig_path.h\u003e#include \u003ciostream\u003e#include \u003cQPainter\u003e //方便进行测试的信shared_info快速创建 UserButton::shared_info UserButton::ButtonInfo::getDefaultButtonInfo(const QString \u0026name, const QString \u0026text, const QString \u0026iconPath) { auto* src = new UserButton::ButtonInfo; src-\u003e_name = name; src-\u003e_text = text; src-\u003e_iconPath = iconPath; return std::shared_ptr\u003cUserButton::ButtonInfo\u003e(src); } //qss样式 inline QString My_StyleSheet(QString const\u0026 icon_path){ return QString(R\"( QPushButton{ border:none; image: url(%1); image-position:left; min-width:120px; padding-top:7px; padding-left:10px; padding-bottom:7px; background:rgb(248, 249, 249); } QPushButton:hover{ border:none; background:rgb(242, 242, 242); } QPushButton:checked{ border:none; background:rgb(235, 235, 235); } )\").arg(icon_path); } UserButton::UserButton(QWidget *parent): QPushButton(parent) { setCheckable(true); setAutoExclusive(true); } UserButton::~UserButton() { } UserButton* UserButton::fromButtonInfo(shared_info const\u0026 info,QWidget* parent){ auto* btn = new UserButton(parent); btn-\u003em_info_ = info; btn-\u003esetStyleSheet(My_StyleSheet(btn-\u003egetMIconPath())); return btn; } //调整内容显示 QRect UserButton::adjustSize(QString \u0026dest){ assert(m_info_!=nullptr); // 三档调节 int num_width = 0; int span = 13; if(m_info_-\u003e_unread\u003c10){ dest = QString::number(m_info_-\u003e_unread); num_width = span; }else if(m_info_-\u003e_unread\u003c99){ dest = QString::number(m_info_-\u003e_unread); num_width = span*2; }else{ dest.append(\"99+\"); num_width = span*3; } return {this-\u003ewidth()-10-num_width,this-\u003eheight()/2+5,num_width,13}; } //根据不同的按钮进行不同的绘制 void UserButton::paintEvent(QPaintEvent *e) { assert(m_info_!=nullptr); QPushButton::paintEvent(e); switch (m_info_-\u003e_type) { case Type::User_Btn: PainUserBtn(); break; case Type::Meg_Btn: PainMessageBtn(); break; } } //持久化二进制数据到本地(主要是图片) std::optional\u003cQString\u003e UserButton::StorageIcon(QByteArray const\u0026 imageSrc,QString const\u0026 name) { QString base_path = QDir::homePath()+DirName_Base; QString user_icon_path = base_path +DirName_Icon; //如果两级文件夹不存在，则先创建文件夹 QDir dir(base_path); if(!dir.exists()){ qDebug()\u003c\u003c\"base_dir not exists,being created\"; if(!dir.mkdir(base_path) ) return {}; } dir.setPath(user_icon_path); if(!dir.exists() ) { qDebug()\u003c\u003c\"icon_dir not exists","date":"2022-04-24","objectID":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/:4:0","tags":["实现实时qq好友搜索框"],"title":"实现实时qq好友搜索框","uri":"/posts/%E5%AE%9E%E7%8E%B0%E5%AE%9E%E6%97%B6qq%E5%A5%BD%E5%8F%8B%E6%90%9C%E7%B4%A2%E6%A1%86/"},{"categories":["C++多线程"],"content":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"条件变量(C++11) ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:1:0","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"为什么要引入条件变量 我们先来看看一个由互斥量加锁构成的生产者消费者模型： // // Created by Alone on 2022-3-27. // #include \u003ciostream\u003e#include \u003cmutex\u003e#include \u003cdeque\u003e#include \u003cthread\u003estd::mutex mtx; std::deque\u003cint\u003e q; // producer void task1(){ int i = 0; while (1){ std::unique_lock\u003cstd::mutex\u003e lock(mtx); //std::this_thread::sleep_for(std::chrono::milliseconds(10)); q.push_back(i); if (i \u003c 9999999) { i++; }else { i = 0; } } } // consumer void task2(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); if(!q.empty()){ data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task2:\"\u003c\u003cdata\u003c\u003cstd::endl; } } } void task3(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); if (!q.empty()) { data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task3:\"\u003c\u003cdata\u003c\u003cstd::endl; } } } int main() { std::thread t1(task1); std::thread t2(task2); std::thread t3(task3); t1.join(); t2.join(); t3.join(); return 0; } 以上代码，由于直接的while(1)循环会导致cpu资源占用的非常厉害，我们可以通过延时sleep_for来进行优化，但这个延时的时间我们并不好控制！ 我们这个生产者、消费者线程，想要实现的愿景就是，当生成者生产出资源后，我们能够及时的唤醒消费者线程，让其获取资源。 但如果是简单的对生产者和消费者进行加锁来实现这一过程，可能中间会有很多过程是在消费者拿到锁后，发现生产者并没有生产出资源，而这个过程很明显就是一个无用功，那么有没有一种方式能够让生产者生产出资源后，立马通知消费者线程来读取，且在没有资源的时候，消费者线程能够阻塞让出cpu时间片呢？实现这个需求有很多种方法，而条件变量就是其中的一种！ ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:1:1","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"条件变量的用法 从C++11起，标准库开始引入条件变量。 它的成员函数也不复杂，就下面这些： 更多详细描述 void wait (unique_lock\u003cmutex\u003e\u0026 lck); 这是非模板成员函数类型，接收一个unique_lock，调用后，会帮你unlock，并且线程陷入等待状态，直到被调用notify唤醒。 有关notify的成员函数也就这两个：notify_one和notify_all。 顾名思义，随机唤醒一个，和唤醒全部处于等待被唤醒的线程。 我们再利用新学的条件变量改造下前面的代码如下： #include \u003ciostream\u003e#include \u003cmutex\u003e#include \u003cdeque\u003e#include \u003cthread\u003e#include \u003ccondition_variable\u003estd::mutex mtx; std::deque\u003cint\u003e q; std::condition_variable cv; // producer void task1(){ int i = 0; while (1){ std::unique_lock\u003cstd::mutex\u003e lock(mtx); q.push_back(i); cv.notify_one(); if (i \u003c 9999999) { i++; }else { i = 0; } } } // consumer void task2(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); if(q.empty()) { cv.wait(lock); } data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task2:\"\u003c\u003cdata\u003c\u003cstd::endl; } } void task3(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); if(q.empty()){ cv.wait(lock); } data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task3:\"\u003c\u003cdata\u003c\u003cstd::endl; } } int main() { std::thread t1(task1); std::thread t2(task2); std::thread t3(task3); t1.join(); t2.join(); t3.join(); return 0; } ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:1:2","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"条件变量引发的虚假唤醒 什么是虚假唤醒？ 前面我们写的利用条件变量写的生产者消费者线程，我可以肯定的告诉你，它是有问题的，运行起来肯定是会报错的！ 这是因为虚假唤醒的原因，那么什么是虚假唤醒呢？ 虚假唤醒的意思是，当一个正在等待条件变量的线程由于条件变量被触发而唤醒时，却发现它等待的条件（共享数据）没有满足(也就是没有共享数据)。 简而言之就是：明明当前线程已经被唤醒了，却得不到需要的数据。 虚假唤醒的产生分析： 那么我们来分析一下，上面的代码是如何发生的虚假唤醒，如果出现以下情形：task1刚好生成出一个数据到q中，而此时task2被唤醒，把数据读出后又pop掉，然后进入mutex争夺，进入阻塞或者是得到锁，按理来说，只要notify_one真的只会唤醒一个在等待的线程，那么一个生产者对应多个消费者的情况下，是不会产生虚假唤醒的。后面我多番查找资料，说是在多核处理器的环境下，notify_one可能会唤醒不止一个线程，所以会产生一个虚假唤醒，这就导致明明q是空的，却在被读取！ 如何避免虚假唤醒？ 一个简单粗暴的避免虚假唤醒的法子就是把if语句改为while语句就行，这个产生的直接作用就是，本来唤醒后会因为没有达到预期情况却还往下执行，而while的加入则确保被唤醒的线程一定要是满足预期情况！ 代码如下： #include \u003ciostream\u003e#include \u003cmutex\u003e#include \u003cdeque\u003e#include \u003cthread\u003e#include \u003ccondition_variable\u003estd::mutex mtx; std::deque\u003cint\u003e q; std::condition_variable cv; // producer void task1(){ int i = 0; while (1){ std::unique_lock\u003cstd::mutex\u003e lock(mtx); q.push_back(i); cv.notify_one(); if (i \u003c 9999999) { i++; }else { i = 0; } } } // consumer void task2(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); while (q.empty()) { cv.wait(lock); } data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task2:\"\u003c\u003cdata\u003c\u003cstd::endl; } } void task3(){ int data = 0; while (1) { std::unique_lock\u003cstd::mutex\u003e lock(mtx); while (q.empty()){ cv.wait(lock); } data = q.front(); q.pop_front(); std::cout\u003c\u003c\"Get value from que task3:\"\u003c\u003cdata\u003c\u003cstd::endl; } } int main() { std::thread t1(task1); std::thread t2(task2); std::thread t3(task3); t1.join(); t2.join(); t3.join(); return 0; } ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:1:3","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"信号量(C++20) 定义于头文件 \u003csemaphore\u003e 信号量是C++20正式加入标准库的，之前使用信号量都是直接调用Linux或者window的底层API，没有统一的接口。 信号量应该算是操作系统里面的一个概念。 具体而言： 维基百科：信号量（英语：Semaphore）又称为信号量、旗语，是一个同步对象，用于保持在0至指定最大值之间的一个计数值。当线程完成一次对该semaphore对象的等待（wait）时，该计数值减一；当线程完成一次对semaphore对象的释放（release）时，计数值加一。当计数值为0，则线程等待该semaphore对象不能成功直至该semaphore对象变成signaled状态。semaphore对象的计数值大于0，为signaled状态；计数值等于0，为nonsignaled状态. 其中，信号量又分为两种：二进制信号量和计数信号量。 对应到C++20里面的semaphore就是： std::binary_semaphore 和 counting_semaphore ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:2:0","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"std::binary_semaphore使用 其实binary_semaphore就是counting_sesmaphore的一个特化而已。 定义如下： using binary_semaphore = std::counting_semaphore\u003c1\u003e; 讲信号量使用前，我们需要讲讲它的基本运用场景，它一般不使用在存在资源竞争的多线程情况下，比如之前的生产者消费者线程，用信号量是非常不适合的。 比较适合的情况是：某些线程需要在满足某个情况后被通知执行，有点类似于Qt的信号槽机制。 以下有个使用示例： 以下代码已经还有充分的注释了，具体而言就是可以通过release方法让计数器+1，从而使得信号量状态发生改变，由于binary_semaphore只有0和1两个状态，当状态为1的时候，会使得被阻塞的线程激活，而被激活后会立马把状态-1为0，使得其他线程还是被阻塞状态，所以binary的信号量只能通知一个线程执行任务。 以下代码定义了两个信号量，一个是从main线程传递到子线程的信号量，一个是从子线程传递到main线程的信号量。 #include \u003ciostream\u003e#include \u003cthread\u003e#include \u003cchrono\u003e#include \u003csemaphore\u003e // global binary semaphore instances // object counts are set to zero // objects are in non-signaled state std::binary_semaphore smphSignalMainToThread{0}, smphSignalThreadToMain{0}; void ThreadProc() { // wait for a signal from the main proc // by attempting to decrement the semaphore smphSignalMainToThread.acquire(); // this call blocks until the semaphore's count // is increased from the main proc std::cout \u003c\u003c \"[thread] Got the signal\\n\"; // response message // wait for 3 seconds to imitate some work // being done by the thread using namespace std::literals; std::this_thread::sleep_for(3s); std::cout \u003c\u003c \"[thread] Send the signal\\n\"; // message // signal the main proc back smphSignalThreadToMain.release(); } int main() { // create some worker thread std::thread thrWorker(ThreadProc); std::cout \u003c\u003c \"[main] Send the signal\\n\"; // message // signal the worker thread to start working // by increasing the semaphore's count smphSignalMainToThread.release(); // wait until the worker thread is done doing the work // by attempting to decrement the semaphore's count smphSignalThreadToMain.acquire(); std::cout \u003c\u003c \"[main] Got the signal\\n\"; // response message thrWorker.join(); } ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:2:1","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["C++多线程"],"content":"counting_semaphore使用 原理与binary版本完全一致只是状态不只是0和1，它能够自定义上限的状态，如下代码，我将上限定为了3，那么release调用的时候可以设置最多+3，那么它就能成功唤醒三个线程. #include \u003ciostream\u003e#include \u003cthread\u003e#include \u003cchrono\u003e#include \u003csemaphore\u003e // global binary semaphore instances // object counts are set to zero // objects are in non-signaled state std::counting_semaphore\u003c3\u003e smphSignalMainToThread{0}, smphSignalThreadToMain{0}; void ThreadProc2() { // wait for a signal from the main proc // by attempting to decrement the semaphore smphSignalMainToThread.acquire(); // this call blocks until the semaphore's count // is increased from the main proc std::cout \u003c\u003c \"[thread] Got the signal2\\n\"; // response message // wait for 3 seconds to imitate some work // being done by the thread using namespace std::literals; std::this_thread::sleep_for(3s); std::cout \u003c\u003c \"[thread] Send the signal2\\n\"; // message // signal the main proc back smphSignalThreadToMain.release(); } void ThreadProc1() { // wait for a signal from the main proc // by attempting to decrement the semaphore smphSignalMainToThread.acquire(); // this call blocks until the semaphore's count // is increased from the main proc std::cout \u003c\u003c \"[thread] Got the signal1\\n\"; // response message // wait for 3 seconds to imitate some work // being done by the thread using namespace std::literals; std::this_thread::sleep_for(3s); std::cout \u003c\u003c \"[thread] Send the signal1\\n\"; // message // signal the main proc back smphSignalThreadToMain.release(); } void ThreadProc3() { // wait for a signal from the main proc // by attempting to decrement the semaphore smphSignalMainToThread.acquire(); // this call blocks until the semaphore's count // is increased from the main proc std::cout \u003c\u003c \"[thread] Got the signal3\\n\"; // response message // wait for 3 seconds to imitate some work // being done by the thread using namespace std::literals; std::this_thread::sleep_for(3s); std::cout \u003c\u003c \"[thread] Send the signal3\\n\"; // message // signal the main proc back smphSignalThreadToMain.release(); } int main() { // create some worker thread std::thread thrWorker1(ThreadProc1); std::thread thrWorker2(ThreadProc2); std::cout \u003c\u003c \"[main] Send the signal\\n\"; // message // signal the worker thread to start working // by increasing the semaphore's count smphSignalMainToThread.release(3); // wait until the worker thread is done doing the work // by attempting to decrement the semaphore's count smphSignalThreadToMain.acquire(); std::cout \u003c\u003c \"[main] Got the signal\\n\"; // response message thrWorker1.join(); thrWorker2.join(); } ","date":"2022-03-27","objectID":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/:2:2","tags":["1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)"],"title":"1.3-多线程控制的另一种姿势-条件变量(condition_variable), 信号量(semaphore)","uri":"/posts/1.3-%E9%94%81%E9%87%8A%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%A7%BF%E5%8A%BF-%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8Fcondition_variable-%E4%BF%A1%E5%8F%B7%E9%87%8Fsemaphore/"},{"categories":["现代C++语法"],"content":"利用string_view优化C++的string","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"一、背景 在日常C/C++编程中，我们常进行数据的传递操作，比如，将数据传给函数。当数据占用的内存较大时，减少数据的拷贝可以有效提高程序的性能。在C中指针是完成这一目的的标准数据结构，而C++引入了安全性更高的引用类型。所以在C++中若传递的数据仅仅只读，const string\u0026成了C++**的天然的方式。但这并非完美，从实践来看，它至少有以下几方面问题： 字符串字面值、字符数组、字符串指针的传递仍要数据拷贝 这三类低级数据类型与string类型不同，传入时，编译器需要做隐式转换，即需要拷贝这些数据生成string临时对象。const string\u0026指向的实际上是这个临时对象。通常字符串字面值较小，性能损耗可以忽略不计；但字符串指针和字符数组某些情况下可能会比较大（比如读取文件的内容），此时会引起频繁的内存分配和数据拷贝，会严重影响程序的性能。 substr O(n) 复杂度 这是一个特别常用的函数，好在std::string提供了这个函数，美中不足的是其每次都返回一个新生成的子串，很容易引起性能热点。实际上我们本意并不是要改变原字符串，为什么不在原字符串基础上返回呢？ 在C++17中引入了string_view，能很好的解决以上两个问题。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:1:0","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"二、std::string_view 从名字出发，我们可以类比数据库视图，view表示该类型不会为数据分配存储空间，而且该数据类型只能用来读。该数据类型可通过{数据的起始指针，数据的长度}两个元素表示，实际上该数据类型的实例不会具体存储原数据，仅仅存储指向的数据的起始指针和长度，所以这个开销是非常小的。 要使用字符串视图，需要引入\u003cstring_view\u003e，下面介绍该数据类型主要的API。这些API基本上都有constexpr修饰，所以能在编译时很好地处理字符串字面值，从而提高程序效率。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:2:0","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"2.1 构造函数 constexpr string_view() noexcept; constexpr string_view(const string_view\u0026 other) noexcept = default; constexpr string_view(const CharT* s, size_type count); constexpr string_view(const CharT* s); 唯一需要说明的是：为什么我们代码string_view foo(string(\"abc\"))可以编译通过，但为什么没有对应的构造函数？ 实际上这是因为string类重载了string到string_view的转换操作符： operator std::basic_string_view\u003cCharT, Traits\u003e() const noexcept; ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:2:1","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"2.2 自定义字面量 自定义字面量也是C++17新增的特性，提高了常量的易读。 下面的代码取值cppreference，能很好地说明自定义字面值和字符串语义的差异。 #include \u003cstring_view\u003e#include \u003ciostream\u003e int main() { using namespace std::literals; std::string_view s1 = \"abc\\0\\0def\"; std::string_view s2 = \"abc\\0\\0def\"sv; std::cout \u003c\u003c \"s1: \" \u003c\u003c s1.size() \u003c\u003c \" \\\"\" \u003c\u003c s1 \u003c\u003c \"\\\"\\n\"; std::cout \u003c\u003c \"s2: \" \u003c\u003c s2.size() \u003c\u003c \" \\\"\" \u003c\u003c s2 \u003c\u003c \"\\\"\\n\"; } 输出： s1: 3 \"abc\" s2: 8 \"abc^@^@def\" 以上例子能很好看清二者的语义区别，\\0对于字符串而言，有其特殊的意义，即表示字符串的结束，字符串视图根本不care，它关心实际的字符个数。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:2:2","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"2.3 成员函数 下面列举其成员函数：忽略了函数的返回值，若函数有重载，括号内用...填充。这样可以对其有个整体轮廓。 // 迭代器 begin() end() cbegin() cend() rbegin() rend() crbegin() crend() // 容量 size() length() max_size() empty() // 元素访问 operator[](size_type pos) at(size_type pos) front() back() data() // 修改器 remove_prefix(size_type n) remove_suffix(size_type n) swap(basic_string_view\u0026 s) copy(charT* s, size_type n, size_type pos = 0) string_view substr(size_type pos = 0, size_type n = npos) compare(...) starts_with(...) ends_with(...) find(...) rfind(...) find_first_of(...) find_last_of(...) find_first_not_of(...) find_last_not_of(...) 从函数列表来看，几乎跟string的只读函数一致，使用string_view的方式跟string基本一致。有几个地方需要特别说明： string_view的substr函数的时间复杂度是O(1)，解决了背景部分的第二个问题。 修改器中的三个函数仅会修改string_view的数据指向，不会修改指向的数据。 除此之外，函数名基本是自解释的。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:2:3","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"2.4 示例 Haskell 中有一个常用函数lines，会将字符串切割成行存储在容器里。下面我们用 C++ 来实现 string-版本 #include \u003cstring\u003e#include \u003ciostream\u003e#include \u003cvector\u003e#include \u003calgorithm\u003e#include \u003csstream\u003e void lines(std::vector\u003cstd::string\u003e \u0026lines, const std::string \u0026str) { auto sep{\"\\n\"}; size_t start{str.find_first_not_of(sep)}; size_t end{}; while (start != std::string::npos) { end = str.find_first_of(sep, start + 1); if (end == std::string::npos) end = str.length(); lines.push_back(str.substr(start, end - start)); start = str.find_first_not_of(sep, end + 1); } } 上面我们用const std::string \u0026类型接收待分割的字符串，若我们传入指向较大内存的字符指针时，会影响程序效率。 使用std::string_view可以避免这种情况： string_view-版本 #include \u003cstring\u003e#include \u003ciostream\u003e#include \u003cvector\u003e#include \u003calgorithm\u003e#include \u003csstream\u003e#include \u003cstring_view\u003e void lines(std::vector\u003cstd::string\u003e \u0026lines, std::string_view str) { auto sep{\"\\n\"}; size_t start{str.find_first_not_of(sep)}; size_t end{}; while (start != std::string_view::npos) { end = str.find_first_of(sep, start + 1); if (end == std::string_view::npos) end = str.length(); lines.push_back(std::string{str.substr(start, end - start)}); start = str.find_first_not_of(sep, end + 1); } } 上面的例子仅仅是把string类型修改成了string_view就获得了性能上的提升。一般情况下，将程序中的string换成string_view的过程是比较直观的，这得益于两者的成员函数的相似性。但并不是所有的“翻译”过程都是这样的，比如： void lines(std::vector\u003cstd::string\u003e \u0026lines, const std::string\u0026 str) { std::stringstream ss(str); std::string line; while (std::getline(ss, line, '\\n')) { lines.push_back(line); } } 这个版本使用stringstream实现lines函数。由于stringstream没有相应的构造函数接收string_view类型参数，所以没法采用直接替换的方式，所以翻译过程要复杂点。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:2:4","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"三、使用陷阱 世上没有免费的午餐。不恰当的使用string_view也会带来一系列的问题。 string_view范围内的字符可能不包含\\0 如： #include \u003ciostream\u003e#include \u003cstring_view\u003e int main() { std::string_view str{\"abc\", 1}; std::cout \u003c\u003c str.data() \u003c\u003c std::endl; return 0; } 本来是要打印a，但输出了abc。这是因为字符串相关的函数都有一条兼容C的约定：\\0代表字符串的结尾。上面的程序打印从开始到字符串结束的所有字符，虽然str包含的有效字符是a，但cout认\\0。好在这块内存空间有合法的字符串结尾符，如果str指向的是一个没有\\0的字符数组，程序很有可能会出现内存问题，所以我们在将string_view类型的数据传入接收字符串的函数时要非常小心。 从[const] char*构造string_view对象时间复杂度O(n) 这是因为获取字符串的长度需要从头开始遍历。如果对[const] char*类型仅仅是一些O(1)的操作，相比直接使用[const] char*，转为string_view是没有性能优势的。只不过是相比const string\u0026，string_view少了拷贝的损耗。实际上我们完全可以用[const] char*接收所有的字符串，但这个类型太底层了，不便使用。在某些情况下，我们转为string_view可能仅仅是想用其中的一些函数，比如substr。 string_view指向的内容的生命周期可能比其本身短 string_view并不拥有其指向内容的所有权，用Rust的术语来说，它仅仅是暂时borrow（借用）了它。如果拥有者提前释放了，你还在使用这些内容，那会出现内存问题，这跟悬挂指针(dangling pointer)或悬挂引用（dangling references）很像。Rust专门有套机制在编译时分析变量的生命期，保证borrow的资源在使用期间不会被释放，但C++没有这样的检查，需要人工保证。下面列出一些典型的问题情况： std::string_view sv = std::string{\"hello world\"}; string_view foo() { std::string s{\"hello world\"}; return string_view{s}; } auto id(std::string_view sv) { return sv; } int main() { std::string s = \"hello\"; auto sv = id(s + \" world\"); } ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:3:0","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"四、测试具体性能 光说不用假把式，直接就地写个测试： #include \u003cstring\u003e#include \u003cstring_view\u003e#include \"../BenchMark/Timer.h\"#include \u003cvector\u003eusing std::vector; using std::string; using std::string_view; //TODO string_view 和 string const\u0026对比。 // 发现string_view在substr()方法和vector的push_back过程中有巨大优势！ std::vector\u003cstring\u003e testStringView(string_view s){ s.substr(0); vector\u003cstring \u003eret; ret.emplace_back(s); return ret; } std::vector\u003cstring\u003e testString(string const\u0026s){ s.substr(0); vector\u003cstring \u003eret; ret.emplace_back(s); return ret; } int main() { //TEST string const\u0026 { Timer t; for (int i = 0; i \u003c 1000000; ++i) { testString(string (100,'0')); } } //TEST string_view { Timer t; for (int i = 0; i \u003c 1000000; ++i) { testStringView(string(100,'0')); } } return 0; } 性能差距： 除了以上代码，我还经过多次测试，发现在用到substr成员函数以及进行各种拷贝的时候，string_view会比string const\u0026快很多。其余情况差不太多。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:4:0","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["现代C++语法"],"content":"五、总结 string_view解决了一些痛点，但同时也引入了指针和引用的一些老问题。C++标准并没有对这个类型做太多的约束，这引来的问题是我们可以像平常的变量一样以多种方式使用它，如，可以传参，可以作为函数返回值，可以做普通变量，甚至我们可以放到容器里。随着使用场景的复杂，人工是很难保证指向的内容的生命周期足够长。所以，推荐的使用方式：仅仅作为函数参数，因为如果该参数仅仅在函数体内使用而不传递出去，这样使用是安全的。 ","date":"2022-03-24","objectID":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/:5:0","tags":["利用string_view优化string"],"title":"","uri":"/posts/%E5%88%A9%E7%94%A8string_view%E4%BC%98%E5%8C%96%E7%8E%B0%E4%BB%A3c++%E8%AF%AD%E6%B3%95/"},{"categories":["手写数据结构"],"content":"通过阅读Redis源码简单实现跳表","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"什么是跳表？ 想要弄清这个，可以查看一篇大佬的文章，把跳表分析的非常透彻，并且剖析了Redis源码，我这里只讲解不带span的Redis源码C++复现。（后续会有带span的完美Redis源码C++复刻） 大佬的讲解 如果想查看Redis源码的各位，可以点进这个链接https://github1s.com/redis/redis/blob/unstable/src/t_zset.c ","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:1:0","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"正式实现 ","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:2:0","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"跳表的创建 Redis实现 跳表结构：sds类型是Redis内部实现的字符串 创建函数，这时前面定义的level[]类型的优势就体现出来了，在C中这个类型算是未完成的类型，所以需要根据你给它分配的内存来进行具体的使用，没有分配内存前，你可以sizeof(zskiplistNode);试一试，你会发现level不计内存！ 销毁函数 cpp实现 struct SkiplistNode { // string ele; 此题不需要维护元素字段，所以舍去 double score; SkiplistNode *backward; struct SkiplistLevel { struct SkiplistNode *forward; // unsigned long span;此题不需要维护跨度，所以省去 }* level; //TODO 构造和析构 SkiplistNode(int level,double score):level(new SkiplistLevel[level]) ,score(score),backward(nullptr){} ~SkiplistNode(){ delete[] level; } }; class Skiplist { struct SkiplistNode *header, *tail; unsigned long length; //当前跳表中的元素个数 int level; //当前跳表中最大表的高度 public: /** * 构造函数和析构函数 */ Skiplist():level(1),length(0),tail(nullptr){ header = new SkiplistNode(Skiplist_MAXLEVEL,0); for (int j = 0; j \u003c Skiplist_MAXLEVEL; j++) { header-\u003elevel[j].forward = nullptr; } header-\u003ebackward = nullptr; } ~Skiplist(){ SkiplistNode* node = header, *next; while(node) { next = node-\u003elevel[0].forward; delete node; node = next; } } }; ","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:2:1","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"Insert插入元素 Redis实现 cpp实现 带span和ele的1:1还原 SkiplistNode* Skiplist::Insert(double score,const string\u0026 ele){ SkiplistNode *update[Skiplist_MAXLEVEL], *x; unsigned int rank[Skiplist_MAXLEVEL]; int i,level; //TODO part1：找到需要插入位置的前一个跳表节点，顺便更新update数组（存储着经过路径中的每个层级最多走到了哪个节点(用于连接新节点的每个层级的forward和更新span)）和rank数组（存储着路径中经过的层级节点到header的长度） x = header; for (i = this-\u003elevel-1; i \u003e= 0; i--) { rank[i] = i == (this-\u003elevel-1) ? 0 : rank[i+1]; cout\u003c\u003crank[i]\u003c\u003cendl; while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score || (x-\u003elevel[i].forward-\u003escore == score \u0026\u0026 x-\u003elevel[i].forward-\u003eele\u003cele))) { rank[i] += x-\u003elevel[i].span; x = x-\u003elevel[i].forward; } update[i] = x; } //TODO 生成节点内存，如果随机生成的节点拥有的层级比当前最高的节点还高，则需要把update数组和rank数组中高于当前level的部分看作是前一个节点是header来更新 level = RandomLevel(); if (level \u003e this-\u003elevel) { for (i = this-\u003elevel; i \u003c level; i++) { rank[i] = 0; update[i] = this-\u003eheader; update[i]-\u003elevel[i].span = this-\u003elength; } this-\u003elevel = level; } x = new SkiplistNode(level,score,ele); //TODO 连接操作，连接的同时把上一个节点的span给转移给我，如果该层级的上一个节点就紧挨着那么可直接转移，否则根据update[i]-\u003elevel[i].span - (rank[0] - rank[i]);来更新，实际上也包含了前一种情况 for (i = 0; i \u003c level; i++) { x-\u003elevel[i].forward = update[i]-\u003elevel[i].forward; update[i]-\u003elevel[i].forward = x; /* update span covered by update[i] as x is inserted here */ x-\u003elevel[i].span = update[i]-\u003elevel[i].span - (rank[0] - rank[i]); update[i]-\u003elevel[i].span = (rank[0] - rank[i]) + 1; } //TODO 最后的善后操作，1.前面高于它的节点，会因为它的产生而使得它们的span+1. 2.更新backward指针，如果不是队尾则需要更新后面的backward，如果插入的是队尾，则更新tail指针 3.更新length /** * 更新插入的节点未能到达的元素的span+1 */ for (i = level; i \u003c this-\u003elevel; i++) { update[i]-\u003elevel[i].span++; } /** * 更新backward，以及判断插入元素是否是队尾，如果是，则更新tail指针 */ x-\u003ebackward = (update[0] == this-\u003eheader) ? nullptr : update[0]; if (x-\u003elevel[0].forward) x-\u003elevel[0].forward-\u003ebackward = x; else this-\u003etail = x; this-\u003elength++; return x; } 不带span和ele SkiplistNode* Skiplist::Insert(double score){ SkiplistNode *update[Skiplist_MAXLEVEL], *x; int i,level; //TODO part1：找到需要插入位置的前一个跳表节点，顺便更新update数组(存储着经过路径中的每个层级最多走到了哪个节点(用于连接新节点的每个层级的forward)) x = this-\u003eheader; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward\u0026\u0026 x-\u003elevel[i].forward-\u003escore \u003c score) { x = x-\u003elevel[i].forward; } update[i] = x; } //TODO 生成节点内存，如果随机生成的节点拥有的层级比当前最高的节点还高，则需要把update数组中高于当前level的部分看作是前一个节点是header来更新 level = RandomLevel(); if (level \u003e this-\u003elevel) { for (i = this-\u003elevel; i \u003c level; i++) { update[i] = this-\u003eheader; } this-\u003elevel = level; } x = new SkiplistNode(level,score); //TODO 连接操作 for (i = 0; i \u003c level; i++) { x-\u003elevel[i].forward = update[i]-\u003elevel[i].forward; update[i]-\u003elevel[i].forward = x; } /** * 更新backward，以及判断插入元素是否是队尾，如果是，则更新tail指针 */ x-\u003ebackward = (update[0] == this-\u003eheader) ? nullptr : update[0]; if (x-\u003elevel[0].forward) x-\u003elevel[0].forward-\u003ebackward = x; else this-\u003etail = x; this-\u003elength++; return x; } ","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:2:2","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"Delete删除元素 Redis实现 delete的helper函数 正式delete cpp实现 1:1还原实现 /** * 该函数处理传入的update数组，并更新删除x节点后的指针连接和span值 * @param zsl 需要处理的跳表 * @param x 需要删除的节点 * @param update 需要处理的的update数组 */ static void Skiplist::DeleteNode(Skiplist *zsl, SkiplistNode *x, SkiplistNode **update) { int i; for (i = 0; i \u003c zsl-\u003elevel; i++) { if (update[i]-\u003elevel[i].forward == x) {//TODO 如果该层级的节点的后一个节点就是x节点，那么删除x节点后span会增加，forward要更新 update[i]-\u003elevel[i].span += x-\u003elevel[i].span - 1; update[i]-\u003elevel[i].forward = x-\u003elevel[i].forward; } else {//TODO 如果后一个节点不是x节点，那么就仅仅只是跨度上减少1而已 update[i]-\u003elevel[i].span -= 1; } } //TODO 和之前的插入处理相同，如果删除的节点是末尾节点，则需要更新tail指针，如果不是，则需要更新backward指针 if (x-\u003elevel[0].forward) { x-\u003elevel[0].forward-\u003ebackward = x-\u003ebackward; } else { zsl-\u003etail = x-\u003ebackward; } while(zsl-\u003elevel \u003e 1 \u0026\u0026 zsl-\u003eheader-\u003elevel[zsl-\u003elevel-1].forward == NULL) zsl-\u003elevel--; zsl-\u003elength--; } /** * 根据score和ele删除节点 * @param score 传入的分值标识 * @param ele 传入的元素字符串标识 * @param node 用于选择是否要传出node，而不是就地删除 * @return 0表示节点未找到，1表示删除处理成功 */ int Skiplist::Delete( double score, string ele, SkiplistNode **node = nullptr) { SkiplistNode *update[Skiplist_MAXLEVEL], *x; int i; x = this-\u003eheader; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score || (x-\u003elevel[i].forward-\u003escore == score \u0026\u0026 x-\u003elevel[i].forward-\u003eele\u003cele))) { x = x-\u003elevel[i].forward; } update[i] = x; } /** * 我们可能有多个相同分数的元素，我们需要找到同时具有正确分数和对象的元素。 */ x = x-\u003elevel[0].forward; if (x \u0026\u0026 score == x-\u003escore \u0026\u0026 x-\u003eele==ele) { DeleteNode(this, x, update);//TODO 处理update数组上的指针连接 if (!node)//TODO 如果外界不需要接住node来进行处理，则该node就地销毁，否则资源传出到外界 delete x; else *node = x; return 1; } return 0; /* not found */ } 仅包含score的还原 /** * 删除节点后的处理过程 * @param zsl 需要处理的跳表 * @param x 需要删除的节点 * @param update 需要处理的update数组 */ static void Skiplist::DeleteHelper(Skiplist *zsl, SkiplistNode *x, SkiplistNode **update) { assert(zsl!= nullptr\u0026\u0026x!= nullptr\u0026\u0026update!= nullptr); int i; for (i = 0; i \u003c zsl-\u003elevel; i++) { if (update[i]-\u003elevel[i].forward == x) { update[i]-\u003elevel[i].forward = x-\u003elevel[i].forward; } } //TODO 和之前的插入处理相同，如果删除的节点是末尾节点，则需要更新tail指针，如果不是，则更新backward指针 if (x-\u003elevel[0].forward) { x-\u003elevel[0].forward-\u003ebackward = x-\u003ebackward; } else { zsl-\u003etail = x-\u003ebackward; } //TODO 删除的节点可能是最高的高度，由于可能存在多个相同的高度，所以我们不能直接判断 // 可从头节点的forward指针是否为空来确认高度是否需要下降 while(zsl-\u003elevel \u003e 1 \u0026\u0026 zsl-\u003eheader-\u003elevel[zsl-\u003elevel-1].forward == nullptr) zsl-\u003elevel--; zsl-\u003elength--; } /** * 根据score和ele删除节点 * @param score 传入的分值标识 * @param node 用于选择是否要传出node，而不是就地删除 * @return 0表示节点未找到，1表示删除处理成功 */ int Skiplist::Delete( double score,SkiplistNode **node = nullptr) { SkiplistNode *update[Skiplist_MAXLEVEL], *x; int i; /** * 查找并更新update数组 */ x = this-\u003eheader; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score)) { x = x-\u003elevel[i].forward; } update[i] = x; } x = x-\u003elevel[0].forward; if (x \u0026\u0026 score == x-\u003escore) { DeleteHelper(this, x, update);//TODO 处理update数组上的指针连接 if (!node)//TODO 如果外界不需要接住node来进行处理，则该node就地销毁，否则资源传出到外界 delete x; else *node = x; return 1; } return 0; /* not found */ } ","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:2:3","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["手写数据结构"],"content":"正式解题 题目链接 前面的增删弄懂了，这个跳表的各种查找也就不在话下了，现在可以正式解题了。 效率还是比较nice的！ 解题源码： #define Skiplist_MAXLEVEL 32 /* Should be enough for 2^64 elements */#define Skiplist_P 0.25 /* Skiplist P = 1/4 */ struct SkiplistNode { // string ele; 此题不需要维护元素字段，所以舍去 double score; SkiplistNode *backward; struct SkiplistLevel { struct SkiplistNode *forward; // unsigned long span;此题不需要维护跨度，所以省去 }* level; SkiplistNode(int level,double score):level(new SkiplistLevel[level]) ,score(score),backward(nullptr){} ~SkiplistNode(){ delete[] level; } }; class Skiplist { public: struct SkiplistNode *header, *tail; unsigned long length; //当前跳表中的元素个数 int level; //当前跳表中最大表的高度 private: /** * * @return 返回随机产生的level高度 */ static int RandomLevel() {//TODO 根据rand()和掩码相与得到对应的随机值(0,0xffff)来产生level int level = 1; while ((rand()\u00260xFFFF) \u003c (Skiplist_P * 0xFFFF)) level += 1; return (level\u003cSkiplist_MAXLEVEL) ? level : Skiplist_MAXLEVEL; } /** * 删除节点后的处理过程 * @param zsl 需要处理的跳表 * @param x 需要删除的节点 * @param update 需要处理的update数组 */ static void DeleteHelper(Skiplist *zsl, SkiplistNode *x, SkiplistNode **update) { assert(zsl!= nullptr\u0026\u0026x!= nullptr\u0026\u0026update!= nullptr); int i; for (i = 0; i \u003c zsl-\u003elevel; i++) { if (update[i]-\u003elevel[i].forward == x) { update[i]-\u003elevel[i].forward = x-\u003elevel[i].forward; } } //TODO 和之前的插入处理相同，如果删除的节点是末尾节点，则需要更新tail指针，如果不是，则更新backward指针 if (x-\u003elevel[0].forward) { x-\u003elevel[0].forward-\u003ebackward = x-\u003ebackward; } else { zsl-\u003etail = x-\u003ebackward; } //TODO 删除的节点可能是最高的高度，由于可能存在多个相同的高度，所以我们不能直接判断 // 可从头节点的forward指针是否为空来确认高度是否需要下降 while(zsl-\u003elevel \u003e 1 \u0026\u0026 zsl-\u003eheader-\u003elevel[zsl-\u003elevel-1].forward == nullptr) zsl-\u003elevel--; zsl-\u003elength--; } public: /** * 构造函数和析构函数 */ Skiplist():level(1),length(0),tail(nullptr){ header = new SkiplistNode(Skiplist_MAXLEVEL,0); for (int j = 0; j \u003c Skiplist_MAXLEVEL; j++) { header-\u003elevel[j].forward = nullptr; } header-\u003ebackward = nullptr; } ~Skiplist(){ SkiplistNode* node = header, *next; while(node) { next = node-\u003elevel[0].forward; delete node; node = next; } } /** * 插入元素 * @param score 用于定位的score * @param ele 存储的元素 * @return 被插入元素的指针 */ SkiplistNode* Insert(double score){ SkiplistNode *update[Skiplist_MAXLEVEL], *x; int i,level; //TODO part1：找到需要插入位置的前一个跳表节点，顺便更新update数组(存储着经过路径中的每个层级最多走到了哪个节点(用于连接新节点的每个层级的forward)) x = header; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward\u0026\u0026 x-\u003elevel[i].forward-\u003escore \u003c score) { x = x-\u003elevel[i].forward; } update[i] = x; } //TODO 生成节点内存，如果随机生成的节点拥有的层级比当前最高的节点还高，则需要把update数组中高于当前level的部分看作是前一个节点是header来更新 level = RandomLevel(); if (level \u003e this-\u003elevel) { for (i = this-\u003elevel; i \u003c level; i++) { update[i] = this-\u003eheader; } this-\u003elevel = level; } x = new SkiplistNode(level,score); //TODO 连接操作 for (i = 0; i \u003c level; i++) { x-\u003elevel[i].forward = update[i]-\u003elevel[i].forward; update[i]-\u003elevel[i].forward = x; } /** * 更新backward，以及判断插入元素是否是队尾，如果是，则更新tail指针 */ x-\u003ebackward = (update[0] == this-\u003eheader) ? nullptr : update[0]; if (x-\u003elevel[0].forward) x-\u003elevel[0].forward-\u003ebackward = x; else this-\u003etail = x; this-\u003elength++; return x; } /** * 根据score和ele删除节点 * @param score 传入的分值标识 * @param node 用于选择是否要传出node，而不是就地删除 * @return 0表示节点未找到，1表示删除处理成功 */ int Delete( double score,SkiplistNode **node = nullptr) { SkiplistNode *update[Skiplist_MAXLEVEL], *x; int i; /** * 查找并更新update数组 */ x = this-\u003eheader; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score)) { x = x-\u003elevel[i].forward; } update[i] = x; } x = x-\u003elevel[0].forward; if (x \u0026\u0026 score == x-\u003escore) { DeleteHelper(this, x, update);//TODO 处理update数组上的指针连接 if (!node)//TODO 如果外界不需要接住node来进行处理，则该node就地销毁，否则资源传出到外界 delete x; else *node = x; return 1; } return 0; /* not found */ } //本题的函数接口 bool search(int target) const { double score = target; SkiplistNode* x; int i; x = this-\u003eheader; for (i = this-\u003elevel-1; i \u003e= 0; i--) { while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score)) { x = x-\u003elevel[i].forward; } } auto val = x-\u003elevel[0].forward; if(val) return val-\u003escore==score; return false; } void add(int num) { In","date":"2022-03-19","objectID":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/:3:0","tags":["通过阅读Redis源码简单实现跳表"],"title":"通过阅读Redis源码简单实现跳表","uri":"/posts/%E9%98%85%E8%AF%BBredis%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%B7%B3%E8%A1%A8/"},{"categories":["C++底层原理"],"content":"宏和模板的对比——预编译和编译的较量","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":" 本文默认你已经拥有基本的gcc编译选项知识，如果没有，可以看看这篇文章 程序的编译过程gcc版。 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:0:0","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"从预编译的角度对比宏定义和模板 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:1:0","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"来测测宏定义 大家都知道，宏定义仅仅只作用于文本的替换，在预编译的时候，把用到宏定义的部分替换为真正的文本而已，缺点就是不会做类型检查，只要语法能过编译就行。 我们定义一个宏来求两者之间的最大值，为了防止预编译的代码太过冗长不易看懂，没有用 include 去包含其他函数库的声明。 代码如下： #define MAX_VALUE(_1,_2) ((_1\u003e_2)?_1:_2) int main(){ MAX_VALUE(1,2); } 经过 gcc -E 命令后得到预处理后的代码如下： # 0 \".\\\\test_template.cpp\" # 0 \"\u003cbuilt-in\u003e\" # 0 \"\u003ccommand-line\u003e\" # 1 \".\\\\test_template.cpp\" # 10 \".\\\\test_template.cpp\" int main(){ ((1\u003e2)?1:2); } 观察以上代码，我们发现，确实是直接的文本替换，前面的宏定义代码都不见了，只有，传入的替换文本了。 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:1:1","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"宏定义的害处 如果我们把下面这段代码传入到宏定义中： #define MAX_VALUE(_1,_2) ((_1\u003e_2)?_1:_2) int main(){ MAX_VALUE(\"abdcdf\",\"fdf\"); } 很明显，这个代码是可以过编译的，但我们肯定不想直接的如下的方式进行字符串的比较，这样的比较毫无意义，只不过是比较的地址而已。 # 0 \".\\\\test_template.cpp\" # 0 \"\u003cbuilt-in\u003e\" # 0 \"\u003ccommand-line\u003e\" # 1 \".\\\\test_template.cpp\" # 10 \".\\\\test_template.cpp\" int main(){ ((\"abdcdf\"\u003e\"fdf\")?\"abdcdf\":\"fdf\"); } 故宏定义的最大危害，就是没法对类型进行检查！这就导致无法灵活的进行优化和调整一些直接文本替换带来的副作用。 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:1:2","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"模板是否会进行预处理操作？ 源代码： template\u003ctypename T\u003e T MAX_VALUE(T _1, T _2){ if(_1\u003c_2) return _2; return _1; } int main(){ MAX_VALUE(\"abdcdf\",\"fdf\"); } 预处理后： # 0 \".\\\\test_template.cpp\" # 0 \"\u003cbuilt-in\u003e\" # 0 \"\u003ccommand-line\u003e\" # 1 \".\\\\test_template.cpp\" template\u003ctypename T\u003e T MAX_VALUE(T _1, T _2){ if(_1\u003c_2) return _2; return _1; } int main(){ MAX_VALUE(\"abdcdf\",\"fdf\"); } 我们发现模板并不会在预处理时被展开，它甚至不会和预处理的调用部分形成任何联系！ 看来模板的展开处理过程是发生在后续的编译过程中，由于本人不清楚如何反汇编和反编译过程，故没有亲身实践。 但从我的日常使用体验和大佬们总结的经验看来，模板也和宏定义的展开类似，就是简单的文本替换，但可以利用编译时期的语法检查和类型判断（type_traits技术），所以能够对不同情况的展开进行不同的处理。 比如上面的代码利用偏特化进行优化： #include \u003ciostream\u003e#include \u003ccstring\u003etemplate\u003ctypename T\u003e int MAX_VALUE(T _1, T _2){ if(_1\u003c_2) return _2; return _1; } using type = const char *; template\u003c\u003e int MAX_VALUE\u003ctype\u003e(type _1, type _2){ std::cout \u003c\u003c \"调用偏特化\"; return strcmp(_1, _2); } int main(){ MAX_VALUE(\"abdcdf\",\"fdf\"); } ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:1:3","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"善用编译期的模板 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:2:0","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"为什么大多数模板库声明和定义都放在一起？ 声明和定义分开处理的原因 我们清楚，声明和定义中，声明可以有无数份，但定义在一个项目里只能存在一份！所以我们在多文件项目的过程中，需要 include\u003cxxx.h\u003e 得到对应的声明，最后再把对应的实现 xxx.c 编译完后，再通过链接过程让声明能够连接到对应的定义来进行使用。所以在通常情况下我们都是在 .h 文件里面进行声明，再写一个 .c 文件实现定义，把声明和定义进行分开，这样无论你在其他的 .c 文件里使用多少个 include 操作，整个项目都只存在一份定义而已。故这种情况下，用 include 把多份相同的声明放在一个main函数里运行也是没问题的。 话说回来，那为什么一般不会去直接把定义和声明写在一起呢？ 很明显，定义和声明分开，无论我们怎么 include 都不会因为重复定义而产生错误，而如果我们写在一个被 include 的文件里面，则整个项目中，每次 include 都会多产生一次定义！ 为什么模板要把声明和定义写在一起？ 那既然如此，为什么写模板的时候声明和定义写在一个文件里面呢？？？ 我们前面已经清楚了，预编译命令 include 就仅仅把里面的代码拷贝到使用它的文件里去，如果我们把模板的定义和声明分开，画风大概会像下面这样： //test.h #ifndef TEST_TEMPLATE_TEST_H #define TEST_TEMPLATE_TEST_H template\u003ctypename T\u003e void print(T val); #endif //TEST_TEMPLATE_TEST_H //test.cpp #include \"test.h\"#include \u003ciostream\u003etemplate\u003ctypename T\u003e void print(T val) { std::cout\u003c\u003cval; } 不出我所料，很快就报错，内容如下： undefined reference to `void print\u003cint\u003e(int)'\r就是链接时找不到对应的函数定义，原因是模板只会在调用的时候实例化展开代码，如果 .h 只存在模板的声明，那么被include后调用，只会被实例化为以下内容： void print([实例化的类型名] val); 因为对应的.cpp文件里面，不会有任何的变更，也就是不会有对应的定义产生。 所以想要把声明和定义分文件且实现模板的实例化展开，几乎是不可能的（我目前不清楚有哪种方式可以（似乎在末尾再include定义的文件就可以但。。和直接include没区别）） 所以在写模板的时候，我们需要让使用到它的人（include调用者）把声明和定义一块给它，而防止找不到实例化展开的定义。 如何防止模板类的重复定义？ 以上描述了，为什么模板声明和定义需要写在同一个文件里，但问题又出现了，如何预防一个项目中产生多个定义呢？ 简单预防： 通过预处理的宏定义控制导入导出的代码，但这样只能够保证一个 .c文件 中不会出现两次 include 相同代码段。而无法保证其他 .c文件 include 后再次产生定义！ 如下代码，我写了一个ttt.h文件，导出一个 print 函数的声明和定义（直接定义就包含声明）。 //ttt.h #ifndef TEST_TEMPLATE_TTT_H #define TEST_TEMPLATE_TTT_H #include \u003ciostream\u003e void print(){ std::cout\u003c\u003c\"hhhh\"; } #endif //TEST_TEMPLATE_TTT_H //test.cpp #include \"ttt.h\"#include \u003ciostream\u003e//main.cpp #include \"ttt.h\"#include \u003ciostream\u003e int main() { print(); return 0; } 运行main函数时，很快就发生了重复定义的报错！ 因为我们在test.cpp文件里面也 include“ttt.h\" 这将导致重复定义跨文件的重复定义是简单的宏定义没法避免的！ 但我们又发现我们导入多次 \u003ciostream\u003e 标准库却不会出现这个问题，这是为什么呢？ 我试着点进 iostream 里面观察观察，发现写的全是声明。 再进入到更深层的include瞧一瞧看看： 好家伙，\u003cbits/c++config.h\u003e 是两千多行的宏定义！ \u003costream\u003e 和 \u003cistream\u003e 也都只是大量的类型定义和各种类的声明而已，还有我看不懂的各种模板语法的运用，追溯到 \u003cios_base.h\u003e 才终于发现各种类和方法的实现，但看起来都很短，还有各种模板技术的运用，感觉还有很多实现没有放在这个库里面。最后总之就是虽然标准库用的模板，但它用各种技术避免了声明和定义放在一个文件里面。。。或者说让文件导入的时候不会重复的去导入了。 以我目前的水平，看标准库就等于是看天书。。。 大概这就是C++劝退的原因之一吧，模板库为了防止重复定义，使得可读性变得极差，反正我是完全看不懂。。。 相对而言，Java的源代码直接就能简单上手看懂，而且比读文档还清晰，只能说C++的痛。。。 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:2:1","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++底层原理"],"content":"一些简单且常用的模板技术 template + 函数声明将模板提前实例化一份 由于我们template定义后，只有调用它的时候，才会实例化展开一份代码，但我们也能让编译器先为我们实例化一份代码，这个我也是最近才清楚有这个用法，以前从碰到过。 template\u003ctypename T\u003e int MAX_VALUE(T _1, T _2){ if(_1\u003c_2) return _2; return _1; } using type = const char *; template int MAX_VALUE(type _1, type _2); type_traits（类型萃取技术） type_traits是编译期就去确定具体的类型，而如何确定的呢，这个只需要用到模板的特化展开进行特定的标记即可。 比如写一个无符号整型的编译期类型判断工具可以像下面这样写： 原理就是利用的模板的实例化展开，再加上特定的偏特化，来对类内变量的值进行一个区分，便可实现类型的判断了。 type_traits + static_assert()的运用 我们都知道C语言有个assert()宏，用于DEBUG模式下的断言（不清楚的可以看看我的 assert源码解析），而C++也有static_assert()进行断言，这个断言比assert宏要强大很多。 assert和static_assert的对比 共同点： 都是用于断言，满足条件则正常运行，否则抛出错误信息。 不同点： assert是在运行时且为DEBUG模式，出错后调用函数来进行报错退出处理。 static_assert()则是在程序编译时，在编译时进行判断，再抛出相应信息。 type_tarits的运用 如下图： 通过type_trait可以实现只接受float类型和int类型。 ","date":"2022-03-05","objectID":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/:2:2","tags":["宏和模板的对比——预编译和编译的较量"],"title":"宏和模板的对比——预编译和编译的较量","uri":"/posts/%E5%AE%8F%E5%92%8C%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%AF%B9%E6%AF%94%E9%A2%84%E7%BC%96%E8%AF%91%E5%92%8C%E7%BC%96%E8%AF%91%E7%9A%84%E8%BE%83%E9%87%8F/"},{"categories":["C++多线程"],"content":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"创建线程(thread)分析程序报错原因 头文件：include\u003cthread\u003e ","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/:1:0","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"理解线程的创建运行过程 你可以试着运行下面这段代码，你会发现，他肯定会报错！ #include\u003cthread\u003e#include\u003ciostream\u003e void task(){ std::cout\u003c\u003c\"线程运行开始\" } int main(){ std::thread th1(task); } 为什么会报错？ Java里的多线程，创建好线程后，需要调用 run 或者 start 方法来启动这个线程。而 C++ 里的 thread 不需要这个过程，直接传值创建这个线程的内存后，会自动的开始运行这个线程（应该是利用的RAII特性在构造函数里面运行了这个线程的代码）。 但上面并不是报错的理由，在Java里面，线程创建之后会自动和主线程形成联系，start方法开始运行后，这个子线程会和主线程的生命周期一致，如果子线程后于主线程结束，那么就会阻塞主线程直到子线程结束才终止整个程序。当然也可以把子线程通过 setDaemon(true) 设置后，使得主线程和子线程直接的关系切断，这样主线程就不会再等待子线程了，这样就形成了守护线程。 好了，现在回归到上面的C++代码，说说为什么会发生错误。 同样，在C++里创建线程后，也会和主线程产生联系，但它并不会像Java一样主线程自动的等待子线程结束，这个等待结束的过程需要你自己去调用这个线程对应的方法。如果不去调用方法等待子线程结束，或者切断他们之间的联系，那程序就会Crash。这就是报错的原因了！ ","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/:1:1","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"解决报错的方式：join()和detach 为了解决上面的报错问题有两种方式解决。 ","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/:2:0","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"方法一：调用join()方法进行合并 由于主线程和子线程之间有联系，我们如果需要正常执行完这个程序，则需要保证整个线程一并结束或者是切断联系。 join() 方法的调用就是等待子线程和主线程汇聚，产生的效果就是主线程被阻塞，直到子线程结束运行，阻塞停止。 join在这里是汇聚汇集之意，你可以想象有多根线，最终结束的时候这几根线需要汇聚到一起，然后全部的线程就结束了，整个程序正常退出！ 代码如下：就仅仅在最后加入了一行join代码，程序bug被解决！ #include\u003cthread\u003e#include\u003ciostream\u003e void task(){ std::cout\u003c\u003c\"线程运行开始\"; } int main(){ std::thread th1(task); th1.join(); } ","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/:2:1","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"方法二：调用detach()方法分离线程联系 前面也讲到，由于主线程和子线程之间的联系，导致主线程需要照顾子线程。那么detach方法的调用就是切断这个联系。调用了这个方法后，主线程不会再等待子线程执行结束，而是直接正常结束，而子线程是否执行完成和我主线程没有任何关系了。 调用这个方法后的线程，可以理解为守护线程了。这种情况下内存安全要引起重视，如果子线程用到了主线程的资源，这个情况下将会导致 指针悬空 这是一个非常严重的内存安全问题！ 代码如下： #include\u003cthread\u003e#include\u003ciostream\u003e void task(){ std::cout\u003c\u003c\"线程运行开始\"; } int main(){ std::thread th1(task); th1.detach(); } ","date":"2022-03-02","objectID":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/:2:2","tags":["1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)"],"title":"1.1-创建线程(thread)、线程的汇聚(join)、线程的分离(detach)","uri":"/posts/1.1-%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8Bthread%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%B1%87%E8%81%9Ajoin%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%86%E7%A6%BBdetach/"},{"categories":["C++多线程"],"content":"1.2-线程安全的保证——互斥量(锁)和原子变量","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"资源竞争引发的线程安全问题 有如下的代码： #include\u003cthread\u003e#include\u003ciostream\u003eint globalVariable = 0; void task(){ for (int i = 0; i \u003c 1000000; ++i) { ++globalVariable; } } int main(){ std::thread th1(task); std::thread th2(task); th1.join(); th2.join(); std::cout\u003c\u003cglobalVariable; } 我们开了两个线程，一共执行了两次 task ，按理来讲 globalVariable 变量应该被加到 2000000 。事实上，你可跑以上代码进行验证，肯定是达不到 2000000 的！ 这又是怎么一回事呢？ 资源竞争的产生： 在多线程中，由于类似于并行的逻辑存在，我们可以想象一下，到 th1 调用 task 函数，且正在为 globalVariable 变量做加法操作的时候，可能此时 th2 也正在为它做加法操作，线程中也是存在对应的工作内存，不是直接更改原内存的值，而是经过 读取-\u003e执行-\u003e写入 的过程。故此时如果两个线程同时进行读取并写入，那么实际上 globalVariable 只加了1，而不是2。 故由于资源竞争的存在，导致结果小于正确的结果！ ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:1:0","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"如何解决资源竞争问题？ 正如标题所示，如何解决资源竞争问题呢？ 我们经过前面的分析，可知，资源竞争问题是因为并行逻辑的存在，扰乱了原本需要的有序逻辑。怎么理解呢，当多个线程同时处理同一个变量时是不安全的，我们只要让同时只有一个线程去处理这个变量即可。 上面所说的正是多线程的 原子性，执行一个操作的时候不会被其他的线程打断，或者说只能有一个线程在执行这个操作。而之前的代码中 ++globalVarible 这句正需要这样的原子性操作！ 而C++里面也有两类方法去实现这样的效果。 ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:2:0","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"法一：加互斥锁mutex（性能较低） 代码如下： #include\u003cthread\u003e#include\u003ciostream\u003e int globalVariable = 0; std::mutex mtx; void task(){ for (int i = 0; i \u003c 1000000; ++i) { mtx.lock(); //上锁 ++globalVariable; mtx.unlock();//解锁 } } int main(){ std::thread th1(task); std::thread th2(task); th1.join(); th2.join(); std::cout\u003c\u003cglobalVariable; } 这下终于可以正确的得到 2000000 这个结果了。 我们来讲讲互斥量解锁和上锁的原理： lock()：形象的描述就是，当调用这个方法的时候，会去互斥量里面拿取这把锁，如果这个锁已经被其他线程持有，则阻塞，直到其他线程把这把锁释放，每个互斥量都是一把相同的锁。 unlock()：字面意思，把我现在持有的锁给释放掉，这样就可以让其他因为没有拿到锁的线程停止阻塞，开始争抢这把锁，谁抢到了谁就能得到下一个CPU的时间片。 最终的结果就是哪个线程先拿下这把锁，那么其他线程再运行到这块代码的位置就会被阻塞，这就使得被上锁的区域是具有原子性的！这样就保证了线程的安全。 ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:2:1","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"法二：转用原子变量（效率更高） C++中可用模板类，把类型转为原子类型，原子变量的实现方式实际上和上锁的过程是类似，但可能由于不同编译器的实现方式，可能会调用计算机的硬件去优化这个加锁解锁的过程，所以效率会更高。 如下代码：(这时就不需要加解锁了，变量本身就是线程安全的) #include\u003cthread\u003e#include\u003ciostream\u003e std::atomic\u003cint\u003e globalVariable = 0; void task(){ for (int i = 0; i \u003c 1000000; ++i) { ++globalVariable; } } int main(){ std::thread th1(task); std::thread th2(task); th1.join(); th2.join(); std::cout\u003c\u003cglobalVariable; } ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:2:2","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"三个常用的互斥量装饰器 ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:3:0","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"std::lock_guard (C++11) 这是一个最简单的互斥量装饰器，就是简单的利用C++构造函数和析构函数的RAII特性，在构造的时候上锁和析构的时候解锁，并不会维持传入的互斥器状态。 故前面的代码我们可以改作： #include\u003cthread\u003e#include\u003ciostream\u003e int globalVariable = 0; std::mutex mtx; void task(){ for (int i = 0; i \u003c 1000000; ++i) { std::lock_guard\u003cstd::mutex\u003e lock(mtx); ++globalVariable; } } int main(){ std::thread th1(task); std::thread th2(task); th1.join(); th2.join(); std::cout\u003c\u003cglobalVariable; } std::lock_guard 还有第二个可选参数用于告知它此传入的互斥器已经被锁上，你无需再次上锁，这种主要用在上锁过程自己完成的情况下。例如很多情况我们为了防止产生死锁，需要调用 std::lock() 函数进行统一的上锁。 死锁的产生 如下代码： #include\u003cthread\u003e#include\u003ciostream\u003e#include \u003cmutex\u003e int globalVariable = 0; std::mutex mtx1; std::mutex mtx2; void task1(){ mtx1.lock(); for (int i = 0; i \u003c 10; ++i) { std::cout\u003c\u003c\"test2\"\u003c\u003c'\\n'; } mtx2.lock(); mtx1.unlock(); mtx2.unlock(); } void task2(){ mtx2.lock(); mtx1.lock(); mtx2.unlock(); mtx1.unlock(); } int main(){ std::thread th1(task1); std::thread th2(task2); th1.join(); th2.join(); std::cout\u003c\u003cglobalVariable; } 以上代码的运行结果大概率是由于死锁产生的程序阻塞。 你想想一个过程：如果 mtx1 在 th1 线程先被上锁，而与此同时 mtx2 在 th2 线程被上锁，在 th1 线程运行完 for 循环代码后，遇到将 mtx2 上锁的代码后，由于此时 th2 线程正持有此锁，而 th1 也正持有 mtx1 这样的互相持有对方所需的锁的时候，将会发生死锁现象，即两个线程都被永远的阻塞了！ 利用std::lock批量上锁防止死锁发生 以上的死锁发生的原因就是因为上锁的顺序所导致的，我们可以采取多个线程上多个锁时采用相同的顺序，便可防止死锁的发生，当然也可以直接调用标准库提供的 std::lock 函数批量上锁，来防止上锁顺序导致的死锁！ 如下代码：（lock函数批量上锁是具有原子性的，不会被其他线程打断） #include\u003cthread\u003e#include\u003ciostream\u003e#include \u003cmutex\u003e std::mutex mtx1; std::mutex mtx2; void task1(){ std::lock(mtx1,mtx2); std::lock_guard\u003cstd::mutex\u003e _1(mtx1,std::adopt_lock); //adopt_lock代表一个标志，表示已经被上锁了，别再调用lock方法了 std::lock_guard\u003cstd::mutex\u003e _2(mtx2,std::adopt_lock); for (int i = 0; i \u003c 5; ++i) { std::cout\u003c\u003c\"test1\\n\"; } } void task2(){ std::lock(mtx1,mtx2); std::lock_guard\u003cstd::mutex\u003e _1(mtx1,std::adopt_lock); //adopt_lock代表一个标志，表示已经被上锁了，别再调用lock方法了 std::lock_guard\u003cstd::mutex\u003e _2(mtx2,std::adopt_lock); for (int i = 0; i \u003c 5; ++i) { std::cout\u003c\u003c\"test2\\n\"; } } int main(){ std::thread th1(task1); std::thread th2(task2); th1.join(); th2.join(); } 代码执行结果： ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:3:1","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"std::unique_lock (C++11) 和 lock_guard 类似，也是用的 RAII 手法进行上锁和解锁。但它还会维持互斥量的状态，你可以通过传入第二个参数告诉它状态。且它是支持无参构造的。 注意：这三个装饰器只有 unique_lock 含有移动构造函数，所以你可以写一个函数简化初始化过程。他们都没有复制构造器！ 如： std::unique_lock\u003cstd::mutex\u003e lock(mtx2,std::defer_lock); 传入的 defer_lock 表示上锁过程暂时不调用，将在后面由我自己上锁。统样也支持 adopt_lock 选项表示已经上了锁。 ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:3:2","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["C++多线程"],"content":"std::scoped_lock（C++17） 这个装饰器，支持同时装饰多个互斥量，且也是通过 RAII 手法进行解锁和上锁过程。 创建 scoped_lock 对象时，它试图取得给定互斥的所有权。控制离开创建 scoped_lock 对象的作用域时，析构 scoped_lock 并以逆序释放互斥。若给出数个互斥，则使用免死锁算法，如同以 std::lock 。 scoped_lock 类不可复制。 如下代码： std::scoped_lock lock(e1.m, e2.m); // 等价代码 1 （用 std::lock 和 std::lock_guard ） // std::lock(e1.m, e2.m); // std::lock_guard\u003cstd::mutex\u003e lk1(e1.m, std::adopt_lock); // std::lock_guard\u003cstd::mutex\u003e lk2(e2.m, std::adopt_lock); // 等价代码 2 （若需要 unique_lock ，例如对于条件变量） // std::unique_lock\u003cstd::mutex\u003e lk1(e1.m, std::defer_lock); // std::unique_lock\u003cstd::mutex\u003e lk2(e2.m, std::defer_lock); // std::lock(lk1, lk2); ","date":"2022-03-02","objectID":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/:3:3","tags":["1.2-线程安全的保证——互斥量(锁)和原子变量"],"title":"1.2-线程安全的保证——互斥量mutex(锁)和原子变量atomic","uri":"/posts/1.2-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E4%BF%9D%E8%AF%81%E4%BA%92%E6%96%A5%E9%87%8F%E9%94%81%E5%92%8C%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F/"},{"categories":["算法——动态规划"],"content":"增量元素之间的最大差值——前缀dp","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["算法——动态规划"],"content":"题目 题目链接 ","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/:1:0","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["算法——动态规划"],"content":"题目解析 ","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/:2:0","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["算法——动态规划"],"content":"法一：暴力枚举 此题由于是简单题，所以直接可以暴力枚举。暴力枚举的时候我们也可以考虑优化一下，比如外层枚举 $nums[i]$ 的时候，内层直接找右边的最大值。 代码如下： class Solution { public: int maximumDifference(vector\u003cint\u003e\u0026 nums) { int n = nums.size(); int ret = INT_MIN,mx; for(int i=0;i\u003cn;i++){ mx = *max_element(nums.begin()+i,nums.begin()+n); if(mx\u003enums[i]) ret = max(ret,mx-nums[i]); } if(ret==INT_MIN)return -1; return ret; } }; ","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/:2:1","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["算法——动态规划"],"content":"法二：dp优化 很明显，时间复杂度实际上还是 $O(n^2)$ ，我们可以通过动态规划，提前求出 $nums[i]$ 之前的最小值，然后我们就可以直接通过 $nums[i]-dp_{min}[i-1]$ 求得，此时时间复杂度被优化为了 $O(n)$ ，但空间复杂度也上升到了 $O(n)$ 。 代码如下： class Solution { public: const int maxn = 0x3f3f3f3f; int maximumDifference(vector\u003cint\u003e\u0026 nums) { //计算dp值 int n = nums.size(); int dp_min[n]; memset(dp_min,0x3f,sizeof(dp_min)); dp_min[0] = nums[0]; int ret = INT_MIN; for(int i=1;i\u003cn;i++) dp_min[i] = min(dp_min[i-1],nums[i]); //得出答案 for(int i=0;i\u003cn;i++){ if(i\u003e0\u0026\u0026nums[i]\u003edp_min[i-1]){ ret = max(ret,nums[i]-dp_min[i-1]); } } if(ret==INT_MIN)return -1; return ret; } }; ","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/:2:2","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["算法——动态规划"],"content":"法三：进一步优化空间复杂度 我们在 $dp$ 求解的时候发现，我们转移的状态依赖并未跨维度，而仅仅只和上一个状态 $dp[i-1]$ 相关，所以我们实际上只需要一个变量来记录 $nums[i]$ 前的最小值，故把所有的处理放到一个循环中实现即可。 代码如下： class Solution { public: int maximumDifference(vector\u003cint\u003e\u0026 nums) { int n = nums.size(); int ret = -1, premin = nums[0]; for (int i = 1; i \u003c n; ++i) { if (nums[i] \u003e premin) { ret = max(ret, nums[i] - premin); } else { premin = nums[i]; } } return ret; } }; ","date":"2022-02-26","objectID":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/:2:3","tags":["增量元素之间的最大差值——前缀dp"],"title":"增量元素之间的最大差值——前缀dp","uri":"/posts/%E5%A2%9E%E9%87%8F%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%E5%89%8D%E7%BC%80dp/"},{"categories":["C++实战"],"content":"bitset与埃氏筛","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"bitset ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:0:0","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"介绍 std::bitset 是标准库中的一个存储 0/1 的大小不可变容器。严格来讲，它并不属于 STL。 bitset 并不属于 STL，而是一种标准库中的 “Special Container”。事实上，它作为一种容器，也并不满足 STL 容器的要求。说它是适配器，它也并不依赖于其它 STL 容器作为底层实现。——摘自《The C++ Standard Library 2nd Edition》 由于内存地址是按字节即 byte 寻址，而非比特 bit，一个 bool 类型的变量，虽然只能表示 0/1, 但是也占了 1 byte 的内存。 bitset 就是通过固定的优化，使得一个字节的八个比特能分别储存 8 位的 0/1。 对于一个 4 字节的 int 变量，在只存 0/1 的意义下，bitset 占用空间只是其 ，计算一些信息时，所需时间也是其 。 在某些情况下通过 bitset 可以优化程序的运行效率。至于其优化的是复杂度还是常数，要看计算复杂度的角度。一般 bitset 的复杂度有以下几种记法：（设原复杂度为 ） $O(n)$，这种记法认为 bitset 完全没有优化复杂度。 $O(n/32)$，这种记法不太严谨（复杂度中不应出现常数），但体现了 bitset 能将所需时间优化至 $1/32$。 $O(n/w)$，其中 $w=32$（计算机的位数），这种记法较为普遍接受。 $O(n/logw)$，其中 $w$ 为计算机一个整型变量的大小。 当然，vector 的一个特化 vector\u003cbool\u003e 的储存方式同 bitset 一样，区别在于其支持动态开空间，bitset 则和我们一般的静态数组一样，是在编译时就开好了的。 然而，bitset 有一些好用的库函数，不仅方便，而且有时可以避免使用 for 循环而没有实质的速度优化。因此，一般不使用 vector\u003cbool\u003e。 ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:1:0","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"使用¶ ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:0","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"头文件¶ #include \u003cbitset\u003e ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:1","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"指定大小¶ bitset\u003c1000\u003e bs; // a bitset with 1000 bits ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:2","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"构造函数¶ bitset(): 每一位都是 false。 bitset(unsigned long val): 设为 val 的二进制形式。 bitset(const string\u0026 str): 设为 串 str。 ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:3","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"运算符¶ operator []: 访问其特定的一位。 operator ==/!=: 比较两个 bitset 内容是否完全一样。 operator \u0026/\u0026=/|/| =/^/^=/~: 进行按位与/或/异或/取反操作。bitset 只能与 bitset 进行位运算，若要和整型进行位运算，要先将整型转换为 bitset。 operator \u003c\u003e/\u003c\u003c=/\u003e\u003e=: 进行二进制左移/右移。 operator \u003c\u003e: 流运算符，这意味着你可以通过 cin/cout 进行输入输出。 ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:4","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"成员函数¶ count(): 返回 true 的数量。 size(): 返回 bitset 的大小。 test(pos): 它和 vector 中的 at() 的作用是一样的，和 [] 运算符的区别就是越界检查。 any(): 若存在某一位是 true 则返回 true，否则返回 false。 none(): 若所有位都是 false 则返回 true，否则返回 false。 all():C++11，若所有位都是 true 则返回 true，否则返回 false。 set(): 将整个 bitset 设置成 true。 set(pos, val = true): 将某一位设置成 true/false。 reset(): 将整个 bitset 设置成 false。 reset(pos): 将某一位设置成 false。相当于 set(pos, false)。 flip(): 翻转每一位。（相当于异或一个全是 1 的 bitset） flip(pos): 翻转某一位。 to_string(): 返回转换成的字符串表达。 to_ulong(): 返回转换成的 unsigned long 表达 (long 在 NT 及 32 位 POSIX 系统下与 int 一样，在 64 位 POSIX 下与 long long 一样）。 to_ullong():C++11，返回转换成的 unsigned long long 表达。 一些文档中没有的成员函数： _Find_first(): 返回 bitset 第一个 true 的下标，若没有 true 则返回 bitset 的大小。 _Find_next(pos): 返回 pos 后面（下标严格大于 pos 的位置）第一个 true 的下标，若 pos 后面没有 true 则返回 bitset 的大小。 ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:2:5","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"应用¶ ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:3:0","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"「LibreOJ β Round #2」贪心只能过样例¶ 这题可以用 dp 做，转移方程很简单： 表示前 个数的平方和能否为 ，那么 （或起来）。 但如果直接做的话是 的，（看起来）过不了。 发现可以用 bitset 优化，左移再或起来就好了：std::bitset 然后发现……被加了几个剪枝的暴力艹了：加了几个剪枝的暴力 然而，可以手写 bitset（只需要支持左移后或起来这一种操作）压 位（unsigned long long）来艹掉暴力：手写 bitset ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:3:1","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"与埃氏筛结合 由于 bitset 快速的连续读写效率（前提是开O2优化），使得它非常适合用于与埃氏筛结合打质数表。 使用的方式也很简单，只需要将埃氏筛中的布尔数组替换成 bitset 即可。 参考代码 bitset\u003cN\u003e vis; void Prime(int n) { vis.set(); vis[0] = vis[1] = 0; for (int i = 2; i * i \u003c= n; i++) { if (vis[i]) { for (int j = i \u003c\u003c 1; j \u003c= n; j += i) vis[j] = 0; } } } ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:3:2","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["C++实战"],"content":"埃氏筛速度测试 测试代码如下： #include\u003ccstdio\u003e#include \"../BenchMark/Timer.h\"using namespace std; #define BIT_SET //通过是否定义BIT_SET宏来控制测试对象 #ifdef BIT_SET #include\u003cbitset\u003e bitset\u003c100001000\u003e vis; #else bool vis[100001000]; #endif int n, ans; #ifdef BIT_SET void Prime() { vis.set(); vis[0] = vis[1] = 0; for (int i = 2; i * i \u003c= n; i++) { if (vis[i]) { for (int j = i \u003c\u003c 1; j \u003c= n; j += i) vis[j] = 0; } } } #else void GetPrimes() { fill(vis,vis+100001000, true); vis[0] = vis[1] = 0; for (int i = 2; i * i \u003c= n; i++) { if (vis[i]) { for (int j = i \u003c\u003c 1; j \u003c= n; j += i) vis[j] = 0; } } } #endif int main() { scanf(\"%d\", \u0026n); { ans = 0; Timer c; #ifdef BIT_SET Prime(); #else GetPrimes(); #endif for (int i = 2; i \u003c= n; i++) if (vis[i]) ans++; printf(\"%d \", ans); } return 0; } 其中的计时器Timer是我封装的一个用于测试速度的类，利用的C++ RAII特性写的。 代码如下： // // Created by Alone on 2022-1-31. // #ifndef BENCHMARK_TIMER_H #define BENCHMARK_TIMER_H #include \u003cchrono\u003e#include \u003ciostream\u003eclass Timer { public: Timer(){ m_StartTimepoint = std::chrono::high_resolution_clock::now(); } ~Timer(){ Stop(); } void Stop(){ auto endTimepoint = std::chrono::high_resolution_clock::now(); auto start = std::chrono::time_point_cast\u003cstd::chrono::microseconds\u003e(m_StartTimepoint).time_since_epoch().count(); auto end = std::chrono::time_point_cast\u003cstd::chrono::microseconds\u003e(endTimepoint).time_since_epoch().count(); auto duration = end-start; //以微秒为单位 double ms = duration * 0.001;//得到毫秒 printf(\"%lld us (%lf ms)\\n\",duration,ms); } private: std::chrono::time_point\u003cstd::chrono::high_resolution_clock\u003em_StartTimepoint; }; #endif //BENCHMARK_TIMER_H 测试结果： 测试中使用的环境为mingw，使用了 $o2$ 级别的优化，所以 bitset 明显会快很多！ 在 n==1e8 的情况下，轮番三次 bool vs bitset 结果如下： 第一次：1351532 us (1351.532000 ms) vs 1171797 us (1171.797000 ms) 第二次：1814639 us (1814.639000 ms) vs 1236894 us (1236.894000 ms) 第三次：1812811 us (1812.811000 ms) vs 5761455 1188555 us (1188.555000 ms) 难道以后都优先使用bitset？别高兴的太早！在我们默认的debug模式下bitset根本就完全不是bool数组的对手，随手一测，直接就是 2397002 us (2397.002000 ms) vs 8512716 us (8512.716000 ms) bitset大败！！！ 所以，在一般的算法题情况下，写质数筛还是用bool数组吧！因为算法题的oj不会给你开优化的！故原始bool数组是最好的选择！ ","date":"2022-02-22","objectID":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/:3:3","tags":["bitset与埃氏筛"],"title":"bitset与埃氏筛","uri":"/posts/bitset%E4%B8%8E%E5%9F%83%E6%B0%8F%E7%AD%9B/"},{"categories":["算法——最短路问题"],"content":"k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造","date":"2022-02-22","objectID":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/","tags":["k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造"],"title":"k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造","uri":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/"},{"categories":["算法——最短路问题"],"content":"题目 oj平台 ","date":"2022-02-22","objectID":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/:1:0","tags":["k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造"],"title":"k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造","uri":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/"},{"categories":["算法——最短路问题"],"content":"BellmanFord算法的动态规划解决(效率一般) 看到k站内，肯定会想到 BellmanFord 算法的动态规划解法，本来优化成按边遍历的动态规划可以不用计较多少次，但这里必须要计较用了多少次，所以我们要在同一次边的选择中，保证另一个边用的是上一次的结果，故通过二维数组进行dp即可写出，要压缩成一维数组也不难，毕竟用的仅仅只是上一行的结果，所以动态规划解决是非常简单的。 class Solution { public: int findCheapestPrice(int n, vector\u003cvector\u003cint\u003e\u003e\u0026 flights, int src, int dst, int k) { const int INF = 0x3f3f3f3f; //用临时数组存储，这样改变了dist也不会改变temp,这样便达到了控制遍历次数的目的 int* dist = new int[n]; memset(dist,0x3f,sizeof(int)*n); dist[src] = 0; int sz = flights.size(); for(int i=0;i\u003c=k;i++){ int* temp = new int[n]; memcpy(temp,dist,n*sizeof(int)); for(int j=0;j\u003csz;j++){ if(temp[flights[j][0]]!=INF){ dist[flights[j][1]] = min(dist[flights[j][1]],temp[flights[j][0]] + flights[j][2]); } } delete[] temp; } return dist[dst]==INF?-1:dist[dst]; } }; ","date":"2022-02-22","objectID":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/:2:0","tags":["k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造"],"title":"k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造","uri":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/"},{"categories":["算法——最短路问题"],"content":"SPFA算法改造成为经典BFS解决(效率高) 为了效率，我建图时候用了链式前向星。 这道题开始拿到的时候，我就再想这个SPFA类似于BFS，那肯定是可以控制次数的，然后就开始行动了，SPFA队列遍历的时候需要判断该结点是否存在于队列中，如果存在，则不能入队，使用的数据都是通过 dist 数组来更新，这样导致完全丧失了BFS的遍历次数信息，使得答案更新的很快是没错，但无法控制在一定的遍历次数范围内(因为可能你本次所用到的 dist 可能不是上一次的)。 那么如何解决这个 BFS 遍历次数的限制问题呢？ 为了解决 SPFA 算法的这个问题我试了两种方式，只有最后有一种是可行的： (错误)利用同等长度的临时数组记录此次遍历后dist数组更新的结果，然后在遍历完的尾部利用该数组对 dist 数组进行更新。 就像这样： 但很快会发现出现一个问题：一次遍历途中可能一个结点更新多次，那么这样就无法保证把所有k次中转内的情况都列举出来。 (正确)利用队列的参数对dist进行更新，如何更新呢？队列中记录每个结点的编号和到src的距离,每次更新新的 dist 的时候直接用正在遍历的编号到src的距离替代直接使用dist数组(这样便防止了更新dist数组后影响后续更新)。 如图： 解题代码： class Solution { public: //用于建图的结构体 struct { int to; int len; int next; }edge[5000]; int findCheapestPrice(int n, vector\u003cvector\u003cint\u003e\u003e\u0026 flights, int src, int dst, int k) { const int INF = 0x3f3f3f3f; memset(head, 0xff, sizeof head); int dist[n];memset(dist,0x3f,sizeof dist); dist[src] = 0; queue\u003cpair\u003cint,int\u003e\u003eQ; int sz = flights.size(); for(int i=0;i\u003csz;i++){ add(flights[i][0],flights[i][1],flights[i][2]); } Q.push({src,dist[src]}); int step = 0; while(!Q.empty()){ if(step\u003ek) break; for(int i=Q.size();i\u003e0;i--){ auto idx = move(Q.front());Q.pop(); for(int j=head[idx.first];j!=-1;j=edge[j].next){ if(idx.second+edge[j].len\u003cdist[edge[j].to]){ dist[edge[j].to] = idx.second+edge[j].len; Q.push({edge[j].to,dist[edge[j].to]}); } } } step++; } if(dist[dst]!=INF)return dist[dst]; return -1; } private: //用于链式前向星建图的函数和数据 int tot = 0; int head[100]; void add(int node,int to,int len){ edge[tot].to = to; edge[tot].len = len; edge[tot].next = head[node]; head[node] = tot; tot++; } }; ","date":"2022-02-22","objectID":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/:3:0","tags":["k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造"],"title":"k站中转内最便宜的航班--BellmanFord算法和SPFA算法的改造","uri":"/posts/k%E7%AB%99%E4%B8%AD%E8%BD%AC%E5%86%85%E6%9C%80%E4%BE%BF%E5%AE%9C%E7%9A%84%E8%88%AA%E7%8F%AD-bellmanford%E7%AE%97%E6%B3%95%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E9%80%A0/"},{"categories":["Linux网络编程"],"content":"TCP协议详解","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":" 本文章为《Linux高性能服务器编程》第四章的笔记，该书描述该章的各种内容的时候，几乎都使用了实践的工具来抓包验证并解释的方式来铺开知识点。我这里几乎是照搬了书本的知识点，但做了个分类处理。 TCP协议 在详细讨论TCP协议之前，我们先简单介绍一下TCP服务的特点，以及它和UDP服务的区别。 传输层协议主要有两个：TCP协议和UDP协议。TCP协议相对于UDP协议的特点是：面向连接、字节流和可靠传输。 面向连接： 使用TCP协议通信的双方必须先建立连接，然后才能开始数据的读写。双方都必须为该连接分配必要的内核资源，以管理连接的状态和连接上数据的传输。TCP连接是全双工的，即双方的数据读写可以通过一个连接进行。完成数据交换之后，通信双方都必须断开连接以释放系统资源。 字节流： 字节流服务和数据报服务的接收双方的最终表现出来的区别是，通信双方是否必须执行相同次数的读、写操作。具体原因在于，TCP通信是有接收和发送缓冲区的，应用程序可以一次性将TCP接收缓冲区中的数据全部读出，也可以分多次读取，这取决于用户指定的应用程序读缓冲区的大小。因此，应用程序执行的读操作次数和TCP模块接收到的TCP报文段个数之间也没有固定的数量关系。 综上所述，发送端执行的写操作次数和接收端执行的读操作次数之间没有任何数量关系，这就是字节流的概念：应用程序对数据的发送和接收是没有边界限制的。UDP则不然。发送端应用程序每执行一次写操作，UDP模块就将其封装成一个UDP数据报并发送之。接收端必须及时针对每一个UDP数据报执行读操作（通过recvfrom系统调用），否则就会丢包（这经常发生在较慢的服务器上）。并且，如果用户没有指定足够的应用程序缓冲区（程序开发者自建的）来读取UDP数据，则UDP数据将被截断。 图3-1和图3-2显示了TCP字节流服务和UDP数据报服务的上述区别。两图中省略了传输层以下的通信细节。 可靠传输： TCP协议采用发送应答机制，即发送端发送的每个TCP报文段都必须得到接收方的应答，才认为这个TCP报文段传输成功。 TCP协议采用超时重传机制，发送端在发送出一个TCP报文段之后启动定时器，如果在定时时间内未收到应答，它将重发该报文段。 因为TCP报文段最终是以IP数据报发送的，而IP数据报到达接收端可能乱序、重复，所以TCP协议还会对接收到的TCP报文段重排、整理，再交付给应用层。 UDP协议则和IP协议一样，提供不可靠服务。它们都需要上层协议来处理数据确认和超时重传。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:0:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP头部信息 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:1:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP头部信息清单 TCP头部结构如图3-3所示，其中的诸多字段为管理TCP连接和控制数据流提供了足够的信息。 16位端口号（port number） 告知主机该报文段是来自哪里（源端口）以及传给哪个上层协议或应用程序（目的端口）的。 进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号。 32位序号（sequence number） 一次TCP通信（从TCP连接建立到断开）过程中某一个传输方向上的字节流的每个字节的编号。 假设主机A和主机B进行TCP通信，A发送给B的第一个TCP报文段中，序号值被系统初始化为某个随机值ISN（InitialSequence Number，初始序号值）。那么在该传输方向上（从A到B），后续的TCP报文段中序号值将被系统设置成ISN加上该报文段所携带数据的第一个字节在整个字节流中的偏移。例如，某个TCP报文段传送的数据是字节流中的第1025～2048字节，那么该报文段的序号值就是ISN+1025。另外一个传输方向（从B到A）的TCP报文段的序号值也具有相同的含义。 32位确认号（acknowledgement number） 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。 假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。 4位头部长度（header length） 标识该TCP头部有多少个32bit字（4字节）。因为4位最大能表示15，所以TCP头部最长是60字节。 6位标志位 ❑URG标志，表示紧急指针（urgent pointer）是否有效。 ❑ACK标志，表示确认号是否有效。我们称携带ACK标志的TCP报文段为确认报文段。 ❑PSH标志，提示接收端应用程序应该立即从TCP接收缓冲区中读走数据，为接收后续数据腾出空间（如果应用程序不将接收到的数据读走，它们就会一直停留在TCP接收缓冲区中）。 ❑RST标志，表示要求对方重新建立连接。我们称携带RST标志的TCP报文段为复位报文段。 ❑SYN标志，表示请求建立一个连接。我们称携带SYN标志的TCP报文段为同步报文段。 ❑FIN标志，表示通知对方本端要关闭连接了。我们称携带FIN标志的TCP报文段为结束报文段。 16位窗口大小（window size） 是TCP流量控制的一个手段。这里说的窗口，指的是接收通告窗口（Receiver Window，RWND）注意这个名词，后面TCP数据流的控制会再碰到。它告诉对方本端的TCP接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 16位校验和（TCP checksum） 由发送端填充，接收端对TCP报文段执行CRC算法以检验TCP报文段在传输过程中是否损坏。注意，这个校验不仅包括TCP头部，也包括数据部分。这也是TCP可靠传输的一个重要保障。 16位紧急指针（urgent pointer） 是一个正的偏移量。它和序号字段的值相加表示最后一个紧急数据的下一字节的序号。因此，确切地说，这个字段是紧急指针相对当前序号的偏移，不妨称之为紧急偏移。TCP的紧急指针是发送端向接收端发送紧急数据的方法。我们将在后面讨论TCP紧急数据。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:1:1","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP头部选项 TCP头部的最后一个选项字段（options）是可变长的可选信息。这部分最多包含40字节，因为TCP头部最长是60字节（其中还包含前面讨论的20字节的固定部分）。典型的TCP头部选项结构如图3-4所示。 选项的第一个字段kind说明选项的类型。有的TCP选项没有后面两个字段，仅包含1字节的kind字段。第二个字段length（如果有的话）指定该选项的总长度，该长度包括kind字段和length字段占据的2字节。第三个字段info（如果有的话）是选项的具体信息。 常见的TCP选项有7种，如图3-5所示。 kind=0是选项表结束选项。 kind=1是空操作（nop）选项，没有特殊含义，一般用于将TCP选项的总长度填充为4字节的整数倍。 kind=2是最大报文段长度选项。重要 TCP连接初始化时，通信双方使用该选项来协商最大报文段长度（Max Segment Size，MSS）。TCP模块通常将MSS设置为（MTU-40）字节（减掉的这40字节包括20字节的TCP头部和20字节的IP头部）。这样携带TCP报文段的IP数据报的长度就不会超过MTU（假设TCP头部和IP头部都不包含选项字段，并且这也是一般情况），从而避免本机发生IP分片。对以太网而言，MSS值是1460（1500-40）字节。 kind=3是窗口扩大因子选项。重要 TCP连接初始化时，通信双方使用该选项来协商接收通告窗口的扩大因子。在TCP的头部中，接收通告窗口大小是用16位表示的，故最大为65 535字节，但实际上TCP模块允许的接收通告窗口大小远不止这个数（为了提高TCP通信的吞吐量）。窗口扩大因子解决了这个问题。假设TCP头部中的接收通告窗口大小是N，窗口扩大因子（移位数）是M，那么TCP报文段的实际接收通告窗口大小是N乘2M ，或者说N左移M位。注意，M的取值范围是0～14。我们可以通过修改/proc/sys/net/ipv4/tcp_window_scaling内核变量来启用或关闭窗口扩大因子选项。 和MSS选项一样，窗口扩大因子选项只能出现在同步报文段中，否则将被忽略。但同步报文段本身不执行窗口扩大操作，即同步报文段头部的接收通告窗口大小就是该TCP报文段的实际接收通告窗口大小。当连接建立好之后，每个数据传输方向的窗口扩大因子就固定不变了。 kind=4是选择性确认（Selective Acknowledgment，SACK）选项。 TCP通信时，如果某个TCP报文段丢失，则TCP模块会重传最后被确认的TCP报文段后续的所有报文段，这样原先已经正确传输的TCP报文段也可能重复发送，从而降低了TCP性能。SACK技术正是为改善这种情况而产生的，它使TCP模块只重新发送丢失的TCP报文段，不用发送所有未被确认的TCP报文段。选择性确认选项用在连接初始化时，表示是否支持SACK技术。我们可以通过修改/proc/sys/net/ipv4/tcp_sack内核变量来启用或关闭选择性确认选项。 kind=5是SACK实际工作的选项。 该选项的参数告诉发送方本端已经收到并缓存的不连续的数据块，从而让发送端可以据此检查并重发丢失的数据块。每个块边沿（edge of block）参数包含一个4字节的序号。其中块左边沿表示不连续块的第一个数据的序号，而块右边沿则表示不连续块的最后一个数据的序号的下一个序号。这样一对参数（块左边沿和块右边沿）之间的数据是没有收到的。因为一个块信息占用8字节，所以TCP头部选项中实际上最多可以包含4个这样的不连续数据块（考虑选项类型和长度占用的2字节）。 kind=8是时间戳选项。 该选项提供了较为准确的计算通信双方之间的回路时间（Round Trip Time，RTT）的方法，从而为TCP流量控制提供重要信息。我们可以通过修改/proc/sys/net/ipv4/tcp_timestamps内核变量来启用或关闭时间戳选项。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:1:2","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"一个真实的抓包验证过程 tcpdump抓取数据包 tcpdump输出Flags[S]，表示该TCP报文段包含SYN标志，因此它是一个同步报文段。如果TCP报文段包含其他标志，则tcpdump也会将该标志的首字母显示在“Flags”后的方括号中。 seq是序号值。因为该同步报文段是从127.0.0.1.41621（客户端IP地址和端口号）到127.0.0.1.23（服务器IP地址和端口号）这个传输方向上的第一个TCP报文段，所以这个序号值也就是此次通信过程中该传输方向的ISN值。并且，因为这是整个通信过程中的第一个TCP报文段，所以它没有针对对方发送来的TCP报文段的确认值（尚未收到任何对方发送来的TCP报文段）。 win是接收通告窗口的大小。因为这是一个同步报文段，所以win值反映的是实际的接收通告窗口大小。 options是TCP选项，其具体内容列在方括号中。mss是发送端（客户端）通告的最大报文段长度。通过ifconfig命令查看回路接口的MTU为16436字节，因此可以预想到TCP报文段的MSS为16396（16436-40）字节。sackOK表示发送端支持并同意使用SACK选项。TS val是发送端的时间戳。ecr是时间戳回显应答。因为这是一次TCP通信的第一个TCP报文段，所以它针对对方的时间戳的应答为0（尚未收到对方的时间戳）。紧接着的nop是一个空操作选项。wscale指出发送端使用的窗口扩大因子为6。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:1:3","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP的建立和关闭 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:2:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"三次握手和四次挥手 三次握手 第1个TCP报文段包含SYN标志，因此它是一个同步报文段，即ernest-laptop（客户端）向Kongming20（服务器）发起连接请求。同时，该同步报文段包含一个ISN值为535734930的序号。 第2个TCP报文段也是同步报文段，表示Kongming20同意与ernest-laptop建立连接。同时它发送自己的ISN值为2159701207的序号，并对第1个同步报文段进行确认。确认值是535734931，即第1个同步报文段的序号值加1。序号值是用来标识TCP数据流中的数据部分的起始字节在整个传输过程中的序号。但同步报文段比较特殊，即使它并没有携带任何应用程序数据，它也要占用一个序号值。 第3个TCP报文段是ernest-laptop对第2个同步报文段的确认。至此，TCP连接就建立起来了。建立TCP连接的这3个步骤被称为TCP三次握手。 四次挥手 后面4个TCP报文段是关闭连接的过程。第4个TCP报文段包含FIN标志，因此它是一个结束报文段，即ernest-laptop要求关闭连接。结束报文段和同步报文段一样，也要占用一个序号值。 Kongming20用TCP报文段5来确认该结束报文段。紧接着Kongming20发送自己的结束报文段6，ernest-laptop则用TCP报文段7给予确认。实际上，仅用于确认目的的确认报文段5是可以省略的，因为结束报文段6也携带了该确认信息。确认报文段5是否出现在连接断开的过程中，取决于TCP的延迟确认特性（除了延迟确认特性外我觉得还能用来实现半关闭状态）。 在连接的关闭过程中，因为ernest-laptop先发送结束报文段（telnet客户端程序主动退出），故称ernest-laptop执行主动关闭，而称Kongming20执行被动关闭。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:2:1","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"半关闭状态 TCP通信的一端可以发送结束报文段给对方，告诉它本端已经完成了数据的发送，但允许继续接收来自对方的数据，直到对方也发送结束报文段以关闭连接。TCP连接的这种状态称为半关闭（half close）状态，如图3-7所示。 socket网络编程接口通过shutdown函数提供了对半关闭的支持，我们将在后续章节讨论它。这里强调一下，虽然我们介绍了半关闭状态，但是使用半关闭的应用程序很少见。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:2:2","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"超时重连的抓包探索 前面我们讨论的是很快建立连接的情况。如果客户端访问一个距离它很远的服务器，或者由于网络繁忙，导致服务器对于客户端发送出的同步报文段没有应答，此时客户端程序将产生什么样的行为呢？显然，对于提供可靠服务的TCP来说，它必然是先进行重连（可能执行多次），如果重连仍然无效，则通知应用程序连接超时。 作者抓包Linux系统后输出包的时间戳，得到以下结果： 我们一共抓取到6个TCP报文段，它们都是同步报文段，并且具有相同的序号值，这说明后面5个同步报文段都是超时重连报文段。观察这些TCP报文段被发送的时间间隔，它们分别为1s、2s、4s、8s和16s（由于定时器精度的问题，这些时间间隔都有一定偏差），可以推断最后一个TCP报文段的超时时间是32s（63s-16s-8s-4s-2s-1s）。因此，TCP模块一共执行了5次重连操作，这是由/proc/sys/net/ipv4/tcp_syn_retries内核变量所定义的。每次重连的超时时间都增加一倍。在5次重连均失败的情况下，TCP模块放弃连接并通知应用程序。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:2:3","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP状态转移过程 TCP连接的任意一端在任一时刻都处于某种状态，当前状态可以通过netstat命令查看。本节我们要讨论的是TCP连接从建立到关闭的整个过程中通信两端状态的变化。图3-8是完整的状态转移图，它描绘了所有的TCP状态以及可能的状态转换。 图3-8中的粗虚线表示典型的服务器端连接的状态转移；粗实线表示典型的客户端连接的状态转移。CLOSED是一个假想的起始点，并不是一个实际的状态。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:3:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP状态转移总图文字详解 服务器状态转移过程 服务器通过listen系统调用进入 LISTEN 状态，被动等待客户端连接，因此执行的是所谓的被动打开。服务器一旦监听到某个连接请求（收到同步报文段），就将该连接放入内核等待队列中，并向客户端发送带SYN标志的确认报文段。此时该连接处于 SYN_RCVD 状态。如果服务器成功地接收到客户端发送回的确认报文段，则该连接转移到 ESTABLISHED 状态。ESTABLISHED状态是连接双方能够进行双向数据传输的状态。 当客户端主动关闭连接时（通过close或shutdown系统调用向服务器发送结束报文段），服务器通过返回确认报文段使连接进入 CLOSE_WAIT 状态。这个状态的含义很明确：等待服务器应用程序关闭连接。通常，服务器检测到客户端关闭连接后，也会立即给客户端发送一个结束报文段来关闭连接。这将使连接转移到 LAST_ACK 状态，以等待客户端对结束报文段的最后一次确认。一旦确认完成，连接就彻底关闭了。 客户端状态转移过程 客户端通过connect系统调用主动与服务器建立连接。connect系统调用首先给服务器发送一个同步报文段，使连接转移到 SYN_SENT 状态。此后，connect系统调用可能因为如下两个原因失败返回： ❑如果connect连接的目标端口不存在（未被任何进程监听），或者该端口仍被处于TIME_WAIT状态的连接所占用（见后文），则服务器将给客户端发送一个复位报文段，connect调用失败。 ❑如果目标端口存在，但connect在超时时间内未收到服务器的确认报文段，则connect调用失败。 connect调用失败将使连接立即返回到初始的CLOSED状态。如果客户端成功收到服务器的同步报文段和确认，则connect调用成功返回，连接转移至 ESTABLISHED 状态。 当客户端执行主动关闭时，它将向服务器发送一个结束报文段，同时连接进入 FIN_WAIT_1 状态。若此时客户端收到服务器专门用于确认目的的确认报文段（比如图3-6中的TCP报文段5），则连接转移至 FIN_WAIT_2 状态。当客户端处于 FIN_WAIT_2 状态时，服务器处于 CLOSE_WAIT 状态，这一对状态是可能发生半关闭的状态。此时如果服务器也关闭连接（发送结束报文段），则客户端将给予确认并进入TIME_WAIT状态。 图3-8还给出了客户端从 FIN_WAIT_1 状态直接进入 TIME_WAIT 状态的一条线路（不经过 FIN_WAIT_2 状态），前提是处于FIN_WAIT_1 状态的服务器直接收到带确认信息的结束报文段（而不是先收到确认报文段，再收到结束报文段）。这种情况对应于图3-6中的服务器不发送TCP报文段5。 前面说过，处于 FIN_WAIT_2 状态的客户端需要等待服务器发送结束报文段，才能转移至 TIME_WAIT 状态，否则它将一直停留在这个状态。如果不是为了在半关闭状态下继续接收数据，连接长时间地停留在 FIN_WAIT_2 状态并无益处（所以客户端处于 FIN_WAIT_2 服务端处于 CLOSE_WAIT 状态时一般是为了保持半关闭状态）。 那么问题来了：客户端执行半关闭后，未等服务器关闭连接就强行退出了。此时客户端连接由内核来接管，可称之为孤儿连接（和孤儿进程类似）。Linux为了防止孤儿连接长时间存留在内核中，定义了两个内核变量：/proc/sys/net/ipv4/tcp_max_orphans和/proc/sys/net/ipv4/tcp_fin_timeout。前者指定内核能接管的孤儿连接数目，后者指定孤儿连接在内核中生存的时间。 至此，我们简单地讨论了服务器和客户端程序的典型TCP状态转移路线。对应于图3-6所示的TCP连接的建立与断开过程，客户端和服务器的状态转移如图3-9所示。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:3:1","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TIME_WAIT状态 客户端处于 TIME_WAIT 状态。在这个状态，客户端连接要等待一段长为2MSL（Maximum Segment Life，报文段最大生存时间）的时间，才能完全关闭。MSL是TCP报文段在网络中的最大生存时间，标准文档RFC 1122的建议值是2 min。 TIME_WAIT 状态存在的原因有两点： ❑可靠地终止TCP连接。 ❑保证让迟来的TCP报文段有足够的时间被识别并丢弃。 第一个原因很好理解。假设图3-9中用于确认服务器结束报文段6的TCP报文段7丢失，那么服务器将重发结束报文段。因此客户端需要停留在某个状态以处理重复收到的结束报文段（即向服务器发送确认报文段）。否则，客户端将以复位报文段来回应服务器，服务器则认为这是一个错误，因为它期望的是一个像TCP报文段7那样的确认报文段。 TIME_WAIT状态持续2MSL的原因 在Linux系统上，一个TCP端口不能被同时打开多次（两次及以上）。当一个TCP连接处于 TIME_WAIT 状态时，我们将无法立即使用该连接占用着的端口来建立一个新连接。反过来思考，如果不存在 TIME_WAIT 状态，则应用程序能够立即建立一个和刚关闭的连接相似的连接（这里说的相似，是指它们具有相同的IP地址和端口号）。这个新的、和原来相似的连接被称为原来的连接的化身（incarnation）。新的化身可能接收到属于原来的连接的、携带应用程序数据的TCP报文段（迟到的报文段），这显然是不应该发生的。这就是TIME_WAIT状态存在的第二个原因。 另外，因为TCP报文段的最大生存时间是MSL，所以坚持2MSL时间的TIME_WAIT状态能够确保网络上两个传输方向上尚未被接收到的、迟到的TCP报文段都已经消失（被中转路由器丢弃）。 因此，一个连接的新的化身可以在2MSL时间之后安全地建立，而绝对不会接收到属于原来连接的应用程序数据，这就是 TIME_WAIT 状态要持续2MSL时间的原因。 TIME_WAIT引发的端口占用问题 关于客户端 既然一个客户端处于 TIME_WAIT 状态时会占用端口号，那我们下次重启程序不久没法再和服务端进行通信了？ 实际上客户端并不用担心这个问题，因为客户端一般使用系统自动分配的临时端口号来建立连接，而由于随机性，临时端口号一般和程序上一次使用的端口号（还处于TIME_WAIT状态的那个连接使用的端口号）不同，所以客户端程序一般可以立即重启。 关于服务端 但如果是服务器主动关闭连接后异常终止，则因为它总是使用同一个知名服务端口号，所以连接的 TIME_WAIT 状态将导致它不能立即重启。不过，我们可以通过socket选项SO_REUSEADDR来强制进程立即使用处于TIME_WAIT状态的连接占用的端口。（当然你也可以把这个端口号对应的进程杀死） ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:3:2","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"复位报文段 在某些特殊条件下，TCP连接的一端会向另一端发送携带RST标志的报文段，即复位报文段，以通知对方关闭连接或重新建立连接。 我们讨论产生复位报文段的3种情况。 情况一：访问不存在的端口 抓包测试如下： 由此可见，ernest-laptop针对Kongming20的连接请求（同步报文段）回应了一个复位报文段（tcpdump输出R标志）。因为复位报文段的接收通告窗口大小为0，所以可以预见：收到复位报文段的一端应该关闭连接或者重新连接，而不能回应这个复位报文段。 实际上，当客户端程序向服务器的某个端口发起连接，而该端口仍被处于TIME_WAIT状态的连接所占用时，客户端程序也将收到复位报文段。 情况二：异常终止连接 前面讨论的连接终止方式都是正常的终止方式：数据交换完成之后，一方给另一方发送结束报文段。TCP提供了异常终止一个连接的方法，即给对方发送一个复位报文段。一旦发送了复位报文段，发送端所有排队等待发送的数据都将被丢弃。 应用程序可以使用socket选项SO_LINGER来发送复位报文段，以异常终止一个连接。 情况三：处理半打开连接 考虑下面的情况：服务器（或客户端）关闭或者异常终止了连接，而对方没有接收到结束报文段（比如发生了网络故障），此时，客户端（或服务器）还维持着原来的连接，而服务器（或客户端）即使重启，也已经没有该连接的任何信息了。我们将这种状态称为半打开状态，处于这种状态的连接称为半打开连接。如果客户端（或服务器）往处于半打开状态的连接写入数据，则对方将回应一个复位报文段。 举例来说，我们在Kongming20上使用nc命令模拟一个服务器程序，使之监听12345端口，然后从ernest-laptop运行telnet命令登录到该端口上，接着拔掉ernest-laptop的网线，并在Kongming20上请求连接中断。显然，此时ernest-laptop上运行的telnet客户端程序维持着一个半打开连接。然后接上ernest-laptop的网线，并从客户端程序往半打开连接写入1字节的数据“a”。 具体的实践抓包： 该输出内容中，前3个TCP报文段是正常建立TCP连接的3次握手的过程。第4个TCP报文段由客户端发送给服务器，它携带了3字节的应用程序数据，这3字节依次是：字母“a”、回车符“\\r”和换行符“\\n”。不过因为服务器程序已经被中断，所以Kongming20对客户端发送的数据回应了一个复位报文段5。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:3:3","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP数据流 TCP报文段所携带的应用程序数据按照长度分为两种：交互数据和成块数据。交互数据仅包含很少的字节。使用交互数据的应用程序（或协议）对实时性要求高，比如telnet、ssh等。成块数据的长度则通常为TCP报文段允许的最大数据长度。使用成块数据的应用程序（或协议）对传输效率要求高，比如ftp。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:4:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"交互数据流抓包讲解 考虑如下情况：在ernest-laptop上执行telnet命令登录到本机，然后在shell命令提示符后执行ls命令，同时用tcpdump抓取这一过程中telnet客户端和telnet服务器交换的TCP报文段。 TCP报文段1由客户端发送给服务器，它携带1个字节的应用程序数据，即字母“l”。 TCP报文段2是服务器对TCP报文段1的确认，同时回显字母“l”。 TCP报文段3是客户端对TCP报文段2的确认。 第4～6个TCP报文段是针对字母“s”的上述过程。 TCP报文段7传送的2字节数据分别是：客户端键入的回车符和流结束符（EOF，本例中是0x00）。 TCP报文段8携带服务器返回的客户查询的目录的内容（ls命令的输出），包括该目录下文件的文件名及其显示控制参数。 TCP报文段9是客户端对TCP报文段8的确认。 TCP报文段10携带的也是服务器返回给客户端的数据，包括一个回车符、一个换行符、客户端登录用户的PS1环境变量（第一级命令提示符）。 TCP报文段11是客户端对TCP报文段10的确认。 延迟确认 在上述过程中，客户端针对服务器返回的数据所发送的确认报文段（TCP报文段6、9和11）都不携带任何应用程序数据（长度为0），而服务器每次发送的确认报文段（TCP报文段2、5、8和10）都包含它需要发送的应用程序数据。 服务器的这种处理方式称为延迟确认，即它不马上确认上次收到的数据，而是在一段延迟时间后查看本端是否有数据需要发送，如果有，则和确认信息一起发出。因为服务器对客户请求处理得很快，所以它发送确认报文段的时候总是有数据一起发送。延迟确认可以减少发送TCP报文段的数量。而由于用户的输入速度明显慢于客户端程序的处理速度，所以客户端的确认报文段总是不携带任何应用程序数据。 Nagle算法 上例是在本地回路运行的结果，在局域网中也能得到基本相同的结果，但在广域网就未必如此了。广域网上的交互数据流可能经受很大的延迟，并且，携带交互数据的微小TCP报文段数量一般很多（一个按键输入就导致一个TCP报文段），这些因素都可能导致拥塞发生。解决该问题的一个简单有效的方法是使用Nagle算法。 Nagle算法要求一个TCP连接的通信双方在任意时刻都最多只能发送一个未被确认的TCP报文段，在该TCP报文段的确认到达之前不能发送其他TCP报文段。另一方面，发送方在等待确认的同时收集本端需要发送的微量数据，并在确认到来时以一个TCP报文段将它们全部发出。这样就极大地减少了网络上的微小TCP报文段的数量。该算法的另一个优点在于其自适应性：确认到达得越快，数据也就发送得越快。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:4:1","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP成块数据流抓包讲解 下面考虑用FTP协议传输一个大文件。在ernest-laptop上启动一个vsftpd服务器程序（升级的、安全版的ftp服务器程序），并执行ftp命令登录该服务器上，然后在ftp命令提示符后输入get命令，从服务器下载一个几百兆的大文件。 注意，客户端发送的最后两个TCP报文段17和18，它们分别是对TCP报文段2和16的确认（从序号值和确认值来判断）。由此可见，当传输大量大块数据的时候，发送方会连续发送多个TCP报文段，接收方可以一次确认所有这些报文段。那么发送方在收到上一次确认后，能连续发送多少个TCP报文段呢？这是由接收通告窗口（还需要考虑拥塞窗口，见后文）的大小决定的。TCP报文段17说明客户端还能接收30 084×64字节（本例中窗口扩大因子为6），即1 925 376字节的数据。而在TCP报文段18中，接收通告窗口大小为1 748 288字节，即客户端能接收的数据量变小了。这表明客户端的TCP接收缓冲区有更多的数据未被应用程序读取而停留在其中，这些数据都来自TCP报文段3～16中的一部分。服务器收到TCP报文段18后，它至少（因为接收通告窗口可能扩大）还能连续发送的未被确认的报文段数量是1 748 288/16 384个，即106个（但一般不会连续发送这么多）。其中，16 384是成块数据的长度（见TCP报文段1～16的length值），很显然它小于但接近MSS规定的16 396字节。 另外一个值得注意的地方是，服务器每发送4个TCP报文段就传送一个PSH标志（tcpdump输出标志P）给客户端，以通知客户端的应用程序尽快读取数据。不过这对服务器来说显然不是必需的，因为它知道客户端的TCP接收缓冲区中还有空闲空间（接收通告窗口大小不为0）。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:4:2","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"带外数据 有些传输层协议具有带外（Out Of Band，OOB）数据的概念，用于迅速通告对方本端发生的重要事件。因此，带外数据比普通数据（也称为带内数据）有更高的优先级，它应该总是立即被发送，而不论发送缓冲区中是否有排队等待发送的普通数据。 带外数据的传输可以使用一条独立的传输层连接，也可以映射到传输普通数据的连接中。 UDP没有实现带外数据传输，TCP也没有真正的带外数据。不过TCP利用其头部中的紧急指针标志和紧急指针两个字段，给应用程序提供了一种紧急方式。 TCP发送带外数据的过程 假设一个进程已经往某个TCP连接的发送缓冲区中写入了N字节的普通数据，并等待其发送。在数据被发送前，该进程又向这个连接写入了3字节的带外数据“abc”。 此时，待发送的TCP报文段的头部将被设置URG标志，并且紧急指针被设置为指向最后一个带外数据的下一字节（进一步减去当前TCP报文段的序号值得到其头部中的紧急偏移值），如图3-10所示。 由图3-10可见，发送端一次发送的多字节的带外数据中只有最后一字节被当作带外数据（字母c），而其他数据（字母a和b）被当成了普通数据。 如果TCP模块以多个TCP报文段来发送图3-10所示TCP发送缓冲区中的内容，则每个TCP报文段都将设置URG标志，并且它们的紧急指针指向同一个位置（数据流中带外数据的下一个位置），但只有一个TCP报文段真正携带带外数据。 TCP接收带外数据的过程 TCP接收端只有在接收到紧急指针标志时才检查紧急指针，然后根据紧急指针所指的位置确定带外数据的位置，并将它读入一个特殊的缓存中。这个缓存只有1字节，称为带外缓存。如果上层应用程序没有及时将带外数据从带外缓存中读出，则后续的带外数据（如果有的话）将覆盖它。 前面讨论的带外数据的接收过程是TCP模块接收带外数据的默认方式。如果我们给TCP连接设置了 SO_OOBINLINE 选项，则带外数据将和普通数据一样被TCP模块存放在TCP接收缓冲区中。此时应用程序需要像读取普通数据一样来读取带外数据。那么这种情况下如何区分带外数据和普通数据呢？显然，紧急指针可以用来指出带外数据的位置，socket编程接口也提供了系统调用来识别带外数据。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:4:3","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"TCP数据流的控制（拥塞控制） ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:5:0","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"超时重传机制 在正式进入拥塞控制之前，先聊聊超时重传机制。 抓包截图就省略了，直接开始分析： 观察TCP报文段6～11被发送的时间间隔，它们分别为0.2 s、0.4 s、0.8 s、1.6 s和3.2 s。由此可见，TCP一共执行5次重传，每次重传超时时间都增加一倍（因此，和TCP超时重连的策略相似）。在5次重传均失败的情况下，底层的IP和ARP开始接管，直到telnet客户端放弃连接为止。 Linux有两个重要的内核参数与TCP超时重传相关：/proc/sys/net/ipv4/tcp_retries1和/proc/sys/net/ipv4/tcp_retries2。前者指定在底层IP接管之前TCP最少执行的重传次数，默认值是3。后者指定连接放弃前TCP最多可以执行的重传次数，默认值是15（一般对应13～30 min）。在我们的实例中，TCP超时重传发生了5次，连接坚持的时间是15 min（可以用date命令来测量）。 虽然超时会导致TCP报文段重传，但TCP报文段的重传可以发生在超时之前，即快速重传（接收到重复的确认报文段）。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:5:1","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux网络编程"],"content":"拥塞控制 拥塞控制算法在Linux下有多种实现，比如reno算法、vegas算法和cubic算法等。它们或者部分或者全部实现了上述四个部分。/proc/sys/net/ipv4/tcp_congestion_control文件指示机器当前所使用的拥塞控制算法。 拥塞控制的最终受控变量是发送端向网络一次连续写入（收到其中第一个数据的确认之前）的数据量，我们称为SWND（Send Window，发送窗口[1]）。不过，发送端最终以TCP报文段来发送数据，所以SWND限定了发送端能连续发送的TCP报文段数量。这些TCP报文段的最大长度（仅指数据部分）称为SMSS（SenderMaximum Segment Size，发送者最大段大小），其值一般等于MSS。 发送端需要合理地选择SWND的大小。如果SWND太小，会引起明显的网络延迟；反之，如果SWND太大，则容易导致网络拥塞。前文提到，接收方可通过其接收通告窗口（RWND）来控制发送端的SWND。但这显然不够，所以发送端引入了一个称为拥塞窗口（Congestion Window，CWND）的状态变量。实际的SWND值是RWND和CWND中的较小者。图3-11显示了拥塞控制的输入和输出（可见，它是一个闭环反馈控制）。 慢启动和拥塞避免 TCP连接建立好之后，CWND将被设置成初始值IW（Initial Window），其大小为2～4个SMSS。但新的Linux内核提高了该初始值，以减小传输滞后。此时发送端最多能发送IW字节的数据。此后发送端每收到接收端的一个确认，其CWND就按照式（3-1）增加： 其中N是此次确认中包含的之前未被确认的字节数。这样一来，CWND将按照指数形式扩大，这就是所谓的慢启动。慢启动算法的理由是，TCP模块刚开始发送数据时并不知道网络的实际情况，需要用一种试探的方式平滑地增加CWND的大小。 但是如果不施加其他手段，慢启动必然使得CWND很快膨胀（可见慢启动其实不慢）并最终导致网络拥塞。因此TCP拥塞控制中定义了另一个重要的状态变量：慢启动门限（slow start threshold size，ssthresh）。当CWND的大小超过该值时，TCP拥塞控制将进入拥塞避免阶段。 拥塞避免算法使得CWND按照线性方式增加，从而减缓其扩大。RFC 5681中提到了如下两种实现方式： ❑每个RTT时间内按照式（3-1）计算新的CWND，而不论该RTT时间内发送端收到多少个确认。 ❑每收到一个对新数据的确认报文段，就按照式（3-2）来更新CWND。 图3-12粗略地描述了慢启动和拥塞避免发生的时机和区别。该图中，我们以SMSS为单位来显示CWND（实际上它是以字节为单位的），以次数为单位来显示RTT，这只是为了方便讨论问题。此外，我们假设当前的ssthresh是16SMSS大小（当然，实际的ssthresh显然远不止这么大）。 判断拥塞是否发生 以上我们讨论了发送端在未检测到拥塞时所采用的积极避免拥塞的方法。接下来介绍拥塞发生时（可能发生在慢启动阶段或者拥塞避免阶段）拥塞控制的行为。不过我们先要搞清楚发送端是如何判断拥塞已经发生的。发送端判断拥塞发生的依据有如下两个： ❑传输超时，或者说TCP重传定时器溢出。 ❑接收到重复的确认报文段。 拥塞控制对这两种情况有不同的处理方式。对第一种情况仍然使用慢启动和拥塞避免。对第二种情况则使用快速重传和快速恢复（如果是真的发生拥塞的话），这种情况将在后面讨论。注意，第二种情况如果发生在重传定时器溢出之后，则也被拥塞控制当成第一种情况来对待。 如果发送端检测到拥塞发生是由于传输超时，即上述第一种情况，那么它将执行重传并做如下调整： 快速重传和快速恢复 在很多情况下，发送端都可能接收到重复的确认报文段，比如TCP报文段丢失，或者接收端收到乱序TCP报文段并重排之等。拥塞控制算法需要判断当收到重复的确认报文段时，网络是否真的发生了拥塞，或者说TCP报文段是否真的丢失了。具体做法是：发送端如果连续收到3个重复的确认报文段，就认为是拥塞发生了。然后它启用快速重传和快速恢复算法来处理拥塞，过程如下： 1）当收到第3个重复的确认报文段时，按照式（3-3）计算ssthresh，然后立即重传丢失的报文段，并按照以下式子设置CWND。 $$CWND=ssthresh+3*SMSS$$ 2）每次收到1个重复的确认时，设置CWND=CWND+SMSS。此时发送端可以发送新的TCP报文段（如果新的CWND允许的话）。 3）当收到新数据的确认时，设置CWND=ssthresh（ssthresh是新的慢启动门限值，由第一步计算得到）。 快速重传和快速恢复完成之后，拥塞控制将恢复到拥塞避免阶段，这一点由第3步操作可得知。 ","date":"2022-02-21","objectID":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/:5:2","tags":["TCP协议详解"],"title":"TCP协议详解","uri":"/posts/tcp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——模拟题"],"content":"1bit与2bit字符——简单模拟题","date":"2022-02-20","objectID":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/","tags":["1bit与2bit字符——简单模拟题"],"title":"1bit与2bit字符——简单模拟题","uri":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/"},{"categories":["算法——模拟题"],"content":"题目 题目链接 ","date":"2022-02-20","objectID":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/:1:0","tags":["1bit与2bit字符——简单模拟题"],"title":"1bit与2bit字符——简单模拟题","uri":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/"},{"categories":["算法——模拟题"],"content":"题目详解 读懂题： 此题是为了让最后以一个字符解码，也就是 $0$ ，而含 $1$ 的只能是两个字符进行解码，所以遇到 $1$ 就必须确保后面有 $0$ 或 $1$ 来用于抵消。 解题法： 法一：正向遍历法 直接通过遇到 $0$ 走一步，遇到 $1$ 走两步，再看最后是否能恰好走到最后一个 $0$ 的位置，如果能则 $true$ 否则 $false$ 。 法二：反向遍历法 由于遇到 $0$ 能直接跳过，而遇到 $1$ 则后面必须含有一个字符被抵消，所以为了让最后一个 $0$ 不被抵消，它前面的连续 $1$ 应该要为偶数个，否则 $0$ 将会被抵消。故具体做法直接记录最后一个 $0$ 前面的 $1$ 的个数即可得出答案。 ","date":"2022-02-20","objectID":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/:2:0","tags":["1bit与2bit字符——简单模拟题"],"title":"1bit与2bit字符——简单模拟题","uri":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/"},{"categories":["算法——模拟题"],"content":"解题代码 法一： class Solution { public: bool isOneBitCharacter(vector\u003cint\u003e\u0026 bits) { int sz = bits.size(); if(sz\u003c2) return true; int start = 0; while(start\u003csz-1){ if(bits[start]==0) start++; else{ start += 2; } } return start==sz-1; } }; 法二： class Solution { public: bool isOneBitCharacter(vector\u003cint\u003e\u0026 bits) { return find(rbegin(bits)+1, rend(bits), 0) - rbegin(bits) \u0026 1; } }; ","date":"2022-02-20","objectID":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/:3:0","tags":["1bit与2bit字符——简单模拟题"],"title":"1bit与2bit字符——简单模拟题","uri":"/posts/1bit%E4%B8%8E2bit%E5%AD%97%E7%AC%A6%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9F%E9%A2%98/"},{"categories":["Java底层原理"],"content":"Java动态代理详解","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":" 在介绍动态代理之前，必须先来聊聊静态代理。 ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:0:0","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"静态代理介绍 假设现在项目经理有一个需求：在项目现有所有类的方法前后打印日志。 你如何在不修改已有代码的前提下，完成这个需求？ 我首先想到的是静态代理。具体做法是： 一、为现有的每一个类都编写一个对应的代理类，并且让它实现和目标类相同的接口（假设都有） 二、在创建代理对象时，通过构造器塞入一个目标对象，然后在代理对象的方法内部调用目标对象同名方法，并在调用前后打印日志。也就是说，代理对象 = 增强代码 + 目标对象（原对象）。有了代理对象后，就不用原对象了 静态代理的缺陷 程序员要手动为每一个目标类编写对应的代理类。如果当前系统已经有成百上千个类，工作量太大了。所以，现在我们的努力方向是：如何少写或者不写代理类，却能完成代理功能？ 复习对象的创建 很多初学Java的朋友眼中创建对象的过程 实际上可以换个角度，也说得通 所谓的Class对象，是Class类的实例，而Class类是描述所有类的，比如Person类，Student类 可以看出，要创建一个实例，最关键的就是得到对应的Class对象。只不过对于初学者来说，new这个关键字配合构造方法，实在太好用了，底层隐藏了太多细节，一句 Person p = new Person();直接把对象返回给你了。我自己刚开始学Java时，也没意识到Class对象的存在。 分析到这里，貌似有了思路： 能否不写代理类，而直接得到代理Class对象，然后根据它创建代理实例（反射）。 Class对象包含了一个类的所有信息，比如构造器、方法、字段等。如果我们不写代理类，这些信息从哪获取呢？苦思冥想，突然灵光一现：代理类和目标类理应实现同一组接口。之所以实现相同接口，是为了尽可能保证代理对象的内部结构和目标对象一致，这样我们对代理对象的操作最终都可以转移到目标对象身上，代理对象只需专注于增强代码的编写。还是上面这幅图： 所以，可以这样说：接口拥有代理对象和目标对象共同的类信息。所以，我们可以从接口那得到理应由代理类提供的信息。但是别忘了，接口是无法创建对象的，怎么办？ ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:1:0","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"走进动态代理 JDK提供了java.lang.reflect.InvocationHandler接口和 java.lang.reflect.Proxy类，这两个类相互配合，入口是Proxy，所以我们先聊它。 Proxy有个静态方法：getProxyClass(ClassLoader, interfaces)，只要你给它传入类加载器和一组接口，它就给你返回代理Class对象。 用通俗的话说，getProxyClass() 这个方法，会从你传入的接口Class中，“拷贝”类结构信息到一个新的Class对象中，但新的Class对象带有构造器，是可以创建对象的。打个比方，一个大内太监（接口Class），空有一身武艺（类信息），但是无法传给后人。现在江湖上有个妙手神医（Proxy类），发明了克隆大法（getProxyClass），不仅能克隆太监的一身武艺，还保留了小DD（构造器）…（这到底是道德の沦丧，还是人性的扭曲，欢迎走进动态代理） 所以，一旦我们明确接口，完全可以通过接口的Class对象，创建一个代理Class，通过代理Class即可创建代理对象。 所以，按我理解，Proxy.getProxyClass()这个方法的本质就是：以Class造Class。 有了Class对象，就很好办了，具体看代码： 根据代理Class的构造器创建对象时，需要传入InvocationHandler。每次调用代理对象的方法，最终都会调用InvocationHandler的invoke()方法： 怎么做到的呢？ 上面不是说了吗，根据代理Class的构造器创建对象时，需要传入InvocationHandler。通过构造器传入一个引用，那么必然有个成员变量去接收。没错，代理对象的内部确实有个成员变量invocationHandler，而且代理对象的每个方法内部都会调用handler.invoke()！ InvocationHandler对象成了代理对象和目标对象的桥梁，不像静态代理这么直接。 大家仔细看上图右侧的动态代理，我在invocationHandler的invoke()方法中并没有写目标对象。因为一开始invocationHandler的invoke()里确实没有目标对象，需要我们手动new。 但这种写法不够优雅，属于硬编码。我这次代理A对象，下次想代理B对象还要进来改invoke()方法，太差劲了。改进一下，让调用者把目标对象作为参数传进来： public class ProxyTest { public static void main(String[] args) throws Throwable { CalculatorImpl target = new CalculatorImpl(); //传入目标对象 //目的：1.根据它实现的接口生成代理对象 2.代理对象调用目标对象方法 Calculator calculatorProxy = (Calculator) getProxy(target); calculatorProxy.add(1, 2); calculatorProxy.subtract(2, 1); } private static Object getProxy(final Object target) throws Exception { //参数1：随便找个类加载器给它， 参数2：目标对象实现的接口，让代理对象实现相同接口 Class proxyClazz = Proxy.getProxyClass(target.getClass().getClassLoader(), target.getClass().getInterfaces()); Constructor constructor = proxyClazz.getConstructor(InvocationHandler.class); Object proxy = constructor.newInstance(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(method.getName() + \"方法开始执行...\"); Object result = method.invoke(target, args); System.out.println(result); System.out.println(method.getName() + \"方法执行结束...\"); return result; } }); return proxy; } } 这样就非常灵活，非常优雅了。无论现在系统有多少类，只要你把实例传进来，getProxy()都能给你返回对应的代理对象。就这样，我们完美地跳过了代理类，直接创建了代理对象！ 不过实际编程中，一般不用getProxyClass()，而是使用Proxy类的另一个静态方法：Proxy.newProxyInstance()，直接返回代理实例，连中间得到代理Class对象的过程都帮你隐藏： public class ProxyTest { public static void main(String[] args) throws Throwable { CalculatorImpl target = new CalculatorImpl(); Calculator calculatorProxy = (Calculator) getProxy(target); calculatorProxy.add(1, 2); calculatorProxy.subtract(2, 1); } private static Object getProxy(final Object target) throws Exception { Object proxy = Proxy.newProxyInstance( target.getClass().getClassLoader(),/*类加载器*/ target.getClass().getInterfaces(),/*让代理对象和目标对象实现相同接口*/ new InvocationHandler(){/*代理对象的方法最终都会被JVM导向它的invoke方法*/ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(method.getName() + \"方法开始执行...\"); Object result = method.invoke(target, args); System.out.println(result); System.out.println(method.getName() + \"方法执行结束...\"); return result; } } ); return proxy; } } 现在，我想大家应该能看懂动态代理了。 那么来张小图做总结： ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:2:0","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"代理的真正作用（实践） ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:3:0","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"静态代理的作用 假如我们有一个字体提供类，有多种实现（从磁盘，从网络，从系统） public interface FontProvider { Font getFont(String name); } public abstract class ProviderFactory { public static FontProvider getFontProvider() { return new FontProviderFromDisk(); } } public class Main() { public static void main(String[] args) { FontProvider fontProvider = ProviderFactory.getFontProvider(); Font font = fontProvider.getFont(\"微软雅黑\"); ...... } } 现在我们希望给他加上一个缓存功能，我们可以用静态代理来完成。 public class CachedFontProvider implements FontProvider { private FontProvider fontProvider; private Map\u003cString, Font\u003e cached; public CachedFontProvider(FontProvider fontProvider) { this.fontProvider = fontProvider; } public Font getFont(String name) { Font font = cached.get(name); if (font == null) { font = fontProvider.getFont(name); cached.put(name, font); } return font; } } /* 对工厂类进行相应修改，代码使用处不必进行任何修改。 这也是面向接口编程以及工厂模式的一个好处 */ public abstract class ProviderFactory { public static FontProvider getFontProvider() { return new CachedFontProvider(new FontProviderFromDisk()); } } 当然，我们直接修改FontProviderFromDisk类也可以实现目的，但是我们还有FontProviderFromNet, FontProviderFromSystem等多种实现类，一一修改太过繁琐且易出错。 况且将来还可能添加日志，权限检查，异常处理等功能显然用代理类更好一点。 ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:3:1","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"动态代理的运用场景（减少重复劳作） 比如：考虑以下各种情况，有多个提供类，每个类都有getXxx(String name)方法，每个类都要加入缓存功能，使用静态代理虽然也能实现，但是也是略显繁琐，需要手动一一创建代理类。 public abstract class ProviderFactory { public static FontProvider getFontProvider() {...} public static ImageProvider getImageProvider() {...} public static MusicProvider getMusicProvider() {...} ...... } 使用动态代理怎么完成呢？(只需一份缓存版本实现即可) public class CachedProviderHandler implements InvocationHandler { private Map\u003cString, Object\u003e cached = new HashMap\u003c\u003e(); private Object target; public CachedProviderHandler(Object target) { this.target = target; } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Type[] types = method.getParameterTypes(); //关键逻辑：当调用的是get方法时，我们就来发挥缓冲区的作用 if (method.getName().matches(\"get.+\") \u0026\u0026 (types.length == 1) \u0026\u0026 (types[0] == String.class)) { String key = (String) args[0]; Object value = cached.get(key); if (value == null) { value = method.invoke(target, args); cached.put(key, value); } return value; } return method.invoke(target, args); } } public abstract class ProviderFactory { public static FontProvider getFontProvider() { Class\u003cFontProvider\u003e targetClass = FontProvider.class; return (FontProvider) Proxy.newProxyInstance(targetClass.getClassLoader(), new Class[] { targetClass }, new CachedProviderHandler(new FontProviderFromDisk())); } } ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:3:2","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["Java底层原理"],"content":"总结 很明显，动态代理在需要大量静态代理的情况下，大大减少了重复劳作，动态代理yyds！(不知道C++如何去实现动态代理。。) ","date":"2022-02-20","objectID":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/:3:3","tags":["Java动态代理详解"],"title":"Java动态代理详解","uri":"/posts/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——排序原理运用"],"content":"PAT甲级--Insertion-or-Heap-Sort","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"题目 OJ平台 ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:1:0","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"题目大意 有很多题目实际不需要看懂题目，只需要看懂输入和输出，比如这题。 此题虽然题目较为学术，且比较长，实际总结下来就是，通过给你一个原数组序列，还有一个用插入排序或者是堆排序排了几轮的数组序列，你要根据这个序列判断所使用的排序方式，并且再以该排序方式往下排一轮。 ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:2:0","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"解题代码拆解 这次由于我使用的接口化函数设计，也就是传入指针进行操作，没有采用全局变量，所以直接input操作写在main函数里面，省空间。 ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:3:0","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"关键的判断函数 isInsert() 设计思路：假设为插入排序后的序列，通过一次循环，找到下一个要被排序的元素位置，按理来说，没有被排序的位置应该和原数组的序列情况一致，如果不一致，则不是插入排序。 //用于确定是否为插入排序，顺便返回此时待插入处理的位置 int isInsert(int* nums, int len) { int i = 1; for (; i \u003c len; i++) { if (nums[i - 1] \u003e nums[i]) break; } for (int j = i; j \u003c len; j++) { if (original[j] != nums[j]) return -1; } return i; } ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:3:1","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"堆排序和插入排序 插入排序 插入排序很简单，我这里直接写插入排序的每一轮处理函数来代替。 //插入排序的单步处理 void InsertSort(int* nums, int numSize, int i) { int j = i; int temp = nums[i]; for (; j \u003e 0 \u0026\u0026 nums[j - 1] \u003e temp; j--) { nums[j] = nums[j - 1]; } nums[j] = temp; } 堆排序 堆排序的原理： 对于一个完整的堆排序，分为两个过程：堆化+维持堆化。 建立大根堆，每次堆化找到最大值，为了维持堆的结构和排序，将堆顶与最后一个元素交换，然后更新堆的范围到 0~i-1 再次堆化，又能找到这个堆中的最大值，长此以往，便完成了排序。 对于堆排序中的每一轮：堆排中的每一轮都是缩小堆的范围，并继续维持大根堆，在堆范围以外的元素就是被排好的元素。所以重点在于从后往前找到已经排到了哪个位置，再进行一次交换和堆化便可完成一轮排序。 堆排的过程： 向下堆化的函数：sift_down() //堆化，得到大根堆 //对于从0开始编号的二叉堆: /* iparent = (i-1)/2, ilchild = i*2+1, irchild = i*2+2 */ void sift_down(int arr[], int start, int end) { // 计算父结点和子结点的下标 int parent = start; int child = parent * 2 + 1; while (child \u003c= end) { // 子结点下标在范围内才做比较 // 先比较两个子结点大小，选择最大的 if (child + 1 \u003c= end \u0026\u0026 arr[child] \u003c arr[child + 1]) child++; // 如果父结点比子结点大，代表调整完毕，直接跳出函数 if (arr[parent] \u003e= arr[child]) return; else { // 否则交换父子内容，子结点再和孙结点比较 swap(arr[parent], arr[child]); parent = child; child = parent * 2 + 1; } } } 先完全堆化，再利用它挑选最大值维持堆化的过程：heap_sort() void heap_sort(int arr[], int len) { // 从最后一个节点的父节点开始 sift down 以完成堆化 (heapify) for (int i = (len - 1 - 1) / 2; i \u003e= 0; i--) sift_down(arr, i, len - 1); // 先将第一个元素和已经排好的元素前一位做交换，再重新调整（刚调整的元素之前的元素），直到排序完毕 for (int i = len - 1; i \u003e 0; i--) { swap(arr[0], arr[i]); sift_down(arr, 0, i - 1); } } 方便更新下一轮堆排的工具： 确定堆排序到哪个位置的函数：findP //由于堆排是从后往前先得出最大值，所以直接从后往前判断最大值位置即可得出堆排排到了哪一轮 int findP(int* nums, int len) { int i = 0; for (; i \u003c len; i++) { int val = *max_element(nums, nums + len - i); if (nums[len - 1 - i] != val) break; } return len - 1 - i; } ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:3:2","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——排序原理运用"],"content":"整合代码进行提交 效率还行！ #include \u003cbits/stdc++.h\u003eusing namespace std; int* original = nullptr; //插入排序的单步处理 void InsertSort(int* nums, int numSize, int i) { int j = i; int temp = nums[i]; for (; j \u003e 0 \u0026\u0026 nums[j - 1] \u003e temp; j--) { nums[j] = nums[j - 1]; } nums[j] = temp; } //堆排序 void sift_down(int arr[], int start, int end) { // 计算父结点和子结点的下标 int parent = start; int child = parent * 2 + 1; while (child \u003c= end) { // 子结点下标在范围内才做比较 // 先比较两个子结点大小，选择最大的 if (child + 1 \u003c= end \u0026\u0026 arr[child] \u003c arr[child + 1]) child++; // 如果父结点比子结点大，代表调整完毕，直接跳出函数 if (arr[parent] \u003e= arr[child]) return; else { // 否则交换父子内容，子结点再和孙结点比较 swap(arr[parent], arr[child]); parent = child; child = parent * 2 + 1; } } } //用于找出堆排已经拍到了哪个位置的最大值。 int findP(int* nums, int len) { int i = 0; for (; i \u003c len; i++) { int val = *max_element(nums, nums + len - i); if (nums[len - 1 - i] != val) break; } return len - 1 - i; } //用于确定是否为插入排序，顺便返回此时待插入处理的位置 int isInsert(int* nums, int len) { int i = 1; for (; i \u003c len; i++) { if (nums[i - 1] \u003e nums[i]) break; } for (int j = i; j \u003c len; j++) { if (original[j] != nums[j]) return -1; } return i; } //统一打印结果函数 void print(int* nums, int len) { cout \u003c\u003c nums[0]; for (int i = 1; i \u003c len; i++) { cout \u003c\u003c ' ' \u003c\u003c nums[i]; } } int main() { //@输入处理 ios::sync_with_stdio(false); int N; cin \u003e\u003e N; int org[N], nums[N]; for (int i = 0; i \u003c N; ++i) { cin \u003e\u003e org[i]; } for (int i = 0; i \u003c N; ++i) { cin \u003e\u003e nums[i]; } //@根据不同的排序方式进行答案的打印 original = org; int flag = isInsert(nums, N); if (flag != -1) { cout \u003c\u003c \"Insertion Sort\" \u003c\u003c endl; InsertSort(nums, N, flag); print(nums, N); } else { cout \u003c\u003c \"Heap Sort\" \u003c\u003c endl; int pos = findP(nums, N); swap(nums[0], nums[pos]); sift_down(nums, 0, pos - 1); print(nums, N); } return 0; } ","date":"2022-02-19","objectID":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/:3:3","tags":["PAT甲级--Insertion-or-Heap-Sort"],"title":"PAT甲级--Insertion-or-Heap-Sort","uri":"/posts/pat%E7%94%B2%E7%BA%A7-insertion-or-heap-sort/"},{"categories":["算法——贪心"],"content":"堆的运用——有序元素的多路归并topk问题","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"题目一：有序矩阵第k小的元素(提炼出做题方法) 题目链接 ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:1:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"解题技法 感觉这张图基本就清楚了这题目如何解。 具体详解过程请看lc大神：题目详解 ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:2:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"解题代码 class Solution { public: //TODO 多路归并 int kthSmallest(vector\u003cvector\u003cint\u003e\u003e\u0026 matrix, int k) { auto cmp = [\u0026](pair\u003cint,int\u003e\u0026a,pair\u003cint,int\u003e\u0026b){ return matrix[a.first][a.second]\u003ematrix[b.first][b.second]; }; priority_queue\u003cpair\u003cint,int\u003e,vector\u003cpair\u003cint,int\u003e\u003e,decltype(cmp)\u003epq(cmp); int n = matrix.size(); for(int i=0;i\u003cmin(k,n);i++){ pq.push({i,0});//TODO 得到第一次的行首元素 } int ret = INT_MAX; while(k--\u0026\u0026!pq.empty()){ auto [x,y] = pq.top();pq.pop(); if(y+1\u003cn)//TODO 更新这一行的下一个元素到堆中 pq.push({x,y+1}); ret = matrix[x][y]; } return ret; } }; ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:3:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"(进阶运用)题目二：查找和最小的K对数字 题目链接 ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:4:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"题目解析 和前面那道题的做法一样，这道题是由于者均有序，所以如果是直接进行两层循环的枚举的话，得到的数字可以看作是一个和上题一模一样的矩阵，也就是把 nums1[0...]+nums2[0] 看作是一行的首元素即可，然后处理过程就和前面的处理过程是完全一致。和前面一题的区别仅仅在于未有确定矩阵的内容而已，而我们需要做的就是确定这个矩阵的内容！ 细节优化：由于矩阵的内容由我们来确定，为了防止初始化矩阵首行元素过多，我们可以采取把长度小的 nums 作为行的标准，那么为了让每次的答案顺序不变，所以需要一个标记。 ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:5:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——贪心"],"content":"解题代码 class Solution { public: bool flag = true; vector\u003cvector\u003cint\u003e\u003e kSmallestPairs(vector\u003cint\u003e\u0026 nums1, vector\u003cint\u003e\u0026 nums2, int k) { vector\u003cvector\u003cint\u003e\u003e ans; int n = nums1.size(), m = nums2.size(); if(n \u003e m) { //始终确保nums1为两数组中长度较少的那个(这样做可以适当的减少堆的初始大小)，这个不处理也可以，只是简单的优化 swap(nums1, nums2); swap(m,n); flag = false;//确保原本的第一个取数的数字时nums1原本的数字 } //定义比较规则 auto cmp = [\u0026](const auto\u0026 a, const auto\u0026 b){ return nums2[a.first] + nums2[a.second] \u003e nums1[b.first] + nums2[b.second]; }; priority_queue\u003c pair\u003cint,int\u003e, vector\u003cpair\u003cint,int\u003e\u003e, decltype(cmp) \u003e q(cmp); for(int i =j 0; i \u003c min(n,k); i++){ q.push( {i, 0} ); } while(k-- and !q.empty()){ auto [a,b] = q.top(); q.pop(); flag ? ans.push_back( {nums1[a], nums2[b]}) : ans.push_back( {nums2[b], nums1[a]}); //TODO 得到这一行的下一个元素，如果超过则不入 if(b + 2 \u003c m) q.push( {a, b + 1} ); } return ans; } }; ","date":"2022-02-19","objectID":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/:6:0","tags":["堆的运用——有序元素的多路归并topk问题"],"title":"堆的运用——有序元素的多路归并topk问题","uri":"/posts/%E5%A0%86%E7%9A%84%E8%BF%90%E7%94%A8%E6%9C%89%E5%BA%8F%E5%85%83%E7%B4%A0%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6topk%E9%97%AE%E9%A2%98/"},{"categories":["算法——排序原理运用"],"content":"归并的运用——计算逆序对","date":"2022-02-19","objectID":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/","tags":["归并的运用——计算逆序数"],"title":"归并的运用——计算逆序数","uri":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/"},{"categories":["算法——排序原理运用"],"content":"题目 题目链接 ","date":"2022-02-19","objectID":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/:1:0","tags":["归并的运用——计算逆序数"],"title":"归并的运用——计算逆序数","uri":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/"},{"categories":["算法——排序原理运用"],"content":"题目解析 很明显此题的问题规模来到了 1e5 的级别，显然不是 O(n^2) 的暴力方式能够解决的。 具体的详细解析，这里有力扣大神在：题目解析 我这里把最关键的图解过程扣了下来： ","date":"2022-02-19","objectID":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/:2:0","tags":["归并的运用——计算逆序数"],"title":"归并的运用——计算逆序数","uri":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/"},{"categories":["算法——排序原理运用"],"content":"解题代码 配上这简洁清晰的解题代码： class Solution { public: int reversePairs(vector\u003cint\u003e\u0026 nums) { vector\u003cint\u003e tmp(nums.size()); return mergeSort(0, nums.size() - 1, nums, tmp); } private: int mergeSort(int l, int r, vector\u003cint\u003e\u0026 nums, vector\u003cint\u003e\u0026 tmp) { // 终止条件 if (l \u003e= r) return 0; // 递归划分 int m = (l + r) / 2; int res = mergeSort(l, m, nums, tmp) + mergeSort(m + 1, r, nums, tmp); // 合并阶段 int i = l, j = m + 1; for (int k = l; k \u003c= r; k++) tmp[k] = nums[k]; for (int k = l; k \u003c= r; k++) { if (i == m + 1) nums[k] = tmp[j++]; else if (j == r + 1 || tmp[i] \u003c= tmp[j]) nums[k] = tmp[i++]; else { nums[k] = tmp[j++]; res += m - i + 1; // 统计逆序对 } } return res; } }; ","date":"2022-02-19","objectID":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/:3:0","tags":["归并的运用——计算逆序数"],"title":"归并的运用——计算逆序数","uri":"/posts/%E5%BD%92%E5%B9%B6%E7%9A%84%E8%BF%90%E7%94%A8%E8%AE%A1%E7%AE%97%E9%80%86%E5%BA%8F%E5%AF%B9/"},{"categories":["算法——排序原理运用"],"content":"煎饼排序——选择排序的运用","date":"2022-02-19","objectID":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/","tags":["煎饼排序——选择排序的运用"],"title":"煎饼排序——选择排序的运用","uri":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/"},{"categories":["算法——排序原理运用"],"content":"题目 题目链接 ","date":"2022-02-19","objectID":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/:1:0","tags":["煎饼排序——选择排序的运用"],"title":"煎饼排序——选择排序的运用","uri":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/"},{"categories":["算法——排序原理运用"],"content":"解题思路 读懂题目： 此题并不是要我们求出类似于示例所给的最优情况的方式得出答案。 他只要能够翻转成有序的操作序列即可。 故我们可以按照选择排序的思路，利用翻转能够将首尾交换，来进行两次翻转把最大值移动到最后的位置，第一次翻转到首位，第二次翻转到后面的位置即可。 具体例子： 例如:[3,2,4,1]—-\u003e[?,?,?,4] 我们可以先找到数字4的位置,将数字4前进行翻转变成[4,2,3,1],接下来我们在整体翻转[1,3,2,4],这样我们把数字4移动列表底. 然后,我们[1,3,2,4]—\u003e[?,?,3,4],还是用刚才方法,首先找到数字3,翻转数字3前面的,再翻转已经排好数字(这里指数字4)前就可以了. ","date":"2022-02-19","objectID":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/:2:0","tags":["煎饼排序——选择排序的运用"],"title":"煎饼排序——选择排序的运用","uri":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/"},{"categories":["算法——排序原理运用"],"content":"解题代码 class Solution { public: vector\u003cint\u003e pancakeSort(vector\u003cint\u003e \u0026arr) { int n = arr.size(); vector\u003cint\u003e ret; while (n \u003e 1) { if (arr[n - 1] != n) { int index = find_if(arr.begin(), arr.end(), [n](auto \u0026num) { if (num == n) return true; return false; }) - arr.begin(); ret.push_back(index + 1); ret.push_back(n); reverse(arr.begin(), arr.begin() + index + 1); reverse(arr.begin(), arr.begin() + n); } --n; } return ret; } }; ","date":"2022-02-19","objectID":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/:3:0","tags":["煎饼排序——选择排序的运用"],"title":"煎饼排序——选择排序的运用","uri":"/posts/%E7%85%8E%E9%A5%BC%E6%8E%92%E5%BA%8F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E7%9A%84%E8%BF%90%E7%94%A8/"},{"categories":["算法——排序原理运用"],"content":"高频面试考点(考察分治思想)：合并k个排序链表","date":"2022-02-19","objectID":"/posts/%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E8%80%83%E7%82%B9%E8%80%83%E5%AF%9F%E5%88%86%E6%B2%BB%E6%80%9D%E6%83%B3%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8/","tags":["高频面试考点(考察分治思想)：合并k个排序链表"],"title":"高频面试考点(考察分治思想)：合并k个排序链表","uri":"/posts/%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E8%80%83%E7%82%B9%E8%80%83%E5%AF%9F%E5%88%86%E6%B2%BB%E6%80%9D%E6%83%B3%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8/"},{"categories":["算法——排序原理运用"],"content":"题目 题目解析 很明显，这种多个有序链表的排序可以分解为，两个过程： 合并两个有序链表的函数。 实现多次调用合并两个有序链表。 关于分治法如何优化该过程的？很明显如果直接从左往右调用多次合并两个有序链表来实现需要调用n-1次，而分治法通过先把数组分割成两个数组元素为一个基本的操作对象，那么很明显可以优化为调用logn次。 以下是两种方式的时间差距： 解题代码 朴素解法 class Solution { public: ListNode* mergeKLists(vector\u003cListNode*\u003e\u0026 lists) { int size = lists.size(); if(size==0) return nullptr; ListNode* res = lists[0]; for(int i=1;i\u003csize;i++){ res = mergeTwo(res,lists[i]); } return res; } private: ListNode* mergeTwo(ListNode* a,ListNode* b){ ListNode h; ListNode * head = \u0026h; ListNode* res = head; while(a\u0026\u0026b){ if(a-\u003eval\u003eb-\u003eval){ head-\u003enext = b; b = b-\u003enext; head = head-\u003enext; }else{ head-\u003enext = a; a = a-\u003enext; head = head-\u003enext; } } if(b){ head-\u003enext = b; }else{ head-\u003enext = a; } return res-\u003enext; } }; 分治法 class Solution { public: ListNode* mergeKLists(vector\u003cListNode*\u003e\u0026 lists) { return merge(lists,0,lists.size()-1); } private: ListNode* merge(vector \u003cListNode*\u003e \u0026lists, int l, int r) { if (l == r) return lists[l]; if (l \u003e r) return nullptr; int mid = (l + r) \u003e\u003e 1; //开始分治 return mergeTwo(merge(lists, l, mid), merge(lists, mid + 1, r)); } ListNode* mergeTwo(ListNode* a,ListNode* b){ ListNode h; ListNode * head = \u0026h; ListNode* res = head; while(a\u0026\u0026b){ if(a-\u003eval\u003eb-\u003eval){ head-\u003enext = b; b = b-\u003enext; head = head-\u003enext; }else{ head-\u003enext = a; a = a-\u003enext; head = head-\u003enext; } } if(b){ head-\u003enext = b; }else{ head-\u003enext = a; } return res-\u003enext; } }; ","date":"2022-02-19","objectID":"/posts/%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E8%80%83%E7%82%B9%E8%80%83%E5%AF%9F%E5%88%86%E6%B2%BB%E6%80%9D%E6%83%B3%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8/:0:0","tags":["高频面试考点(考察分治思想)：合并k个排序链表"],"title":"高频面试考点(考察分治思想)：合并k个排序链表","uri":"/posts/%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E8%80%83%E7%82%B9%E8%80%83%E5%AF%9F%E5%88%86%E6%B2%BB%E6%80%9D%E6%83%B3%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8/"},{"categories":["算法——动态规划"],"content":"骑士在棋盘上的概率——dp棋盘概率题","date":"2022-02-18","objectID":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/","tags":["骑士在棋盘上的概率——dp棋盘概率题"],"title":"骑士在棋盘上的概率——dp棋盘概率题","uri":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"题目 题目链接 ","date":"2022-02-18","objectID":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/:1:0","tags":["骑士在棋盘上的概率——dp棋盘概率题"],"title":"骑士在棋盘上的概率——dp棋盘概率题","uri":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"题目详解 一个骑士有 $8$ 种可能的走法，骑士会从中以等概率随机选择一种。部分走法可能会让骑士离开棋盘，另外的走法则会让骑士移动到棋盘的其他位置，并且剩余的移动次数会减少 1。 定义 $dp[step][i][j]$ 表示其实从棋盘商店的点 $(i,j)$ 出发，走了 $step$ 步时仍然留在棋盘上的概率。 当点 $(i,j)$ 不在棋盘上的时候，$dp[step][i][j] = 0;$ 当点 $(i,j)$ 在棋盘上且 $step = 0$ 时，$dp[step][i][j]=1$ 。 对于其他情况，$dp[step][i][j]=1/8×∑dp[step-1][i+di][j+dj]$。 其中$(di,dj)$ 表示走法对坐标的偏移量，具体为 $(−2,−1),(−2,1),(2,−1),(2,1),(−1,−2),(−1,2),(1,−2),(1,2)$ 共 $8$ 种。 ","date":"2022-02-18","objectID":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/:2:0","tags":["骑士在棋盘上的概率——dp棋盘概率题"],"title":"骑士在棋盘上的概率——dp棋盘概率题","uri":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"解题代码 class Solution { public: vector\u003cvector\u003cint\u003e\u003e dirs = {{-2, -1}, {-2, 1}, {2, -1}, {2, 1}, {-1, -2}, {-1, 2}, {1, -2}, {1, 2}}; double knightProbability(int n, int k, int row, int column) { vector\u003cvector\u003cvector\u003cdouble\u003e\u003e\u003e dp(k + 1, vector\u003cvector\u003cdouble\u003e\u003e(n, vector\u003cdouble\u003e(n))); for (int step = 0; step \u003c= k; step++) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c n; j++) { if (step == 0) { dp[step][i][j] = 1; } else { for (auto \u0026 dir : dirs) { int ni = i + dir[0], nj = j + dir[1]; if (ni \u003e= 0 \u0026\u0026 ni \u003c n \u0026\u0026 nj \u003e= 0 \u0026\u0026 nj \u003c n) { dp[step][i][j] += dp[step - 1][ni][nj] / 8; } } } } } } return dp[k][row][column]; } }; ","date":"2022-02-18","objectID":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/:3:0","tags":["骑士在棋盘上的概率——dp棋盘概率题"],"title":"骑士在棋盘上的概率——dp棋盘概率题","uri":"/posts/%E9%AA%91%E5%A3%AB%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87dp%E6%A3%8B%E7%9B%98%E6%A6%82%E7%8E%87%E9%A2%98/"},{"categories":["手写数据结构"],"content":"C++手撕哈希表详解","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"关于实现源码 实现源码仓库在线查看链接： C语言实现 C++实现 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:0:0","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"哈希表的理论知识 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:1:0","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"哈希表的定义 哈希表也叫散列表，我们先来看看哈希表的定义： 哈希表是保存键值映射关系的查找表，通过关键字可以很快找到对应的值。 简单说来说，哈希表由两个要素构成：桶数组 和 散列函数（哈希函数）。 桶数组：用于存储键值对的空间。 散列函数：用于给键值对在桶数组中的位置指路。 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:1:1","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"桶数组 我们可能知道，有一类基础的数据结构 线性表，而线性表又分两种，数组 和 链表。 哈希表数据结构里，存储元素的数据结构就是数组，数组里的每个单元都可以想象成一个 桶（Bucket）。 而我们每次都是把我们需要存入的键值对加入到这样的桶子中。 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:1:2","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"散列函数 我们需要在元素和 桶数组 对应位置建立一种映射映射关系，这种映射关系就是 散列函数 ，也可以叫哈希函数。 比如我们平时生活中，碰到排队型的时候，都需要根据高矮来进行一定的队形调整，这个调整过程也可以看做是散列函数的一种体现。 散列函数的构造 散列函数有很多类，这其中的奥妙来自于数学，而不是我们程序员需要过于操心的事情。 在Java语言中只要是继承自Object类的所有类都有默认的一个 hashcode() 方法，而对于具体过程我们不需要多想，我们来看看最常用的字符串的哈希过程： static inline size_t strHashcode(char *key) { size_t hash = 0; size_t index = 0; while (key[index] != '\\0') { hash = hash * 31 + key[index++]; } return hash; } 很明显得出的 hashcode 是一个以31为底的次方数，关于为什么以31为底，我这里简单的描述一下： 以31为底能很快得出比较散且比较大的二进制码（底层的01串），这样结合子掩码的与运算有利于减少哈希冲突的产生。 31 * i == (i \u003c\u003c 5) - i 因为这个等式的成立使得运算性能也有很大的提升。（编译器一般会对 31*i 进行优化的） 更多细节看看这篇文章：关于为什么选31 扰动函数 和 按位与 我们通过散列函数得到一堆 01 串后，我们该怎么做？ 接下来一般就是通过和桶数组的长度取余然后得到对应的位置进行插入。这样虽然也可以，但我们有更好的方式进行替代，那就是位运算。 既然要讲位运算，那么我先讲讲一个二进制串。 在只有一个 1 的二进制串里面，我们对它再减去 1 的时候，我们很快得到它的低位掩码。 比如： 0001000 - 1 = 0000111，得到的 0000111 有什么用处呢？ 如果我们把一个 hashcode 和这个数进行按位与，则得到的结果肯定是介于 000~111 之间，也就是 0~7 之间，这个时候我们思考一下，如果把这个 0001000 看做是桶数组的长度，那么这个按位与的结果就可以当做需要存入的桶的具体位置了。 基于这个理论，我们只要桶数组长度是 2的倍数 则 hashcode%size 可用 hashcode\u0026(size-1) 来替代。 这样做有以下好处： 位运算的性能更好。 便于控制 hashcode 最终得出的结果，有些时候我们得到的 hashcode不够均匀 高位的1比较多，而低位的1比较少，这个时候可以利用 hashcode^(hashcode\u003e\u003e16) 进行一定程度的打散，而这个打散的过程我们一般把它叫做 扰动函数 。 哈希冲突 当出现键值运算结果得到的桶子位置是同一个的时候便产生了哈希冲突。 而解决哈希冲突的方案一般有以下三种： 链地址法 一旦发生哈希冲突，直接生成链表往后继续延伸。 开放地址法 简单来说就是给冲突的元素再在桶数组里找到一个空闲的位置。 而常用采用的方法有： 线性探查法: 从冲突的位置开始，依次判断下一个位置是否空闲，直至找到空闲位置。 平方探查法: 从冲突的位置x开始，第一次增加 1^2 个位置，第二次增加 2^2…，直至找到空闲的位置。 双散列函数探查法 …… 再哈希法 再哈希法完全可以配备以上两类方法进行使用。 当然也可单独使用，单独使用的话就行配备多个哈希函数，一个不行的话换另一个哈希函数，直到不产生哈希冲突为止。 最终的选择： 而我们常用的是 链地址法 + 再哈希 ，为了能够尽量减少内存空间的使用，我们默认从容量为 16 的桶数组开始，一旦装入的键值对超过 capacity * factor 个时，我们进行一次两倍的（左移1位）扩容，而由于扩容会导致 capacity 改变，所以通过 哈希函数 + 与运算 得出的位置也会出错，故需要经过 再哈希 。 我们其实可以继续细想，我们左移一位后，得出的结果再减一，它也仅仅多出一位掩码，而我们的 hashcode 只要在这一位上为 0 则最后得到的桶位置不会有任何改变，只有在这一位上为 1 的才会发生改变，所以根据这个特点，Java 8 进行了一些优化，更厉害的优化方式在于，只要链比较长，它还会转红黑树（这就在我的能力范围之外了）。 以上内容参考的文章为：一个HashMap和面试官扯了半小时 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:1:3","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"HashMap实现 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:0","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"类型定义（键值对以及对应节点） C #define DEFAULT_CAPACITY 16 //初始的表长 #define DEFAULT_FACTOR 0.75f //初始的装载因子 /*类型定义 和 装载因子初始化*/ typedef int key_t; typedef int val_t; static const float factor = DEFAULT_FACTOR; //装载因子 typedef struct node {//每个哈希表的键值对 key_t key; val_t val; struct node *next; } Node; C++ 全程用的泛型模板 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:1","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"哈希表的数据 C typedef struct { size_t size; //记录已经存下的键值对数目 size_t capacity; //记录表长 Node **buckets; //桶子：用于记录的哈希桶，桶子中每个元素是Node* } HashMap; C++ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:2","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"初始化方法（构造方法） C HashMap *init() { //初始化得到一个哈希表 HashMap *ret = (HashMap *) malloc(sizeof(HashMap)); assert(ret != NULL); ret-\u003esize = 0; ret-\u003ecapacity = DEFAULT_CAPACITY; ret-\u003ebuckets = (Node **) calloc(DEFAULT_CAPACITY, sizeof(Node *)); assert(ret-\u003ebuckets != NULL); return ret; } C++ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:3","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"根据散列函数得到位置 C static inline size_t getHashcode(key_t key) { return key ^ (key \u003e\u003e 16);//这是32位数的扰动函数 } static inline size_t getIndex(key_t key, size_t bucket_size) {//由于bucketsize一定是2的次方，所以size-1和key相与得到的就是下标 return getHashcode(key) \u0026 (bucket_size - 1); } C++ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:4","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"put方法 C void put(HashMap *map, key_t key, val_t val) { assert(map != NULL); //判断是否需要扩容 if (map-\u003esize \u003e= map-\u003ecapacity * factor) resize(map); putVal(key, val, map-\u003ebuckets, \u0026map-\u003esize, map-\u003ecapacity); } static void putVal(key_t key, val_t val, Node **buckets, size_t *returnSize, size_t bucketSize) { //获取位置 size_t index = getIndex(key, bucketSize); Node *node = buckets[index]; if (node == NULL) {//插入位置为空 node = (Node *) malloc(sizeof(Node)); assert(node != NULL); node-\u003eval = val; node-\u003ekey = key; node-\u003enext = NULL; buckets[index] = node; (*returnSize)++; //哈希表内的元素增加 return; } //插入位置不为空，说明发生冲突，使用链地址法，遍历链表 while (node != NULL) { //如果key相同就覆盖 if (node-\u003ekey == key) { node-\u003eval = val; return; } node = node-\u003enext; } //当前的key不在链表中，则插入链表头部 Node *newNode = (Node *) malloc(sizeof(Node)); assert(newNode != NULL); newNode-\u003enext = buckets[index]; buckets[index] = newNode; (*returnSize)++; //哈希表内元素增加 } C++ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:5","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"扩容方法 C static void resize(HashMap *map) { map-\u003ecapacity \u003c\u003c= 1; //扩大两倍容量，相当于左移一位 Node **tmp = map-\u003ebuckets; //存下之前的内存地址 map-\u003ebuckets = (Node **) calloc(map-\u003ecapacity, sizeof(Node *)); //重新分配 assert(map-\u003ebuckets != NULL); rehash(map, tmp);//重新哈希处理 free(tmp); //释放之前的内存 } static void rehash(HashMap *map, Node **preTable) {//采取java1.7的方式进行rehash也就是最简单直接的直接重新哈希插入 size_t preCap = map-\u003ecapacity / 2; //改变前的有效区域 for (size_t i = 0; i \u003c preCap; i++) { if (preTable[i] != NULL) {//判断对应的key是否需要重新换位置,如果对最新掩码多出来的1敏感则需要rehash Node *preNode; Node *curNode = preTable[i]; while (curNode != NULL) { preNode = curNode; curNode = curNode-\u003enext; insert(map, preNode); } } } } C++ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:6","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"get方法 C val_t *get(HashMap *map, key_t key) {//前面的写好后，那么get就很好写了 int index = getIndex(key, map-\u003ecapacity); Node *node = map-\u003ebuckets[index]; while (node != NULL) { if (node-\u003ekey == key) { return \u0026(node-\u003eval); } node = node-\u003enext; } return NULL;//没找到返回NULL指针 } C++ C++标准中主要采用重载下标运算符的方式进行get。 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:7","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"完整代码 C完整代码 C++完整代码 ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:8","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"测试：LeetCode 1.两数之和 这效率在用C语言哈希表的方法里面应该是无敌的存在了。。。试了下比UT_HASH还要快一些☺ 再看看之前写的比较烂的哈希表的效率😂 测试源码： // // Created by L_B__ on 2021/11/24. // //底层的表长都用2的次方，然后-1后可以得到低位掩码 //该设计理念急于java1.7的源代码，本来是想基于1.8的实现， // 因为1.8巧妙太多，比如对rehash尽量能不动的就不动，再比如对于链比较长的结构直接转红黑树。可惜能力不能及 /** * 简单说明：static修饰的函数为单文件的中间委托功能函数，不对外公开 * 简单的功能函数介绍： * init()得到一个默认表长的哈希表 * put()插入键值，内部自动进行内存的申请 * get()得到key对应的val的地址，若不存在该键值对返回NULL * insert()外界已经分配好node的内存和key\u0026val对，进行哈希运算后直接插入即可 * destroy()把整个哈希表牵扯的所有内存释放 * **/ #include \u003cassert.h\u003e#include \u003cstdlib.h\u003e#ifndef MY_TINY_STL_HASHMAP_C_H #define MY_TINY_STL_HASHMAP_C_H #define DEFAULT_CAPACITY 128 //初始的表长 #define DEFAULT_FACTOR 0.75f //初始的装载因子 /*类型定义 和 装载因子初始化*/ typedef int key_t; typedef int val_t ; static const float factor = DEFAULT_FACTOR; //装载因子 typedef struct node{//每个哈希表的键值对 key_t key; val_t val; struct node* next; }Node; typedef struct { size_t size; //记录已经存下的键值对数目 size_t capacity; //记录表长 Node** buckets; //桶子：用于记录的哈希桶，桶子中每个元素是Node* }HashMap; /*函数的声明*/ HashMap* init(); void put(HashMap*,key_t,val_t); void insert(HashMap*,Node*); //直接把已经分配好的内存插入哈希表 static void putVal(key_t,val_t,Node**,size_t*,size_t); //这个是put的委托函数,用于直接更新桶子，并更新HashMap的size static inline size_t getHashcode(key_t ); //得到key对应的hashcode static inline size_t getIndex(key_t,size_t); //通过桶的大小和key映射位置，算是包含了关键的哈希函数：由于C不支持泛型也就无法针对不同类型作出不同的哈希了，我这里默认key为int static void resize(HashMap*); //如果插入的元素过多，*2进行重新哈希分配 static void rehash(HashMap*,Node**); //重新设置长度则需要重新哈希一些key的位置 val_t* get(HashMap*,key_t); //得到key对应的val static void del_nodes(Node*); //把单个链表销毁 void destroy(HashMap*); //把哈希表的内存销毁 /*函数实现*/ HashMap* init(){ //初始化得到一个哈希表 HashMap * ret = (HashMap*)malloc(sizeof(HashMap)); assert(ret!=NULL); ret-\u003esize = 0; ret-\u003ecapacity = DEFAULT_CAPACITY; ret-\u003ebuckets = (Node**)calloc(DEFAULT_CAPACITY,sizeof(Node*)); assert(ret-\u003ebuckets!=NULL); return ret; } void insert(HashMap* map,Node* node){ assert(map!=NULL\u0026\u0026node!=NULL); size_t index = getIndex(node-\u003ekey,map-\u003ecapacity); if(map-\u003ebuckets[index]==NULL){ node-\u003enext = NULL; map-\u003ebuckets[index] = node; }else{ node-\u003enext = map-\u003ebuckets[index]; map-\u003ebuckets[index] = node; } } void put(HashMap* map,key_t key,val_t val){ assert(map != NULL); //判断是否需要扩容 if(map-\u003esize \u003e= map-\u003ecapacity*factor) resize(map); putVal(key,val,map-\u003ebuckets,\u0026map-\u003esize,map-\u003ecapacity); } static inline size_t getHashcode(key_t key){ return key ^ (key\u003e\u003e16);//这是32位数的扰动函数 } static inline size_t getIndex(key_t key,size_t bucket_size){//由于bucketsize一定是2的次方，所以size-1和key相与得到的就是下标 return getHashcode(key)\u0026(bucket_size-1); } static void putVal(key_t key,val_t val,Node** buckets,size_t* returnSize,size_t bucketSize){ //获取位置 size_t index = getIndex(key,bucketSize); Node * node = buckets[index]; if(node==NULL){//插入位置为空 node = (Node*)malloc(sizeof(Node)); assert(node!=NULL); node-\u003eval = val; node-\u003ekey = key; node-\u003enext = NULL; buckets[index] = node; (*returnSize)++; //哈希表内的元素增加 return; } //插入位置不为空，说明发生冲突，使用链地址法，遍历链表 while (node!=NULL){ //如果key相同就覆盖 if(node-\u003ekey==key){ node-\u003eval = val; return; } node = node-\u003enext; } //当前的key不在链表中，则插入链表头部 Node* newNode = (Node*)malloc(sizeof(Node)); assert(newNode != NULL); newNode-\u003enext = buckets[index]; buckets[index] = newNode; (*returnSize)++; //哈希表内元素增加 } static void resize(HashMap* map){ map-\u003ecapacity \u003c\u003c= 1; //扩大两倍容量，相当于左移一位 Node ** tmp = map-\u003ebuckets; //存下之前的内存地址 map-\u003ebuckets = (Node**)calloc(map-\u003ecapacity,sizeof(Node*)); //重新分配 assert(map-\u003ebuckets!=NULL); rehash(map,tmp);//重新哈希处理 free(tmp); //释放之前的内存 } static void rehash(HashMap* map,Node** preTable){//采取java1.7的方式进行rehash也就是最简单直接的直接重新哈希插入 size_t preCap = map-\u003ecapacity / 2; //改变前的有效区域 for(size_t i=0;i\u003cpreCap;i++){ if(preTable[i] != NULL){//判断对应的key是否需要重新换位置,如果对最新掩码多出来的1敏感则需要rehash Node *preNode; Node *curNode = preTable[i]; while (curNode!=NULL){ preNode = curNode; curNode = curNode-\u003enext; insert(map,preNode); } } } } val_t* get(HashMap* map,key_t key){//前面的写好后，那么get就很好写了 int index = getIndex(key,map-\u003ecapacity); Node * node = map-\u003ebuckets[index]; while (node != NULL){ if(node-\u003ekey==key){ retur","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:2:9","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"总结 通过这次手撕比较规范的哈希表的收获： 更加深入的理解了哈希表的基本实现思想。 从LeetCode刷题无用的感觉到有用，比如时刻需要的链表增删操作。 有点遗憾，没能按照java1.8的思路再进一步优化哈希表。 看了很多大厂的面试题，现在对哈希表这块数据结构的考量是越来越严苛了，比如你如果走的 java 后端岗位，面试可能需要你回答 java HashMap 源码的相关实现部分，深挖原理，而不是死记硬背了，所以最好的学习方式就是学习源码，然后根据学到的思想自己实现一些功能。 参考： [1]. 《数据结构与算法》 [2]. HashMap跟面试官扯了半个小时 [3]. hashCode 为什么乘以 31？ ","date":"2022-02-16","objectID":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/:3:0","tags":["C/C++手撕哈希表详解"],"title":"C/C++手撕哈希表详解","uri":"/posts/c++%E6%89%8B%E6%92%95%E5%93%88%E5%B8%8C%E8%A1%A8%E8%AF%A6%E8%A7%A3/"},{"categories":["JavaWeb笔记"],"content":"Java连接数据库","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Java与数据库 通过Java如何去使用数据库来帮助我们存储数据呢，这将是本章节讨论的重点。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"初识JDBC JDBC是什么？JDBC英文名为：Java Data Base Connectivity(Java数据库连接)，官方解释它是Java编程语言和广泛的数据库之间独立于数据库的连接标准的Java API，根本上说JDBC是一种规范，它提供的接口，一套完整的，允许便捷式访问底层数据库。可以用JAVA来写不同类型的可执行文件：JAVA应用程序、JAVA Applets、Java Servlet、JSP等，不同的可执行文件都能通过JDBC访问数据库，又兼备存储的优势。简单说它就是Java与数据库的连接的桥梁或者插件，用Java代码就能操作数据库的增删改查、存储过程、事务等。 我们可以发现，JDK自带了一个java.sql包，而这里面就定义了大量的接口，不同类型的数据库，都可以通过实现此接口，编写适用于自己数据库的实现类。而不同的数据库厂商实现的这套标准，我们称为数据库驱动。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"准备工作 那么我们首先来进行一些准备工作，以便开始JDBC的学习： 将idea连接到我们的数据库，以便以后调试。 将mysql驱动jar依赖导入到项目中（推荐6.0版本以上，这里用到是8.0） 向Jetbrians申请一个学生/教师授权，用于激活idea终极版（进行JavaWeb开发需要用到，一般申请需要3-7天时间审核）不是大学生的话…emmm…懂的都懂。 教育授权申请地址：https://www.jetbrains.com/shop/eform/students 一个Java程序并不是一个人的战斗，我们可以在别人开发的基础上继续向上开发，其他的开发者可以将自己编写的Java代码打包为jar，我们只需要导入这个jar作为依赖，即可直接使用别人的代码，就像我们直接去使用JDK提供的类一样。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用JDBC连接数据库 **注意：**6.0版本以上，不用手动加载驱动，我们直接使用即可！ //1. 通过DriverManager来获得数据库连接 try (Connection connection = DriverManager.getConnection(\"连接URL\",\"用户名\",\"密码\"); //2. 创建一个用于执行SQL的Statement对象 Statement statement = connection.createStatement()){ //注意前两步都放在try()中，因为在最后需要释放资源！ //3. 执行SQL语句，并得到结果集 ResultSet set = statement.executeQuery(\"select * from 表名\"); //4. 查看结果 while (set.next()){ ... } }catch (SQLException e){ e.printStackTrace(); } //5. 释放资源，try-with-resource语法会自动帮助我们close 其中，连接的URL如果记不住格式，我们可以打开idea的数据库连接配置，复制一份即可。（其实idea本质也是使用的JDBC，整个idea程序都是由Java编写的，实际上idea就是一个Java程序） ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:2","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"了解DriverManager 我们首先来了解一下DriverManager是什么东西，它其实就是管理我们的数据库驱动的： public static synchronized void registerDriver(java.sql.Driver driver, DriverAction da) throws SQLException { /* Register the driver if it has not already been added to our list */ if(driver != null) { registeredDrivers.addIfAbsent(new DriverInfo(driver, da)); //在刚启动时，mysql实现的驱动会被加载，我们可以断点调试一下。 } else { // This is for compatibility with the original DriverManager throw new NullPointerException(); } println(\"registerDriver: \" + driver); } 我们可以通过调用getConnection()来进行数据库的链接： @CallerSensitive public static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(\"user\", user); } if (password != null) { info.put(\"password\", password); } return (getConnection(url, info, Reflection.getCallerClass())); //内部有实现 } 我们可以手动为驱动管理器添加一个日志打印： static { DriverManager.setLogWriter(new PrintWriter(System.out)); //这里直接设定为控制台输出 } 现在我们执行的数据库操作日志会在控制台实时打印。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:3","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"了解Connection Connection是数据库的连接对象，可以通过连接对象来创建一个Statement用于执行SQL语句： Statement createStatement() throws SQLException; 我们发现除了普通的Statement，还存在PreparedStatement： PreparedStatement prepareStatement(String sql) throws SQLException; 在后面我们会详细介绍PreparedStatement的使用，它能够有效地预防SQL注入式攻击。 它还支持事务的处理，也放到后面来详细进行讲解。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:4","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"了解Statement 我们发现，我们之前使用了executeQuery()方法来执行select语句，此方法返回给我们一个ResultSet对象，查询得到的数据，就存放在ResultSet中！ Statement除了执行这样的DQL语句外，我们还可以使用executeUpdate()方法来执行一个DML或是DDL语句，它会返回一个int类型，表示执行后受影响的行数，可以通过它来判断DML语句是否执行成功。 也可以通过excute()来执行任意的SQL语句，它会返回一个boolean来表示执行结果是一个ResultSet还是一个int，我们可以通过使用getResultSet()或是getUpdateCount()来获取。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:5","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"执行DML操作 我们通过几个例子来向数据库中插入数据。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:6","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"执行DQL操作 执行DQL操作会返回一个ResultSet对象，我们来看看如何从ResultSet中去获取数据： //首先要明确，select返回的数据类似于一个excel表格 while (set.next()){ //每调用一次next()就会向下移动一行，首次调用会移动到第一行 } 我们在移动行数后，就可以通过set中提供的方法，来获取每一列的数据。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:7","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"执行批处理操作 当我们要执行很多条语句时，可以不用一次一次地提交，而是一口气全部交给数据库处理，这样会节省很多的时间。 public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(); Statement statement = connection.createStatement()){ statement.addBatch(\"insert into user values ('f', 1234)\"); statement.addBatch(\"insert into user values ('e', 1234)\"); //添加每一条批处理语句 statement.executeBatch(); //一起执行 }catch (SQLException e){ e.printStackTrace(); } } ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:8","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"将查询结果映射为对象 既然我们现在可以从数据库中获取数据了，那么现在就可以将这些数据转换为一个类来进行操作，首先定义我们的实体类： public class Student { Integer sid; String name; String sex; public Student(Integer sid, String name, String sex) { this.sid = sid; this.name = name; this.sex = sex; } public void say(){ System.out.println(\"我叫：\"+name+\"，学号为：\"+sid+\"，我的性别是：\"+sex); } } 现在我们来进行一个转换： while (set.next()){ Student student = new Student(set.getInt(1), set.getString(2), set.getString(3)); student.say(); } **注意：**列的下标是从1开始的。 我们也可以利用反射机制来将查询结果映射为对象，使用反射的好处是，无论什么类型都可以通过我们的方法来进行实体类型映射： private static \u003cT\u003e T convert(ResultSet set, Class\u003cT\u003e clazz){ try { Constructor\u003cT\u003e constructor = clazz.getConstructor(clazz.getConstructors()[0].getParameterTypes()); //默认获取第一个构造方法 Class\u003c?\u003e[] param = constructor.getParameterTypes(); //获取参数列表 Object[] object = new Object[param.length]; //存放参数 for (int i = 0; i \u003c param.length; i++) { //是从1开始的 object[i] = set.getObject(i+1); if(object[i].getClass() != param[i]) throw new SQLException(\"错误的类型转换：\"+object[i].getClass()+\" -\u003e \"+param[i]); } return constructor.newInstance(object); } catch (ReflectiveOperationException | SQLException e) { e.printStackTrace(); return null; } } 现在我们就可以通过我们的方法来将查询结果转换为一个对象了： while (set.next()){ Student student = convert(set, Student.class); if(student != null) student.say(); } 实际上，在后面我们会学习Mybatis框架，它对JDBC进行了深层次的封装，而它就进行类似上面反射的操作来便于我们对数据库数据与实体类的转换。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:9","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"实现登陆与SQL注入攻击 在使用之前，我们先来看看如果我们想模拟登陆一个用户，我们该怎么去写： try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); Statement statement = connection.createStatement(); Scanner scanner = new Scanner(System.in)){ ResultSet res = statement.executeQuery(\"select * from user where username='\"+scanner.nextLine()+\"'and pwd='\"+scanner.nextLine()+\"';\"); while (res.next()){ String username = res.getString(1); System.out.println(username+\" 登陆成功！\"); } }catch (SQLException e){ e.printStackTrace(); } 用户可以通过自己输入用户名和密码来登陆，乍一看好像没啥问题，那如果我输入的是以下内容呢： Test1111' or 1=1; -- # Test 登陆成功！ 1=1一定是true，那么我们原本的SQL语句会变为： select*fromuserwhereusername='Test'andpwd='1111'or1=1;-- ' 我们发现，如果允许这样的数据插入，那么我们原有的SQL语句结构就遭到了破坏，使得用户能够随意登陆别人的账号。因此我们可能需要限制用户的输入来防止用户输入一些SQL语句关键字，但是关键字非常多，这并不是解决问题的最好办法。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:10","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用PreparedStatement 我们发现，如果单纯地使用Statement来执行SQL命令，会存在严重的SQL注入攻击漏洞！而这种问题，我们可以使用PreparedStatement来解决： public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); PreparedStatement statement = connection.prepareStatement(\"select * from user where username= ? and pwd=?;\"); Scanner scanner = new Scanner(System.in)){ statement.setString(1, scanner.nextLine()); statement.setString(2, scanner.nextLine()); System.out.println(statement); //打印查看一下最终执行的 ResultSet res = statement.executeQuery(); while (res.next()){ String username = res.getString(1); System.out.println(username+\" 登陆成功！\"); } }catch (SQLException e){ e.printStackTrace(); } } 我们发现，我们需要提前给到PreparedStatement一个SQL语句，并且使用?作为占位符，它会预编译一个SQL语句，通过直接将我们的内容进行替换的方式来填写数据。使用这种方式，我们之前的例子就失效了！我们来看看实际执行的SQL语句是什么： com.mysql.cj.jdbc.ClientPreparedStatement: select * from user where username= 'Test' and pwd='123456'' or 1=1; -- ';\r我们发现，我们输入的参数一旦出现'时，会被变为转义形式\\'，而最外层有一个真正的'来将我们输入的内容进行包裹，因此它能够有效地防止SQL注入攻击！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:11","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"管理事务 JDBC默认的事务处理行为是自动提交，所以前面我们执行一个SQL语句就会被直接提交（相当于没有启动事务），所以JDBC需要进行事务管理时，首先要通过Connection对象调用setAutoCommit(false) 方法, 将SQL语句的提交（commit）由驱动程序转交给应用程序负责。 con.setAutoCommit(); //关闭自动提交后相当于开启事务。 // SQL语句 // SQL语句 // SQL语句 con.commit();或 con.rollback(); 一旦关闭自动提交，那么现在执行所有的操作如果在最后不进行commit()来提交事务的话，那么所有的操作都会丢失，只有提交之后，所有的操作才会被保存！也可以使用rollback()来手动回滚之前的全部操作！ public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); Statement statement = connection.createStatement()){ connection.setAutoCommit(false); //关闭自动提交，现在将变为我们手动提交 statement.executeUpdate(\"insert into user values ('a', 1234)\"); statement.executeUpdate(\"insert into user values ('b', 1234)\"); statement.executeUpdate(\"insert into user values ('c', 1234)\"); connection.commit(); //如果前面任何操作出现异常，将不会执行commit()，之前的操作也就不会生效 }catch (SQLException e){ e.printStackTrace(); } } 我们来接着尝试一下使用回滚操作： public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); Statement statement = connection.createStatement()){ connection.setAutoCommit(false); //关闭自动提交，现在将变为我们手动提交 statement.executeUpdate(\"insert into user values ('a', 1234)\"); statement.executeUpdate(\"insert into user values ('b', 1234)\"); connection.rollback(); //回滚，撤销前面全部操作 statement.executeUpdate(\"insert into user values ('c', 1234)\"); connection.commit(); //提交事务（注意，回滚之前的内容都没了） }catch (SQLException e){ e.printStackTrace(); } } 同样的，我们也可以去创建一个回滚点来实现定点回滚： public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); Statement statement = connection.createStatement()){ connection.setAutoCommit(false); //关闭自动提交，现在将变为我们手动提交 statement.executeUpdate(\"insert into user values ('a', 1234)\"); Savepoint savepoint = connection.setSavepoint(); //创建回滚点 statement.executeUpdate(\"insert into user values ('b', 1234)\"); connection.rollback(savepoint); //回滚到回滚点，撤销前面全部操作 statement.executeUpdate(\"insert into user values ('c', 1234)\"); connection.commit(); //提交事务（注意，回滚之前的内容都没了） }catch (SQLException e){ e.printStackTrace(); } } 通过开启事务，我们就可以更加谨慎地进行一些操作了，如果我们想从事务模式切换为原有的自动提交模式，我们可以直接将其设置回去： public static void main(String[] args) throws ClassNotFoundException { try (Connection connection = DriverManager.getConnection(\"URL\",\"用户名\",\"密码\"); Statement statement = connection.createStatement()){ connection.setAutoCommit(false); //关闭自动提交，现在将变为我们手动提交 statement.executeUpdate(\"insert into user values ('a', 1234)\"); connection.setAutoCommit(true); //重新开启自动提交，开启时把之前的事务模式下的内容给提交了 statement.executeUpdate(\"insert into user values ('d', 1234)\"); //没有commit也成功了！ }catch (SQLException e){ e.printStackTrace(); } 通过学习JDBC，我们现在就可以通过Java来访问和操作我们的数据库了！为了更好地衔接，我们还会接着讲解主流持久层框架——Mybatis，加深JDBC的记忆。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:1:12","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用Lombok 我们发现，在以往编写项目时，尤其是在类进行类内部成员字段封装时，需要编写大量的get/set方法，这不仅使得我们类定义中充满了get和set方法，同时如果字段名称发生改变，又要挨个进行修改，甚至当字段变得很多时，构造方法的编写会非常麻烦！ 通过使用Lombok（小辣椒）就可以解决这样的问题！ 我们来看看，使用原生方式和小辣椒方式编写类的区别，首先是传统方式： public class Student { private Integer sid; private String name; private String sex; public Student(Integer sid, String name, String sex) { this.sid = sid; this.name = name; this.sex = sex; } public Integer getSid() { //长！ return sid; } public void setSid(Integer sid) { //到！ this.sid = sid; } public String getName() { //爆！ return name; } public void setName(String name) { //炸！ this.name = name; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } } 而使用Lombok之后： @Getter @Setter @AllArgsConstructor public class Student { private Integer sid; private String name; private String sex; } 我们发现，使用Lombok之后，只需要添加几个注解，就能够解决掉我们之前长长的一串代码！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"配置Lombok 首先我们需要导入Lombok的jar依赖，和jdbc依赖是一样的，放在项目目录下直接导入就行了。可以在这里进行下载：https://projectlombok.org/download 然后我们要安装一下Lombok插件，由于IDEA默认都安装了Lombok的插件，因此直接导入依赖后就可以使用了。 重启IDEA Lombok是一种插件化注解API，是通过添加注解来实现的，然后在javac进行编译的时候，进行处理。 Java的编译过程可以分成三个阶段： 所有源文件会被解析成语法树。 调用注解处理器。如果注解处理器产生了新的源文件，新文件也要进行编译。 最后，语法树会被分析并转化成类文件。 实际上在上述的第二阶段，会执行*lombok.core.AnnotationProcessor*，它所做的工作就是我们上面所说的，修改语法树。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:2:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用Lombok 我们通过实战来演示一下Lombok的实用注解： 我们通过添加@Getter和@Setter来为当前类的所有字段生成get/set方法，他们可以添加到类或是字段上，注意静态字段不会生成，final字段无法生成set方法。 我们还可以使用@Accessors来控制生成Getter和Setter的样式。 我们通过添加@ToString来为当前类生成预设的toString方法。 我们可以通过添加@EqualsAndHashCode来快速生成比较和哈希值方法。 我们可以通过添加@AllArgsConstructor和@NoArgsConstructor来快速生成全参构造和无参构造。 我们可以添加@RequiredArgsConstructor来快速生成参数只包含final或被标记为@NonNull的成员字段。 使用@Data能代表@Setter、@Getter、@RequiredArgsConstructor、@ToString、@EqualsAndHashCode全部注解。 一旦使用@Data就不建议此类有继承关系，因为equal方法可能不符合预期结果（尤其是仅比较子类属性）。 使用@Value与@Data类似，但是并不会生成setter并且成员属性都是final的。 使用@SneakyThrows来自动生成try-catch代码块。 使用@Cleanup作用与局部变量，在最后自动调用其close()方法（可以自由更换） 使用@Builder来快速生成建造者模式。 通过使用@Builder.Default来指定默认值。 通过使用@Builder.ObtainVia来指定默认值的获取方式。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:2:2","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"认识Mybatis 在前面JDBC的学习中，虽然我们能够通过JDBC来连接和操作数据库，但是哪怕只是完成一个SQL语句的执行，都需要编写大量的代码，更不用说如果我还需要进行实体类映射，将数据转换为我们可以直接操作的实体类型，JDBC很方便，但是还不够方便，我们需要一种更加简洁高效的方式来和数据库进行交互。 **再次强调：**学习厉害的框架或是厉害的技术，并不是为了一定要去使用它，而是它们能够使得我们在不同的开发场景下，合理地使用这些技术，以灵活地应对需要解决的问题。 MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Ordinary Java Object,普通的 Java对象)映射成数据库中的记录。 我们依然使用传统的jar依赖方式，从最原始开始讲起，不使用Maven，有关Maven内容我们会在后面统一讲解！全程围绕官方文档讲解！ 这一块内容很多很杂，再次强调要多实践！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"XML语言概述 在开始介绍Mybatis之前，XML语言发明最初是用于数据的存储和传输，它可以长这样： \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003couter\u003e \u003cname\u003e阿伟\u003c/name\u003e \u003cdesc\u003e怎么又在玩电动啊\u003c/desc\u003e \u003cinner type=\"1\"\u003e \u003cage\u003e10\u003c/age\u003e \u003csex\u003e男\u003c/sex\u003e \u003c/inner\u003e \u003c/outer\u003e 如果你学习过前端知识，你会发现它和HTML几乎长得一模一样！但是请注意，虽然它们长得差不多，但是他们的意义却不同，HTML主要用于通过编排来展示数据，而XML主要是存放数据，它更像是一个配置文件！当然，浏览器也是可以直接打开XML文件的。 一个XML文件存在以下的格式规范： 必须存在一个根节点，将所有的子标签全部包含。 可以但不必须包含一个头部声明（主要是可以设定编码格式） 所有的标签必须成对出现，可以嵌套但不能交叉嵌套 区分大小写。 标签中可以存在属性，比如上面的type=\"1\"就是inner标签的一个属性，属性的值由单引号或双引号包括。 XML文件也可以使用注释： \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003c!-- 注释内容 --\u003e 通过IDEA我们可以使用Ctrl+/来快速添加注释文本（不仅仅适用于XML，还支持很多种类型的文件） 那如果我们的内容中出现了\u003c或是\u003e字符，那该怎么办呢？我们就可以使用XML的转义字符来代替： 如果嫌一个一个改太麻烦，也可以使用CD来快速创建不解析区域： \u003ctest\u003e \u003cname\u003e\u003c![CDATA[我看你\u003c\u003e\u003c\u003e\u003c\u003e是一点都不懂哦\u003e\u003e\u003e]]\u003e\u003c/name\u003e \u003c/test\u003e 那么，我们现在了解了XML文件的定义，现在该如何去解析一个XML文件呢？比如我们希望将定义好的XML文件读取到Java程序中，这时该怎么做呢？ JDK为我们内置了一个叫做org.w3c的XML解析库，我们来看看如何使用它来进行XML文件内容解析： // 创建DocumentBuilderFactory对象 DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); // 创建DocumentBuilder对象 try { DocumentBuilder builder = factory.newDocumentBuilder(); Document d = builder.parse(\"file:mappers/test.xml\"); // 每一个标签都作为一个节点 NodeList nodeList = d.getElementsByTagName(\"test\"); // 可能有很多个名字为test的标签 Node rootNode = nodeList.item(0); // 获取首个 NodeList childNodes = rootNode.getChildNodes(); // 一个节点下可能会有很多个节点，比如根节点下就囊括了所有的节点 //节点可以是一个带有内容的标签（它内部就还有子节点），也可以是一段文本内容 for (int i = 0; i \u003c childNodes.getLength(); i++) { Node child = childNodes.item(i); if(child.getNodeType() == Node.ELEMENT_NODE) //过滤换行符之类的内容，因为它们都被认为是一个文本节点 System.out.println(child.getNodeName() + \"：\" +child.getFirstChild().getNodeValue()); // 输出节点名称，也就是标签名称，以及标签内部的文本（内部的内容都是子节点，所以要获取内部的节点） } } catch (Exception e) { e.printStackTrace(); } 当然，学习和使用XML只是为了更好地去认识Mybatis的工作原理，以及如何使用XML来作为Mybatis的配置文件，这是在开始之前必须要掌握的内容（使用Java读取XML内容不要求掌握，但是需要知道Mybatis就是通过这种方式来读取配置文件的） 不仅仅是Mybatis，包括后面的Spring等众多框架都会用到XML来作为框架的配置文件！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"初次使用Mybatis 那么我们首先来感受一下Mybatis给我们带来的便捷，就从搭建环境开始，中文文档网站：https://mybatis.org/mybatis-3/zh/configuration.html 我们需要导入Mybatis的依赖，Jar包需要在github上下载，如果卡得一匹，连不上可以在视频简介处从分享的文件中获取。同样地放入到项目的根目录下，右键作为依赖即可！（依赖变多之后，我们可以将其放到一个单独的文件夹，不然会很繁杂） 依赖导入完成后，我们就可以编写Mybatis的配置文件了（现在不是在Java代码中配置了，而是通过一个XML文件去配置，这样就使得硬编码的部分大大减少，项目后期打包成Jar运行不方便修复，但是通过配置文件，我们随时都可以去修改，就变得很方便了，同时代码量也大幅度减少，配置文件填写完成后，我们只需要关心项目的业务逻辑而不是如何去读取配置文件）我们按照官方文档给定的提示，在项目根目录下新建名为mybatis-config.xml的文件，并填写以下内容： \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003c!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"\u003e \u003cconfiguration\u003e \u003cenvironments default=\"development\"\u003e \u003cenvironment id=\"development\"\u003e \u003ctransactionManager type=\"JDBC\"/\u003e \u003cdataSource type=\"POOLED\"\u003e \u003cproperty name=\"driver\" value=\"${驱动类（含包名）}\"/\u003e \u003cproperty name=\"url\" value=\"${数据库连接URL}\"/\u003e \u003cproperty name=\"username\" value=\"${用户名}\"/\u003e \u003cproperty name=\"password\" value=\"${密码}\"/\u003e \u003c/dataSource\u003e \u003c/environment\u003e \u003c/environments\u003e \u003c/configuration\u003e 我们发现，在最上方还引入了一个叫做DTD（文档类型定义）的东西，它提前帮助我们规定了一些标签，我们就需要使用Mybatis提前帮助我们规定好的标签来进行配置（因为只有这样Mybatis才能正确识别我们配置的内容） 通过进行配置，我们就告诉了Mybatis我们链接数据库的一些信息，包括URL、用户名、密码等，这样Mybatis就知道该链接哪个数据库、使用哪个账号进行登陆了（也可以不使用配置文件，这里不做讲解，还请各位小伙伴自行阅读官方文档） 配置文件完成后，我们需要在Java程序启动时，让Mybatis对配置文件进行读取并得到一个SqlSessionFactory对象： public static void main(String[] args) throws FileNotFoundException { SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(new FileInputStream(\"mybatis-config.xml\")); try (SqlSession sqlSession = sqlSessionFactory.openSession(true)){ //暂时还没有业务 } } 直接运行即可，虽然没有干什么事情，但是不会出现错误，如果之前的配置文件编写错误，直接运行会产生报错！那么现在我们来看看，SqlSessionFactory对象是什么东西： 每个基于 MyBatis 的应用都是以一个 SqlSessionFactory 的实例为核心的，我们可以通过SqlSessionFactory来创建多个新的会话，SqlSession对象，每个会话就相当于我不同的地方登陆一个账号去访问数据库，你也可以认为这就是之前JDBC中的Statement对象，会话之间相互隔离，没有任何关联。 而通过SqlSession就可以完成几乎所有的数据库操作，我们发现这个接口中定义了大量数据库操作的方法，因此，现在我们只需要通过一个对象就能完成数据库交互了，极大简化了之前的流程。 我们来尝试一下直接读取实体类，读取实体类肯定需要一个映射规则，比如类中的哪个字段对应数据库中的哪个字段，在查询语句返回结果后，Mybatis就会自动将对应的结果填入到对象的对应字段上。首先编写实体类，，直接使用Lombok是不是就很方便了： import lombok.Data; @Data public class Student { int sid; //名称最好和数据库字段名称保持一致，不然可能会映射失败导致查询结果丢失 String name; String sex; } 在根目录下重新创建一个mapper文件夹，新建名为TestMapper.xml的文件作为我们的映射器，并填写以下内容： \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003c!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"\u003e \u003cmapper namespace=\"TestMapper\"\u003e \u003cselect id=\"selectStudent\" resultType=\"com.test.entity.Student\"\u003e select * from student \u003c/select\u003e \u003c/mapper\u003e 其中namespace就是命名空间，每个Mapper都是唯一的，因此需要用一个命名空间来区分，它还可以用来绑定一个接口。我们在里面写入了一个select标签，表示添加一个select操作，同时id作为操作的名称，resultType指定为我们刚刚定义的实体类，表示将数据库结果映射为Student类，然后就在标签中写入我们的查询语句即可。 编写好后，我们在配置文件中添加这个Mapper映射器： \u003cmappers\u003e \u003cmapper url=\"file:mappers/TestMapper.xml\"/\u003e \u003c!-- 这里用的是url，也可以使用其他类型，我们会在后面讲解 --\u003e \u003c/mappers\u003e 最后在程序中使用我们定义好的Mapper即可： public static void main(String[] args) throws FileNotFoundException { SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(new FileInputStream(\"mybatis-config.xml\")); try (SqlSession sqlSession = sqlSessionFactory.openSession(true)){ List\u003cStudent\u003e student = sqlSession.selectList(\"selectStudent\"); student.forEach(System.out::println); } } 我们会发现，Mybatis非常智能，我们只需要告诉一个映射关系，就能够直接将查询结果转化为一个实体类！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:2","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"配置Mybatis 在了解了Mybatis为我们带来的便捷之后，现在我们就可以正式地去学习使用Mybatis了！ 由于SqlSessionFactory一般只需要创建一次，因此我们可以创建一个工具类来集中创建SqlSession，这样会更加方便一些： public class MybatisUtil { //在类加载时就进行创建 private static SqlSessionFactory sqlSessionFactory; static { try { sqlSessionFactory = new SqlSessionFactoryBuilder().build(new FileInputStream(\"mybatis-config.xml\")); } catch (FileNotFoundException e) { e.printStackTrace(); } } /** * 获取一个新的会话 * @param autoCommit 是否开启自动提交（跟JDBC是一样的，如果不自动提交，则会变成事务操作） * @return SqlSession对象 */ public static SqlSession getSession(boolean autoCommit){ return sqlSessionFactory.openSession(autoCommit); } } 现在我们只需要在main方法中这样写即可查询结果了： public static void main(String[] args) { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ List\u003cStudent\u003e student = sqlSession.selectList(\"selectStudent\"); student.forEach(System.out::println); } } 之前我们演示了，如何创建一个映射器来将结果快速转换为实体类，但是这样可能还是不够方便，我们每次都需要去找映射器对应操作的名称，而且还要知道对应的返回类型，再通过SqlSession来执行对应的方法，能不能再方便一点呢？ 现在，我们可以通过namespace来绑定到一个接口上，利用接口的特性，我们可以直接指明方法的行为，而实际实现则是由Mybatis来完成。 public interface TestMapper { List\u003cStudent\u003e selectStudent(); } 将Mapper文件的命名空间修改为我们的接口，建议同时将其放到同名包中，作为内部资源： \u003cmapper namespace=\"com.test.mapper.TestMapper\"\u003e \u003cselect id=\"selectStudent\" resultType=\"com.test.entity.Student\"\u003e select * from student \u003c/select\u003e \u003c/mapper\u003e 作为内部资源后，我们需要修改一下配置文件中的mapper定义，不使用url而是resource表示是Jar内部的文件： \u003cmappers\u003e \u003cmapper resource=\"com/test/mapper/TestMapper.xml\"/\u003e \u003c/mappers\u003e 现在我们就可以直接通过SqlSession获取对应的实现类，通过接口中定义的行为来直接获取结果： public static void main(String[] args) { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); List\u003cStudent\u003e student = testMapper.selectStudent(); student.forEach(System.out::println); } } 那么肯定有人好奇，TestMapper明明是一个我们自己定义接口啊，Mybatis也不可能提前帮我们写了实现类啊，那这接口怎么就出现了一个实现类呢？我们可以通过调用getClass()方法来看看实现类是个什么： TestMapper testMapper = sqlSession.getMapper(TestMapper.class); System.out.println(testMapper.getClass()); 我们发现，实现类名称很奇怪，名称为com.sun.proxy.$Proxy4，它是通过动态代理生成的，相当于动态生成了一个实现类，而不是预先定义好的，有关Mybatis这一部分的原理，我们放在最后一节进行讲解。 接下来，我们再来看配置文件，之前我们并没有对配置文件进行一个详细的介绍： \u003cconfiguration\u003e \u003cenvironments default=\"development\"\u003e \u003cenvironment id=\"development\"\u003e \u003ctransactionManager type=\"JDBC\"/\u003e \u003cdataSource type=\"POOLED\"\u003e \u003cproperty name=\"driver\" value=\"com.mysql.cj.jdbc.Driver\"/\u003e \u003cproperty name=\"url\" value=\"jdbc:mysql://localhost:3306/study\"/\u003e \u003cproperty name=\"username\" value=\"test\"/\u003e \u003cproperty name=\"password\" value=\"123456\"/\u003e \u003c/dataSource\u003e \u003c/environment\u003e \u003c/environments\u003e \u003cmappers\u003e \u003cmapper resource=\"com/test/mapper/TestMapper.xml\"/\u003e \u003c/mappers\u003e \u003c/configuration\u003e 首先就从environments标签说起，一般情况下，我们在开发中，都需要指定一个数据库的配置信息，包含连接URL、用户、密码等信息，而environment就是用于进行这些配置的！实际情况下可能会不止有一个数据库连接信息，比如开发过程中我们一般会使用本地的数据库，而如果需要将项目上传到服务器或是防止其他人的电脑上运行时，我们可能就需要配置另一个数据库的信息，因此，我们可以提前定义好所有的数据库信息，该什么时候用什么即可！ 在environments标签上有一个default属性，来指定默认的环境，当然如果我们希望使用其他环境，可以修改这个默认环境，也可以在创建工厂时选择环境： sqlSessionFactory = new SqlSessionFactoryBuilder() .build(new FileInputStream(\"mybatis-config.xml\"), \"环境ID\"); 我们还可以给类型起一个别名，以简化Mapper的编写： \u003c!-- 需要在environments的上方 --\u003e \u003ctypeAliases\u003e \u003ctypeAlias type=\"com.test.entity.Student\" alias=\"Student\"/\u003e \u003c/typeAliases\u003e 现在Mapper就可以直接使用别名了： \u003cmapper namespace=\"com.test.mapper.TestMapper\"\u003e \u003cselect id=\"selectStudent\" resultType=\"Student\"\u003e select * from student \u003c/select\u003e \u003c/mapper\u003e 如果这样还是很麻烦，我们也可以直接让Mybatis去扫描一个包，并将包下的所有类自动起别名（别名为首字母小写的类名） \u003ctypeAliases\u003e \u003cpackage name=\"com.test.entity\"/\u003e \u003c/typeAliases\u003e 也可以为指定实体类添加一个注解，来指定别名： @Data @Alias(\"lbwnb\") public class Student { private int sid; private String name; private String sex; } 当然，Mybatis也包含许多的基础配置，通过使用： \u003csettings\u003e \u003csetting name=\"\" value=\"\"/\u003e \u003c/settings\u003e 所有的配置项可以在中文文档处查询，本文不会进行详细介绍，在后面我们会提出一些比较重要的配置项。 有关配置文件的介绍就暂时到这里为止，我们讨论的重心应该是Mybatis的应用，而不是配置文件，所以省略了一部分内容的讲解。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:3","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"增删改查 在了解了Mybatis的一些基本配置之后，我们就可以正式来使用Mybatis来进行数据库操作了！ 在前面我们演示了如何快速进行查询，我们只需要编写一个对应的映射器既可以了： \u003cmapper namespace=\"com.test.mapper.TestMapper\"\u003e \u003cselect id=\"studentList\" resultType=\"Student\"\u003e select * from student \u003c/select\u003e \u003c/mapper\u003e 当然，如果你不喜欢使用实体类，那么这些属性还可以被映射到一个Map上： \u003cselect id=\"selectStudent\" resultType=\"Map\"\u003e select * from student \u003c/select\u003e public interface TestMapper { List\u003cMap\u003e selectStudent(); } Map中就会以键值对的形式来存放这些结果了。 通过设定一个resultType属性，让Mybatis知道查询结果需要映射为哪个实体类，要求字段名称保持一致。那么如果我们不希望按照这样的规则来映射呢？我们可以自定义resultMap来设定映射规则： \u003cresultMap id=\"Test\" type=\"Student\"\u003e \u003cresult column=\"sid\" property=\"sid\"/\u003e \u003cresult column=\"sex\" property=\"name\"/\u003e \u003cresult column=\"name\" property=\"sex\"/\u003e \u003c/resultMap\u003e 通过指定映射规则，我们现在名称和性别一栏就发生了交换，因为我们将其映射字段进行了交换。 如果一个类中存在多个构造方法，那么很有可能会出现这样的错误： ### Exception in thread \"main\" org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: org.apache.ibatis.executor.ExecutorException: No constructor found in com.test.entity.Student matching [java.lang.Integer, java.lang.String, java.lang.String] ### The error may exist in com/test/mapper/TestMapper.xml ### The error may involve com.test.mapper.TestMapper.getStudentBySid ### The error occurred while handling results ### SQL: select * from student where sid = ? ### Cause: org.apache.ibatis.executor.ExecutorException: No constructor found in com.test.entity.Student matching [java.lang.Integer, java.lang.String, java.lang.String] at org.apache.ibatis.exceptions.ExceptionFactory.wrapException(ExceptionFactory.java:30) ... 这时就需要使用constructor标签来指定构造方法： \u003cresultMap id=\"test\" type=\"Student\"\u003e \u003cconstructor\u003e \u003carg column=\"sid\" javaType=\"Integer\"/\u003e \u003carg column=\"name\" javaType=\"String\"/\u003e \u003c/constructor\u003e \u003c/resultMap\u003e 值得注意的是，指定构造方法后，若此字段被填入了构造方法作为参数，将不会通过反射给字段单独赋值，而构造方法中没有传入的字段，依然会被反射赋值，有关resultMap的内容，后面还会继续讲解。 如果数据库中存在一个带下划线的字段，我们可以通过设置让其映射为以驼峰命名的字段，比如my_test映射为myTest \u003csettings\u003e \u003csetting name=\"mapUnderscoreToCamelCase\" value=\"true\"/\u003e \u003c/settings\u003e 如果不设置，默认为不开启，也就是默认需要名称保持一致。 我们接着来看看条件查询，既然是条件查询，那么肯定需要我们传入查询条件，比如现在我们想通过sid字段来通过学号查找信息： Student getStudentBySid(int sid); \u003cselect id=\"getStudentBySid\" parameterType=\"int\" resultType=\"Student\"\u003e select * from student where sid = #{sid} \u003c/select\u003e 我们通过使用#{xxx}或是${xxx}来填入我们给定的属性，实际上Mybatis本质也是通过PreparedStatement首先进行一次预编译，有效地防止SQL注入问题，但是如果使用${xxx}就不再是通过预编译，而是直接传值，因此我们一般都使用#{xxx}来进行操作。 使用parameterType属性来指定参数类型（非必须，可以不用，推荐不用） 接着我们来看插入、更新和删除操作，其实与查询操作差不多，不过需要使用对应的标签，比如插入操作： \u003cinsert id=\"addStudent\" parameterType=\"Student\"\u003e insert into student(name, sex) values(#{name}, #{sex}) \u003c/insert\u003e int addStudent(Student student); 我们这里使用的是一个实体类，我们可以直接使用实体类里面对应属性替换到SQL语句中，只需要填写属性名称即可，和条件查询是一样的。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:4","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"复杂查询 一个老师可以教授多个学生，那么能否一次性将老师的学生全部映射给此老师的对象呢，比如： @Data public class Teacher { int tid; String name; List\u003cStudent\u003e studentList; } 映射为Teacher对象时，同时将其教授的所有学生一并映射为List列表，显然这是一种一对多的查询，那么这时就需要进行复杂查询了。而我们之前编写的都非常简单，直接就能完成映射，因此我们现在需要使用resultMap来自定义映射规则： \u003cselect id=\"getTeacherByTid\" resultMap=\"asTeacher\"\u003e select *, teacher.name as tname from student inner join teach on student.sid = teach.sid inner join teacher on teach.tid = teacher.tid where teach.tid = #{tid} \u003c/select\u003e \u003cresultMap id=\"asTeacher\" type=\"Teacher\"\u003e \u003cid column=\"tid\" property=\"tid\"/\u003e \u003cresult column=\"tname\" property=\"name\"/\u003e \u003ccollection property=\"studentList\" ofType=\"Student\"\u003e \u003cid property=\"sid\" column=\"sid\"/\u003e \u003cresult column=\"name\" property=\"name\"/\u003e \u003cresult column=\"sex\" property=\"sex\"/\u003e \u003c/collection\u003e \u003c/resultMap\u003e 可以看到，我们的查询结果是一个多表联查的结果，而联查的数据就是我们需要映射的数据（比如这里是一个老师有N个学生，联查的结果也是这一个老师对应N个学生的N条记录），其中id标签用于在多条记录中辨别是否为同一个对象的数据，比如上面的查询语句得到的结果中，tid这一行始终为1，因此所有的记录都应该是tid=1的教师的数据，而不应该变为多个教师的数据，如果不加id进行约束，那么会被识别成多个教师的数据！ 通过使用collection来表示将得到的所有结果合并为一个集合，比如上面的数据中每个学生都有单独的一条记录，因此tid相同的全部学生的记录就可以最后合并为一个List，得到最终的映射结果，当然，为了区分，最好也设置一个id，只不过这个例子中可以当做普通的result使用。 了解了一对多，那么多对一又该如何查询呢，比如每个学生都有一个对应的老师，现在Student新增了一个Teacher对象，那么现在又该如何去处理呢？ @Data @Accessors(chain = true) public class Student { private int sid; private String name; private String sex; private Teacher teacher; } @Data public class Teacher { int tid; String name; } 现在我们希望的是，每次查询到一个Student对象时都带上它的老师，同样的，我们也可以使用resultMap来实现（先修改一下老师的类定义，不然会很麻烦）： \u003cresultMap id=\"test2\" type=\"Student\"\u003e \u003cid column=\"sid\" property=\"sid\"/\u003e \u003cresult column=\"name\" property=\"name\"/\u003e \u003cresult column=\"sex\" property=\"sex\"/\u003e \u003cassociation property=\"teacher\" javaType=\"Teacher\"\u003e \u003cid column=\"tid\" property=\"tid\"/\u003e \u003cresult column=\"tname\" property=\"name\"/\u003e \u003c/association\u003e \u003c/resultMap\u003e \u003cselect id=\"selectStudent\" resultMap=\"test2\"\u003e select *, teacher.name as tname from student left join teach on student.sid = teach.sid left join teacher on teach.tid = teacher.tid \u003c/select\u003e 通过使用association进行关联，形成多对一的关系，实际上和一对多是同理的，都是对查询结果的一种处理方式罢了。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:5","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"事务操作 我们可以在获取SqlSession关闭自动提交来开启事务模式，和JDBC其实都差不多： public static void main(String[] args) { try (SqlSession sqlSession = MybatisUtil.getSession(false)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); testMapper.addStudent(new Student().setSex(\"男\").setName(\"小王\")); testMapper.selectStudent().forEach(System.out::println); } } 我们发现，在关闭自动提交后，我们的内容是没有进入到数据库的，现在我们来试一下在最后提交事务： sqlSession.commit(); 在事务提交后，我们的内容才会被写入到数据库中。现在我们来试试看回滚操作： try (SqlSession sqlSession = MybatisUtil.getSession(false)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); testMapper.addStudent(new Student().setSex(\"男\").setName(\"小王\")); testMapper.selectStudent().forEach(System.out::println); sqlSession.rollback(); sqlSession.commit(); } 回滚操作也印证成功。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:6","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"动态SQL 动态 SQL 是 MyBatis 的强大特性之一。如果你使用过 JDBC 或其它类似的框架，你应该能理解根据不同条件拼接 SQL 语句有多痛苦，例如拼接时要确保不能忘记添加必要的空格，还要注意去掉列表最后一个列名的逗号。利用动态 SQL，可以彻底摆脱这种痛苦。 我们直接使用官网的例子进行讲解。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:7","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"缓存机制 MyBatis 内置了一个强大的事务性查询缓存机制，它可以非常方便地配置和定制。 其实缓存机制我们在之前学习IO流的时候已经提及过了，我们可以提前将一部分内容放入缓存，下次需要获取数据时，我们就可以直接从缓存中读取，这样的话相当于直接从内存中获取而不是再去向数据库索要数据，效率会更高。 因此Mybatis内置了一个缓存机制，我们查询时，如果缓存中存在数据，那么我们就可以直接从缓存中获取，而不是再去向数据库进行请求。 Mybatis存在一级缓存和二级缓存，我们首先来看一下一级缓存，默认情况下，只启用了本地的会话缓存，它仅仅对一个会话中的数据进行缓存（一级缓存无法关闭，只能调整），我们来看看下面这段代码： public static void main(String[] args) throws InterruptedException { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); Student student1 = testMapper.getStudentBySid(1); Student student2 = testMapper.getStudentBySid(1); System.out.println(student1 == student2); } } 我们发现，两次得到的是同一个Student对象，也就是说我们第二次查询并没有重新去构造对象，而是直接得到之前创建好的对象。如果还不是很明显，我们可以修改一下实体类： @Data @Accessors(chain = true) public class Student { public Student(){ System.out.println(\"我被构造了\"); } private int sid; private String name; private String sex; } 我们通过前面的学习得知Mybatis在映射为对象时，在只有一个构造方法的情况下，无论你构造方法写成什么样子，都会去调用一次构造方法，如果存在多个构造方法，那么就会去找匹配的构造方法。我们可以通过查看构造方法来验证对象被创建了几次。 结果显而易见，只创建了一次，也就是说当第二次进行同样的查询时，会直接使用第一次的结果，因为第一次的结果已经被缓存了。 那么如果我修改了数据库中的内容，缓存还会生效吗： public static void main(String[] args) throws InterruptedException { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); Student student1 = testMapper.getStudentBySid(1); testMapper.addStudent(new Student().setName(\"小李\").setSex(\"男\")); Student student2 = testMapper.getStudentBySid(1); System.out.println(student1 == student2); } } 我们发现，当我们进行了插入操作后，缓存就没有生效了，我们再次进行查询得到的是一个新创建的对象。 也就是说，一级缓存，在进行DML操作后，会使得缓存失效，也就是说Mybatis知道我们对数据库里面的数据进行了修改，所以之前缓存的内容可能就不是当前数据库里面最新的内容了。还有一种情况就是，当前会话结束后，也会清理全部的缓存，因为已经不会再用到了。但是一定注意，一级缓存只针对于单个会话，多个会话之间不相通。 public static void main(String[] args) { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); Student student2; try(SqlSession sqlSession2 = MybatisUtil.getSession(true)){ TestMapper testMapper2 = sqlSession2.getMapper(TestMapper.class); student2 = testMapper2.getStudentBySid(1); } Student student1 = testMapper.getStudentBySid(1); System.out.println(student1 == student2); } } **注意：**一个会话DML操作只会重置当前会话的缓存，不会重置其他会话的缓存，也就是说，其他会话缓存是不会更新的！ 一级缓存给我们提供了很高速的访问效率，但是它的作用范围实在是有限，如果一个会话结束，那么之前的缓存就全部失效了，但是我们希望缓存能够扩展到所有会话都能使用，因此我们可以通过二级缓存来实现，二级缓存默认是关闭状态，要开启二级缓存，我们需要在映射器XML文件中添加： \u003ccache/\u003e 可见二级缓存是Mapper级别的，也就是说，当一个会话失效时，它的缓存依然会存在于二级缓存中，因此如果我们再次创建一个新的会话会直接使用之前的缓存，我们首先根据官方文档进行一些配置： \u003ccache eviction=\"FIFO\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/\u003e 我们来编写一个代码： public static void main(String[] args) { Student student; try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); student = testMapper.getStudentBySid(1); } try (SqlSession sqlSession2 = MybatisUtil.getSession(true)){ TestMapper testMapper2 = sqlSession2.getMapper(TestMapper.class); Student student2 = testMapper2.getStudentBySid(1); System.out.println(student2 == student); } } 我们可以看到，上面的代码中首先是第一个会话在进行读操作，完成后会结束会话，而第二个操作重新创建了一个新的会话，再次执行了同样的查询，我们发现得到的依然是缓存的结果。 那么如果我不希望某个方法开启缓存呢？我们可以添加useCache属性来关闭缓存： \u003cselect id=\"getStudentBySid\" resultType=\"Student\" useCache=\"false\"\u003e select * from student where sid = #{sid} \u003c/select\u003e 我们也可以使用flushCache=“false\"在每次执行后都清空缓存，通过这这个我们还可以控制DML操作完成之后不清空缓存。 \u003cselect id=\"getStudentBySid\" resultType=\"Student\" flushCache=\"true\"\u003e select * from student where sid = #{sid} \u003c/select\u003e 添加了二级缓存之后，会先从二级缓存中查找数据，当二级缓存中没有时，才会从一级缓存中获取，当一级缓存中都还没有数据时，才会请求数据库，因此我们再来执行上面的代码： public static void main(String[] args) { try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); Student student2; try(SqlSession sqlSession2 = MybatisUtil.getSession(true)){ TestMapper testMapper2 = sqlSession2.getMapper(TestMapper.class); student2 = testMapper2.getStudentBySid(1); } Student student1 = testMapper.getStudentBySid(1); System.out.println(student1 == student2); } } 得到的结果就会是同一个对象了，因为现在是优先从二级缓存中获取。 读取顺序：二级缓存 =\u003e 一级缓存 =\u003e 数据库 虽然缓存机制给我","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:8","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用注解开发 在之前的开发中，我们已经体验到Mybatis为我们带来的便捷了，我们只需要编写对应的映射器，并将其绑定到一个接口上，即可直接通过该接口执行我们的SQL语句，极大的简化了我们之前JDBC那样的代码编写模式。那么，能否实现无需xml映射器配置，而是直接使用注解在接口上进行配置呢？答案是可以的，也是现在推荐的一种方式（也不是说XML就不要去用了，由于Java 注解的表达能力和灵活性十分有限，可能相对于XML配置某些功能实现起来会不太好办，但是在大部分场景下，直接使用注解开发已经绰绰有余了） 首先我们来看一下，使用XML进行映射器编写时，我们需要现在XML中定义映射规则和SQL语句，然后再将其绑定到一个接口的方法定义上，然后再使用接口来执行： \u003cinsert id=\"addStudent\"\u003e insert into student(name, sex) values(#{name}, #{sex}) \u003c/insert\u003e int addStudent(Student student); 而现在，我们可以直接使用注解来实现，每个操作都有一个对应的注解： @Insert(\"insert into student(name, sex) values(#{name}, #{sex})\") int addStudent(Student student); 当然，我们还需要修改一下配置文件中的映射器注册： \u003cmappers\u003e \u003cmapper class=\"com.test.mapper.MyMapper\"/\u003e \u003c!-- 也可以直接注册整个包下的 \u003cpackage name=\"com.test.mapper\"/\u003e --\u003e \u003c/mappers\u003e 通过直接指定Class，来让Mybatis知道我们这里有一个通过注解实现的映射器。 我们接着来看一下，如何使用注解进行自定义映射规则： @Results({ @Result(id = true, column = \"sid\", property = \"sid\"), @Result(column = \"sex\", property = \"name\"), @Result(column = \"name\", property = \"sex\") }) @Select(\"select * from student\") List\u003cStudent\u003e getAllStudent(); 直接通过@Results注解，就可以直接进行配置了，此注解的value是一个@Result注解数组，每个@Result注解都都一个单独的字段配置，其实就是我们之前在XML映射器中写的： \u003cresultMap id=\"test\" type=\"Student\"\u003e \u003cid property=\"sid\" column=\"sid\"/\u003e \u003cresult column=\"name\" property=\"sex\"/\u003e \u003cresult column=\"sex\" property=\"name\"/\u003e \u003c/resultMap\u003e 现在我们就可以通过注解来自定义映射规则了。那么如何使用注解来完成复杂查询呢？我们还是使用一个老师多个学生的例子： @Results({ @Result(id = true, column = \"tid\", property = \"tid\"), @Result(column = \"name\", property = \"name\"), @Result(column = \"tid\", property = \"studentList\", many = @Many(select = \"getStudentByTid\") ) }) @Select(\"select * from teacher where tid = #{tid}\") Teacher getTeacherBySid(int tid); @Select(\"select * from student inner join teach on student.sid = teach.sid where tid = #{tid}\") List\u003cStudent\u003e getStudentByTid(int tid); 我们发现，多出了一个子查询，而这个子查询是单独查询该老师所属学生的信息，而子查询结果作为@Result注解的一个many结果，代表子查询的所有结果都归入此集合中（也就是之前的collection标签） \u003cresultMap id=\"asTeacher\" type=\"Teacher\"\u003e \u003cid column=\"tid\" property=\"tid\"/\u003e \u003cresult column=\"tname\" property=\"name\"/\u003e \u003ccollection property=\"studentList\" ofType=\"Student\"\u003e \u003cid property=\"sid\" column=\"sid\"/\u003e \u003cresult column=\"name\" property=\"name\"/\u003e \u003cresult column=\"sex\" property=\"sex\"/\u003e \u003c/collection\u003e \u003c/resultMap\u003e 同理，@Result也提供了@One子注解来实现一对一的关系表示，类似于之前的assocation标签： @Results({ @Result(id = true, column = \"sid\", property = \"sid\"), @Result(column = \"sex\", property = \"name\"), @Result(column = \"name\", property = \"sex\"), @Result(column = \"sid\", property = \"teacher\", one = @One(select = \"getTeacherBySid\") ) }) @Select(\"select * from student\") List\u003cStudent\u003e getAllStudent(); 如果现在我希望直接使用注解编写SQL语句但是我希望映射规则依然使用XML来实现，这时该怎么办呢？ @ResultMap(\"test\") @Select(\"select * from student\") List\u003cStudent\u003e getAllStudent(); 提供了@ResultMap注解，直接指定ID即可，这样我们就可以使用XML中编写的映射规则了，这里就不再演示了。 那么如果出现之前的两个构造方法的情况，且没有任何一个构造方法匹配的话，该怎么处理呢？ @Data @Accessors(chain = true) public class Student { public Student(int sid){ System.out.println(\"我是一号构造方法\"+sid); } public Student(int sid, String name){ System.out.println(\"我是二号构造方法\"+sid+name); } private int sid; private String name; private String sex; } 我们可以通过@ConstructorArgs注解来指定构造方法： @ConstructorArgs({ @Arg(column = \"sid\", javaType = int.class), @Arg(column = \"name\", javaType = String.class) }) @Select(\"select * from student where sid = #{sid} and sex = #{sex}\") Student getStudentBySidAndSex(@Param(\"sid\") int sid, @Param(\"sex\") String sex); 得到的结果和使用constructor标签效果一致，这里就不多做讲解了。 我们发现，当参数列表中出现两个以上的参数时，会出现错误： @Select(\"select * from student where sid = #{sid} and sex = #{sex}\") Student getStudentBySidAndSex(int sid, String sex); Exception in thread \"main\" org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: org.apache.ibatis.binding.BindingException: Parameter 'sid' not found. Available parameters are [arg1, arg0, param1, param2] ### Cause: org.apache.ibatis.binding.BindingException: Parameter 'sid' not found. Available parameters are [arg1, arg0, param1, param2] at org.apache.ibatis.exceptions.ExceptionFactory.wrapException(ExceptionFactory.java:30) at org.apache.ibatis.session.defaults.","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:9","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"探究Mybatis的动态代理机制 在探究动态代理机制之前，我们要先聊聊什么是代理：其实顾名思义，就好比我开了个大棚，里面栽种的西瓜，那么西瓜成熟了是不是得去卖掉赚钱，而我们的西瓜非常多，一个人肯定卖不过来，肯定就要去多找几个开水果摊的帮我们卖，这就是一种代理。实际上是由水果摊老板在帮我们卖瓜，我们只告诉老板卖多少钱，而至于怎么卖的是由水果摊老板决定的。 那么现在我们来尝试实现一下这样的类结构，首先定义一个接口用于规范行为： public interface Shopper { //卖瓜行为 void saleWatermelon(String customer); } 然后需要实现一下卖瓜行为，也就是我们要告诉老板卖多少钱，这里就直接写成成功出售： public class ShopperImpl implements Shopper{ //卖瓜行为的实现 @Override public void saleWatermelon(String customer) { System.out.println(\"成功出售西瓜给 ===\u003e \"+customer); } } 最后老板代理后肯定要用自己的方式去出售这些西瓜，成交之后再按照我们告诉老板的价格进行出售： public class ShopperProxy implements Shopper{ private final Shopper impl; public ShopperProxy(Shopper impl){ this.impl = impl; } //代理卖瓜行为 @Override public void saleWatermelon(String customer) { //首先进行 代理商讨价还价行为 System.out.println(customer + \"：哥们，这瓜多少钱一斤啊？\"); System.out.println(\"老板：两块钱一斤。\"); System.out.println(customer + \"：你这瓜皮子是金子做的，还是瓜粒子是金子做的？\"); System.out.println(\"老板：你瞅瞅现在哪有瓜啊，这都是大棚的瓜，你嫌贵我还嫌贵呢。\"); System.out.println(customer + \"：给我挑一个。\"); impl.saleWatermelon(customer); //讨价还价成功，进行我们告诉代理商的卖瓜行为 } } 现在我们来试试看： public class Main { public static void main(String[] args) { Shopper shopper = new ShopperProxy(new ShopperImpl()); shopper.saleWatermelon(\"小强\"); } } 这样的操作称为静态代理，也就是说我们需要提前知道接口的定义并进行实现才可以完成代理，而Mybatis这样的是无法预知代理接口的，我们就需要用到动态代理。 JDK提供的反射框架就为我们很好地解决了动态代理的问题，在这里相当于对JavaSE阶段反射的内容进行一个补充。 public class ShopperProxy implements InvocationHandler { Object target; public ShopperProxy(Object target){ this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { String customer = (String) args[0]; System.out.println(customer + \"：哥们，这瓜多少钱一斤啊？\"); System.out.println(\"老板：两块钱一斤。\"); System.out.println(customer + \"：你这瓜皮子是金子做的，还是瓜粒子是金子做的？\"); System.out.println(\"老板：你瞅瞅现在哪有瓜啊，这都是大棚的瓜，你嫌贵我还嫌贵呢。\"); System.out.println(customer + \"：行，给我挑一个。\"); return method.invoke(target, args); } } 通过实现InvocationHandler来成为一个动态代理，我们发现它提供了一个invoke方法，用于调用被代理对象的方法并完成我们的代理工作。现在就可以通过 Proxy.newProxyInstance来生成一个动态代理类： public static void main(String[] args) { Shopper impl = new ShopperImpl(); Shopper shopper = (Shopper) Proxy.newProxyInstance(impl.getClass().getClassLoader(), impl.getClass().getInterfaces(), new ShopperProxy(impl)); shopper.saleWatermelon(\"小强\"); System.out.println(shopper.getClass()); } 通过打印类型我们发现，就是我们之前看到的那种奇怪的类：class com.sun.proxy.$Proxy0，因此Mybatis其实也是这样的来实现的（肯定有人问了：Mybatis是直接代理接口啊，你这个不还是要把接口实现了吗？）那我们来改改，现在我们不代理任何类了，直接做接口实现： public class ShopperProxy implements InvocationHandler { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { String customer = (String) args[0]; System.out.println(customer + \"：哥们，这瓜多少钱一斤啊？\"); System.out.println(\"老板：两块钱一斤。\"); System.out.println(customer + \"：你这瓜皮子是金子做的，还是瓜粒子是金子做的？\"); System.out.println(\"老板：你瞅瞅现在哪有瓜啊，这都是大棚的瓜，你嫌贵我还嫌贵呢。\"); System.out.println(customer + \"：行，给我挑一个。\"); return null; } } public static void main(String[] args) { Shopper shopper = (Shopper) Proxy.newProxyInstance(Shopper.class.getClassLoader(), new Class[]{ Shopper.class }, //因为本身就是接口，所以直接用就行 new ShopperProxy()); shopper.saleWatermelon(\"小强\"); System.out.println(shopper.getClass()); } 我们可以去看看Mybatis的源码。 Mybatis的学习差不多就到这里为止了，不过，同样类型的框架还有很多，Mybatis属于半自动框架，SQL语句依然需要我们自己编写，虽然存在一定的麻烦，但是会更加灵活，而后面我们还会学习JPA，它是全自动的框架，你几乎见不到SQL的影子！ ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:3:10","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用JUnit进行单元测试 首先一问：我们为什么需要单元测试？ 随着我们的项目逐渐变大，比如我们之前编写的图书管理系统，我们都是边在写边在测试，而我们当时使用的测试方法，就是直接在主方法中运行测试，但是，在很多情况下，我们的项目可能会很庞大，不可能每次都去完整地启动一个项目来测试某一个功能，这样显然会降低我们的开发效率，因此，我们需要使用单元测试来帮助我们针对于某个功能或是某个模块单独运行代码进行测试，而不是启动整个项目。 同时，在我们项目的维护过程中，难免会涉及到一些原有代码的修改，很有可能出现改了代码导致之前的功能出现问题（牵一发而动全身），而我们又不一定能立即察觉到，因此，我们可以提前保存一些测试用例，每次完成代码后都可以跑一遍测试用例，来确保之前的功能没有因为后续的修改而出现问题。 我们还可以利用单元测试来评估某个模块或是功能的耗时和性能，快速排查导致程序运行缓慢的问题，这些都可以通过单元测试来完成，可见单元测试对于开发的重要性。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:4:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"尝试JUnit 首先需要导入JUnit依赖，我们在这里使用Junit4进行介绍，最新的Junit5放到Maven板块一起讲解，Jar包已经放在视频下方简介中，直接去下载即可。同时IDEA需要安装JUnit插件（默认是已经捆绑安装的，因此无需多余配置） 现在我们创建一个新的类，来编写我们的单元测试用例： public class TestMain { @Test public void method(){ System.out.println(\"我是测试用例1\"); } @Test public void method2(){ System.out.println(\"我是测试用例2\"); } } 我们可以点击类前面的测试按钮，或是单个方法前的测试按钮，如果点击类前面的测试按钮，会执行所有的测试用例。 运行测试后，我们发现控制台得到了一个测试结果，显示为绿色表示测试通过。 只需要通过打上@Test注解，即可将一个方法标记为测试案例，我们可以直接运行此测试案例，但是我们编写的测试方法有以下要求： 方法必须是public的 不能是静态方法 返回值必须是void 必须是没有任何参数的方法 对于一个测试案例来说，我们肯定希望测试的结果是我们所期望的一个值，因此，如果测试的结果并不是我们所期望的结果，那么这个测试就应该没有成功通过！ 我们可以通过断言工具类来进行判定： public class TestMain { @Test public void method(){ System.out.println(\"我是测试案例！\"); Assert.assertEquals(1, 2); //参数1是期盼值，参数2是实际测试结果值 } } 通过运行代码后，我们发现测试过程中抛出了一个错误，并且IDEA给我们显示了期盼结果和测试结果，那么现在我们来测试一个案例，比如我们想查看冒泡排序的编写是否正确： @Test public void method(){ int[] arr = {0, 4, 5, 2, 6, 9, 3, 1, 7, 8}; //错误的冒泡排序 for (int i = 0; i \u003c arr.length - 1; i++) { for (int j = 0; j \u003c arr.length - 1 - i; j++) { if(arr[j] \u003e arr[j + 1]){ int tmp = arr[j]; arr[j] = arr[j+1]; // arr[j+1] = tmp; } } } Assert.assertArrayEquals(new int[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, arr); } 通过测试，我们发现得到的结果并不是我们想要的结果，因此现在我们需要去修改为正确的冒泡排序，修改后，测试就能正确通过了。我们还可以再通过一个案例来更加深入地了解测试，现在我们想测试从数据库中取数据是否为我们预期的数据： @Test public void method(){ try (SqlSession sqlSession = MybatisUtil.getSession(true)){ TestMapper mapper = sqlSession.getMapper(TestMapper.class); Student student = mapper.getStudentBySidAndSex(1, \"男\"); Assert.assertEquals(new Student().setName(\"小明\").setSex(\"男\").setSid(1), student); } } 那么如果我们在进行所有的测试之前需要做一些前置操作该怎么办呢，一种办法是在所有的测试用例前面都加上前置操作，但是这样显然是很冗余的，因为一旦发生修改就需要挨个进行修改，因此我们需要更加智能的方法，我们可以通过@Before注解来添加测试用例开始之前的前置操作： public class TestMain { private SqlSessionFactory sqlSessionFactory; @Before public void before(){ System.out.println(\"测试前置正在初始化...\"); try { sqlSessionFactory = new SqlSessionFactoryBuilder() .build(new FileInputStream(\"mybatis-config.xml\")); } catch (FileNotFoundException e) { e.printStackTrace(); } System.out.println(\"测试初始化完成，正在开始测试案例...\"); } @Test public void method1(){ try (SqlSession sqlSession = sqlSessionFactory.openSession(true)){ TestMapper mapper = sqlSession.getMapper(TestMapper.class); Student student = mapper.getStudentBySidAndSex(1, \"男\"); Assert.assertEquals(new Student().setName(\"小明\").setSex(\"男\").setSid(1), student); System.out.println(\"测试用例1通过！\"); } } @Test public void method2(){ try (SqlSession sqlSession = sqlSessionFactory.openSession(true)){ TestMapper mapper = sqlSession.getMapper(TestMapper.class); Student student = mapper.getStudentBySidAndSex(2, \"女\"); Assert.assertEquals(new Student().setName(\"小红\").setSex(\"女\").setSid(2), student); System.out.println(\"测试用例2通过！\"); } } } 同理，在所有的测试完成之后，我们还想添加一个收尾的动作，那么只需要使用@After注解即可添加结束动作： @After public void after(){ System.out.println(\"测试结束，收尾工作正在进行...\"); } 有关JUnit的使用我们就暂时只介绍这么多。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:4:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"JUL日志系统 首先一问：我们为什么需要日志系统？ 我们之前一直都在使用System.out.println来打印信息，但是，如果项目中存在大量的控制台输出语句，会显得很凌乱，而且日志的粒度是不够细的，假如我们现在希望，项目只在debug的情况下打印某些日志，而在实际运行时不打印日志，采用直接输出的方式就很难实现了，因此我们需要使用日志框架来规范化日志输出。 而JDK为我们提供了一个自带的日志框架，位于java.util.logging包下，我们可以使用此框架来实现日志的规范化打印，使用起来非常简单： public class Main { public static void main(String[] args) { // 首先获取日志打印器 Logger logger = Logger.getLogger(Main.class.getName()); // 调用info来输出一个普通的信息，直接填写字符串即可 logger.info(\"我是普通的日志\"); } } 我们可以在主类中使用日志打印，得到日志的打印结果： 十一月 15, 2021 12:55:37 下午 com.test.Main main 信息: 我是普通的日志 我们发现，通过日志输出的结果会更加规范。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"JUL日志讲解 日志分为7个级别，详细信息我们可以在Level类中查看： SEVERE（最高值）- 一般用于代表严重错误 WARNING - 一般用于表示某些警告，但是不足以判断为错误 INFO （默认级别） - 常规消息 CONFIG FINE FINER FINEST（最低值） 我们之前通过info方法直接输出的结果就是使用的默认级别的日志，我们可以通过log方法来设定该条日志的输出级别： public static void main(String[] args) { Logger logger = Logger.getLogger(Main.class.getName()); logger.log(Level.SEVERE, \"严重的错误\", new IOException(\"我就是错误\")); logger.log(Level.WARNING, \"警告的内容\"); logger.log(Level.INFO, \"普通的信息\"); logger.log(Level.CONFIG, \"级别低于普通信息\"); } 我们发现，级别低于默认级别的日志信息，无法输出到控制台，我们可以通过设置来修改日志的打印级别： public static void main(String[] args) { Logger logger = Logger.getLogger(Main.class.getName()); //修改日志级别 logger.setLevel(Level.CONFIG); //不使用父日志处理器 logger.setUseParentHandlers(false); //使用自定义日志处理器 ConsoleHandler handler = new ConsoleHandler(); handler.setLevel(Level.CONFIG); logger.addHandler(handler); logger.log(Level.SEVERE, \"严重的错误\", new IOException(\"我就是错误\")); logger.log(Level.WARNING, \"警告的内容\"); logger.log(Level.INFO, \"普通的信息\"); logger.log(Level.CONFIG, \"级别低于普通信息\"); } 每个Logger都有一个父日志打印器，我们可以通过getParent()来获取： public static void main(String[] args) throws IOException { Logger logger = Logger.getLogger(Main.class.getName()); System.out.println(logger.getParent().getClass()); } 我们发现，得到的是java.util.logging.LogManager$RootLogger这个类，它默认使用的是ConsoleHandler，且日志级别为INFO，由于每一个日志打印器都会直接使用父类的处理器，因此我们之前需要关闭父类然后使用我们自己的处理器。 我们通过使用自己日志处理器来自定义级别的信息打印到控制台，当然，日志处理器不仅仅只有控制台打印，我们也可以使用文件处理器来处理日志信息，我们继续添加一个处理器： //添加输出到本地文件 FileHandler fileHandler = new FileHandler(\"test.log\"); fileHandler.setLevel(Level.WARNING); logger.addHandler(fileHandler); 注意，这个时候就有两个日志处理器了，因此控制台和文件的都会生效。如果日志的打印格式我们不喜欢，我们还可以自定义打印格式，比如我们控制台处理器就默认使用的是SimpleFormatter，而文件处理器则是使用的XMLFormatter，我们可以自定义： //使用自定义日志处理器(控制台) ConsoleHandler handler = new ConsoleHandler(); handler.setLevel(Level.CONFIG); handler.setFormatter(new XMLFormatter()); logger.addHandler(handler); 我们可以直接配置为想要的打印格式，如果这些格式还不能满足你，那么我们也可以自行实现： public static void main(String[] args) throws IOException { Logger logger = Logger.getLogger(Main.class.getName()); logger.setUseParentHandlers(false); //为了让颜色变回普通的颜色，通过代码块在初始化时将输出流设定为System.out ConsoleHandler handler = new ConsoleHandler(){{ setOutputStream(System.out); }}; //创建匿名内部类实现自定义的格式 handler.setFormatter(new Formatter() { @Override public String format(LogRecord record) { SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\"); String time = format.format(new Date(record.getMillis())); //格式化日志时间 String level = record.getLevel().getName(); // 获取日志级别名称 // String level = record.getLevel().getLocalizedName(); // 获取本地化名称（语言跟随系统） String thread = String.format(\"%10s\", Thread.currentThread().getName()); //线程名称（做了格式化处理，留出10格空间） long threadID = record.getThreadID(); //线程ID String className = String.format(\"%-20s\", record.getSourceClassName()); //发送日志的类名 String msg = record.getMessage(); //日志消息 //\\033[33m作为颜色代码，30~37都有对应的颜色，38是没有颜色，IDEA能显示，但是某些地方可能不支持 return \"\\033[38m\" + time + \" \\033[33m\" + level + \" \\033[35m\" + threadID + \"\\033[38m --- [\" + thread + \"] \\033[36m\" + className + \"\\033[38m : \" + msg + \"\\n\"; } }); logger.addHandler(handler); logger.info(\"我是测试消息1...\"); logger.log(Level.INFO, \"我是测试消息2...\"); logger.log(Level.WARNING, \"我是测试消息3...\"); } 日志可以设置过滤器，如果我们不希望某些日志信息被输出，我们可以配置过滤规则： public static void main(String[] args) throws IOException { Logger logger = Logger.getLogger(Main.class.getName()); //自定义过滤规则 logger.setFilter(record -\u003e !record.getMessage().contains(\"普通\")); logger.log(Level.SEVERE, \"严重的错误\", new IOException(\"我就是错误\")); logger.log(Level.WARNING, \"警告的内容\"); logger.log(Level.INFO, \"普通的信息\"); } 实际上，整个日志的输出流程如下： ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Properties配置文件 Properties文件是Java的一种配置文件，我们之前学习了XML，但是我们发现XML配置文件读取实在是太麻烦，那么能否有一种简单一点的配置文件呢？我们可以使用Properties文件： name=Test\rdesc=Description\r该文件配置很简单，格式为配置项=配置值，我们可以直接通过Properties类来将其读取为一个类似于Map一样的对象： public static void main(String[] args) throws IOException { Properties properties = new Properties(); properties.load(new FileInputStream(\"test.properties\")); System.out.println(properties); } 我们发现，Properties类是继承自Hashtable，而Hashtable是实现的Map接口，也就是说，Properties本质上就是一个Map一样的结构，它会把所有的配置项映射为一个Map，这样我们就可以快速地读取对应配置的值了。 我们也可以将已经存在的Properties对象放入输出流进行保存，我们这里就不保存文件了，而是直接打印到控制台，我们只需要提供输出流即可： public static void main(String[] args) throws IOException { Properties properties = new Properties(); // properties.setProperty(\"test\", \"lbwnb\"); //和put效果一样 properties.put(\"test\", \"lbwnb\"); properties.store(System.out, \"????\"); //properties.storeToXML(System.out, \"????\"); 保存为XML格式 } 我们可以通过System.getProperties()获取系统的参数，我们来看看： public static void main(String[] args) throws IOException { System.getProperties().store(System.out, \"系统信息：\"); } ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:2","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"编写日志配置文件 我们可以通过进行配置文件来规定日志打印器的一些默认值： # RootLogger 的默认处理器为\rhandlers= java.util.logging.ConsoleHandler\r# RootLogger 的默认的日志级别\r.level= CONFIG\r我们来尝试使用配置文件来进行配置： public static void main(String[] args) throws IOException { //获取日志管理器 LogManager manager = LogManager.getLogManager(); //读取我们自己的配置文件 manager.readConfiguration(new FileInputStream(\"logging.properties\")); //再获取日志打印器 Logger logger = Logger.getLogger(Main.class.getName()); logger.log(Level.CONFIG, \"我是一条日志信息\"); //通过自定义配置文件，我们发现默认级别不再是INFO了 } 我们也可以去修改ConsoleHandler的默认配置： # 指定默认日志级别\rjava.util.logging.ConsoleHandler.level = ALL\r# 指定默认日志消息格式\rjava.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter\r# 指定默认的字符集\rjava.util.logging.ConsoleHandler.encoding = UTF-8\r其实，我们阅读ConsoleHandler的源码就会发现，它就是通过读取配置文件来进行某些参数设置： // Private method to configure a ConsoleHandler from LogManager // properties and/or default values as specified in the class // javadoc. private void configure() { LogManager manager = LogManager.getLogManager(); String cname = getClass().getName(); setLevel(manager.getLevelProperty(cname +\".level\", Level.INFO)); setFilter(manager.getFilterProperty(cname +\".filter\", null)); setFormatter(manager.getFormatterProperty(cname +\".formatter\", new SimpleFormatter())); try { setEncoding(manager.getStringProperty(cname +\".encoding\", null)); } catch (Exception ex) { try { setEncoding(null); } catch (Exception ex2) { // doing a setEncoding with null should always work. // assert false; } } } ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:3","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用Lombok快速开启日志 我们发现，如果我们现在需要全面使用日志系统，而不是传统的直接打印，那么就需要在每个类都去编写获取Logger的代码，这样显然是很冗余的，能否简化一下这个流程呢？ 前面我们学习了Lombok，我们也体会到Lombok给我们带来的便捷，我们可以通过一个注解快速生成构造方法、Getter和Setter，同样的，Logger也是可以使用Lombok快速生成的。 @Log public class Main { public static void main(String[] args) { System.out.println(\"自动生成的Logger名称：\"+log.getName()); log.info(\"我是日志信息\"); } } 只需要添加一个@Log注解即可，添加后，我们可以直接使用一个静态变量log，而它就是自动生成的Logger。我们也可以手动指定名称： @Log(topic = \"打工是不可能打工的\") public class Main { public static void main(String[] args) { System.out.println(\"自动生成的Logger名称：\"+log.getName()); log.info(\"我是日志信息\"); } } ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:4","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Mybatis日志系统 Mybatis也有日志系统，它详细记录了所有的数据库操作等，但是我们在前面的学习中没有开启它，现在我们学习了日志之后，我们就可以尝试开启Mybatis的日志系统，来监控所有的数据库操作，要开启日志系统，我们需要进行配置： \u003csetting name=\"logImpl\" value=\"STDOUT_LOGGING\" /\u003e logImpl包括很多种配置项，包括 SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING，而默认情况下是未配置，也就是说不打印。我们这里将其设定为STDOUT_LOGGING表示直接使用标准输出将日志信息打印到控制台，我们编写一个测试案例来看看效果： public class TestMain { private SqlSessionFactory sqlSessionFactory; @Before public void before(){ try { sqlSessionFactory = new SqlSessionFactoryBuilder() .build(new FileInputStream(\"mybatis-config.xml\")); } catch (FileNotFoundException e) { e.printStackTrace(); } } @Test public void test(){ try(SqlSession sqlSession = sqlSessionFactory.openSession(true)){ TestMapper mapper = sqlSession.getMapper(TestMapper.class); System.out.println(mapper.getStudentBySidAndSex(1, \"男\")); System.out.println(mapper.getStudentBySidAndSex(1, \"男\")); } } } 我们发现，两次获取学生信息，只有第一次打开了数据库连接，而第二次并没有。 现在我们学习了日志系统，那么我们来尝试使用日志系统输出Mybatis的日志信息： \u003csetting name=\"logImpl\" value=\"JDK_LOGGING\" /\u003e 将其配置为JDK_LOGGING表示使用JUL进行日志打印，因为Mybatis的日志级别都比较低，因此我们需要设置一下logging.properties默认的日志级别： handlers= java.util.logging.ConsoleHandler\r.level= ALL\rjava.util.logging.ConsoleHandler.level = ALL\r代码编写如下： @Log public class TestMain { private SqlSessionFactory sqlSessionFactory; @Before public void before(){ try { sqlSessionFactory = new SqlSessionFactoryBuilder() .build(new FileInputStream(\"mybatis-config.xml\")); LogManager manager = LogManager.getLogManager(); manager.readConfiguration(new FileInputStream(\"logging.properties\")); } catch (IOException e) { e.printStackTrace(); } } @Test public void test(){ try(SqlSession sqlSession = sqlSessionFactory.openSession(true)){ TestMapper mapper = sqlSession.getMapper(TestMapper.class); log.info(mapper.getStudentBySidAndSex(1, \"男\").toString()); log.info(mapper.getStudentBySidAndSex(1, \"男\").toString()); } } } 但是我们发现，这样的日志信息根本没法看，因此我们需要修改一下日志的打印格式，我们自己创建一个格式化类： public class TestFormatter extends Formatter { @Override public String format(LogRecord record) { SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\"); String time = format.format(new Date(record.getMillis())); //格式化日志时间 return time + \" : \" + record.getMessage() + \"\\n\"; } } 现在再来修改一下默认的格式化实现： handlers= java.util.logging.ConsoleHandler\r.level= ALL\rjava.util.logging.ConsoleHandler.level = ALL\rjava.util.logging.ConsoleHandler.formatter = com.test.TestFormatter\r现在就好看多了，当然，我们还可以继续为Mybatis添加文件日志，这里就不做演示了。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:5:5","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"使用Maven管理项目 **注意：**开始之前，看看你C盘空间够不够，最好预留2GB空间以上！ **吐槽：**很多电脑预装系统C盘都给得巨少，就算不装软件，一些软件的缓存文件也能给你塞满，建议有时间重装一下系统重新分配一下磁盘空间。 Maven 翻译为\"专家”、“内行”，是 Apache 下的一个纯 Java 开发的开源项目。基于项目对象模型（缩写：POM）概念，Maven利用一个中央信息片断能管理一个项目的构建、报告和文档等步骤。Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。Maven 也可被用于构建和管理各种项目，例如 C#，Ruby，Scala 和其他语言编写的项目。Maven 曾是 Jakarta 项目的子项目，现为由 Apache 软件基金会主持的独立 Apache 项目。 通过Maven，可以帮助我们做： 项目的自动构建，包括代码的编译、测试、打包、安装、部署等操作。 依赖管理，项目使用到哪些依赖，可以快速完成导入。 我们之前并没有讲解如何将我们的项目打包为Jar文件运行，同时，我们导入依赖的时候，每次都要去下载对应的Jar包，这样其实是很麻烦的，并且还有可能一个Jar包依赖于另一个Jar包，就像之前使用JUnit一样，因此我们需要一个更加方便的包管理机制。 Maven也需要安装环境，但是IDEA已经自带了Maven环境，因此我们不需要再去进行额外的环境安装（无IDEA也能使用Maven，但是配置过程很麻烦，并且我们现在使用的都是IDEA的集成开发环境，所以这里就不讲解Maven命令行操作了）我们直接创建一个新的Maven项目即可。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven项目结构 我们可以来看一下，一个Maven项目和我们普通的项目有什么区别： 那么首先，我们需要了解一下POM文件，它相当于是我们整个Maven项目的配置文件，它也是使用XML编写的： \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cgroupId\u003eorg.example\u003c/groupId\u003e \u003cartifactId\u003eMavenTest\u003c/artifactId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003cproperties\u003e \u003cmaven.compiler.source\u003e8\u003c/maven.compiler.source\u003e \u003cmaven.compiler.target\u003e8\u003c/maven.compiler.target\u003e \u003c/properties\u003e \u003c/project\u003e 我们可以看到，Maven的配置文件是以project为根节点，而modelVersion定义了当前模型的版本，一般是4.0.0，我们不用去修改。 groupId、artifactId、version这三个元素合在一起，用于唯一区别每个项目，别人如果需要将我们编写的代码作为依赖，那么就必须通过这三个元素来定位我们的项目，我们称为一个项目的基本坐标，所有的项目一般都有自己的Maven坐标，因此我们通过Maven导入其他的依赖只需要填写这三个基本元素就可以了，无需再下载Jar文件，而是Maven自动帮助我们下载依赖并导入。 groupId 一般用于指定组名称，命名规则一般和包名一致，比如我们这里使用的是org.example，一个组下面可以有很多个项目。 artifactId 一般用于指定项目在当前组中的唯一名称，也就是说在组中用于区分于其他项目的标记。 version 代表项目版本，随着我们项目的开发和改进，版本号也会不断更新，就像LOL一样，每次赛季更新都会有一个大版本更新，我们的Maven项目也是这样，我们可以手动指定当前项目的版本号，其他人使用我们的项目作为依赖时，也可以根本版本号进行选择（这里的SNAPSHOT代表快照，一般表示这是一个处于开发中的项目，正式发布项目一般只带版本号） properties中一般都是一些变量和选项的配置，我们这里指定了JDK的源代码和编译版本为1.8，无需进行修改。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:1","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven依赖导入 现在我们尝试使用Maven来帮助我们快速导入依赖，我们需要导入之前的JDBC驱动依赖、JUnit依赖、Mybatis依赖、Lombok依赖，那么如何使用Maven来管理依赖呢？ 我们可以创建一个dependencies节点： \u003cdependencies\u003e //里面填写的就是所有的依赖 \u003c/dependencies\u003e 那么现在就可以向节点中填写依赖了，那么我们如何知道每个依赖的坐标呢？我们可以在：https://mvnrepository.com/ 进行查询（可能打不开，建议用流量，或是直接百度某个项目的Maven依赖），我们直接搜索lombok即可，打开后可以看到已经给我们写出了依赖的坐标： \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cversion\u003e1.18.22\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e 我们直接将其添加到dependencies节点中即可，现在我们来编写一个测试用例看看依赖导入成功了没有： public class Main { public static void main(String[] args) { Student student = new Student(\"小明\", 18); System.out.println(student); } } @Data @AllArgsConstructor public class Student { String name; int age; } 项目运行成功，表示成功导入了依赖。那么，Maven是如何进行依赖管理呢，以致于如此便捷的导入依赖，我们来看看Maven项目的依赖管理流程： 通过流程图我们得知，一个项目依赖一般是存储在中央仓库中，也有可能存储在一些其他的远程仓库（私服），几乎所有的依赖都被放到了中央仓库中，因此，Maven可以直接从中央仓库中下载大部分的依赖（Maven第一次导入依赖是需要联网的），远程仓库中下载之后 ，会暂时存储在本地仓库，我们会发现我们本地存在一个.m2文件夹，这就是Maven本地仓库文件夹，默认建立在C盘，如果你C盘空间不足，会出现问题！ 在下次导入依赖时，如果Maven发现本地仓库中就已经存在某个依赖，那么就不会再去远程仓库下载了。 可能在导入依赖时，小小伙伴们会出现卡顿的问题，我们建议配置一下IDEA自带的Maven插件远程仓库地址，我们打开IDEA的安装目录，找到安装根目录/plugins/maven/lib/maven3/conf文件夹，找到settings.xml文件，打开编辑： 找到mirros标签，添加以下内容： \u003cmirror\u003e \u003cid\u003enexus-aliyun\u003c/id\u003e \u003cmirrorOf\u003e*\u003c/mirrorOf\u003e \u003cname\u003eNexus aliyun\u003c/name\u003e \u003curl\u003ehttp://maven.aliyun.com/nexus/content/groups/public\u003c/url\u003e \u003c/mirror\u003e 这样，我们就将默认的远程仓库地址（国外），配置为国内的阿里云仓库地址了（依赖的下载速度就会快起来了） ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:2","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven依赖作用域 除了三个基本的属性用于定位坐标外，依赖还可以添加以下属性： type：依赖的类型，对于项目坐标定义的packaging。大部分情况下，该元素不必声明，其默认值为jar scope：依赖的范围（作用域，着重讲解） optional：标记依赖是否可选 exclusions：用来排除传递性依赖（一个项目有可能依赖于其他项目，就像我们的项目，如果别人要用我们的项目作为依赖，那么就需要一起下载我们项目的依赖，如Lombok） 我们着重来讲解一下scope属性，它决定了依赖的作用域范围： compile ：为默认的依赖有效范围。如果在定义依赖关系的时候，没有明确指定依赖有效范围的话，则默认采用该依赖有效范围。此种依赖，在编译、运行、测试时均有效。 provided ：在编译、测试时有效，但是在运行时无效，也就是说，项目在运行时，不需要此依赖，比如我们上面的Lombok，我们只需要在编译阶段使用它，编译完成后，实际上已经转换为对应的代码了，因此Lombok不需要在项目运行时也存在。 runtime ：在运行、测试时有效，但是在编译代码时无效。比如我们如果需要自己写一个JDBC实现，那么肯定要用到JDK为我们指定的接口，但是实际上在运行时是不用自带JDK的依赖，因此只保留我们自己写的内容即可。 test ：只在测试时有效，例如：JUnit，我们一般只会在测试阶段使用JUnit，而实际项目运行时，我们就用不到测试了，那么我们来看看，导入JUnit的依赖： 同样的，我们可以在网站上搜索Junit的依赖，我们这里导入最新的JUnit5作为依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter\u003c/artifactId\u003e \u003cversion\u003e5.8.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e 我们所有的测试用例全部编写到Maven项目给我们划分的test目录下，位于此目录下的内容不会在最后被打包到项目中，只用作开发阶段测试使用： public class MainTest { @Test public void test(){ System.out.println(\"测试\"); //Assert在JUnit5时名称发生了变化Assertions Assertions.assertArrayEquals(new int[]{1, 2, 3}, new int[]{1, 2}); } } 因此，一般仅用作测试的依赖如JUnit只保留在测试中即可，那么现在我们再来添加JDBC和Mybatis的依赖： \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e8.0.27\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis\u003c/groupId\u003e \u003cartifactId\u003emybatis\u003c/artifactId\u003e \u003cversion\u003e3.5.7\u003c/version\u003e \u003c/dependency\u003e 我们发现，Maven还给我们提供了一个resource文件夹，我们可以将一些静态资源，比如配置文件，放入到这个文件夹中，项目在打包时会将资源文件夹中文件一起打包的Jar中，比如我们在这里编写一个Mybatis的配置文件： \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003c!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"\u003e \u003cconfiguration\u003e \u003csettings\u003e \u003csetting name=\"mapUnderscoreToCamelCase\" value=\"true\"/\u003e \u003csetting name=\"cacheEnabled\" value=\"true\"/\u003e \u003csetting name=\"logImpl\" value=\"JDK_LOGGING\" /\u003e \u003c/settings\u003e \u003c!-- 需要在environments的上方 --\u003e \u003ctypeAliases\u003e \u003cpackage name=\"com.test.entity\"/\u003e \u003c/typeAliases\u003e \u003cenvironments default=\"development\"\u003e \u003cenvironment id=\"development\"\u003e \u003ctransactionManager type=\"JDBC\"/\u003e \u003cdataSource type=\"POOLED\"\u003e \u003cproperty name=\"driver\" value=\"com.mysql.cj.jdbc.Driver\"/\u003e \u003cproperty name=\"url\" value=\"jdbc:mysql://localhost:3306/study\"/\u003e \u003cproperty name=\"username\" value=\"test\"/\u003e \u003cproperty name=\"password\" value=\"123456\"/\u003e \u003c/dataSource\u003e \u003c/environment\u003e \u003c/environments\u003e \u003cmappers\u003e \u003cmapper class=\"com.test.mapper.TestMapper\"/\u003e \u003c/mappers\u003e \u003c/configuration\u003e 现在我们创建一下测试用例，顺便带大家了解一下Junit5的一些比较方便的地方： public class MainTest { //因为配置文件位于内部，我们需要使用Resources类的getResourceAsStream来获取内部的资源文件 private static SqlSessionFactory factory; //在JUnit5中@Before被废弃，它被细分了： @BeforeAll // 一次性开启所有测试案例只会执行一次 (方法必须是static) // @BeforeEach 一次性开启所有测试案例每个案例开始之前都会执行一次 @SneakyThrows public static void before(){ factory = new SqlSessionFactoryBuilder() .build(Resources.getResourceAsStream(\"mybatis.xml\")); } @DisplayName(\"Mybatis数据库测试\") //自定义测试名称 @RepeatedTest(3) //自动执行多次测试 public void test(){ try (SqlSession sqlSession = factory.openSession(true)){ TestMapper testMapper = sqlSession.getMapper(TestMapper.class); System.out.println(testMapper.getStudentBySid(1)); } } } 那么就有人提问了，如果我需要的依赖没有上传的远程仓库，而是只有一个Jar怎么办呢？我们可以使用第四种作用域： system：作用域和provided是一样的，但是它不是从远程仓库获取，而是直接导入本地Jar包： \u003cdependency\u003e \u003cgroupId\u003ejavax.jntm\u003c/groupId\u003e \u003cartifactId\u003elbwnb\u003c/artifactId\u003e \u003cversion\u003e2.0\u003c/version\u003e \u003cscope\u003esystem\u003c/scope\u003e \u003csystemPath\u003eC://学习资料/4K高清无码/test.jar\u003c/systemPath\u003e \u003c/dependency\u003e 比如上面的例子，如果scope为system，那么我们需要添加一个systemPath来指定jar文件的位置，这里就不再演示了。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:3","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven可选依赖 当项目中的某些依赖不希望被使用此项目作为依赖的项目使用时，我们可以给依赖添加optional标签表示此依赖是可选的，默认在导入依赖时，不会导入可选的依赖： \u003coptional\u003etrue\u003c/optional\u003e 比如Mybatis的POM文件中，就存在大量的可选依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-api\u003c/artifactId\u003e \u003cversion\u003e1.7.30\u003c/version\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-log4j12\u003c/artifactId\u003e \u003cversion\u003e1.7.30\u003c/version\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003elog4j\u003c/groupId\u003e \u003cartifactId\u003elog4j\u003c/artifactId\u003e \u003cversion\u003e1.2.17\u003c/version\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e ... 由于Mybatis要支持多种类型的日志，需要用到很多种不同的日志框架，因此需要导入这些依赖来做兼容，但是我们项目中并不一定会使用这些日志框架作为Mybatis的日志打印器，因此这些日志框架仅Mybatis内部做兼容需要导入使用，而我们可以选择不使用这些框架或是选择其中一个即可，也就是说我们导入Mybatis之后想用什么日志框架再自己加就可以了。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:4","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven排除依赖 我们了解了可选依赖，现在我们可以让使用此项目作为依赖的项目默认不使用可选依赖，但是如果存在那种不是可选依赖，但是我们导入此项目有不希望使用此依赖该怎么办呢，这个时候我们就可以通过排除依赖来防止添加不必要的依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter\u003c/artifactId\u003e \u003cversion\u003e5.8.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter-engine\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e 我们这里演示了排除JUnit的一些依赖，我们可以在外部库中观察排除依赖之后和之前的效果。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:5","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven继承关系 一个Maven项目可以继承自另一个Maven项目，比如多个子项目都需要父项目的依赖，我们就可以使用继承关系来快速配置。 我们右键左侧栏，新建一个模块，来创建一个子项目： \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cparent\u003e \u003cartifactId\u003eMavenTest\u003c/artifactId\u003e \u003cgroupId\u003eorg.example\u003c/groupId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003c/parent\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cartifactId\u003eChildModel\u003c/artifactId\u003e \u003cproperties\u003e \u003cmaven.compiler.source\u003e8\u003c/maven.compiler.source\u003e \u003cmaven.compiler.target\u003e8\u003c/maven.compiler.target\u003e \u003c/properties\u003e \u003c/project\u003e 我们可以看到，IDEA默认给我们添加了一个parent节点，表示此Maven项目是父Maven项目的子项目，子项目直接继承父项目的groupId，子项目会直接继承父项目的所有依赖，除非依赖添加了optional标签，我们来编写一个测试用例尝试一下: import lombok.extern.java.Log; @Log public class Main { public static void main(String[] args) { log.info(\"我是日志信息\"); } } 可以看到，子项目也成功继承了Lombok依赖。 我们还可以让父Maven项目统一管理所有的依赖，包括版本号等，子项目可以选取需要的作为依赖，而版本全由父项目管理，我们可以将dependencies全部放入dependencyManagement节点，这样父项目就完全作为依赖统一管理。 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cversion\u003e1.18.22\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter\u003c/artifactId\u003e \u003cversion\u003e5.8.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e8.0.27\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis\u003c/groupId\u003e \u003cartifactId\u003emybatis\u003c/artifactId\u003e \u003cversion\u003e3.5.7\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 我们发现，子项目的依赖失效了，因为现在父项目没有依赖，而是将所有的依赖进行集中管理，子项目需要什么再拿什么即可，同时子项目无需指定版本，所有的版本全部由父项目决定，子项目只需要使用即可： \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 当然，父项目如果还存在dependencies节点的话，里面的内依赖依然是直接继承： \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter\u003c/artifactId\u003e \u003cversion\u003e5.8.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cdependencyManagement\u003e \u003cdependencies\u003e ... ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:6","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven常用命令 我们可以看到在IDEA右上角Maven板块中，每个Maven项目都有一个生命周期，实际上这些是Maven的一些插件，每个插件都有各自的功能，比如： clean命令，执行后会清理整个target文件夹，在之后编写Springboot项目时可以解决一些缓存没更新的问题。 validate命令可以验证项目的可用性。 compile命令可以将项目编译为.class文件。 install命令可以将当前项目安装到本地仓库，以供其他项目导入作为依赖使用 verify命令可以按顺序执行每个默认生命周期阶段（validate，compile，package等） ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:7","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven测试项目 通过使用test命令，可以一键测试所有位于test目录下的测试案例，请注意有以下要求： 测试类的名称必须是以Test结尾，比如MainTest 测试方法上必须标注@Test注解，实测@RepeatedTest无效 这是由于JUnit5比较新，我们需要重新配置插件升级到高版本，才能完美的兼容Junit5： \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-surefire-plugin\u003c/artifactId\u003e \u003c!-- JUnit 5 requires Surefire version 2.22.0 or higher --\u003e \u003cversion\u003e2.22.0\u003c/version\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e 现在@RepeatedTest、@BeforeAll也能使用了。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:8","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"Maven打包项目 我们的项目在编写完成之后，要么作为Jar依赖，供其他模型使用，要么就作为一个可以执行的程序，在控制台运行，我们只需要直接执行package命令就可以直接对项目的代码进行打包，生成jar文件。 当然，以上方式仅适用于作为Jar依赖的情况，如果我们需要打包一个可执行文件，那么我不仅需要将自己编写的类打包到Jar中，同时还需要将依赖也一并打包到Jar中，因为我们使用了别人为我们通过的框架，自然也需要运行别人的代码，我们需要使用另一个插件来实现一起打包： \u003cplugin\u003e \u003cartifactId\u003emaven-assembly-plugin\u003c/artifactId\u003e \u003cversion\u003e3.1.0\u003c/version\u003e \u003cconfiguration\u003e \u003cdescriptorRefs\u003e \u003cdescriptorRef\u003ejar-with-dependencies\u003c/descriptorRef\u003e \u003c/descriptorRefs\u003e \u003carchive\u003e \u003cmanifest\u003e \u003caddClasspath\u003etrue\u003c/addClasspath\u003e \u003cmainClass\u003ecom.test.Main\u003c/mainClass\u003e \u003c/manifest\u003e \u003c/archive\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003emake-assembly\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e 在打包之前也会执行一次test命令，来保证项目能够正常运行，当测试出现问题时，打包将无法完成，我们也可以手动跳过，选择执行Maven目标来手动执行Maven命令，输入mvn package -Dmaven.test.skip=true 来以跳过测试的方式进行打包。 最后得到我们的Jar文件，在同级目录下输入java -jar xxxx.jar来运行我们打包好的Jar可执行程序（xxx代表文件名称） deploy命令用于发布项目到本地仓库和远程仓库，一般情况下用不到，这里就不做讲解了。 site命令用于生成当前项目的发布站点，暂时不需要了解。 我们之前还讲解了多模块项目，那么多模块下父项目存在一个packing打包类型标签，所有的父级项目的packing都为pom，packing默认是jar类型，如果不作配置，maven会将该项目打成jar包。作为父级项目，还有一个重要的属性，那就是modules，通过modules标签将项目的所有子项目引用进来，在build父级项目时，会根据子模块的相互依赖关系整理一个build顺序，然后依次build。 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:6:9","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["JavaWeb笔记"],"content":"实战：基于Mybatis+JUL+Lombok+Maven的图书管理系统（带单元测试） 项目需求： 在线录入学生信息和书籍信息 查询书籍信息列表 查询学生信息列表 查询借阅信息列表 完整的日志系统 ","date":"2022-02-16","objectID":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/:7:0","tags":["Java连接数据库"],"title":"Java连接数据库","uri":"/posts/java%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"categories":["手写数据结构"],"content":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"AVL树简介 AVL树的名字来源于它的发明作者G.M. Adelson-Velsky 和 E.M. Landis。AVL树是最先发明的自平衡二叉查找树（Self-Balancing Binary Search Tree,简称平衡二叉树）。 一棵AVL树有如下必要条件： 条件一：它必须是二叉查找树。 条件二：每个节点的左子树和右子树的高度差至多为1。 图一中左边二叉树的节点45的左孩子46比45大，不满足二叉搜索树的条件，因此它也不是一棵平衡二叉树。 右边二叉树满足二叉搜索树的条件，同时它满足条件二，因此它是一棵平衡二叉树。 左边二叉树的节点45左子树高度2，右子树高度0，左右子树高度差为2-0=2，不满足条件二；右边二叉树的节点均满足左右子树高度差至多为1，同时它满足二叉搜索树的要求，因此它是一棵平衡二叉树。 AVL树的查找、插入、删除操作在平均和最坏的情况下都是O（logn），这得益于它时刻维护着二叉树的平衡。但由于每次插入都需要不断的调整和维护，所以，实际上如果插入操作次数太多则同样会陷入超时的死局，最具优势的操作在于查找，因为它的底层设计使得它无论插入多少个元素，这颗二叉树总是严格平衡的，所以AVL树适用于插入操作不是很频繁，但查找操作极度频繁的情况，如果需要在插入和查找操作找一个均衡点，那么就只能选择红黑树了。 ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:1:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"AVL树的相关概念 平衡因子：将二叉树上节点的左子树高度减去右子树高度的值称为该节点的平衡因子BF(Balance Factor)。 在图二右边的AVL树上： 节点50的左子树高度为3，右子树高度为2，BF= 3-2 = 1； 节点45的左子树高度为2，右子树高度为1，BF= 2-1 = 1； 节点46的左子树高度为0，右子树高度为0，BF= 0-0 = 0； 节点65的左子树高度为0，右子树高度为1，BF= 0-1 = -1； 对于平衡二叉树，BF的取值范围为[-1,1]。如果发现某个节点的BF值不在此范围，则需要对树进行调整。 最小不平衡树：距离插入节点最近的，且平衡因子的绝对值大于1的节点为根的子树。 在图三中，左边二叉树的节点45的BF = 1，插入节点43后，节点45的BF = 2。节点45是距离插入点43最近的BF不在[-1,1]范围内的节点，因此以节点45为根的子树为最小不平衡子树。(这正好对应了递归的后序返回操作 中序的前驱和后继：顾名思义，就是中序遍历下的前一个结点和后一个结点，由于时二叉搜索树，所以中序遍历的前一个结点对应比这个结点小的最大结点，而后一个结点对应比这个结点大的最小结点。(这个可以看后面代码再进行理解)这个概念在进行删除结点的操作时很有用，因为删除结点后需要同时保证仍然为二叉搜索树。 关于对前驱和后继的一些寻找方法，请看我的另一篇博客：面试题 04.06. 后继者 ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:2:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"AVL树的实现详解 总体思维导图实现。 ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"1. 结点结构 struct node { int val; int depth; node *lchild; node *rchild; node() : val(0), lchild(nullptr), rchild(nullptr) {} node(int x) : val(x), lchild(nullptr), rchild(nullptr) {} }; val，结点的值。 depth，该结点的高度(它的左右子树中最高的高度+1)。 lchild，左孩子。 rchild，右孩子。 ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:1","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"2. AVL树的抽象数据结构（ADT） class AVLTree { /*date part*/ node *head; int length; public: /*construct and destruct part*/ AVLTree() : head(nullptr), length(0) {} AVLTree(int x) : head(new node(x)), length(1) {} ~AVLTree() { destroy(head); } public: /*iterator part*/ class iterator {//封装迭代器：内部类--只能调用外部类的静态函数 node *head; node *root; public: iterator(node *head, node *root) : head(head), root(root) {} iterator \u0026operator++(); bool operator==(const iterator \u0026x); bool operator!=(const iterator \u0026x); iterator operator++(int); iterator \u0026operator--(); iterator operator--(int); int operator*(); }; private: /*static member function*/ /*Rotate Part*/ static node *rotateRight(node *root); static node *rotateLeft(node *root); static node *rotateLeftRight(node *root); static node *rotateRightLeft(node *root); /*Destruct*/ static void destroy(node *root); /*Getter*/ static node *getNext(node *root, node *p); static node *getPre(node *root, node *p); static node *getMinNode(node *root); static node *getMaxNode(node *root); static int get_depth(node *root); static void update_depth(node *root); /*Insert\u0026Remove*/ static node *Insert(int x, node *root, int \u0026size); static node *remove(int x, node *root, int \u0026size); /*print_order*/ static void inorder(node *root); public: /*public interface*/ /*clear\u0026empty*/ void clear(); bool isEmpty(); /*find*/ bool find(int x); /*insert\u0026remove*/ void insert(int x); void remove(int x); /*size*/ int size(); /*begin\u0026end*/ iterator begin(); iterator end(); /*print*/ void inorder_print(); }; ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:2","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"3. AVL树高度相关操作 得到高度 static int get_depth(node *root) {//得到深度 if (root == nullptr) return 0; return root-\u003edepth; } 更新高度 static void update_depth(node *root) { if (root == nullptr) return; root-\u003edepth = std::max(get_depth(root-\u003elchild), get_depth(root-\u003erchild)) + 1; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:3","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"4. 得到子树中最大/最小结点 原理：根据二叉搜索树中结点的左子树一定小于该结点，右子树一定大于该结点。 得到最大：直接遍历得出该结点的最右结点。 static node* getMaxNode(node* root) { if (root == nullptr) return nullptr; while (root-\u003erchild != nullptr) root = root-\u003erchild; return root; } 得到最小：直接遍历得出该结点的最左结点。 static node* getMinNode(node* root) { if (root == nullptr) return nullptr; while (root-\u003elchild != nullptr) root = root-\u003elchild; return root; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:4","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"5. 得到结点的前驱和后继 注意：二叉搜索树的前驱后继一般指的是它中序遍历的前一个和后一个结点，也就是从小到大排的前一个和后一个结点。 具体可以看我之前的博客–后继者 后继结点求解：如果有右子树，就是右子树的最小结点，如果没有，则是距离该节点最近的处于该节点右边的父节点。 static node* getNext(node* root, node* p) { //得到p节点的后继结点 if (root == nullptr || p == nullptr) return nullptr; if (p-\u003eval \u003e= root-\u003eval) { return getNext(root-\u003erchild, p); } else { node* left = getNext(root-\u003elchild, p); return left ? left : root; } } 前驱结点求解：如果有左子树，就是左子树的最大结点，如果没有，则是距离该节点最近的处于该节点左边的父节点。 static node* getPre(node* root, node* p) { //得到p节点的前驱结点 if (root == nullptr || p == nullptr)return nullptr; if (p-\u003eval \u003c= root-\u003eval) { return getPre(root-\u003elchild, p); } else { node* right = getPre(root-\u003erchild, p); return right ? right : root; } } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:5","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"6. AVL树失衡的调整 节点的插入或删除都有可能导致AVL树失去平衡，因此，失衡调整是插入与删除操作的基础。 AVL树的失衡调整可以分为四种情况，我们逐一分析。 假设我们要为数组a[]={4，5，6，3，2，8，7，0，1}构建一棵AVL树。 情况一：左旋 首先插入{4，5，6}，在插入元素6后出现不平衡的情况： 当我们在右子树插入右孩子导致AVL失衡时，我们需要进行单左旋调整。旋转围绕最小失衡子树的根节点进行。 在删除新节点时也有可能会出现需要单左旋的情况。 左旋代码如下： static node *rotateLeft(node *root) { node *son = root-\u003erchild; root-\u003erchild = son-\u003elchild; son-\u003elchild = root; update_depth(root); update_depth(son); return son; } 情况二：右旋 我们继续插入元素{3，2}，此时二叉树为： 插入3、2后出现了不平衡的情况。此时的插入情况是“在左子树上插入左孩子导致AVL树失衡”，我们需要进行单右旋调整。 右旋代码： static node *rotateRight(node *root) {//右旋 node *son = root-\u003elchild; root-\u003elchild = son-\u003erchild; son-\u003erchild = root; update_depth(root);//更新深度(右旋只会对这两结点产生影响 update_depth(son); return son; } 情况三：先左旋后右旋 需要进行两次旋转的原因是第一次旋转后，AVL树仍旧处于不平衡的状态，第二次旋转再次进行调整。 我们继续插入元素{8，7} 这种情况，总结起来就是“在右子树上插入左孩子导致AVL树失衡\",此时我们需要进行先右旋后左旋的调整。 调整的代码为： static node *rotateLeftRight(node *root) { root-\u003elchild = rotateLeft(root-\u003elchild); return rotateRight(root); } 结合例子进行分析： 首先对最小不平衡子树的根节点（也就是节点6）的右孩子（也就是8）进行右旋操作 再对节点6进行一次左旋操作 情况四：先右旋再左旋 根据对称性原理，当我们“在左子树上插入右孩子导致AVL树失衡\",此时我们需要进行先左旋后右旋的调整。如果你不理解接着看图。 我们接着插入节点{0，1} 调整的代码: static node *rotateRightLeft(node *root) { root-\u003erchild = rotateRight(root-\u003erchild); return rotateLeft(root); } 结合例子进行分析： 首先对最小不平衡子树的根节点（也就是节点2）的左孩子（也就是0）进行左旋操作 再对节点2进行一次右旋操作 总结：四种失衡调整 ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:6","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"7. 插入新结点 //需要是否兼容相等的元素，可通过对 x\u003croot-\u003eval 或 x\u003eroot-\u003eval 这两个中的一个取等号即可 static node *Insert(int x, node *root, int\u0026 size) { //所有的deep的更新都在后序遍历后 if (root == nullptr) { root = new node(x); size++;//创建结点后size++ } else if (x \u003c root-\u003eval) { root-\u003elchild = Insert(x, root-\u003elchild, size); //由于在更新该root结点之前，当平衡度未达到该要求之前肯定以及是进行了update_depth操作 if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == 2) root = x \u003c root-\u003elchild-\u003eval ? rotateRight(root) : rotateLeftRight(root); } else if (x \u003e root-\u003eval) { root-\u003erchild = Insert(x, root-\u003erchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == -2) root = x \u003e root-\u003erchild-\u003eval ? rotateLeft(root) : rotateRightLeft(root); } update_depth(root); return root; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:7","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"8. 删除结点 失衡的处理： 删除节点也可能导致AVL树的失衡，实际上删除节点和插入节点是一种互逆的操作： 删除右子树的节点导致AVL树失衡时，相当于在左子树插入节点导致AVL树失衡，即情况情况二或情况四。 删除左子树的节点导致AVL树失衡时，相当于在右子树插入节点导致AVL树失衡，即情况情况一或情况三。 维持排序树的处理： 另外，AVL树也是一棵二叉排序树，因此在删除节点时也要维护二叉排序树的性质。 如果删除结点为叶子结点，则直接删除，并不会改变搜索树的性质。 如果删除结点只有左子树或者右子树，则直接把要删除的结点的数据用下一个结点覆盖，然后删除下一个结点，由于复制了下一层的左右孩子指针，所以不会出现断层的。 如果删除结点左右子树都有，则找出该节点的前驱结点或后继结点的值进行覆盖(不覆盖指针，这样便仍然是排序二叉树了，然后**继续递归寻找对应的前驱或者后继结点进行删除，**因为左右子树都有，所以它们的前驱或者后继只能是叶子结点，找到直接删除即可。 删除处理代码： 我这里对删除操作进行了进一步优化，如果被删除结点的左右子树都存在，则查看左右子树的高度，如果左边高于右边则选择前驱结点进行删除，反之则后继。 static node *remove(int x, node *root, int\u0026 size) { if (root == nullptr) return nullptr; if (x == root-\u003eval) { /*左右子树均不为空---用中序的前驱或者后继来进行替换*/ if (root-\u003elchild != nullptr \u0026\u0026 root-\u003erchild != nullptr) { /*根据左右子树的深度来选择删除替换哪边的*/ if (get_depth(root-\u003elchild) \u003e get_depth(root-\u003erchild)) { node* t = getMaxNode(root-\u003elchild); root-\u003eval = t-\u003eval; root-\u003elchild = remove(t-\u003eval, root-\u003elchild, size); } else { node* t = getMinNode(root-\u003erchild); root-\u003eval = t-\u003eval; root-\u003erchild = remove(t-\u003eval, root-\u003erchild, size); } } /*左右子树至少有一个为空的情况，直接往下走一步即可*/ else { node* tmp = root-\u003elchild == nullptr ? root-\u003erchild : nullptr; if (tmp != nullptr) { *root = *tmp; delete tmp; } else { delete root; root = nullptr; } //删除时size-- size--; } } else if (x \u003c root-\u003eval) { root-\u003elchild = remove(x, root-\u003elchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == -2) root = get_depth(root-\u003erchild-\u003elchild) \u003e get_depth(root-\u003erchild-\u003erchild) ? rotateRightLeft(root) : rotateLeft(root); } else { root-\u003erchild = remove(x, root-\u003erchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == 2) root = get_depth(root-\u003elchild-\u003erchild) \u003e get_depth(root-\u003elchild-\u003elchild) ? rotateLeftRight(root) : rotateRight(root); } return root; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:8","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"9. 查找元素 二叉树是一种递归的定义，因此，二叉树的许多操作都可以通过递归简单地实现，例如遍历二叉树、查找指定元素、销毁二叉树等。 这里使用了迭代方式。 bool find(int x) { //查找直接迭代方式即可 node *f = head; while (f != nullptr) { if (x == f-\u003eval) return true; else if (x \u003c f-\u003eval) f = f-\u003elchild; else f = f-\u003erchild; } return false; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:9","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"10. 遍历二叉树 我这里只提供了中序遍历的打印，方便验证二叉搜索树的情况。 static void inorder(node *root) { if (root != nullptr) { inorder(root-\u003elchild); printf(\"%d \", root-\u003eval); inorder(root-\u003erchild); } } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:10","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"11. AVL树的销毁 直接利用后序先处理完左右子树再处理根节点。 static void destroy(node *root) { if (root == nullptr) return; destroy(root-\u003elchild); destroy(root-\u003erchild); delete root; root = nullptr; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:11","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"12. 迭代器的设计 关于C++里面的迭代器，其实就是方便遍历容器中的元素，而迭代器需要模拟指针的行为，所以在C++中迭代器其实就是对指针特别包装的类，对其进行一些运算符的重载即可。 内部类充当迭代器 由于需要满足顺序容器的迭代顺序，所以++和–操作对应后继和前驱。 /*iterator part*/ class iterator { //封装迭代器 node* head; node* root; public: iterator(node* head, node* root): head(head), root(root) {} iterator\u0026 operator++() { //直接把root加为当前的后继结点 root = getNext(head, root); return *this; } bool operator==(const iterator\u0026 x) { return this-\u003eroot == x.root; } bool operator!=(const iterator\u0026 x) { return this-\u003eroot != x.root; } iterator operator++(int) { iterator t = *this; root = getNext(head, root); return t; } iterator\u0026 operator--() { //直接把root赋值为前驱结点 root = getPre(head, root); return *this; } iterator operator--(int) { iterator t = *this; root = getPre(head, root); return t; } node\u0026 operator*() { //解引用的重载 return *root; } }; 外部类提供外界begin()和end()接口得到迭代器的始端和末端 iterator begin() { node* min = getMinNode(head); return iterator(head, min); } iterator end() { //end表示结束标记 return iterator(head, nullptr); } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:3:12","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"完整代码 我的GitHub ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:4:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"AVLTree.h // // Created by Alone on 2021/10/12. // #include \u003calgorithm\u003e#include \u003ccstdio\u003e#include \u003ccassert\u003e #ifndef MY_TINY_STL_AVLTREE_H #define MY_TINY_STL_AVLTREE_H namespace L_B__ { struct node { int val; int depth; node *lchild; node *rchild; node() : val(0), lchild(nullptr), rchild(nullptr) {} node(int x) : val(x), lchild(nullptr), rchild(nullptr) {} }; class AVLTree { /*date part*/ node *head; int length; public: /*construct and destruct part*/ AVLTree() : head(nullptr), length(0) {} AVLTree(int x) : head(new node(x)), length(1) {} ~AVLTree() { destroy(head); } public: /*iterator part*/ class iterator {//封装迭代器：内部类--只能调用外部类的静态函数 node *head; node *root; public: iterator(node *head, node *root) : head(head), root(root) {} iterator \u0026operator++(); bool operator==(const iterator \u0026x); bool operator!=(const iterator \u0026x); iterator operator++(int); iterator \u0026operator--(); iterator operator--(int); int operator*(); }; private: /*static member function*/ /*Rotate Part*/ static node *rotateRight(node *root); static node *rotateLeft(node *root); static node *rotateLeftRight(node *root); static node *rotateRightLeft(node *root); /*Destruct*/ static void destroy(node *root); /*Getter*/ static node *getNext(node *root, node *p); static node *getPre(node *root, node *p); static node *getMinNode(node *root); static node *getMaxNode(node *root); static int get_depth(node *root); static void update_depth(node *root); /*Insert\u0026Remove*/ static node *Insert(int x, node *root, int \u0026size); static node *remove(int x, node *root, int \u0026size); /*print_order*/ static void inorder(node *root); public: /*public interface*/ /*clear\u0026empty*/ void clear(); bool isEmpty(); /*find*/ bool find(int x); /*insert\u0026remove*/ void insert(int x); void remove(int x); /*size*/ int size(); /*begin\u0026end*/ iterator begin(); iterator end(); /*print*/ void inorder_print(); }; } #endif //MY_TINY_STL_AVLTREE_H ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:4:1","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"AVLTree.cpp // // Created by Alone on 2021/10/12. // #include \"AVLTree.h\" /*Rotate*/ L_B__::node *L_B__::AVLTree::rotateRight(L_B__::node *root) {//右旋 node *son = root-\u003elchild; root-\u003elchild = son-\u003erchild; son-\u003erchild = root; update_depth(root);//更新深度(右旋只会对这两结点产生影响 update_depth(son); return son; } L_B__::node *L_B__::AVLTree::rotateLeft(L_B__::node *root) { node *son = root-\u003erchild; root-\u003erchild = son-\u003elchild; son-\u003elchild = root; update_depth(root); update_depth(son); return son; } L_B__::node *L_B__::AVLTree::rotateLeftRight(L_B__::node *root) { root-\u003elchild = rotateLeft(root-\u003elchild); return rotateRight(root); } L_B__::node *L_B__::AVLTree::rotateRightLeft(L_B__::node *root) { root-\u003erchild = rotateRight(root-\u003erchild); return rotateLeft(root); } /*Destruct*/ void L_B__::AVLTree::destroy(L_B__::node *root) { if (root == nullptr) return; destroy(root-\u003elchild); destroy(root-\u003erchild); delete root; root = nullptr; } /*Getter*/ L_B__::node *L_B__::AVLTree::getNext(L_B__::node *root, L_B__::node *p) { if (root == nullptr || p == nullptr) return nullptr; if (p-\u003eval \u003e= root-\u003eval) { return getNext(root-\u003erchild, p); } else { node *left = getNext(root-\u003elchild, p); return left ? left : root; } } L_B__::node *L_B__::AVLTree::getPre(L_B__::node *root, L_B__::node *p) { if (root == nullptr || p == nullptr)return nullptr; if (p-\u003eval \u003c= root-\u003eval) { return getPre(root-\u003elchild, p); } else { node *right = getPre(root-\u003erchild, p); return right ? right : root; } } L_B__::node *L_B__::AVLTree::getMinNode(L_B__::node *root) { if (root == nullptr) return nullptr; while (root-\u003elchild != nullptr) root = root-\u003elchild; return root; } L_B__::node *L_B__::AVLTree::getMaxNode(L_B__::node *root) { if (root == nullptr) return nullptr; while (root-\u003erchild != nullptr) root = root-\u003erchild; return root; } int L_B__::AVLTree::get_depth(L_B__::node *root) { if (root == nullptr) return 0; return root-\u003edepth; } void L_B__::AVLTree::update_depth(L_B__::node *root) { if (root == nullptr) return; root-\u003edepth = std::max(get_depth(root-\u003elchild), get_depth(root-\u003erchild)) + 1; } /*Insert\u0026remove*/ L_B__::node *L_B__::AVLTree::Insert(int x, L_B__::node *root, int \u0026size) { if (root == nullptr) { root = new node(x); size++;//创建结点后size++ } else if (x \u003c root-\u003eval) { root-\u003elchild = Insert(x, root-\u003elchild, size); //由于在更新该root结点之前，当平衡度未达到该要求之前肯定以及是进行了update_depth操作 if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == 2) root = x \u003c root-\u003elchild-\u003eval ? rotateRight(root) : rotateLeftRight(root); } else if (x \u003e root-\u003eval) { root-\u003erchild = Insert(x, root-\u003erchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == -2) root = x \u003e root-\u003erchild-\u003eval ? rotateLeft(root) : rotateRightLeft(root); } update_depth(root); return root; } L_B__::node *L_B__::AVLTree::remove(int x, L_B__::node *root, int \u0026size) { if (root == nullptr) return nullptr; if (x == root-\u003eval) { /*左右子树均不为空---用中序的前驱或者后继来进行替换*/ if (root-\u003elchild != nullptr \u0026\u0026 root-\u003erchild != nullptr) { /*根据左右子树的深度来选择删除替换哪边的*/ if (get_depth(root-\u003elchild) \u003e get_depth(root-\u003erchild)) { node *t = getMaxNode(root-\u003elchild); root-\u003eval = t-\u003eval; root-\u003elchild = remove(t-\u003eval, root-\u003elchild, size); } else { node *t = getMinNode(root-\u003erchild); root-\u003eval = t-\u003eval; root-\u003erchild = remove(t-\u003eval, root-\u003erchild, size); } } /*左右子树至少有一个为空的情况，直接往下走一步即可*/ else { node *tmp = root-\u003elchild == nullptr ? root-\u003erchild : nullptr; if (tmp != nullptr) { *root = *tmp; delete tmp; } else { delete root; root = nullptr; } //删除时size-- size--; } } else if (x \u003c root-\u003eval) { root-\u003elchild = remove(x, root-\u003elchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == -2) root = get_depth(root-\u003erchild-\u003elchild) \u003e get_depth(root-\u003erchild-\u003erchild) ? rotateRightLeft(root) : rotateLeft(root); } else { root-\u003erchild = remove(x, root-\u003erchild, size); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == 2) root = get_depth(root-\u003elchild-\u003erchild) \u003e get_depth(root-\u003elchild-\u003elchild) ? rotateLeftRight(root) : rotateRight(root); } return root; } /*print part*/ void L_B__::AVL","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:4:2","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"测试 注意：以下数据由于存在大量相同的值，而我写的这个AVLTree并未对相同的值进行存储，所以节省了大量插入时候的调整时间，所以才能达到不错的插入性能，实际上只要实际插入的数据够多，和红黑树的差距就越大，我之前试过十亿不重复数据插入AVL和RB的测试，AVL运行几分钟，而RB一分钟内解决。但查找和删除方面AVL仍然是吊打RB(毕竟严格平衡树 与STL中的set（红黑树）进行对比： ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:5:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"测试代码 int main() { using namespace std; AVLTree x; set\u003cint\u003eQ; printf(\"插入测试\\n\"); auto start = clock(); for (int i = 0; i \u003c 100000000; ++i) { x.insert(i%10000000); } std::cout\u003c\u003c\"AVLTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; start = clock(); for (int i = 0; i \u003c 100000000; ++i) { Q.insert(i%10000000); } std::cout\u003c\u003c\"RBTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; printf(\"迭代测试\\n\"); start = clock(); for(auto it = x.begin();it!=x.end();++it){ continue; } std::cout\u003c\u003c\"AVLTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; start = clock(); for(auto it = Q.begin();it!=Q.end();++it){ continue; } std::cout\u003c\u003c\"RBTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; printf(\"查找测试\\n\"); start = clock(); for (int i = 0; i \u003c 100000000; ++i) { x.find(i); } std::cout\u003c\u003c\"AVLTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; start = clock(); for(int i = 0;i\u003c100000000;++i){ Q.count(i); } std::cout\u003c\u003c\"RBTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cstd::endl; printf(\"删除测试\\n\"); start = clock(); for(int i=0;i\u003c10000000;i++){ x.remove(i); } std::cout\u003c\u003c\"AVLTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003c\"length\"\u003c\u003cx.size()\u003c\u003cstd::endl; start = clock(); for(int i=0;i\u003c10000000;i++){ Q.erase(i); } std::cout\u003c\u003c\"RBTree\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003c\"length\"\u003c\u003cQ.size()\u003c\u003cstd::endl; return 0; } ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:5:1","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"测试总结 通过不断对比红黑树(RB)和AVL，得出以下结论： 插入操作红黑树比AVL快很多，数据量越大优势越明显。 查找和删除操作红黑树却是比AVL慢很多，同样也是数据量越大越明显。 总的来说，如果所需要管理的数据量很大，并且需要频繁的插入，那么红黑树更适合你，如果只需要插入一次后，对数据进行查找管理，那么AVL更加的适合你！ ","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:5:2","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"解题测试 OJ平台 在以上设计的基础上加个get_head方法即可。 #include \u003cbits/stdc++.h\u003e struct node { int val; int depth; node *lchild; node *rchild; node() : val(0), lchild(nullptr), rchild(nullptr) {} node(int x) : val(x), lchild(nullptr), rchild(nullptr) {} }; class AVLTree { /*date part*/ node *head; int size; public: /*construct and destruct part*/ AVLTree() : head(nullptr),size(0) {} AVLTree(int x) : head(new node(x)),size(1) {} ~AVLTree() { destroy(head); } private: /*static function part*/ static node *rotateRight(node *root) {//右旋 node *son = root-\u003elchild; root-\u003elchild = son-\u003erchild; son-\u003erchild = root; update_depth(root);//更新深度(右旋只会对这两结点产生影响 update_depth(son); return son; } static node *rotateLeft(node *root) { node *son = root-\u003erchild; root-\u003erchild = son-\u003elchild; son-\u003elchild = root; update_depth(root); update_depth(son); return son; } static node *rotateLeftRight(node *root) { root-\u003elchild = rotateLeft(root-\u003elchild); return rotateRight(root); } static node *rotateRightLeft(node *root) { root-\u003erchild = rotateRight(root-\u003erchild); return rotateLeft(root); } static void destroy(node *root) { if (root == nullptr) return; destroy(root-\u003elchild); destroy(root-\u003erchild); delete root; root = nullptr; } static node* getMinNode(node* root){ if(root== nullptr) return nullptr; while(root-\u003elchild!= nullptr) root = root-\u003elchild; return root; } static node* getMaxNode(node* root){ if(root== nullptr) return nullptr; while(root-\u003erchild!= nullptr) root = root-\u003erchild; return root; } static int get_depth(node *root) {//得到深度 if (root == nullptr) return 0; return root-\u003edepth; } static void update_depth(node *root) { if (root == nullptr) return; root-\u003edepth = std::max(get_depth(root-\u003elchild), get_depth(root-\u003erchild)) + 1; } static node *Insert(int x, node *root) {//所有的deep的更新都在后序遍历后 if (root == nullptr) { root = new node(x); } else if (x \u003c= root-\u003eval) { root-\u003elchild = Insert(x, root-\u003elchild); //由于在更新该root结点之前，当平衡度未达到该要求之前肯定以及是进行了update_depth操作 if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == 2) root = x \u003c= root-\u003elchild-\u003eval ? rotateRight(root) : rotateLeftRight(root); } else if (x \u003e root-\u003eval) { root-\u003erchild = Insert(x, root-\u003erchild); if (get_depth(root-\u003elchild) - get_depth(root-\u003erchild) == -2) root = x \u003e= root-\u003erchild-\u003eval ? rotateLeft(root) : rotateRightLeft(root); } update_depth(root); return root; } static node *remove(int x, node *root) { if (root == nullptr) return nullptr; if (x == root-\u003eval) { /*左右子树均不为空---用中序的前驱或者后继来进行替换*/ if (root-\u003elchild != nullptr \u0026\u0026 root-\u003erchild != nullptr) { /*根据左右子树的深度来选择删除替换哪边的*/ if(get_depth(root-\u003elchild)\u003eget_depth(root-\u003erchild)){ node* t = getMaxNode(root-\u003elchild); root-\u003eval = t-\u003eval; root-\u003elchild = remove(t-\u003eval,root-\u003elchild); }else{ node* t = getMinNode(root-\u003erchild); root-\u003eval = t-\u003eval; root-\u003erchild = remove(t-\u003eval,root-\u003erchild); } } /*左右子树至少有一个为空的情况，直接往下走一步即可*/ else{ node* tmp = root-\u003elchild== nullptr?root-\u003erchild: nullptr; if(tmp!= nullptr){ *root = *tmp; delete tmp; } else{ delete root; root = nullptr; } } } else if (x \u003c root-\u003eval) { root-\u003elchild = remove(x, root-\u003elchild); if(get_depth(root-\u003elchild)-get_depth(root-\u003erchild)==-2) root = get_depth(root-\u003erchild-\u003elchild)\u003eget_depth(root-\u003erchild-\u003erchild)?rotateRightLeft(root): rotateLeft(root); } else { root-\u003erchild = remove(x, root-\u003erchild); if(get_depth(root-\u003elchild)-get_depth(root-\u003erchild)==2) root = get_depth(root-\u003elchild-\u003erchild)\u003eget_depth(root-\u003elchild-\u003elchild)?rotateLeftRight(root): rotateRight(root); } return root; } static void inorder(node *root) { if (root != nullptr) { inorder(root-\u003elchild); printf(\"%d \", root-\u003eval); inorder(root-\u003erchild); } } public: /*public function part*/ void insert(int x) { //递归方式插入，方便后续处理 head = Insert(x, head); size++; } void remove(int x){ assert(size!=0); head = remove(x,head); size--; } void clear() { destroy(head); } bool isEmpty() { return head == nullptr; } bool find(int x) { //查找直接迭代方式即可 node *f = head; while (f != nullptr) { if (x == f-\u003eval) return true; else if (x \u003c f-\u003eval) f = f-\u003elchild; else f = f-\u003erchild; } return false; } void i","date":"2022-02-16","objectID":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/:6:0","tags":["徒手写的AVL竟然比STL中的红黑树效率更高？✨"],"title":"徒手写的AVL竟然比STL中的红黑树效率更高？✨","uri":"/posts/%E5%BE%92%E6%89%8B%E5%86%99%E7%9A%84avl%E7%AB%9F%E7%84%B6%E6%AF%94stl%E4%B8%AD%E7%9A%84%E7%BA%A2%E9%BB%91%E6%A0%91%E6%95%88%E7%8E%87%E6%9B%B4%E9%AB%98/"},{"categories":["手写数据结构"],"content":"新手用C++写了个泛型堆，效率竟比STL的更快？","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"关于为什么突然想写一个模板类？ 嗯。。主要是因为最近在翻看《STL源码剖析》，然后发现原来STL源码是如此的庞大且复杂，而又及其具有条理，而其中最难的就是各个组件的关系，而对外所展现的效果就是泛型编程，对我这个初入门菜鸟来说的话，我之前对模板仅仅停留在知道，但没用过的阶段🤣 由于STL库中对模板的骚操作一个接一个，没个模板的基础，根本就看不懂，所以我痛定思痛，一定要亲手用模板实现一个数据结构(之前用非模板实现过一些)。 好了，目标有了，用模板实现一个数据结构，那数据结构这么多，我到底实现哪个呢？要不就把堆给冲了？我一拍大腿，可行！👍 最后关于它的具体实现，我肯定不会像STL那样考虑的那么周到，组件分工齐全，毕竟STL是包含六大组件的！ 具体如下图： ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:1:0","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"对刚用模板的C++新手而言的几大坑点 ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:2:0","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"习惯性写.h和.cpp文件 对于对C++有一定掌握，但基本没用过模板的人而言，声明与定义早就烂熟于心的.h和.cpp文件。 然而，当你使用模板的时候，再去吧声明和定义分开，将会产生一个错误！除非在.h文件末尾有包含.cpp文件。然而这样的操作是很多余的！ 所以在用模板实现类的时候，最好只写一个.h文件！ ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:2:1","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"对模板特化运用和理解很少 我现在就处于这个状态，说不上来该怎么规范🤣 ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:2:2","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"我的Heap实现 ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:3:0","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"总览Heap类 这是我画的树状图，我设计的Heap一共分为以下三个部分： 一、 成员变量 这部分没啥好说的，就用一个 nums 指针存储变量数据，length 记录当前的 nums 已经使用的长度，capacity 存储当前 nums 的最大容量。 二、 静态成员函数 我将堆的基本操作封装为静态成员变量的初衷是方便，对外实现堆化的功能，即使使用外部数组也能实现堆。 下面简单的介绍一下这几个函数（后面再详解）： sift_down()：接受四个参数，依次是用于向下堆化的数组，起始的堆化位置，结束的堆化位置，以及一个仿函数(用于设定最大/最小堆)。 static void sift_down(T \u0026nums, size_t start, size_t len, _CMP cmp); sift_up()：向上堆化的操作，与上面的类似，少了结束信息，因为结束信息一般都是0. static void sift_up(T \u0026nums, size_t start, _CMP cmp); heapify()：调用向下堆化函数，实现完全堆化，接收两个参数，待堆化数组和数组长度 static void heapify(T \u0026nums, size_t len); print()：主要实现一个用于测试功能的泛型打印接口，接收的参数肯定就是数组和数组长度了。(略过，不重要) 三、类的内部成员函数(放源代码详解) 构造和析构函数：很简单的，我直接放源代码出来。 Heap() : length(0), capacity(1) {//暂时没有对构造函数拓展的打算 nums = new _T[capacity]; } ~Heap(){ delete []nums; } push()：每次入队后进行一次向上堆化，这个简单，直接调上面的静态接口就行了。注意：我这里采用的是倍增的方式进行延伸内存，一旦出现capacity不够用的情况，就重新分配内存，其中数据拷贝方面用了copy函数。 void push(_T val) { if (length \u003e= capacity) {//两倍两倍的扩容 _T *t = nums; capacity *= 2; nums = new _T[capacity]; std::copy(t, t + length, nums); delete[] t; } nums[length] = val; sift_up(nums, length, _CMP()); length++; } pop()：这个操作是需要一点技巧的–为了不破坏整体堆化的结构，我们直接把根部与尾部元素进行交换，再从根部往下重新堆化一次(注意此时堆化的终点应该要比原来小1)。 还采取了C语言的断言方式，防止pop操作的时候，堆中已经没有元素。 void pop() { if (length == 0) assert(0); length--; std::swap(nums[0], nums[length]);//实际上pop操作就相当于堆排的一次过程 sift_down(nums, 0, length, _CMP()); } top()：直接取根节点的值。 _T top() { if (length == 0) assert(0); return nums[0]; } print()：用于打印内部数据的接口。 void print() {//内部print方便测试 print(nums, length);//调用静态print } ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:3:1","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"关于堆化函数的实现 我直接开放代码实现，具体的理解靠大家了。 sift_down() template\u003ctypename T\u003e static void sift_down(T \u0026nums, size_t start, size_t len, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = len; int parent = start; int child = parent * 2 + 1; while (child \u003c end) { if (child + 1 \u003c end \u0026\u0026 cmp(nums[child], nums[child + 1])) child++; if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); parent = child; child = parent * 2 + 1; } } } sift_up template\u003ctypename T\u003e static void sift_up(T \u0026nums, size_t start, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = 0; int child = start; int parent = (child - 1) / 2; while (child \u003e end) { if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); child = parent; parent = (child - 1) / 2; } } } heapify() template\u003ctypename T\u003e static void heapify(T \u0026nums, size_t len) {//用于对数组进行堆化 for (int i = len - 1; i \u003e= 0; i--) { sift_down\u003cT\u003e(nums, i, len, _CMP()); } } print() template\u003ctypename T\u003e static void print(T \u0026nums, size_t length) { for (int i = 0; i \u003c length; i++) { std::cout \u003c\u003c nums[i] \u003c\u003c ' '; } } ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:3:2","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"关于所用模板的说明 类模板(全局均可用的类型参数)： 其中 _T 用于表示数组元素的类型，默认为int，_CMP 则是一个类(仿函数)的类型，默认为已经提供的cmp仿函数，所以如果为自定义类型构建堆，则一定要自己写好相应的仿函数进行传递。 template\u003ctypename T = int\u003e //用于默认排序的仿函数，默认为大顶堆 class cmp { public: bool operator()(T \u0026a, T \u0026b) { return a \u003c b; } }; template\u003ctypename _T = int, typename _CMP = cmp\u003c_T\u003e\u003e 成员函数模板 为了便于外界使用静态成员函数进行相应的操作，所以每个静态成员函数都提供了对应的类型模板，但比较操作的仿函数用的还是和类模板一样的类型。 如： template\u003ctypename T\u003e static void sift_up(T \u0026nums, size_t start, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = 0; int child = start; int parent = (child - 1) / 2; while (child \u003e end) { if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); child = parent; parent = (child - 1) / 2; } } } ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:3:3","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"整合所有代码，实现Heap类模板 这里用一个命名空间套住是最好，防止命名冲突！我这里用的自己常用的网名hhh // // Created by Alone on 2021/10/2. // #ifndef MY_TINY_STL_HEAP_H #define MY_TINY_STL_HEAP_H #include \u003ciostream\u003e#include \u003ccassert\u003e#include \u003calgorithm\u003e namespace L_B__ { template\u003ctypename T=int\u003e//用于默认排序的仿函数，默认为大顶堆 class cmp { public: bool operator()(T \u0026a, T \u0026b) { return a \u003c b; } }; //@模板类的实现 template\u003ctypename _T=int, typename _CMP = cmp\u003c_T\u003e\u003e class Heap { //私有成员 _T *nums; size_t length; size_t capacity; public://@静态成员函数，对外对内都能实现功能 template\u003ctypename T\u003e static void sift_down(T \u0026nums, size_t start, size_t len, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = len; int parent = start; int child = parent * 2 + 1; while (child \u003c end) { if (child + 1 \u003c end \u0026\u0026 cmp(nums[child], nums[child + 1])) child++; if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); parent = child; child = parent * 2 + 1; } } } template\u003ctypename T\u003e static void sift_up(T \u0026nums, size_t start, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = 0; int child = start; int parent = (child - 1) / 2; while (child \u003e end) { if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); child = parent; parent = (child - 1) / 2; } } } template\u003ctypename T\u003e static void heapify(T \u0026nums, size_t len) {//用于对数组进行堆化 for (int i = len - 1; i \u003e= 0; i--) { sift_down\u003cT\u003e(nums, i, len, _CMP()); } } template\u003ctypename T\u003e static void print(T \u0026nums, size_t length) {//专为打印原始数组 for (int i = 0; i \u003c length; i++) { std::cout \u003c\u003c nums[i] \u003c\u003c ' '; } } public://@基本的内部成员函数 Heap() : length(0), capacity(1) {//暂时没有对构造函数拓展的打算 nums = new _T[capacity]; } ~Heap() { delete[]nums; } void push(_T val) { if (length \u003e= capacity) {//两倍两倍的扩容 _T *t = nums; capacity *= 2; nums = new _T[capacity]; std::copy(t, t + length, nums); delete[] t; } nums[length] = val; sift_up(nums, length, _CMP()); length++; } void pop() { if (length == 0) assert(0); length--; std::swap(nums[0], nums[length]);//实际上pop操作就相当于堆排的一次过程 sift_down(nums, 0, length, _CMP()); } _T top() { if (length == 0) assert(0); return nums[0]; } void print() {//内部print方便测试 print(nums, length); } }; } #endif //MY_TINY_STL_HEAP_H ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:3:4","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"我的Heap测试 ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:4:0","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"正确性测试(与STL priority_queue对比) 经历这数千万数据的测试，仍然和STL的正确性是一样的，说明正确性是有了保障的。 测试代码 #include \"Heap.h\"#include \u003cctime\u003e#include \u003cqueue\u003eusing namespace std; #define MAX_SIZE 10000000 int main() { L_B__::Heap\u003cint\u003e a; priority_queue\u003cint\u003e Q; int step = 1; for(int i=1;i\u003cMAX_SIZE;i++){ a.push(step%i); Q.push(step%i); step += 2; } cout\u003c\u003c\"STL-\u003e\"; for (int i = 0; i \u003c 10; i++) { auto t = move(Q.top()); cout \u003c\u003c t\u003c\u003c' '; Q.pop(); } cout \u003c\u003c endl; cout \u003c\u003c \"My-\u003e\"; for (int i = 0; i \u003c 10; i++) { auto t = move(a.top()); cout \u003c\u003c t\u003c\u003c' '; a.pop(); } } ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:4:1","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"效率测试(与STL priority_queue对比) 1000w数据两者的push和pop速度对比 欧耶！比STL快两倍！！🐱‍🏍 其实这样不是很奇怪，STL本来就要实现和考虑很多事情，各种组件的套娃，而我们只是实现了这样一个简单的堆，那当然快很多了。 测试源代码 #include \"Heap.h\"#include \u003cctime\u003e#include \u003cqueue\u003eusing namespace std; #define MAX_SIZE 10000000 int main() { L_B__::Heap\u003cint\u003e a; priority_queue\u003cint\u003e Q; auto start = clock(); //@push测试 int step = 1; for(int i=1;i\u003cMAX_SIZE;i++){ Q.push(step%i); step += 2; } cout\u003c\u003c\"STL Push:\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cendl; start = clock(); step = 1; for(int i=1;i\u003cMAX_SIZE;i++){ a.push(step%i); step += 2; } cout\u003c\u003c\"My Push:\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cendl; //@pop测试 start = clock(); for(int i=1;i\u003cMAX_SIZE;i++){ Q.pop(); } cout\u003c\u003c\"STL Pop:\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cendl; start = clock(); for(int i=1;i\u003cMAX_SIZE;i++){ a.pop(); } cout\u003c\u003c\"My Pop:\"\u003c\u003cclock()-start\u003c\u003c\"ms\"\u003c\u003cendl; } ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:4:2","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"解题实测(LeetCode 347. 前 K 个高频元素) 我去，果然比stl的快不少🤣 直接把代码cv放到LC前面，然后测试使用自定义类型的情况。 解题代码： struct S{ int a,b; }; class cmp{ public: bool operator()(S\u0026a,S\u0026b){ return a.b\u003cb.b; } }; namespace L_B__ { template\u003ctypename T=int\u003e//用于默认排序的仿函数，默认为大顶堆 class cmp { public: bool operator()(T \u0026a, T \u0026b) { return a \u003c b; } }; //@模板类的实现 template\u003ctypename _T=int, typename _CMP = cmp\u003c_T\u003e\u003e class Heap { //私有成员 _T *nums; size_t length; size_t capacity; public://@静态成员函数，对外对内都能实现功能 template\u003ctypename T\u003e static void sift_down(T \u0026nums, size_t start, size_t len, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = len; int parent = start; int child = parent * 2 + 1; while (child \u003c end) { if (child + 1 \u003c end \u0026\u0026 cmp(nums[child], nums[child + 1])) child++; if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); parent = child; child = parent * 2 + 1; } } } template\u003ctypename T\u003e static void sift_up(T \u0026nums, size_t start, _CMP cmp) {//最小堆还是最大堆由cmp决定 int end = 0; int child = start; int parent = (child - 1) / 2; while (child \u003e end) { if (!cmp(nums[parent], nums[child])) { break; } else { std::swap(nums[parent], nums[child]); child = parent; parent = (child - 1) / 2; } } } template\u003ctypename T\u003e static void heapify(T \u0026nums, size_t len) {//用于对数组进行堆化 for (int i = len - 1; i \u003e= 0; i--) { sift_down\u003cT\u003e(nums, i, len, _CMP()); } } template\u003ctypename T\u003e static void print(T \u0026nums, size_t length) {//专为打印原始数组 for (int i = 0; i \u003c length; i++) { std::cout \u003c\u003c nums[i] \u003c\u003c ' '; } } public://@基本的内部成员函数 Heap() : length(0), capacity(1) {//暂时没有对构造函数拓展的打算 nums = new _T[capacity]; } ~Heap() { delete[]nums; } void push(_T val) { if (length \u003e= capacity) {//两倍两倍的扩容 _T *t = nums; capacity *= 2; nums = new _T[capacity]; std::copy(t, t + length, nums); delete[] t; } nums[length] = val; sift_up(nums, length, _CMP()); length++; } void pop() { if (length == 0) assert(0); length--; std::swap(nums[0], nums[length]);//实际上pop操作就相当于堆排的一次过程 sift_down(nums, 0, length, _CMP()); } _T top() { if (length == 0) assert(0); return nums[0]; } void print() {//内部print方便测试 print(nums, length); } }; } class Solution { public: vector\u003cint\u003e topKFrequent(vector\u003cint\u003e\u0026 nums, int k) { unordered_map\u003cint,int\u003et; //开始映射hashmap for(int s:nums){ t[s]++; } L_B__::Heap\u003cS,cmp\u003eQ; for(unordered_map\u003cint,int\u003e::iterator it=t.begin();it!=t.end();it++){ Q.push({it-\u003efirst,it-\u003esecond}); } vector\u003cint\u003eres; //得到前k个最大的元素 for(int i=0;i\u003ck;i++){ auto t = Q.top();Q.pop(); res.push_back(t.a); } return res; } }; ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:4:3","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"总结 对于模板的运用，现在才刚开始入门(入门都不到…)，给我的感觉是STL源码的包袱很重，各个组件高度关联，所以能够通过一个容器实现的算法有很多很多！我这个新手还不配高攀，只能仰望了，当然平时随便写写乐色代码也是可以滴，至少可以吹牛说比STL还快了🤣 ","date":"2022-02-16","objectID":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/:5:0","tags":["新手用C++写了个泛型堆，效率竟比STL的更快？"],"title":"新手用C++写了个泛型堆，效率竟比STL的更快？","uri":"/posts/%E6%96%B0%E6%89%8B%E7%94%A8c++%E5%86%99%E4%BA%86%E4%B8%AA%E6%B3%9B%E5%9E%8B%E5%A0%86%E6%95%88%E7%8E%87%E7%AB%9F%E6%AF%94stl%E7%9A%84%E6%9B%B4%E5%BF%AB/"},{"categories":["手写数据结构"],"content":"来自上帝的骰子---Treap(树堆)详解","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"为什么说是上帝的骰子？ 解释这个问题，首先由这个数据结构的名字开始，Treap = Tree + Heap，即为树堆之意，然而实际上用到堆的地方就是利用了一个随机的值标记每个结点，然后根据这个值对树进行左旋、右旋操作来调整父子树直接的关系，你可以严格让它遵循小根堆也可以遵循大根堆，这都无所谓。 也就是说每个结点的结构需要额外存储一个随机值，来决定它是否旋转调整，这对比普通的BST就要优越很多了，但这个也看运气，如果骰子没摇好，出现极端的情况，则可能即便是旋转了多次还是效率低下，我后面对BST、Treap、AVL进行了各方面的对比。 相对AVL，它不需要记录深度，不需要根据深度来判断是否旋转，旋转这件事就完全交给老天了，而且也不存在复杂的旋转情况，只有左旋和右旋。 注意：我写的这个Treap是以大根堆的方式进行维护的！也就是父节点大于子节点的随机值。 ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:1:0","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"Treap实现 ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:0","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"1. 结点的结构 struct node { //值、优先级(随机数、该数的总结点数、该结点的val次数 int val, priority, length, cnt; node *lchild; node *rchild; node() : val(0), length(1), cnt(1), lchild(nullptr), rchild(nullptr) { srand((unsigned) time(NULL));//记得重新设定种子 priority = rand(); } node(int val) : val(val), length(1), cnt(1), lchild(nullptr), rchild(nullptr) { srand((unsigned) time(NULL)); priority = rand(); } void update() { length = cnt; if (lchild != nullptr)length += lchild-\u003elength; if (rchild != nullptr)length += rchild-\u003elength; } }; ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:1","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"2. Treap的抽象数据结构 class Treap { node *head; int length; public: /*construct\u0026destruct*/ Treap() : head(nullptr), length(0) {} Treap(int val) : head(new node(val)), length(1) {} public://内部类设计迭代器 class iterator { node *head; node *root; public: /*迭代器部分*/ iterator(node *head, node *root) : head(head), root(root) {} iterator \u0026operator++() { root = queryNext(head, root-\u003eval); return *this; } iterator operator++(int) { iterator t = *this; root = queryNext(head, root-\u003eval); return t; } iterator \u0026operator--() { root = queryPre(head, root-\u003eval); return *this; } iterator operator--(int) { iterator t = *this; root = queryPre(head, root-\u003eval); return t; } int operator*() { return root-\u003eval; } bool operator!=(const iterator \u0026t) { return t.root != root; } }; private: /*static function*/ /*rotate*/ static node *rotateLeft(node *root); static node *rotateRight(node *root); /*insert\u0026remove*/ static node *insert(node *root, int val, int \u0026size); static node *remove(node *root, int val, int \u0026size); /*query rank\u0026value*/ static int getLength(node *root); static int queryRank(node *root, int val);//快速查询val的排名 static int queryValue(node *root, int rank);//快速查询排名为rank的数 /*query pre\u0026next*/ static node *queryPre(node *root, int val); static node *queryNext(node *root, int val); static void inorder_print(node *root); static void destroy(node *root); public: /*public function*/ /*insert\u0026remove*/ void insert(int val) { head = insert(head, val, length); } void remove(int val) { head = remove(head, val, length); } int size() { return length; } bool isEmpty() { return length == 0; } /*query rank\u0026value*/ int queryRank(int val) { return queryRank(head, val); } int queryValue(int rank) { return queryValue(head, rank); } void inorder_print() { inorder_print(head); } /*begin\u0026end*/ iterator begin() { node *t = head; while (t-\u003elchild != nullptr) { t = t-\u003elchild; } return iterator(head, t); } iterator end() { return iterator(head, nullptr); } }; ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:2","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"3. 左旋和右旋 具体可以看我之前AVL树对左旋右旋的描述,这里只开发源码查看，文章链接 左旋： node *Treap::rotateLeft(node *root) { node *son = root-\u003erchild; root-\u003erchild = son-\u003elchild; son-\u003elchild = root; root-\u003eupdate();//记得先更新底下的情况 son-\u003eupdate(); return son; } 右旋： node *Treap::rotateRight(node *root) { node *son = root-\u003elchild; root-\u003elchild = son-\u003erchild; son-\u003erchild = root; root-\u003eupdate(); son-\u003eupdate(); return son; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:3","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"4. 插入和删除 插入： node *Treap::insert(node *root, int val, int \u0026size) { if (root == nullptr) { ++size; return new node(val); } if (root-\u003eval == val) { root-\u003ecnt++; size++; } else if (root-\u003eval \u003e val) { root-\u003elchild = insert(root-\u003elchild, val, size); //根据优先级判断是否右旋，因为只可能在左边增加长度，通过维持优先级的大根堆 if (root-\u003epriority \u003c root-\u003elchild-\u003epriority) root = rotateRight(root); } else if (root-\u003eval \u003c val) { root-\u003erchild = insert(root-\u003erchild, val, size); if (root-\u003epriority \u003c root-\u003erchild-\u003epriority) root = rotateLeft(root); } root-\u003eupdate();//注意更新长度信息 return root; } 删除： 删除这里还是详解一下： 找到要删除的目标节点后，我们根据让树旋转使得优先级较大的孩子替换掉父亲（目标节点）。然后继续追杀父亲结点，直到该结点被逼到叶子结点，删除即可。 node *Treap::remove(node *root, int val, int \u0026size) { if (root == nullptr)return nullptr;//没找到 if (root-\u003eval == val) { //含有多个相同值，直接操作cnt即可 if (root-\u003ecnt \u003e 1) { root-\u003ecnt--; --size; } //分为两类情况：叶子结点情况和非叶子结点情况 else if (root-\u003elchild != nullptr || root-\u003erchild != nullptr) { //只有左子树或者左子树优先级大于右子树情况 if (root-\u003erchild == nullptr || root-\u003elchild != nullptr \u0026\u0026 root-\u003elchild-\u003epriority \u003e root-\u003erchild-\u003epriority) { root = rotateRight(root);//右旋后继续追杀 root-\u003erchild = remove(root-\u003erchild, val, size); } else {//只有右子树或者右子树优先级大于左子树的情况 root = rotateLeft(root);//左旋后继续追杀 root-\u003elchild = remove(root-\u003elchild, val, size); } } else {//叶子结点情况，直接删除，然后把 delete root; root = nullptr; --size; } } else if (root-\u003eval \u003e val) { root-\u003elchild = remove(root-\u003elchild, val, size); } else if (root-\u003eval \u003c val) { root-\u003erchild = remove(root-\u003erchild, val, size); } if (root) root-\u003eupdate(); return root; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:4","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"5. 查询排名 原理：由于是排序二叉树，而且记录了树的结点个数，所以我们根据左边个数（小于当前结点的个数），如果我们的值小于当前结点的值，则小于它的个数范围肯定是在当前结点的左边，直接迁移到左孩子即可，如果大于当前结点，则当前结点的左边大小+它自身也是无法满足要查询的值的排名，rank加上该值后root继续右移。如果出现找到该元素的情况，则直接返回rank+左边长度。如果其中不存在，则最后得出的rank肯定是需要+1的。 inline int Treap::getLength(node *root) { if (root == nullptr) return 0; return root-\u003elength; } int Treap::queryRank(node *root, int val) {//相当于查询有多少个数小于等于val int rank = 0; while (root != nullptr) { if (root-\u003eval == val)return rank + getLength(root-\u003elchild) + root-\u003ecnt; else if (root-\u003eval \u003e val)root = root-\u003elchild; else rank += getLength(root-\u003elchild) + root-\u003ecnt, root = root-\u003erchild; } return rank + 1;//如果未找到，则在原来的基础上+1 } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:5","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"6. 按排名查询值 inline int Treap::getLength(node *root) { if (root == nullptr) return 0; return root-\u003elength; } int Treap::queryValue(node *root, int rank) {//相当于得到第k大的数：支持重复元素是最骚的！！！ while (root != nullptr) { if (getLength(root-\u003elchild) + root-\u003ecnt \u003e rank) { if (getLength(root-\u003elchild) + 1 \u003e rank) root = root-\u003elchild; else return root-\u003eval; } else if (getLength(root-\u003elchild) + root-\u003ecnt \u003c rank) { rank -= getLength(root-\u003elchild) + root-\u003ecnt; root = root-\u003erchild; } else {//rank与左子树的大小相等的情况 return root-\u003eval; } } return 0; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:6","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"7. 查询前驱后继 前驱： node *Treap::queryPre(node *root, int val) {//一样的道理：如果有左子树，就是左子树中最大的结点，如果没有则是最接近该结点的父节点(应在它的右侧 node *res = nullptr; while (root != nullptr) { if (root-\u003eval \u003c val)res = root, root = root-\u003erchild; else root = root-\u003erchild; } return res; } 后继： node *Treap::queryNext(node *root, int val) {//寻找后继 node *res = nullptr; while (root != nullptr) { if (root-\u003eval \u003e val)res = root, root = root-\u003elchild; else root = root-\u003erchild; } return res; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:7","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"8. 销毁Treap void Treap::destroy(node *root) { if (root == nullptr) return; destroy(root-\u003elchild); destroy(root-\u003erchild); delete root; root = nullptr; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:8","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"9. 迭代器的设计 public://内部类设计迭代器 class iterator { node *head; node *root; public: /*迭代器部分*/ iterator(node *head, node *root) : head(head), root(root) {} iterator \u0026operator++() { root = queryNext(head, root-\u003eval); return *this; } iterator operator++(int) { iterator t = *this; root = queryNext(head, root-\u003eval); return t; } iterator \u0026operator--() { root = queryPre(head, root-\u003eval); return *this; } iterator operator--(int) { iterator t = *this; root = queryPre(head, root-\u003eval); return t; } int operator*() { return root-\u003eval; } bool operator!=(const iterator \u0026t) { return t.root != root; } }; ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:2:9","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"完整源代码 ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:3:0","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"Treap.h // // Created by Alone on 2021/10/14. // #ifndef MY_TINY_STL_TREAP_H #define MY_TINY_STL_TREAP_H #include \u003ccstdio\u003e#include \u003ccstdlib\u003e#include \u003cctime\u003e struct node { int val, priority, length, cnt; node *lchild; node *rchild; node() : val(0), length(1), cnt(1), lchild(nullptr), rchild(nullptr) { srand((unsigned) time(NULL));//记得重新设定种子 priority = rand(); } node(int val) : val(val), length(1), cnt(1), lchild(nullptr), rchild(nullptr) { srand((unsigned) time(NULL)); priority = rand(); } void update() { length = cnt; if (lchild != nullptr)length += lchild-\u003elength; if (rchild != nullptr)length += rchild-\u003elength; } }; class Treap { node *head; int length; public: /*construct\u0026destruct*/ Treap() : head(nullptr), length(0) {} Treap(int val) : head(new node(val)), length(1) {} ~Treap(){ destroy(head); } public://内部类设计迭代器 class iterator { node *head; node *root; public: /*迭代器部分*/ iterator(node *head, node *root) : head(head), root(root) {} iterator \u0026operator++() { root = queryNext(head, root-\u003eval); return *this; } iterator operator++(int) { iterator t = *this; root = queryNext(head, root-\u003eval); return t; } iterator \u0026operator--() { root = queryPre(head, root-\u003eval); return *this; } iterator operator--(int) { iterator t = *this; root = queryPre(head, root-\u003eval); return t; } int operator*() { return root-\u003eval; } bool operator!=(const iterator \u0026t) { return t.root != root; } }; private: /*static function*/ /*rotate*/ static node *rotateLeft(node *root); static node *rotateRight(node *root); /*insert\u0026remove*/ static node *insert(node *root, int val, int \u0026size); static node *remove(node *root, int val, int \u0026size); /*query rank\u0026value*/ static int getLength(node *root); static int queryRank(node *root, int val);//快速查询val的排名 static int queryValue(node *root, int rank);//快速查询排名为rank的数 /*query pre\u0026next*/ static node *queryPre(node *root, int val); static node *queryNext(node *root, int val); static void inorder_print(node *root); static void destroy(node *root); public: /*public function*/ /*insert\u0026remove*/ void insert(int val) { head = insert(head, val, length); } void remove(int val) { head = remove(head, val, length); } int size() { return length; } bool isEmpty() { return length == 0; } /*query rank\u0026value*/ int queryRank(int val) { return queryRank(head, val); } int queryValue(int rank) { return queryValue(head, rank); } void inorder_print() { inorder_print(head); } /*begin\u0026end*/ iterator begin() { node *t = head; while (t-\u003elchild != nullptr) { t = t-\u003elchild; } return iterator(head, t); } iterator end() { return iterator(head, nullptr); } }; #endif //MY_TINY_STL_TREAP_H ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:3:1","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"Treap.cpp // // Created by Alone on 2021/10/14. // #include \"Treap.h\" node *Treap::rotateLeft(node *root) { node *son = root-\u003erchild; root-\u003erchild = son-\u003elchild; son-\u003elchild = root; root-\u003eupdate();//记得先更新底下的情况 son-\u003eupdate(); return son; } node *Treap::rotateRight(node *root) { node *son = root-\u003elchild; root-\u003elchild = son-\u003erchild; son-\u003erchild = root; root-\u003eupdate(); son-\u003eupdate(); return son; } node *Treap::insert(node *root, int val, int \u0026size) { if (root == nullptr) { ++size; return new node(val); } if (root-\u003eval == val) { root-\u003ecnt++; size++; } else if (root-\u003eval \u003e val) { root-\u003elchild = insert(root-\u003elchild, val, size); //根据优先级判断是否右旋，因为只可能在左边增加长度，通过维持优先级的大根堆 if (root-\u003epriority \u003c root-\u003elchild-\u003epriority) root = rotateRight(root); } else if (root-\u003eval \u003c val) { root-\u003erchild = insert(root-\u003erchild, val, size); if (root-\u003epriority \u003c root-\u003erchild-\u003epriority) root = rotateLeft(root); } root-\u003eupdate(); return root; } node *Treap::remove(node *root, int val, int \u0026size) { if (root == nullptr)return nullptr;//没找到 if (root-\u003eval == val) { //含有多个相同值，直接操作cnt即可 if (root-\u003ecnt \u003e 1) { root-\u003ecnt--; --size; } //分为两类情况：叶子结点情况和非叶子结点情况 else if (root-\u003elchild != nullptr || root-\u003erchild != nullptr) { //只有左子树或者左子树优先级大于右子树情况 if (root-\u003erchild == nullptr || root-\u003elchild != nullptr \u0026\u0026 root-\u003elchild-\u003epriority \u003e root-\u003erchild-\u003epriority) { root = rotateRight(root);//右旋后继续追杀 root-\u003erchild = remove(root-\u003erchild, val, size); } else {//只有右子树或者右子树优先级大于左子树的情况 root = rotateLeft(root);//左旋后继续追杀 root-\u003elchild = remove(root-\u003elchild, val, size); } } else {//叶子结点情况，直接删除，然后把 delete root; root = nullptr; --size; } } else if (root-\u003eval \u003e val) { root-\u003elchild = remove(root-\u003elchild, val, size); } else if (root-\u003eval \u003c val) { root-\u003erchild = remove(root-\u003erchild, val, size); } if (root) root-\u003eupdate(); return root; } int Treap::queryRank(node *root, int val) {//相当于查询有多少个数小于等于val int rank = 0; while (root != nullptr) { if (root-\u003eval == val)return rank + getLength(root-\u003elchild) + root-\u003ecnt; else if (root-\u003eval \u003e val)root = root-\u003elchild; else rank += getLength(root-\u003elchild) + root-\u003ecnt, root = root-\u003erchild; } return rank + 1;//如果未找到，则在原来的基础上+1 } int Treap::queryValue(node *root, int rank) {//相当于得到第k大的数：支持重复元素是最骚的！！！ while (root != nullptr) { if (getLength(root-\u003elchild) + root-\u003ecnt \u003e rank) { if (getLength(root-\u003elchild) + 1 \u003e rank) root = root-\u003elchild; else return root-\u003eval; } else if (getLength(root-\u003elchild) + root-\u003ecnt \u003c rank) { rank -= getLength(root-\u003elchild) + root-\u003ecnt; root = root-\u003erchild; } else {//rank与左子树的大小相等的情况 return root-\u003eval; } } return 0; } node *Treap::queryPre(node *root, int val) {//一样的道理：如果有左子树，就是左子树中最大的结点，如果没有则是最接近该结点的父节点(应在它的右侧 node *res = nullptr; while (root != nullptr) { if (root-\u003eval \u003c val)res = root, root = root-\u003erchild; else root = root-\u003erchild; } return res; } node *Treap::queryNext(node *root, int val) {//寻找后继 node *res = nullptr; while (root != nullptr) { if (root-\u003eval \u003e val)res = root, root = root-\u003elchild; else root = root-\u003erchild; } return res; } void Treap::inorder_print(node *root) { if (root == nullptr) return; inorder_print(root-\u003elchild); printf(\"%d \", root-\u003eval); inorder_print(root-\u003erchild); } void Treap::destroy(node *root) { if (root == nullptr) return; destroy(root-\u003elchild); destroy(root-\u003erchild); delete root; root = nullptr; } inline int Treap::getLength(node *root) { if (root == nullptr) return 0; return root-\u003elength; } ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:3:2","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"测试 ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:4:0","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["手写数据结构"],"content":"AVL vs Treap vs 普通BST 测试数据量：1000w 直接来结论： AVL极端情况下(插入的数据有序)，完爆所有平衡树。 Treap随机情况的插入表示不错！大部分时间可以和AVL持平。 BST别想了，这个1000w数据只有随机情况能用几分钟过，如果极端情况直接程序运行出错！ 整体小结： 第一轮：insert操作 随机情况Treap(看运气，毕竟随机事件) \u003e=\u003c AVL\u003e\u003e BST 极端情况AVL \u003e\u003e Treap， BST直接暴毙 第二轮: remove操作–Treap被AVL吊打，BST就别提了 下面就没得继续展开的比较了，总结就是Treap相对好写一点，效率高不高看运气。 还有就是Treap在直接处理第K大的值具有优势，以及按排位查找具有优势。 优势：由于treap每个结点都含有它整个结点个数的记录，所以可以很快的查出值的排名和排名的值。 ","date":"2022-02-16","objectID":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/:4:1","tags":["来自上帝的骰子---Treap(树堆)详解"],"title":"来自上帝的骰子---Treap(树堆)详解","uri":"/posts/%E6%9D%A5%E8%87%AA%E4%B8%8A%E5%B8%9D%E7%9A%84%E9%AA%B0%E5%AD%90-treap%E6%A0%91%E5%A0%86%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——二段性相关(二分)"],"content":"leetcode情人节特辑——寻找单身狗","date":"2022-02-14","objectID":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/","tags":["leetcode情人节特辑——寻找单身狗"],"title":"leetcode情人节特辑——寻找单身狗","uri":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/"},{"categories":["算法——二段性相关(二分)"],"content":"题目 题目链接 ","date":"2022-02-14","objectID":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/:1:0","tags":["leetcode情人节特辑——寻找单身狗"],"title":"leetcode情人节特辑——寻找单身狗","uri":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/"},{"categories":["算法——二段性相关(二分)"],"content":"题目详解 这题本应是简单题，就是简单的异或规律，但是题目要求使用 O(logn) 时间复杂度， O(1) 空间复杂度，而如果直接异或，只会是 O(n) 的时间复杂度。 那么该如何去做呢？ 这题有二段性，什么叫二段性呢，就是能有一个分界点把特性一分为二。比如此题由于数据是有序的，所以数量为两个的元素会挨在一起，而且在 单身狗 左边连续的元素下标会有以下规律：两个相邻的相同元素中，第一个元素下标会是偶数，第二个是奇数。右边的连续元素下标会有以下规律：两个相邻的相同元素中，第一个元素下标是奇数，第二个是偶数。 根据以上二段性可很快构建出二分版本的代码！ ","date":"2022-02-14","objectID":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/:2:0","tags":["leetcode情人节特辑——寻找单身狗"],"title":"leetcode情人节特辑——寻找单身狗","uri":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/"},{"categories":["算法——二段性相关(二分)"],"content":"解题代码 未简化二分版本 class Solution { public: int singleNonDuplicate(vector\u003cint\u003e\u0026 nums) { int l=0,r=nums.size()-1; int maxr = r; int mid; while(l\u003cr){ mid = (l+r)/2; if(mid\u003cmaxr\u0026\u0026nums[mid+1]==nums[mid]){ if(mid%2==0){ l = mid+2; }else{ r = mid; } }else if(mid\u003e0\u0026\u0026nums[mid-1]==nums[mid]){ if((mid-1)%2==0){ l = mid+1; }else{ r = mid; } }else{ return nums[mid]; } } return nums[l]; } }; 简化的二分版本 class Solution { public: int singleNonDuplicate(vector\u003cint\u003e\u0026 nums) { int low = 0, high = nums.size() - 1; while (low \u003c high) { int mid = (high - low) / 2 + low; if (nums[mid] == nums[mid ^ 1]) { low = mid + 1; } else { high = mid; } } return nums[low]; } }; ","date":"2022-02-14","objectID":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/:3:0","tags":["leetcode情人节特辑——寻找单身狗"],"title":"leetcode情人节特辑——寻找单身狗","uri":"/posts/leetcode%E6%83%85%E4%BA%BA%E8%8A%82%E7%89%B9%E8%BE%91%E5%AF%BB%E6%89%BE%E5%8D%95%E8%BA%AB%E7%8B%97/"},{"categories":["算法——二段性相关(二分)"],"content":"牛客-wyh的物品——通过验证得出二分的搜索区间","date":"2022-02-14","objectID":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/","tags":["牛客-wyh的物品——通过验证得出二分的搜索区间"],"title":"牛客-wyh的物品——通过验证得出二分的搜索区间","uri":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/"},{"categories":["算法——二段性相关(二分)"],"content":"题目 题目链接 ","date":"2022-02-14","objectID":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/:1:0","tags":["牛客-wyh的物品——通过验证得出二分的搜索区间"],"title":"牛客-wyh的物品——通过验证得出二分的搜索区间","uri":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/"},{"categories":["算法——二段性相关(二分)"],"content":"题目详解 以下为手写详解 下面总结这个做题步骤： 二分搜索可能的最大单位价值。 根据这个值得到每个数的单位价值情况s，根据s的值排序，得到前k大的s，相加得出我们枚举的这个最大单位价值是大了还是小了，方便后续二分！ 最后按保留两位小数，输出即可。 ","date":"2022-02-14","objectID":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/:2:0","tags":["牛客-wyh的物品——通过验证得出二分的搜索区间"],"title":"牛客-wyh的物品——通过验证得出二分的搜索区间","uri":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/"},{"categories":["算法——二段性相关(二分)"],"content":"解题代码 #include\u003cbits/stdc++.h\u003eusing namespace std; const int maxn = 1e5+5; int w[maxn],v[maxn]; double s[maxn]; //TODO 存下用于比对实际单位价值的信息 int n,k; using namespace std; int solve(double mid) { double ss = 0; for(int i = 0;i\u003cn;i++) { s[i] = 1.0*v[i] - w[i]*mid; } sort(s,s+n,greater\u003cdouble\u003e());//TODO 从大到小排序，因为要取前k个最大值 for(int i = 0;i\u003ck;i++) ss += s[i]; //TODO 得到前k个最大的s信息的和， // 如果大于0，则说明取小了，小于0则取大了，等于0则正好满足！ if(ss\u003e0) return 1; else if(ss==0)return -1; else return 0; } int main() { int t; cin\u003e\u003et; while(t--) { scanf(\"%d %d\",\u0026n,\u0026k); double low = 0,high = 100000,mid; for(int i = 0;i\u003cn;i++) { scanf(\"%d %d\",\u0026w[i],\u0026v[i]); } //TODO 二分搜索，由于double类型的二分，故取一个小数形式进行比对 // 注意double类型的二分，千万别+1或者-1！这个跨度太大了，因为我们搜的是个分数！ while(high - low \u003e0.00001) { mid = (low + high)/2.0; int flag = solve(mid); if(flag==1) low = mid;//TODO 收缩左边界 else if(flag == -1)break;//TODO 枚举得到答案 else high = mid;//TODO 收缩右边界 } printf(\"%.2lf\\n\",mid); } return 0; } ","date":"2022-02-14","objectID":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/:3:0","tags":["牛客-wyh的物品——通过验证得出二分的搜索区间"],"title":"牛客-wyh的物品——通过验证得出二分的搜索区间","uri":"/posts/%E7%89%9B%E5%AE%A2-wyh%E7%9A%84%E7%89%A9%E5%93%81%E9%80%9A%E8%BF%87%E9%AA%8C%E8%AF%81%E5%BE%97%E5%87%BA%E4%BA%8C%E5%88%86%E7%9A%84%E6%90%9C%E7%B4%A2%E5%8C%BA%E9%97%B4/"},{"categories":["C++实战"],"content":"Hugo博客图形化写作工具","date":"2022-02-12","objectID":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/","tags":["Hugo博客图形化写作工具"],"title":"Hugo博客图形化写作工具","uri":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"软件使用 视频教程 ","date":"2022-02-12","objectID":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/:1:0","tags":["Hugo博客图形化写作工具"],"title":"Hugo博客图形化写作工具","uri":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"项目介绍 项目所在地 温馨提示：如果本地还未搭建 hugo 博客，可以使用我的另一个 hugo 博客自动搭建工具 QtRun： 介绍：一个用纯 C++ 写的命令行工具。 主要作用：根据提供的 hugo 博客本地地址进行命令行式的自动化写作，会把每一篇文章的图片、分类、标题等内容自动化完成。 构建方式：确保生成 exe 文件的目录下含有以下文件，且确保编译器支持 C++17。 BlogPath.txt #提供本地hugo博客路径 categories.txt #提供可供选择的分类(没有也没关系) initImg.txt #提供可选择的图片 mob.txt #提供用于生成的模板 ed_Path.txt #提供打开的编辑器路径(没有也没关系) 使用方式：可查看源码得到更详尽的解答 QtRun [title name] [category name] Qtrun [-op] QtRunBlog： 介绍：使用 Qt+cmake 搭建的图形化 hugo 自动化工具，写作的相关部分都是调用的 QtRun ，所以运行时 QtRun 的配置文件和 QtRun 都必须在它的 exe 目录之下。而其他其他部分调用的 git 命令行，所以需要本地有 git 工具。 主要作用：提供图形化的 hugo 写作体验。 构建方式：本地需要 Qt6 环境，选择本项目目录便可完成构建。 ","date":"2022-02-12","objectID":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/:2:0","tags":["Hugo博客图形化写作工具"],"title":"Hugo博客图形化写作工具","uri":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"QtRun C++ 源代码 实现了自动化命令行写作，QtRunBlog图形化界面调用的就是它的命令行 // // Created by Alone on 2022-1-24. // //TODO aaaaaaa得出感悟：1.数据较为复杂的情况下尽量不要使用全局变量 2.在构造函数初始化的时候千万不要直接new空间给它，记得随时随地nullptr #include \u003cfstream\u003e#include \u003ciostream\u003e#include \u003cstring\u003e#include \u003csys/stat.h\u003e#include \u003cunistd.h\u003e#include \u003cwindows.h\u003e#include \u003cvector\u003e#include \u003cunordered_map\u003e#include \u003cctime\u003e#include \u003cfilesystem\u003e#include \u003crandom\u003e #define IMGS_PATH \"./initImg.txt\" #define MOB_PATH \"./mob.txt\" #define CATEGORIES_PATH \"./categories.txt\" #define BLOG_SRC \"./BlogPath.txt\" std::filesystem::path POSTS_PATH;//用于获取post_path using namespace std; //TODO 建立枚举映射 enum class SHOW_ARGS : int { EMPTY, CATEGORIES, IMG, BLOG_PATH }; //TODO 命令行参数的枚举映射 unordered_map\u003cstring, SHOW_ARGS\u003e MAP{ {\"-sc\", SHOW_ARGS::CATEGORIES}, {\"-si\", SHOW_ARGS::IMG}, {\"-sp\", SHOW_ARGS::BLOG_PATH} }; //TODO 封装文件读取类 class FileReader { stringstream in_buf; ifstream reader; public: FileReader() = default; FileReader(const FileReader \u0026) = delete; FileReader(FileReader \u0026\u0026) = delete; ~FileReader() { if (reader.is_open()) reader.close(); } void open(const string \u0026path) { reader.open(path); if (!reader.is_open()) { perror(\"reader open failed\"); exit(1); } in_buf \u003c\u003c reader.rdbuf(); } bool readAll(string \u0026dst) { if (in_buf.good()) dst = in_buf.str(); else return false; return true; } bool readline(string \u0026dst) { if (in_buf.good()) getline(in_buf, dst); else return false; return true; } }; //TODO 封装文件写入类 class FileWriter { char *out_buf; ofstream writer; size_t cur_buf_size; size_t max_buf_size; private: void _write() { //缓冲区写满，写入文件中 writer.write(out_buf, max_buf_size); cur_buf_size = 0; } public: FileWriter() : cur_buf_size(0), max_buf_size(512), out_buf(nullptr) {}; FileWriter(const FileReader \u0026) = delete; FileWriter(FileReader \u0026\u0026) = delete; FileWriter(const string \u0026path, ios::openmode mode = ios::out) { writer.open(path, mode); if (!writer.is_open()) { perror(\"writer open failed\"); exit(1); } cur_buf_size = 0; max_buf_size = 512; out_buf = new char[max_buf_size + 5]; } ~FileWriter() { if (cur_buf_size \u003e 0) { writer.write(out_buf, cur_buf_size); cur_buf_size = 0; } delete[] out_buf; out_buf = nullptr; writer.flush(); writer.close(); } static bool exist(const string \u0026path) { return (access(path.c_str(), F_OK) != -1); } void open(const string \u0026path, ios::openmode mode = ios::out) { writer.open(path, mode); if (!writer.is_open()) { perror(\"writer open failed\"); exit(1); } out_buf = new char[max_buf_size + 5]; } void write(const string \u0026src) {//TODO 缓冲机制的重要组成 if (writer.is_open()) {//只有在open文件后才能写入 if (src.empty()) return; if (cur_buf_size == max_buf_size) _write(); size_t psize = src.size() + cur_buf_size;//如果全盘写入缓冲区后，缓冲区需要的大小 int startp = 0, maxLen; while (psize \u003e max_buf_size) { //当这次写入缓冲区的数据量大于缓冲区的大小，则进行不断写满更新操作 maxLen = max_buf_size - cur_buf_size; copy(src.begin() + startp, src.begin() + startp + maxLen, out_buf + cur_buf_size);//copy到满状态，再来一次write _write(); startp += maxLen; psize -= max_buf_size; } //如果写入数据不超出缓冲区大小，则直接写入 copy(src.begin() + startp, src.end(), out_buf + cur_buf_size); cur_buf_size += src.size() - startp; } } FileWriter \u0026append(const string \u0026src) {//TODO 和write没区别，只是支持链式调用 write(src); return *this; } }; //TODO 整个项目需要操作的变量（很不推荐用全局变量，我就是因为这玩意就导致了bug） FileReader readImg, readText, readCategories;//用于文件io的变量 FileWriter appendCategories, fileWriter; vector\u003cstring\u003e imgs, categories; //用于存下磁盘到内存的数据，根据名字判断存的啥 time_t now = time(NULL); //TODO 打开typora软件 void open_exe_from_path(const char *path) { WinExec(path, SW_SHOWNORMAL); cout \u003c\u003c \"open your custom editor successfully!\" \u003c\u003c endl; } //TODO 打印内容 void show(vector\u003cstring\u003e \u0026src) { for (int i = 0; i \u003c src.size(); i++) { if (!src[i].empty()) printf(\"%d: %s\\n\", i, src[i].c_str()); } } //TODO 替换string的内容 void to_replace(string \u0026s, const string \u0026target, const string \u0026replacement) { int i = 0; int find_ret; int tar_len = target.size(); while ((find_ret = s.find(target, i)) != -1) { s.replace(find_ret, tar_len, replacement); i = find_ret; } } //TODO 打印出错的信息，并退出程序 void exit_print(","date":"2022-02-12","objectID":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/:3:0","tags":["Hugo博客图形化写作工具"],"title":"Hugo博客图形化写作工具","uri":"/posts/hugo%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/"},{"categories":["算法——搜索类问题"],"content":"leetcode每日一题——1020飞地的数量","date":"2022-02-12","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/","tags":["leetcode每日一题——1020飞地的数量"],"title":"并查集/dfs解决——leetcode每日一题——1020飞地的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——搜索类问题"],"content":"题目描述 题目链接 ","date":"2022-02-12","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/:1:0","tags":["leetcode每日一题——1020飞地的数量"],"title":"并查集/dfs解决——leetcode每日一题——1020飞地的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——搜索类问题"],"content":"题目解析 一、以边界值为对象进行搜索解决 一开始很快就想到用比较暴力的直接dfs深搜，然后就超时了。 注意此题由于以 1 是否能延申到整个边界以外来判断是否为有效的 1 所以我们需要取巧，应该以所有边界的 1 为对象先把所有能超出的 1 搜出来，然后剩余的 1 就是答案了。 二、并查集合并+是否接壤边界属性更新 创建一个并查集，用一维数组存下所有二维数组的元素，同时再增加一个一维数组用于判断是否边界接壤，每次 merge 操作的时候判断需要同时执行合并操作和是否接壤的更新。 先利用并查集 merge 所有的 1，然后再挨个判断是否接壤即可。 ","date":"2022-02-12","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/:2:0","tags":["leetcode每日一题——1020飞地的数量"],"title":"并查集/dfs解决——leetcode每日一题——1020飞地的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——搜索类问题"],"content":"解题代码 dfs方法： class Solution { public: vector\u003cvector\u003cint\u003e\u003e dirs = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}; int numEnclaves(vector\u003cvector\u003cint\u003e\u003e\u0026 grid) { this-\u003em = grid.size(); this-\u003en = grid[0].size(); this-\u003evisited = vector\u003cvector\u003cbool\u003e\u003e(m, vector\u003cbool\u003e(n, false)); for (int i = 0; i \u003c m; i++) { dfs(grid, i, 0); dfs(grid, i, n - 1); } for (int j = 1; j \u003c n - 1; j++) { dfs(grid, 0, j); dfs(grid, m - 1, j); } int enclaves = 0; for (int i = 1; i \u003c m - 1; i++) { for (int j = 1; j \u003c n - 1; j++) { if (grid[i][j] == 1 \u0026\u0026 !visited[i][j]) { enclaves++; } } } return enclaves; } void dfs(const vector\u003cvector\u003cint\u003e\u003e \u0026 grid, int row, int col) { if (row \u003c 0 || row \u003e= m || col \u003c 0 || col \u003e= n || grid[row][col] == 0 || visited[row][col]) { return; } visited[row][col] = true; for (auto \u0026 dir : dirs) { dfs(grid, row + dir[0], col + dir[1]); } } private: int m, n; vector\u003cvector\u003cbool\u003e\u003e visited; }; 并查集方法： //这个并查集写的好 class UnionFind { public: UnionFind(const vector\u003cvector\u003cint\u003e\u003e \u0026 grid) { int m = grid.size(), n = grid[0].size(); this-\u003eparent = vector\u003cint\u003e(m * n); //存储并查集的联通关系 this-\u003eonEdge = vector\u003cbool\u003e(m * n, false);//查找是否有在边界元素的关键所在 this-\u003erank = vector\u003cint\u003e(m * n); for (int i = 0; i \u003c m; i++) { //根据传过来的二维数组更新并查集，同时更新onEdge边界元素为true for (int j = 0; j \u003c n; j++) { if (grid[i][j] == 1) { int index = i * n + j; parent[index] = index; if (i == 0 || i == m - 1 || j == 0 || j == n - 1) { onEdge[index] = true; } } } } } int find(int i) { if (parent[i] != i) { parent[i] = find(parent[i]); } return parent[i]; } void uni(int x, int y) { int rootx = find(x); int rooty = find(y); if (rootx != rooty) { if (rank[rootx] \u003e rank[rooty]) {//这里时按秩优化处理 parent[rooty] = rootx; onEdge[rootx] = onEdge[rootx] | onEdge[rooty];//每次合并元素的时候同时把这一堆是否与边界接壤的关系更新 } else if (rank[rootx] \u003c rank[rooty]) { parent[rootx] = rooty; onEdge[rooty] = onEdge[rooty] | onEdge[rootx]; } else { parent[rooty] = rootx; onEdge[rootx] = onEdge[rootx] | onEdge[rooty]; rank[rootx]++; } } } bool isOnEdge(int i) { return onEdge[find(i)]; } private: vector\u003cint\u003e parent; //并查集的必备 vector\u003cbool\u003e onEdge; //判断是否接壤边界 vector\u003cint\u003e rank; //按秩 }; class Solution { public: int numEnclaves(vector\u003cvector\u003cint\u003e\u003e\u0026 grid) { int m = grid.size(), n = grid[0].size(); UnionFind uf(grid); //先把所有的1连接起来，然后再判断是否接壤边界即可 //由于循环是从上往下，从左往右，故左和上方向不需要考虑 for (int i = 0; i \u003c m; i++) { for (int j = 0; j \u003c n; j++) { if (grid[i][j] == 1) { int index = i * n + j; if (j + 1 \u003c n \u0026\u0026 grid[i][j + 1] == 1) { uf.uni(index, index + 1); } if (i + 1 \u003c m \u0026\u0026 grid[i + 1][j] == 1) { uf.uni(index, index + n); } } } } //判断这个1是否和边界接壤 int enclaves = 0; for (int i = 1; i \u003c m - 1; i++) { for (int j = 1; j \u003c n - 1; j++) { if (grid[i][j] == 1 \u0026\u0026 !uf.isOnEdge(i * n + j)) { enclaves++; } } } return enclaves; } }; ","date":"2022-02-12","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/:3:0","tags":["leetcode每日一题——1020飞地的数量"],"title":"并查集/dfs解决——leetcode每日一题——1020飞地的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981020%E9%A3%9E%E5%9C%B0%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["C++实战"],"content":"自动化搭建博客工具","date":"2022-02-12","objectID":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/","tags":["自动化搭建博客工具"],"title":"自动化搭建博客工具","uri":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"软件使用 使用前请下载好git工具 视频教程 ","date":"2022-02-12","objectID":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/:1:0","tags":["自动化搭建博客工具"],"title":"自动化搭建博客工具","uri":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"项目介绍 项目所在地 温馨提示：本地搭建完网站框架后，每次新建文章的写作体验较差，可以看看我的另一个 hugo 博客自动化写作工具 src目录：存放图形化项目的源代码，使用 Qt6 可直接启动 QHugoInit 项目。 bin目录：存放项目的可执行二进制文件。 exec_code.bat：用于写入代码执行脚本 hugo.exe：构建博客的基石 QHugoInit.exe：软件启动程序 log.txt：整个程序执行的日志 ","date":"2022-02-12","objectID":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/:2:0","tags":["自动化搭建博客工具"],"title":"自动化搭建博客工具","uri":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/"},{"categories":["C++实战"],"content":"项目关键源码 通过多线程防止下载步骤把画面给阻塞 //TODO 简单封装一个用于多线程通信的类 class run_thread:public QThread{ public: run_thread() = delete; run_thread(const std::function\u003cvoid()\u003e\u0026Runnable,QObject* parent = nullptr):m_task(nullptr){ m_task = Runnable; } void run()override{ if(m_task!=nullptr); m_task(); } private: std::function\u003cvoid()\u003em_task; }; 通过文件io控制命令行的代码执行来执行对应的每一步 void MainWindow::on_right_Btn_clicked() { auto path = ui-\u003einput-\u003etext(); if(!QDir(path).exists()||path.isEmpty()){ QMessageBox::warning(nullptr,\"提示\",\"文件夹路径不存在\"); return; } ui-\u003ew2-\u003esetVisible(false); ui-\u003ew4-\u003esetVisible(false); ui-\u003ew3-\u003esetVisible(true); ui-\u003el1-\u003esetText(\"一切即将准备就绪\"); ui-\u003el2-\u003esetText(\"正在初始化您的hugo网站\"); ui-\u003el3-\u003esetOpenExternalLinks(true); ui-\u003el3-\u003esetText(R\"(\u003chtml\u003e \u003cstyle\u003e a { color:#3281b8; } \u003c/style\u003e\u003chead/\u003e\u003cbody\u003e\u003cp\u003e这可能会耗费几分钟，请不要强制关闭应用程序\u003cbr/\u003e\u003cbr/\u003e闲得无聊？\u003ca href=\"https://github.com/ACking-you/AutoHugoSetup\"\u003e给个star\u003c/a\u003e\u003c/p\u003e\u003c/body\u003e\u003c/html\u003e)\"); QProcess qp; std::ofstream writer; m_Path = path; auto std_str = m_Path.toStdString(); //first step writer.open(\"./exec_code.bat\"); if(!writer.is_open()){ QMessageBox::warning(nullptr,\"提示\",\"第一次写入文件打开失败\"); writer.close(); QCoreApplication::exit(1); } ui-\u003einfo_text-\u003esetText(\"正在初始化 hugo 博客\"); std::replace(std_str.begin(),std_str.end(),'/','\\\\'); //这里注意：cmd32命令只支持反斜杠！ writer\u003c\u003cR\"(chcp 65001)\"\u003c\u003c'\\n'; //设置编码为utf-8 writer\u003c\u003cR\"(copy .\\hugo.exe )\"\u003c\u003cstd_str\u003c\u003c'\\n'; //copy一份hugo.exe到目标目录下 writer\u003c\u003c\"cd /d\"\u003c\u003cstd_str\u003c\u003c'\\n'; //切换到创建目录，这里/d代表直接一步到位的切换目录 writer\u003c\u003cR\"(.\\hugo new site myBlog)\"\u003c\u003c'\\n'; //初始化hugo命令 writer.close(); qp.startCommand(R\"(.\\exec_code.bat)\"); if(qp.waitForFinished()){ QString str = qp.readAll(); m_logWriter\u003c\u003c\"-----1st step-----\\n\\r\"\u003c\u003cstr.toStdString()\u003c\u003c'\\n'; ui-\u003einfo_text-\u003esetText(\" hugo 博客初始化完成\"); } //second step：执行此步之前先判断git是否可用，此步执行时间最久，不要让它卡死主线程，故需要用到多线程技术 qp.startCommand(\"git\"); if(!qp.waitForFinished()){ QMessageBox::warning(nullptr,\"提示\",\"未安装git工具或未设置到环境变量\"); QCoreApplication::exit(1); } writer.open(\"./exec_code.bat\"); if(!writer.is_open()){ QMessageBox::warning(nullptr,\"提示\",\"第二次写入文件打开失败\"); writer.close(); QCoreApplication::exit(1); } ui-\u003einfo_text-\u003esetText(\"正在下载 FeelIt 主题\"); std::filesystem::path blog_path = std_str; blog_path /= \"myBlog\"; m_Path = blog_path.string().c_str(); writer\u003c\u003c\"cd /d\"\u003c\u003cblog_path\u003c\u003c\"\\\\themes\"\u003c\u003c'\\n'; //cd到themes文件夹目录下 writer\u003c\u003c\"git clone https://gitee.com/acking-you/FeelIt.git\"\u003c\u003c'\\n';//开始通过git下载主题包 writer.close(); QThread* sub_thread = new run_thread([\u0026]{ //最耗时间的工作别去干扰主线程的正常运行，否则主线程可能看起来会陷入瘫痪 QProcess tqp; //信号槽机制，主线程等待子线程完成任务发送信号后再执行最后的步骤，因为这个过程肯定是要同步进行，故需要信号槽来等待 connect(\u0026tqp,\u0026QProcess::finished,[\u0026](int exitCode, QProcess::ExitStatus exitStatus){ //step third std::ofstream writer; QProcess qp; writer.open(\"./exec_code.bat\"); if(!writer.is_open()){ QMessageBox::warning(nullptr,\"提示\",\"第三次写入文件打开失败\"); writer.close(); QCoreApplication::exit(1); } ui-\u003einfo_text-\u003esetText(\"配置本地主题中...\"); writer\u003c\u003c\"cd /d\"\u003c\u003cm_Path.toStdString()\u003c\u003c'\\n'; //cd到blog_path writer\u003c\u003cR\"(del .\\config.toml)\"\u003c\u003c'\\n'; //删除原本的config文件 writer\u003c\u003cR\"(move .\\themes\\FeelIt\\config.toml .\\)\"\u003c\u003c'\\n'; //将我的配置文件放到顶级目录去 writer\u003c\u003cR\"(move .\\themes\\FeelIt\\exampleSite\\static\\* .\\static\\)\"\u003c\u003c'\\n';//移动static图片资源 writer\u003c\u003cR\"(move ..\\hugo.exe .\\)\"\u003c\u003c'\\n';//将之前的hugo.exe移动到真正的博客目录下 writer\u003c\u003cR\"(mkdir .\\content\\posts)\"\u003c\u003c'\\n'; //创建用于写文章的目录 writer.close(); qp.startCommand(R\"(.\\exec_code.bat)\"); if(qp.waitForFinished(120000)){ ui-\u003einfo_text-\u003esetText(\"配置完成 \"); ending(); m_logWriter\u003c\u003c\"-----3rd step-----\\n\\r\"\u003c\u003cqp.readAll().toStdString(); } }); tqp.startCommand(R\"(.\\exec_code.bat)\"); if(!tqp.waitForFinished(120000)){ QMessageBox::warning(nullptr,\"提示\",\"下载主题响应超时\"); QCoreApplication::exit(1); }else{ m_logWriter\u003c\u003c\"-----2nd step-----\\n\\r\"\u003c\u003ctqp.readAll().toStdString(); } }); sub_thread-\u003estart(); } ","date":"2022-02-12","objectID":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/:3:0","tags":["自动化搭建博客工具"],"title":"自动化搭建博客工具","uri":"/posts/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7/"},{"categories":["JavaWeb笔记"],"content":"数据库基础","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库基础 数据库是学习JavaWeb的一个前置，只有了解了数据库的操作和使用，我们才能更好地组织和管理网站应用产生的数据。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:0:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"什么是数据库 数据库是数据管理的有效技术，是由一批数据构成的有序集合，这些数据被存放在结构化的数据表里。数据表之间相互关联，反映客观事物间的本质联系。数据库能有效地帮助一个组织或企业科学地管理各类信息资源。简而言之，我们的数据可以交给数据库来帮助我们进行管理，同时数据库能够为我们提供高效的访问性能。 在JavaSE学习阶段中，我们学习了如何使用文件I/O来将数据保存到本地，这样就可以将一个数据持久地存储在本地，即使程序重新打开，我们也能加载回上一次的数据，但是当我们的数据变得非常多的时候，这样的方式就显得不太方便了。同时我们如果需要查找众多数据的中的某一个，就只能加载到内存再进行查找，这样显然是很难受的！ 而数据库就是专门做这事的，我们可以快速查找想要的数据，便捷地插入、修改和删除数据，并且数据库不仅能做这些事，还能提供更多便于管理数据和操作数据的功能！ ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:1:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"常见的数据库 常见的数据库有很多种，包括但不限于： MySQL - 免费，用的最多的，开源数据库，适用于中小型 Microsoft SQL Server - 收钱的，但是提供技术支持，适用于Windows Server Oracle - 收钱的，大型数据库系统 而我们要学习的是MySQL数据，其实无论学习哪种数据库，SQL语句大部分都是通用的，只有少许语法是不通用的，因此我们只需要学习一种数据库其他的也就差不多都会了。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:1:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据模型 数据模型与现实世界中的模型一样，是对现实世界数据特征的一种抽象。实际上，我们之前学习的类就是对现实世界数据的一种抽象，比如一个学生的特征包括姓名，年龄，年级，学号，专业等，这些特征也称为实体的一种属性，属性具有以下特点： 属性不可再分 一个实体的属性可以有很多个 用于唯一区分不同实体的的属性，称为Key，比如每个同学的学号都是不一样的 属性取值可以有一定的约束，比如性别只能是男或是女 实体或是属性之间可以具有一定的联系，比如一个老师可以教很多个学生，而学生相对于老师就是被教授的关系；又比如每个同学都有一个学号与其唯一对应，因此学号和学生之间也有一种联系。而像一个老师教多个学生的联系就是一种一对多的联系（1:n），而学号唯一对应，就是一种一对一的联系（1:1）；每一个老师不仅可以教多个学生，每一个学生也可以有多个教师，这就是一种多对多的联系（n:m） MySQL就是一种关系型数据库，通过使用关系型数据库，我们就可以很好地存储这样带有一定联系的数据。 通过构建一个ER图，我们就能很好地理清不同数据模型之间的关系和特点。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:1:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库的创建 既然了解了属性和联系，那么我们就来尝试创建一个数据库，并在数据库中添加用于存放数据的表，每一张表都代表一种实体的数据。首先我们要明确，我们需要创建什么样子的表： 学生表：用于存放所有学生的数据，学生（学号，姓名，性别） 教师表：用于存放所有教师的数据，教师（教师号，姓名） 授课表：用于存放教师与学生的授课信息，授课（学号，教师号） 其中，标注下划线的属性，作为Key，用于区别于其他实体数据的唯一标记。 为了理解起来更加轻松，我们从图形界面操作再讲到SQL语句，请不要着急。我们现在通过Navicat或idea自带的数据库客户端来创建一个数据库和上述三个表。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:2:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库的规范化 要去设计存放一个实体的表，我们就需要了解数据库的关系规范化，尽可能减少“不好”的关系存在，如何设计一个优良的关系模型是最关键的内容！简而言之，我们要学习一下每一个表该如何去设计。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:3:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"第一范式（1NF） 第一范式是指数据库的每一列都是不可分割的基本数据项，而下面这样的就存在可分割的情况： 学生（姓名，电话号码） 电话号码实际上包括了家用座机电话和移动电话，因此它可以被拆分为： 学生（姓名，座机号码，手机号码） 满足第一范式是关系型数据库最基本的要求！ ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:3:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"第二范式（2NF） 第二范式要求表中必须存在主键，且其他的属性必须完全依赖于主键，比如： 学生（学号，姓名，性别） 学号是每个学生的唯一标识，每个学生都有着不同的学号，因此此表中存在一个主键，并且每个学生的所有属性都依赖于学号，学号发生改变就代表学生发生改变，姓名和性别都会因此发生改变，所有此表满足第二范式。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:3:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"第三范式（3NF） 在满足第二范式的情况下，所有的属性都不传递依赖于主键，满足第三范式。 学生借书情况（借阅编号，学生学号，书籍编号，书籍名称，书籍作者） 实际上书籍编号依赖于借阅编号，而书籍名称和书籍作者依赖于书籍编号，因此存在传递依赖的情况，我们可以将书籍信息进行单独拆分为另一张表： 学生借书情况（借阅编号，学生学号，书籍编号） 书籍（书籍编号，书籍名称，书籍作者） 这样就消除了传递依赖，从而满足第三范式。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:3:3","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"BCNF BCNF作为第三范式的补充，假设仓库管理关系表为StorehouseManage(仓库ID, 存储物品ID, 管理员ID, 数量)，且有一个管理员只在一个仓库工作；一个仓库可以存储多种物品。这个数据库表中存在如下决定关系： (仓库ID, 存储物品ID) →(管理员ID, 数量) (管理员ID, 存储物品ID) → (仓库ID, 数量) 所以，(仓库ID, 存储物品ID)和(管理员ID, 存储物品ID)都是StorehouseManage的候选关键字，表中的唯一非关键字段为数量，它是符合第三范式的。但是，由于存在如下决定关系： (仓库ID) → (管理员ID) (管理员ID) → (仓库ID) 即存在关键字段决定关键字段的情况，如果修改管理员ID，那么就必须逐一进行修改，所以其不符合BCNF范式。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:3:4","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"认识SQL语句 结构化查询语言（Structured Query Language）简称SQL，这是一种特殊的语言，它专门用于数据库的操作。每一种数据库都支持SQL，但是他们之间会存在一些细微的差异，因此不同的数据库都存在自己的“方言”。 SQL语句不区分大小写（关键字推荐使用大写），它支持多行，并且需要使用;进行结尾！ SQL也支持注释，通过使用--或是#来编写注释内容，也可以使用/*来进行多行注释。 我们要学习的就是以下四种类型的SQL语言： 数据查询语言（Data Query Language, DQL）基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块。 数据操纵语言（Data Manipulation Language, DML）是SQL语言中，负责对数据库对象运行数据访问工作的指令集，以INSERT、UPDATE、DELETE三种指令为核心，分别代表插入、更新与删除，是开发以数据为中心的应用程序必定会使用到的指令。 数据库定义语言DDL(Data Definition Language)，是用于描述数据库中要存储的现实世界实体的语言。 DCL（Data Control Language）是数据库控制语言。是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。在默认状态下，只有sysadmin,dbcreator,db_owner或db_securityadmin等人员才有权力执行DCL。 我们平时所说的CRUD其实就是增删改查（Create/Retrieve/Update/Delete） ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:4:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库定义语言（DDL） ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库操作 我们可以通过create database来创建一个数据库： createdatabase数据库名为了能够支持中文，我们在创建时可以设定编码格式： CREATEDATABASEIFNOTEXISTS数据库名DEFAULTCHARSETutf8COLLATEutf8_general_ci;如果我们创建错误了，我们可以将此数据库删除，通过使用drop database来删除一个数据库： dropdatabase数据库名","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"创建表 数据库创建完成后，我们一般通过create table语句来创建一张表： createtable表名(列名数据类型[列级约束条件],列名数据类型[列级约束条件],...[,表级约束条件])","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"SQL数据类型 以下的数据类型用于字符串存储： char(n)可以存储任意字符串，但是是固定长度为n，如果插入的长度小于定义长度时，则用空格填充。 varchar(n)也可以存储任意数量字符串，长度不固定，但不能超过n，不会用空格填充。 以下数据类型用于存储数字： smallint用于存储小的整数，范围在 (-32768，32767) int用于存储一般的整数，范围在 (-2147483648，2147483647) bigint用于存储大型整数，范围在 (-9,223,372,036,854,775,808，9,223,372,036,854,775,807) float用于存储单精度小数 double用于存储双精度的小数 以下数据类型用于存储时间： date存储日期 time存储时间 year存储年份 datetime用于混合存储日期+时间 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:3","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"列级约束条件 列级约束有六种：主键Primary key、外键foreign key 、唯一 unique、检查 check （MySQL不支持）、默认default 、非空/空值 not null/ null ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:4","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"表级约束条件 表级约束有四种：主键、外键、唯一、检查 现在我们通过SQL语句来创建我们之前提到的三张表。 [CONSTRAINT\u003c外键名\u003e]FOREIGNKEY字段名[，字段名2，…]REFERENCES\u003c主表名\u003e主键列1[，主键列2，…]","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:5","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"修改表 如果我们想修改表结构，我们可以通过alter table来进行修改： ALTERTABLE表名[ADD新列名数据类型[列级约束条件]][DROPCOLUMN列名[restrict|cascade]][ALTERCOLUMN列名新数据类型]我们可以通过ADD来添加一个新的列，通过DROP来删除一个列，不过我们可以添加restrict或cascade，默认是restrict，表示如果此列作为其他表的约束或视图引用到此列时，将无法删除，而cascade会强制连带引用此列的约束、视图一起删除。还可以通过ALTER来修改此列的属性。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:6","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"删除表 我们可以通过drop table来删除一个表： DROPTABLE表名[restrict|cascade]其中restrict和cascade上面的效果一致。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:5:7","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库操纵语言（DML） 前面我们已经学习了如何使用SQL语句来创建、修改、删除数据库以及表，而如何向数据库中插入、删除、更新数据，将是本版块讨论的重点。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:6:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"插入数据 通过使用insert into语句来向数据库中插入一条数据（一条记录）： INSERTINTO表名VALUES(值1,值2,值3)如果插入的数据与列一一对应，那么可以省略列名，但是如果希望向指定列上插入数据，就需要给出列名： INSERTINTO表名(列名1,列名2)VALUES(值1,值2)我们也可以一次性向数据库中插入多条数据： INSERTINTO表名(列名1,列名2)VALUES(值1,值2),(值1,值2),(值1,值2)我们来试试看向我们刚刚创建的表中添加三条数据。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:6:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"修改数据 我们可以通过update语句来更新表中的数据： UPDATE表名SET列名=值,...WHERE条件注意，SQL语句中的等于判断是= **警告：**如果忘记添加WHERE字句来限定条件，将使得整个表中此列的所有数据都被修改！ ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:6:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"删除数据 我们可以通过使用delete来删除表中的数据： DELETEFROM表名通过这种方式，将删除表中全部数据，我们也可以使用where来添加条件，只删除指定的数据： DELETEFROM表名WHERE条件 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:6:3","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库查询语言（DQL） 数据库的查询是我们整个数据库学习中的重点内容，面对数据库中庞大的数据，该如何去寻找我们想要的数据，就是我们主要讨论的问题。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"单表查询 单表查询是最简单的一种查询，我们只需要在一张表中去查找数据即可，通过使用select语句来进行单表查询： -- 指定查询某一列数据 SELECT列名[,列名]FROM表名-- 会以别名显示此列 SELECT列名别名FROM表名-- 查询所有的列数据 SELECT*FROM表名-- 只查询不重复的值 SELECTDISTINCT列名FROM表名我们也可以添加where字句来限定查询目标： SELECT*FROM表名WHERE条件","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"常用查询条件 一般的比较运算符，包括=、\u003e、\u003c、\u003e=、\u003c=、!=等。 是否在集合中：in、not in 字符模糊匹配：like，not like 多重条件连接查询：and、or、not 我们来尝试使用一下上面这几种条件。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"排序查询 我们可以通过order by来将查询结果进行排序： SELECT*FROM表名WHERE条件ORDERBY列名ASC|DESC使用ASC表示升序排序，使用DESC表示降序排序，默认为升序。 我们也可以可以同时添加多个排序： SELECT*FROM表名WHERE条件ORDERBY列名1ASC|DESC,列名2ASC|DESC这样会先按照列名1进行排序，每组列名1相同的数据再按照列名2排序。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:3","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"聚集函数 聚集函数一般用作统计，包括： count([distinct]*)统计所有的行数（distinct表示去重再统计，下同） count([distinct]列名)统计某列的值总和 sum([distinct]列名)求一列的和（注意必须是数字类型的） avg([distinct]列名)求一列的平均值（注意必须是数字类型） max([distinct]列名)求一列的最大值 min([distinct]列名)求一列的最小值 一般聚集函数是这样使用的： SELECTcount(distinct列名)FROM表名WHERE条件","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:4","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"分组和分页查询 通过使用group by来对查询结果进行分组，它需要结合聚合函数一起使用： SELECTsum(*)FROM表名WHERE条件GROUPBY列名我们还可以添加having来限制分组条件： SELECTsum(*)FROM表名WHERE条件GROUPBY列名HAVING约束条件我们可以通过limit来限制查询的数量，只取前n个结果： SELECT*FROM表名LIMIT数量我们也可以进行分页： SELECT*FROM表名LIMIT起始位置,数量","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:5","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"多表查询 多表查询是同时查询的两个或两个以上的表，多表查询会提通过连接转换为单表查询。 SELECT*FROM表1,表2直接这样查询会得到两张表的笛卡尔积，也就是每一项数据和另一张表的每一项数据都结合一次，会产生庞大的数据。 SELECT*FROM表1,表2WHERE条件这样，只会从笛卡尔积的结果中得到满足条件的数据。 **注意：**如果两个表中都带有此属性吗，需要添加表名前缀来指明是哪一个表的数据。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:6","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"自身连接查询 自身连接，就是将表本身和表进行笛卡尔积计算，得到结果，但是由于表名相同，因此要先起一个别名： SELECT*FROM表名别名1,表名别名2其实自身连接查询和前面的是一样的，只是连接对象变成自己和自己了。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:7","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"外连接查询 外连接就是专门用于联合查询情景的，比如现在有一个存储所有用户的表，还有一张用户详细信息的表，我希望将这两张表结合到一起来查看完整的数据，我们就可以通过使用外连接来进行查询，外连接有三种方式： 通过使用inner join进行内连接，只会返回两个表满足条件的交集部分： 通过使用left join进行左连接，不仅会返回两个表满足条件的交集部分，也会返回左边表中的全部数据，而在右表中缺失的数据会使用null来代替（右连接right join同理，只是反过来而已，这里就不再介绍了）： ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:8","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"嵌套查询 我们可以将查询的结果作为另一个查询的条件，比如： SELECT*FROM表名WHERE列名=(SELECT列名FROM表名WHERE条件)我们来再次尝试编写一下在最开始我们查找某教师所有学生的SQL语句。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:7:9","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"数据库控制语言（DCL） 庞大的数据库不可能由一个人来管理，我们需要更多的用户来一起管理整个数据库。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:8:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"创建用户 我们可以通过create user来创建用户： CREATEUSER用户名identifiedby密码;也可以不带密码： CREATEUSER用户名;我们可以通过@来限制用户登录的登录IP地址，%表示匹配所有的IP地址，默认使用的就是任意IP地址。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:8:1","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"登陆用户 首先需要添加一个环境变量，然后我们通过cmd去登陆mysql： login-u用户名-p输入密码后即可登陆此用户，我们输入以下命令来看看能否访问所有数据库： showdatabases;我们发现，虽然此用户能够成功登录，但是并不能查看完整的数据库列表，这是因为此用户还没有权限！ ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:8:2","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"用户授权 我们可以通过使用grant来为一个数据库用户进行授权： grantall|权限1,权限2...(列1,...)on数据库.表to用户[withgrantoption]其中all代表授予所有权限，当数据库和表为*，代表为所有的数据库和表都授权。如果在最后添加了with grant option，那么被授权的用户还能将已获得的授权继续授权给其他用户。 我们可以使用revoke来收回一个权限： revokeall|权限1,权限2...(列1,...)on数据库.表from用户 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:8:3","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"视图 视图本质就是一个查询的结果，不过我们每次都可以通过打开视图来按照我们想要的样子查看数据。既然视图本质就是一个查询的结果，那么它本身就是一个虚表，并不是真实存在的，数据实际上还是存放在原来的表中。 我们可以通过create view来创建视图; CREATEVIEW视图名称(列名)as子查询语句[WITHCHECKOPTION];WITH CHECK OPTION是指当创建后，如果更新视图中的数据，是否要满足子查询中的条件表达式，不满足将无法插入，创建后，我们就可以使用select语句来直接查询视图上的数据了，因此，还能在视图的基础上，导出其他的视图。 若视图是由两个以上基本表导出的，则此视图不允许更新。 若视图的字段来自字段表达式或常数，则不允许对此视图执行INSERT和UPDATE操作，但允许执行DELETE操作。 若视图的字段来自集函数，则此视图不允许更新。 若视图定义中含有GROUP BY子句，则此视图不允许更新。 若视图定义中含有DISTINCT短语，则此视图不允许更新。 若视图定义中有嵌套查询，并且内层查询的FROM子句中涉及的表也是导出该视图的基本表，则此视图不允许更新。例如将成绩在平均成绩之上的元组定义成一个视图GOOD_SC： CREATE VIEW GOOD_SC AS SELECT Sno, Cno, Grade FROM SC WHERE Grade \u003e (SELECT AVG(Grade) FROM SC); 导出视图GOOD_SC的基本表是SC，内层查询中涉及的表也是SC，所以视图GOOD_SC是不允许更新的。 一个不允许更新的视图上定义的视图也不允许更新 通过drop来删除一个视图： dropviewapptest ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:9:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"索引 在数据量变得非常庞大时，通过创建索引，能够大大提高我们的查询效率，就像Hash表一样，它能够快速地定位元素存放的位置，我们可以通过下面的命令创建索引： -- 创建索引 CREATEINDEX索引名称ON表名(列名)-- 查看表中的索引 showINDEXFROMstudent我们也可以通过下面的命令删除一个索引： dropindex索引名称on表名虽然添加索引后会使得查询效率更高，但是我们不能过度使用索引，索引为我们带来高速查询效率的同时，也会在数据更新时产生额外建立索引的开销，同时也会占用磁盘资源。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:10:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"触发器 触发器就像其名字一样，在某种条件下会自动触发，在select/update/delete时，会自动执行我们预先设定的内容，触发器通常用于检查内容的安全性，相比直接添加约束，触发器显得更加灵活。 触发器所依附的表称为基本表，当触发器表上发生select/update/delete等操作时，会自动生成两个临时的表（new表和old表，只能由触发器使用） 比如在insert操作时，新的内容会被插入到new表中；在delete操作时，旧的内容会被移到old表中，我们仍可在old表中拿到被删除的数据；在update操作时，旧的内容会被移到old表中，新的内容会出现在new表中。 CREATETRIGGER触发器名称[BEFORE|AFTER][INSERT|UPDATE|DELETE]ON表名/视图名FOREACHROWDELETEFROMstudentWHEREstudent.sno=new.snoFOR EACH ROW表示针对每一行都会生效，无论哪行进行指定操作都会执行触发器！ 通过下面的命令来查看触发器： SHOWTRIGGERS如果不需要，我们就可以删除此触发器： DROPTRIGGER触发器名称 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:11:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"事务 当我们要进行的操作非常多时，比如要依次删除很多个表的数据，我们就需要执行大量的SQL语句来完成，这些数据库操作语句就可以构成一个事务！只有Innodb引擎支持事务，我们可以这样来查看支持的引擎： SHOWENGINES;MySQL默认采用的是Innodb引擎，我们也可以去修改为其他的引擎。 事务具有以下特性： **原子性：**一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 **一致性：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 **隔离性：**数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 **持久性：**事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 我们通过以下例子来探究以下事务： begin;#开始事务...rollback;#回滚事务savepoint回滚点;#添加回滚点rollbackto回滚点;#回滚到指定回滚点...commit;#提交事务-- 一旦提交，就无法再进行回滚了！ ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:12:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["JavaWeb笔记"],"content":"选学内容 函数和存储过程并没有包含在我们的教程当中，但是这并不代表它们就不重要，通过学习它们能够让你的数据库管理能力更上一层楼，它们能够捆绑一组SQL语句运行，并且可以反复使用，大大提高工作效率。 ","date":"2022-02-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/:13:0","tags":["数据库基础"],"title":"数据库基础","uri":"/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/"},{"categories":["算法——最短路问题"],"content":"BellmanFord和SPFA算法详解","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"*关于Bellman ford和SPFA算法的详解 我是白嫖的leetcode会员，然后看了关于图单源最短路径的讲解，讲解的非常好(虽然没代码演示，但基本上一看思路就有了)。 为了让大家也白嫖到视频资源，我把视频上传到了YouTube(国内会有版权问题，发不出) 大家有能力上油管的建议去看看，否则这代码肯定是看不懂的。。。 视频链接： Bellman ford算法详解(两种方式及其优化) 由Bellman ford算法的缺陷引出SPFA算法 适用性分析(先看视频) Blellman ford算法 DP方法：以 dp[i][j] 表示选择最多 i 条边，从起点到 j 的最短距离。每次的更新依赖于上一行 dp[i-1][j] 的答案，故可滚动数组优化为一维数组。时间复杂度O(N^3) 多次遍历边的更新方法：提前记录好哪两个结点有边，每进行一次整个边的遍历，就相当于完成了最多选择一条边到达目的地的最短距离的效果。平均时间复杂度 O(N*V)（V是边的个数，极端情况下会掉到 O(N*N*V)的复杂度，因为最多是可以进行 N-1 次循环的) 很明显无论是哪种方式实现，最终都是依赖选择多少条边的结果，所以该算法适用于指定最多经过k条边的最短路径题目。 正好有道例题适合他 K 站中转内最便宜的航班 SPFA算法 这个算法只是Bellman ford算法的再优化，使得每次选择的边的关系达到最优，大大减少了边的遍历次数,时间复杂度较为稳定(相对Bellmanford稳定很多)的在 O(N*V)。 这个原本也是基于Bellmanford算法优化的，除了无法表示最多经过k条边，其余效率比之前的算法更快，所以适用于求存在负权值的单源最短路径问题，而无法精确为最多经过了多少条边。 以题代讲 ","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/:0:0","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"蓝桥杯–最短路 ","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/:1:0","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"Bellman ford的动态规划解决(超时,过三个) #include\u003cbits/stdc++.h\u003eusing namespace std; #define LL long long int n,m; vector\u003cint\u003edp(20001,INT_MAX/2); map\u003cint,map\u003cint,int\u003e \u003e MAP; LL read() { LL res = 0; bool f = 1; char c; //先耗掉一个getchar来进行判断符号 c = getchar(); if(c == '-')f = 0; else res+= (c-'0'); while (isdigit(c = getchar())) { res = (LL)res * 10 + (c-'0'); } if (f) return res; return res*-1; } int main(){ n = read(); m = read(); for(int i=0;i\u003cm;i++){ int a =read(),c = read(),len = read(); MAP[a][c] = len; if(a == 1) dp[c] = len; } dp[1] = 0; vector\u003cint\u003epre = dp; //外层循环经过最多i条路到达该结点的最短距离，最多经过n-1条 //里面几层都是用于更新没一行的数据 for(int i=2;i\u003c=n-1;i++){ for(int j=2;j\u003c=n;j++){ for(int k = 1;k\u003c=n;k++){ int t = MAP[k][j]; if(t) dp[j] = min(dp[j],pre[k]+t); } } pre = dp; } for(int i=2;i\u003c=n;i++){ cout\u003c\u003cdp[i]\u003c\u003cendl; } } ","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/:2:0","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"Bellman ford按边遍历解决(速度竟比SPFA快，我惊了) #include\u003cbits/stdc++.h\u003eusing namespace std; #define LL long long int n,m; //以边为单位遍历更新 struct pos{ int i; int j; int len; }; vector\u003cint\u003edp(20001,INT_MAX/2); LL read() { LL res = 0; bool f = 1; char c; //先耗掉一个getchar来进行判断符号 c = getchar(); if(c == '-')f = 0; else res+= (c-'0'); while (isdigit(c = getchar())) { res = (LL)res * 10 + (c-'0'); } if (f) return res; return res*-1; } int main(){ n = read(); m = read(); //记录m条边的关系 pos MAP[m]; for(int i=0;i\u003cm;i++){ int a =read(),c = read(),len = read(); MAP[i] = {a,c,len}; } dp[1] = 0; //外面一层代表遍历边的次数，最多为n-1次 for(int i=1;i\u003c=n-1;i++){ bool flag = true; for(int k=0;k\u003cm;k++){ if(dp[MAP[k].j]\u003edp[MAP[k].i]+MAP[k].len){ dp[MAP[k].j] = dp[MAP[k].i]+MAP[k].len; flag = false; } } //一旦有一轮遍历未更新一次，则弹出循环，得出答案 if(flag) break; } for(int i=2;i\u003c=n;i++){ cout\u003c\u003cdp[i]\u003c\u003cendl; } } ","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/:3:0","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"最终优化–SPFA算法 毕竟SPFA的全称为Shortest Path Faster Algorithm，也得当担得起这个名字啊🤣 主要因为用STL容器存储数据的原因，所以似乎稍慢。 #include\u003cbits/stdc++.h\u003eusing namespace std; #define LL long long int n,m; //以边为单位遍历更新,再进一步优化便得到得到SPFA算法 //我们需要构造一个以任一点为起点的，它所连接的通路的结构，以方便队列进行操作，用哈希表进行映射最好 map\u003cint,vector\u003cpair\u003cint,int\u003e \u003e \u003eMAP; vector\u003cint\u003edp(20001,INT_MAX/2); queue\u003cint\u003eQ; //标记结点是否在队列之中 bool check[20001] = {false}; LL read() { LL res = 0; bool f = 1; char c; //先耗掉一个getchar来进行判断符号 c = getchar(); if(c == '-')f = 0; else res+= (c-'0'); while (isdigit(c = getchar())) { res = (LL)res * 10 + (c-'0'); } if (f) return res; return res*-1; } int main(){ n = read(); m = read(); for(int i=0;i\u003cm;i++){ int a =read(),c = read(),len = read(); MAP[a].push_back(make_pair(c,len)); } dp[1] = 0; Q.push(1); check[1] = true; while(!Q.empty()){ int node = Q.front();Q.pop();check[node] = false; vector\u003cpair\u003cint,int\u003e \u003e\u0026 t = MAP[node]; //以node为起点开始更新，一旦被更新且队中无该结点，则入队。 for(int i=0;i\u003ct.size();i++) { if(dp[t[i].first]\u003edp[node]+t[i].second){ dp[t[i].first] = dp[node]+t[i].second; //入队操作 if(!check[t[i].first]) Q.push(t[i].first); check[t[i].first] = true; } } } for(int i=2;i\u003c=n;i++){ cout\u003c\u003cdp[i]\u003c\u003cendl; } } ","date":"2022-02-07","objectID":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/:4:0","tags":["BellmanFord和SPFA算法详解"],"title":"BellmanFord和SPFA算法详解","uri":"/posts/bellmanford%E5%92%8Cspfa%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"Dijkstra算法模板讲解","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"(也就5min)先点开链接把Dijkstra算法过程看一看(否则肯定看不懂代码). 视频详解Dijkstra算法过程 此方法最短路径的适用范围：单源带权图，要是不带权完全可以用bfs。 详解代码实现过程(请结合视频过程分析) ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:0:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"用到的基本数据表示 根据视频中讲解的实现原理，我们需要通过多个数组来实现该过程。 dist[i]数组(下标表示第几个结点)用于标记起点S到i的最短距离，初始值全为无穷大，这个值只要足够大就行。 visit[i]数组(下标同上)用于标记每个已经得到最短距离的结点，初始值全为false,表示该结点还未找到最短距离。 path[i]数组(下标同上)用于标记每个已经得到最短距离的结点的前驱(最短路径中的上一个结点)，初始值全为-1。 Graph[i][j]这是一个带权矩阵，表示任意i结点到j结点间边的距离,若两者间无边则初始为无穷大。 ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:1:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"各个函数模块实现(重在Dijkstra函数) init() void init()//在读入数据之前初始化图 {//Graph和dist在读取数据之前都初始化为INF，path初始化为-1 for(int i = 1; i \u003c= N; i++){ path[i] = -1; dist[i] = INF; for(int j = 1; j \u003c= N; j++){ Graph[j][i] = INF;//INF为自定义的较大的值 } } memset(visit,0,sizeof(visit));//起初，没有一个结点被标记 } FIndMin() int FindMin()//找出未被visit标记的结点中最小的距离结点,并返回该结点 { int minV = S;//初始为S，如果未被更新则路径更新过程结束 int minDist = INF; for(int i = 0; i \u003c N; i++){ if(!visit[i] \u0026\u0026 dist[i] \u003c minDist){//没有被标记\u0026\u0026距离最小 minV = i; minDist = dist[i]; } }//返回没有被标记的最小结点 return minV; } Dijkstra() void Dijkstra() {//S表示起点，T表示终点 dist[S] = 0;//起点到自己的距离设为0 visit[S] = true;//将起点标记 for(int i = 1; i \u003c= N; i++){ //更新与起点S相连的结点的距离值 int t = Graph[i][S]; if(t \u003c INF){ dist[i] = t; path[i] = S; } } //要么无法到达，要么就是找到到达终点的最小值，否则一直循环。 while(1){ //得到未被标记的最小结点，将其标记 int v = FindMin(); if(v==S)//如果未被更新，则无需再更新了，要么全被标记，要么就是剩下的无法到达 return; visit[v] = true;//将该结点标记 if(visit[T])return;//一旦T被标记，则说明到达终点的最小值已经找到 for(int i = 1; i \u003c= N; i++){ //这个结合视频的更新过程想想就懂了 if(!visit[i]\u0026\u0026Graph[v][i]!=INF){//没被标记\u0026\u0026两者之间存在边 if(dist[v] + Graph[v][i] \u003c dist[i]){ dist[i] = dist[v] + Graph[v][i]; path[i] = v; } } } } } 以题代讲–具体实现 ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:2:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"蓝桥杯–文化之旅 看完题目，唯一的区别在于还需要额外判断文化是否排斥，这个在函数中多添加一个判断条件就行。 ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:3:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"解题过程 我们按照上述的过程三步走试试： 初始化过程(我比较习惯用vector容器所以就没有单独写init函数了) //vector容器的初始化方法--vector\u003c类型\u003e变量名(长度,初始值); const int size = 505;//用于初始化一个size长度的数组 vector\u003cint\u003epath(size,-1); vector\u003cint\u003edist(size,INT_MAX); vector\u003cbool\u003evisit(size,false); vector\u003cvector\u003cint\u003e \u003eGraphics(size,vector\u003cint\u003e(size,INT_MAX)); //下面两个是本题新加的属性，我们建立所属文化以及文化关系数组来存储。 vector\u003cint\u003ecultures(size); bool cultureLinks[size][size] = {false}; 找最小结点的函数FindMin() int FindMin() { int minV = S; int minDist = INT_MAX; for(int i = 1; i \u003c= N; i++){ if(!visit[i] \u0026\u0026 dist[i] \u003c minDist){ minV = i; minDist = dist[i]; } } return minV; } Dijkstra() 函数 void Dijkstra() { dist[S] = 0; visit[S] = true; for(int i = 1; i \u003c= N; i++){ int t = Graphics[S][i]; if(t \u003c INT_MAX\u0026\u0026!cultureLinks[cultures[i]][cultures[S]]){ dist[i] = t; path[i] = S;} } while(1){ int v = FindMin(); if(v==S) return; visit[v] = true;//将此次最小路径结点标记 //一旦T终点被标记则说明答案已经出现 if(visit[T])return; for(int i = 1; i \u003c= N; i++){ if(!visit[i]\u0026\u0026Graphics[v][i]!=INT_MAX\u0026\u0026!cultureLinks[cultures[i]][cultures[v]]){//只有结点未被标记 \u0026\u0026 文化不会排斥 \u0026\u0026 路径长度不是无穷大 if(dist[v] + Graphics[v][i] \u003c dist[i]){ dist[i] = dist[v] + Graphics[v][i]; path[i] = v; } } } } } main函数测试接口 int main(){ cin\u003e\u003eN\u003e\u003eK\u003e\u003eM\u003e\u003eS\u003e\u003eT; for(int i=1;i\u003c=N;i++) cin\u003e\u003ecultures[i]; for(int i=1;i\u003c=K;i++) for(int j=1;j\u003c=K;j++) cin\u003e\u003ecultureLinks[i][j]; for(int i=1;i\u003c=M;i++){ int u,v,d;cin\u003e\u003eu\u003e\u003ev\u003e\u003ed;//无向图 Graphics[u][v] = Graphics[v][u] = d; } Dijkstra(); if(dist[T]!=INT_MAX) cout\u003c\u003cdist[T]; else cout\u003c\u003c-1; return 0; } ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:4:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["算法——最短路问题"],"content":"汇总代码提交 #include\u003cbits/stdc++.h\u003eusing namespace std; const int size = 505; vector\u003cint\u003epath(size,-1); vector\u003cint\u003edist(size,INT_MAX); vector\u003cbool\u003evisit(size,false); vector\u003cvector\u003cint\u003e \u003eGraphics(size,vector\u003cint\u003e(size,INT_MAX)); vector\u003cint\u003ecultures(size); bool cultureLinks[size][size] = {false}; int N,K,M,S,T; int FindMin() { int minV = S; int minDist = INT_MAX; for(int i = 1; i \u003c= N; i++){ if(!visit[i] \u0026\u0026 dist[i] \u003c minDist){ minV = i; minDist = dist[i]; } } return minV; } void Dijkstra() { dist[S] = 0; visit[S] = true; for(int i = 1; i \u003c= N; i++){ int t = Graphics[S][i]; if(t \u003c INT_MAX\u0026\u0026!cultureLinks[cultures[i]][cultures[S]]){ dist[i] = t; path[i] = S;} } while(1){ int v = FindMin(); if(v==S) return; visit[v] = true;//将此次最小路径结点标记 //一旦T终点被标记则说明答案已经出现 if(visit[T])return; for(int i = 1; i \u003c= N; i++){ if(!visit[i]\u0026\u0026Graphics[v][i]!=INT_MAX\u0026\u0026!cultureLinks[cultures[i]][cultures[v]]){//只有结点未被标记 \u0026\u0026 文化不会排斥 \u0026\u0026 路径长度不是无穷大 if(dist[v] + Graphics[v][i] \u003c dist[i]){ dist[i] = dist[v] + Graphics[v][i]; path[i] = v; } } } } } int main(){ cin\u003e\u003eN\u003e\u003eK\u003e\u003eM\u003e\u003eS\u003e\u003eT; for(int i=1;i\u003c=N;i++) cin\u003e\u003ecultures[i]; for(int i=1;i\u003c=K;i++) for(int j=1;j\u003c=K;j++) cin\u003e\u003ecultureLinks[i][j]; for(int i=1;i\u003c=M;i++){ int u,v,d;cin\u003e\u003eu\u003e\u003ev\u003e\u003ed;//无向图 Graphics[u][v] = Graphics[v][u] = d; } Dijkstra(); if(dist[T]!=INT_MAX) cout\u003c\u003cdist[T]; else cout\u003c\u003c-1; return 0; } ","date":"2022-02-07","objectID":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/:5:0","tags":["Dijkstra算法模板讲解"],"title":"Dijkstra算法模板讲解","uri":"/posts/dijkstra%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF%E8%AE%B2%E8%A7%A3/"},{"categories":["C++实战"],"content":"C++图形化实现学生管理系统","date":"2022-02-05","objectID":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/","tags":["C++图形化实现学生管理系统"],"title":"C++图形化实现学生管理系统","uri":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"categories":["C++实战"],"content":"Qt学生管理系统 想查看源码或者直接下载软件安装包可以到下面的链接： 软件安装包大概在这个位置 GitHub地址：链接 Gitee地址(方便国内访问)：链接 ","date":"2022-02-05","objectID":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/:0:0","tags":["C++图形化实现学生管理系统"],"title":"C++图形化实现学生管理系统","uri":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"categories":["C++实战"],"content":"基本源码介绍 本项目基于Qt6+cmake，故直接拿到源码是无法跑起来的，需要Qt6的库。上传源码主要是记录源码里面的一些思路。 以下是源码的解析图：（其中的ui布局界面和qss界面美化是最耗时间的部分！） ui界面布局用到的特殊之处： 重写界面顶部逻辑。 重写一些事件。 增加软件运行效率的特殊之处： 通过开一个线程将数据库中的数据提前载入内存，后续的任何查询等操作都是直接和内存打交道，大大加快了运行效率。 对数据库的增删，不是一次一次的进行，而是开一个缓冲区，当缓冲满了，再一次性增或者删。减少了磁盘io次数，大大增加了软件运行效率。 使用的数据库为Qt自带的sqlite数据库。 ","date":"2022-02-05","objectID":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/:1:0","tags":["C++图形化实现学生管理系统"],"title":"C++图形化实现学生管理系统","uri":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"categories":["C++实战"],"content":"实现效果 基本学生/用户数据的增删改查。 数据存储的持久化。 较为优美的图形化界面。 还有很多功能没有进行拓展：比如用户权限没有进行任何的限制，比如Excle表格读取没有进行任何的设置，只是把excle读取的功能给加上了，具体读取到的数据没有进行任何操作，这些大家都可以后续有兴趣添加一个。 ","date":"2022-02-05","objectID":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/:2:0","tags":["C++图形化实现学生管理系统"],"title":"C++图形化实现学生管理系统","uri":"/posts/c++%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"categories":["现代C++语法"],"content":"C++右值语义的基石——完美转发","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["现代C++语法"],"content":"什么是完美转发？ 熟悉现代C++语法的都应该清楚，C++把变量分为左值和右值，为了实现对资源的转移而不是拷贝，右值和对应的移动构造函数应运而生，但我们发现，很多时候我们并不能把左值和右值精确的传递给对应版本的函数进行处理，比如下面一个简单的代码，你会发现即使我们把函数的参数类型设置为右值引用，但当拿它去调用对应的构造函数时，它给出的竟然是拷贝构造！故这个转发还不够完美！ #include\u003ciostream\u003eusing namespace std; class test{ public: test() = default; test(test\u0026\u0026 p){ cout\u003c\u003c\"move construct call\"\u003c\u003cendl; } test(const test\u0026 p){ cout\u003c\u003c\"copy construct call\"\u003c\u003cendl; } }; void test_fun(test\u0026\u0026 p){ test q(p); return; } int main() { test_fun(test()); return 0; } 为什么会出现这种情况呢？ 因为无论传入的形参是左值还是右值，对于函数内部来说，形参既有名称又能寻址，因此它都被认为是左值。 ","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/:1:0","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["现代C++语法"],"content":"如何实现完美转发？ 实现完美转发很简单，我们在现代C++中只需要 forward\u003cT\u003e 这个模板函数即可完成，其实际原理就是利用的 C++11 模板中提供的折叠引用的语法，最终达到的效果就是，把参数的类型强制转换为它该有的类型，是左值就转为左值，是右值就转为右值，从而实现该调用哪个版本的函数就调用哪个版本的函数，不再只被认定为右值了！ 先前的代码可以如此实现完美转发： #include\u003ciostream\u003eusing namespace std; class test{ public: test() = default; test(test\u0026\u0026 p){ cout\u003c\u003c\"move construct call\"\u003c\u003cendl; } test(const test\u0026 p){ cout\u003c\u003c\"copy construct call\"\u003c\u003cendl; } }; void test_fun(test\u0026\u0026 p){ test q(forward\u003ctest\u003e(p)); //修改的地方 return; } int main() { test_fun(test()); return 0; } ","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/:2:0","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["现代C++语法"],"content":"任何模板库都离不开完美转发 其实现在只要是C++的模板库，没有哪个是不用完美转发的，同时完美转发的问题也是产生自模板，而 forward 函数的实现其实也不是什么难事，实际就是利用 C++11 对模板提供的万能折叠语义： 当实参为左值或者左值引用（A\u0026）时，函数模板中 T\u0026\u0026 将转变为 A\u0026（A\u0026 \u0026\u0026 = A\u0026）； 当实参为右值或者右值引用（A\u0026\u0026）时，函数模板中 T\u0026\u0026 将转变为 A\u0026\u0026（A\u0026\u0026 \u0026\u0026 = A\u0026\u0026）。 以下为一个简单的利用完美转发设计的创建工厂： #include\u003ciostream\u003e#include \u003cmemory\u003e using namespace std; class test{ public: test() = default; test(int\u0026\u0026 arg):m_iData(arg){ cout\u003c\u003c\"move construct call\"\u003c\u003cendl; } test(const int\u0026 arg):m_iData(arg){ cout\u003c\u003c\"copy construct call\"\u003c\u003cendl; } private: int m_iData; }; template\u003ctypename T,typename Arg\u003e //不直接用T\u0026\u0026的原因在于，如果只使用一个模板参数会导致factory参数无法获得万能引用的效果 shared_ptr\u003cT\u003e factory(Arg\u0026\u0026 arg){ return shared_ptr\u003cT\u003e(new T(forward\u003cArg\u003e(arg)));//使用完美转发调用正确的构造函数 } int main() { int val = 5; auto p1 = factory\u003ctest\u003e(val); auto p2 = factory\u003ctest\u003e(5); return 0; } ","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/:3:0","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["现代C++语法"],"content":"std::forward的实现原理 gcc 的源代码实现如下： template\u003ctypename _Tp\u003e constexpr _Tp \u0026\u0026 forward(typename std::remove_reference\u003c_Tp\u003e::type \u0026__t) noexcept { return static_cast\u003c_Tp \u0026\u0026\u003e(__t); } template\u003ctypename _Tp\u003e constexpr _Tp \u0026\u0026 forward(typename std::remove_reference\u003c_Tp\u003e::type \u0026\u0026__t) noexcept { static_assert(!std::is_lvalue_reference\u003c_Tp\u003e::value, \"template argument\" \" substituting _Tp is an lvalue reference type\"); return static_cast\u003c_Tp \u0026\u0026\u003e(__t); } 我们发现，源代码中实现了两个模板特化，_Tp\u0026 和 _Tp\u0026\u0026 但最终都是通过 static_cast + 折叠引用的特性来实现强制转化的。也就是简单的强转而已！ ","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/:4:0","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["现代C++语法"],"content":"收获 在设计模板库的时候，如果需要根据左值右值语义比较清晰的实现转发，一定要用forward，否则参数只会被当作左值！ ","date":"2022-02-04","objectID":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/:5:0","tags":["C++右值语义的基石——完美转发"],"title":"C++右值语义的基石——完美转发","uri":"/posts/c++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"},{"categories":["算法——贪心"],"content":"leetcode每日一题——和为K的最少斐波那契数字数目","date":"2022-02-03","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/","tags":["fib的贪心可行性"],"title":"leetcode每日一题——和为K的最少斐波那契数字数目","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/"},{"categories":["算法——贪心"],"content":"题目 题目链接 ","date":"2022-02-03","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/:1:0","tags":["fib的贪心可行性"],"title":"leetcode每日一题——和为K的最少斐波那契数字数目","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/"},{"categories":["算法——贪心"],"content":"题目详解 我开始是想着构造好fib数组的值，然后用背包问题去解决它。可惜，直接超时了！ 后面直接用最简单的贪心方式没想到真的可行。。 然后看了题解才知道原来fib的「每次选择不超过当前 k 的最大数」这是一个特有的结论，然后大家都在证明他，虽然我看不懂，但我大受震撼😂 详解链接 ","date":"2022-02-03","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/:2:0","tags":["fib的贪心可行性"],"title":"leetcode每日一题——和为K的最少斐波那契数字数目","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/"},{"categories":["算法——贪心"],"content":"解题代码 class Solution { public: int findMinFibonacciNumbers(int k) { vector\u003cint\u003eitems(2,1); int ret = 0; while(items.back()\u003ck){ items.push_back(items.back()+items[items.size()-2]); } for(int i=items.size()-1;i\u003e=0;i--){ ret += k/items[i]; k %= items[i]; } return ret; } }; ","date":"2022-02-03","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/:3:0","tags":["fib的贪心可行性"],"title":"leetcode每日一题——和为K的最少斐波那契数字数目","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%92%8C%E4%B8%BAk%E7%9A%84%E6%9C%80%E5%B0%91%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%AD%97%E6%95%B0%E7%9B%AE/"},{"categories":["算法——字符串"],"content":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"题目一：至少有 K 个重复字符的最长子串 395. 至少有 K 个重复字符的最长子串 ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:1:0","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"题目解析 有两种方法： 递归分治解决：该分治法的应用对象：解决那种不会去跨越任何一个段更新答案的题目。比如此题这种关于字符串子串的题。首先在整个字符串大范围内可以确定哪些字符没有达到k次，故只要存在这些字符的子串都被排除在外。具体到递归分治的代码上就是： 分治的每一段都不含上一个总字符串的被 ban (被禁)字符，故每个分治的对象都要进行以下几个步骤： 一、计算 [l,r] 之间所有字符的出现次数。 二、根据出现次数计算出被 ban 掉的字符类型。 三、根据是否有被 ban 的字符类型，来确定是否还需要再往下递归分治，如果没有被 ban 的字符类型，则该字符串就是一个符合条件的字符串。否则继续往下递归分治，分治的子对象都不能含有该被 ban 字符类型。 根据枚举类型的滑动窗口解决：按照字符类型的滑动窗口技巧，最外层用于枚举固定当前窗口内最多有多少类型。这个技巧就是，当我们无法直接找到滑动窗口的边界时，我们可以根据有限的类型来构造滑动窗口的边界，由于最多有26个类型的字符，而我们滑动窗口过程可以根据某个类型的字符数量是否符合条件来得到窗口内符合条件的类型数量，只要窗口内的类型数量等于符合条件的类型数量，那么该窗口内的字符串即为一个答案。但问题是，我们不清楚窗口内的类型数量是多大，这也就是我们没有明确的边界，此时我们根据枚举 1~26 个类型来进行窗口的滑动限制即可。 ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:1:1","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"解题代码 法一：递归分治 class Solution { public: int dfs(string\u0026 s,int l,int r,int k){ int cnt[26] = {0}; for(int i=l;i\u003c=r;i++){//计算字符的次数方便计算ban掉的字符类型 cnt[s[i]-'a']++; } char split = 0; for(int i=0;i\u003c26;i++){ if(cnt[i]!=0\u0026\u0026cnt[i]\u003ck){ //很明显在这个大范围内要是这个字符类型次数都小于k了，肯定就是要被ban的 split = i+'a'; break; } } if(split==0)//如果该段字符串没有被ban的字符类型则该字符串就是符合条件的字符串 return r-l+1; int ret = 0; //开始枚举分治到下面去 while(l\u003c=r){ while(l\u003c=r\u0026\u0026s[l]==split){//不让起始点从被ban的字符开始 l++; } if(l\u003er) break;//说明全是被ban的字符 int start = l; while(l\u003c=r\u0026\u0026s[l]!=split){//计算本次分段的长度（结束位置） l++; } int length = dfs(s,start,l-1,k); ret = max(ret,length); } return ret; } int longestSubstring(string s, int k) { return dfs(s,0,s.size()-1,k); } }; 法二：枚举类型的滑动窗口 class Solution { public: int longestSubstring(string s, int k) { int n = s.size(); int cnt[26]{0};//用于记录窗口内字符的出现次数 int maxLen = 0; for(int i=1;i\u003c=26;i++){ memset(cnt,0,sizeof(cnt)); for(int l=0,r=0,sum=0,tot=0;r\u003cn;r++){//sum、tot分别表示窗口内的有效字符种类数和总的种类数 cnt[s[r]-'a']++; if(cnt[s[r]-'a']==1) //增加种类数 tot++; if(cnt[s[r]-'a']==k) //为了防止多次更新千万别取大于号 sum++; //增加有效种类数 while(tot\u003ei){ //达到收缩窗口条件，因为窗口内的种类数限制为i个 int dup = (s[l++]-'a'); cnt[dup]--; if(cnt[dup]==0) //种类数此时需要-1 tot--; if(cnt[dup]==k-1)//有效种类数-1 sum--; } if(sum==tot) maxLen = max(maxLen,r-l+1); } } return maxLen; } }; ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:1:2","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"题目二：最长的美好子字符串 最长的美好子字符串 ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:2:0","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"题目解析 这题虽然由于数据量的关系，被划分为简单题，但实际上完全不亚于第一题。甚至还得用上一些位运算的思想。 这题我会的做法只有两种： 普通位运算枚举法：其实就是此题的暴力解法，只是用了位运算使得更为优雅，由于此题需要求最长的大小写都包含的字符串，我们用一个int位来表示所有小写字母的出现，用另一个int位来表示所有大写字母的出现，则对于每个字符串，都可以通过这两个int是否相等来判断是否正好是大小写都含有，然后就是暴力的遍历所有子串的过程了。 递归分治法：此题和上题差不多，也是不会跨越任何一个段去更新答案，所以也能使用递归分治的方式来进行解决。但前期处理被ban字符和上题是不一样的，其余都一样，这题要根据 [l,r] 之间字符的位运算结果 lower 和 upper 再与运算得到两者的交集来判断是否有被 ban 字符，或者是不被ban的字符。 ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:2:1","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——字符串"],"content":"解题代码 方法一：位运算+暴力遍历 class Solution { public: string longestNiceSubstring(string s) { int sz = s.size(); int start,len=0; for(int i=0;i\u003csz;i++){ int lower = 0,upper = 0; for(int j=i;j\u003csz;j++){ if(islower(s[j])){ lower |= (1\u003c\u003c(s[j]-'a')); }else{ upper |= (1\u003c\u003c(s[j]-'A')); } if(lower==upper\u0026\u0026(j-i+1\u003elen)){ start = i; len = j-i+1; } } } return len==0?\"\":s.substr(start,len); } }; 方法二：递归分治 class Solution { public: void dfs(string \u0026s, int l, int r, pair\u003cint, int\u003e \u0026ret) { int lower = 0, upper = 0; for (int i = l; i \u003c= r; i++) { if (islower(s[i])) { lower |= (1 \u003c\u003c (s[i] - 'a')); } else { upper |= (1 \u003c\u003c (s[i] - 'A')); } } if (lower == upper) {//我之前这里的判断条件导致了无限循环，一旦满足第一个条件，但不满足第二个条件，就发生无限循环！！！所以注意放到下面去 if (r - l + 1 \u003e ret.second) { ret.first = l; ret.second = r - l + 1; } return; } int valid = lower \u0026 upper;//这两的交集代表大小写都出现的类型，为符合条件的类型，而被ban的就是不处于这里面的字符类型 int start; while (l \u003c= r) { while (l \u003c= r \u0026\u0026 !(valid \u0026 (1 \u003c\u003c (tolower(s[l]) - 'a')))) {//让start处于符合条件的类型 l++; } if (l \u003e r) break; start = l; while (l \u003c= r \u0026\u0026 (valid \u0026 (1 \u003c\u003c (tolower(s[l]) - 'a')))) {//得到符合条件的分段右端点 l++; } dfs(s, start, l - 1, ret); } } string longestNiceSubstring(string s) { pair\u003cint, int\u003e ret{0, 0}; dfs(s, 0, s.size() - 1, ret); return s.substr(ret.first, ret.second); } }; ","date":"2022-02-01","objectID":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/:2:2","tags":["递归分治、滑动窗口、位运算"],"title":"字符串类型滑动窗口或递归分治解被ban字符求最长子串","uri":"/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%88%96%E9%80%92%E5%BD%92%E5%88%86%E6%B2%BB%E8%A7%A3%E8%A2%ABban%E5%AD%97%E7%AC%A6%E6%B1%82%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"},{"categories":["算法——动态规划"],"content":"划分数问题","date":"2022-01-30","objectID":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/","tags":["划分数问题"],"title":"划分数问题","uri":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"题目 题目链接 ","date":"2022-01-30","objectID":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/:1:0","tags":["划分数问题"],"title":"划分数问题","uri":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"题目详解 划分数类型题目都是dp解决，而且都有固定的套路和公式，但我们还是需要在前人的公式上加以理解！ 划分数问题dp总结 我这里就提两个比较常见的划分数问题的dp原理： 如果对数字划分较为抽象，那么我们把这个数字可以比作苹果，即有n个苹果需要划分到m个盘子里面，而这几个盘子的顺序肯定是不考虑的，也就是5个苹果划分给3个盘子，则1 2 2和2 1 2是完全一样的情况，只看具体的数字组合不看内部排列！ 将n划分成不大于m的划分。 $dp[n][m] = dp[n-m][m]+dp[n][m-1]$ 对于以上的状态转移方程， dp[n-m][m] 表示n个苹果放入m个盘子中，无空盘的情况。 dp[n][m-1] 表示n个苹果放入m个盘子中，有空盘的情况(这就是划分成不大于m盘的关键所在)。这么写肯定是有些难以理解，但当你去举例子，将它递归往下写的时候，你就会发现这个dp[n][m-1] 包含了从 dp[n][1] 到 dp[n][m-1] 的所有情况！ 底层的基本case有： 当 n==1||m==1 ，即盘子或者苹果数量为1个的时候，那肯定就只有一种情况。即dp[n][m]=1。 当 n\u003em ，则还能继续划分即dp[n][m] = dp[n-m][m]+dp[n][m-1]。 当 n==m ，则有两种情况，当划分为m个时，结果为1，然后继续空盘子划分，即 dp[n][m]=dp[n][m-1]+1。 当 n\u003cm，由于可以空盘，所以是允许存在的，但此时不可能满盘，所以等于空盘的情况，即 dp[n][m] = dp[n][m-1]。 写成代码形式就是（我比较喜欢写记忆化dfs，毕竟不需要考虑初始化问题，只需考虑最后的跳出）： 以下的两个判断条件就把所有是以上四种情况包含在内了！ int memo[202][8];//记忆化的备忘录 //TODO 划分数记忆化方式 int dfs(int n,int m){ if(n\u003c0||m\u003c0) return 0; if(n==1||n==0||m==1) return 1; return memo[n][m] = dfs(n-m,m)+dfs(n,m-1); } 将n严格划分为m个数。(即n个苹果严格划分为m盘，不要有空位！) $dp[n][m] = dp[n-m][m]+dp[n-1][m-1]$ 下面给出我的一段手写推导： base case也在手写题解里面提到了，所以直接上代码： int memo[202][8]; //TODO 划分数n划成k份的记忆化方式 int dfs(int n,int k){ if(n\u003ck) return 0; if(n==k||k==1) return 1; if(memo[n][k])return memo[n][k]; return memo[n][k] = dfs(n-k, k) + dfs(n-1,k-1); } ","date":"2022-01-30","objectID":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/:2:0","tags":["划分数问题"],"title":"划分数问题","uri":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/"},{"categories":["算法——动态规划"],"content":"解题代码 前面已经介绍了两种划分数的dp，那么本题就属于第二种划分数的dp！ 直接把上面的代码拿下来直接秒！ #include\u003cbits/stdc++.h\u003eusing namespace std; using ll = long long; ll memo[202][8]; int n,k; //TODO 划分数记忆化方式 ll dfs(int a,int b){ if(a\u003cb) return 0; if(a==b||b==1) return 1; if(memo[a][b])return memo[a][b]; return memo[a][b] = dfs(a-b, b) + dfs(a-1,b-1); } int main(){ cin\u003e\u003en\u003e\u003ek; cout\u003c\u003cdfs(n, k); return 0; } ","date":"2022-01-30","objectID":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/:3:0","tags":["划分数问题"],"title":"划分数问题","uri":"/posts/%E5%88%92%E5%88%86%E6%95%B0%E9%97%AE%E9%A2%98/"},{"categories":["算法——路径更新问题"],"content":"leetcode每日一题——地图中的最高点","date":"2022-01-29","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/","tags":["leetcode每日一题——地图中的最高点"],"title":"leetcode每日一题——地图中的最高点","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/"},{"categories":["算法——路径更新问题"],"content":"题目 题目链接 ","date":"2022-01-29","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/:1:0","tags":["leetcode每日一题——地图中的最高点"],"title":"leetcode每日一题——地图中的最高点","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/"},{"categories":["算法——路径更新问题"],"content":"解题思路 两种解题思路，都是根据题目的意思更新路径信息即可： bfs思路：由于相邻的两个格子必须高度差为1，而水域必须高度为0，所以，直接以水域为bfs源点，进行bfs把整个区域的值给更新就行了。这是bfs思路。 dp思路：由于dp都依赖上一次更新的结果，而我们一般就是从左到右的遍历更新，而这题是和四个位置相关，所以，我们分为：从上到下从左到右更新，可以把依赖上和左的答案给更新，从下到上，从右到左更新，可以把依赖下和右的结果给更新完。 ","date":"2022-01-29","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/:2:0","tags":["leetcode每日一题——地图中的最高点"],"title":"leetcode每日一题——地图中的最高点","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/"},{"categories":["算法——路径更新问题"],"content":"解题代码 BFS代码 class Solution { public: const int dx[4]={-1,1,0,0}; const int dy[4]={0,0,-1,1}; int n,m; bool isValid(int x,int y){ return x\u003cn\u0026\u0026x\u003e=0\u0026\u0026y\u003cm\u0026\u0026y\u003e=0; } const int maxn = 1e3+5; vector\u003cvector\u003cint\u003e\u003e highestPeak(vector\u003cvector\u003cint\u003e\u003e\u0026 isWater) { n = isWater.size(); m = isWater[0].size(); bool visit[maxn][maxn]; memset(visit,0,sizeof(visit)); queue\u003cpair\u003cint,int\u003e\u003eQ; for(int i=0;i\u003cn;i++){ for(int j=0;j\u003cm;j++){ if(isWater[i][j]){ visit[i][j] = 1; isWater[i][j] = 0; Q.push({i,j}); } } } int step = 1; while(!Q.empty()){ for(int i=Q.size();i\u003e0;i--){ auto[x,y] = Q.front();Q.pop(); for(int k=0;k\u003c4;k++){ int nx = x+dx[k]; int ny = y+dy[k]; if(isValid(nx,ny)\u0026\u0026!visit[nx][ny]){ visit[nx][ny] = 1; isWater[nx][ny] = step; Q.push({nx,ny}); } } } step++; } return isWater; } }; dp代码 class Solution { public: vector\u003cvector\u003cint\u003e\u003e highestPeak(vector\u003cvector\u003cint\u003e\u003e\u0026 isWater) { int n = isWater.size(); int m = isWater[0].size(); vector\u003cvector\u003cint\u003e\u003e dp(n, vector\u003cint\u003e(m, 1e9+7)); for(int i=0; i\u003cn; i++) { //从上到下从左到右 for(int j=0; j\u003cm; j++) { if(isWater[i][j]) dp[i][j] = 0; else { if(i \u003e 0) dp[i][j] = min(dp[i][j], dp[i-1][j]+1); if(j \u003e 0) dp[i][j] = min(dp[i][j], dp[i][j-1]+1); } } } for(int i=n-1; i\u003e=0; i--) { //从下到上从右到左 for(int j=m-1; j\u003e=0; j--) { if(isWater[i][j]) dp[i][j] = 0; else { if(i \u003c n-1) dp[i][j] = min(dp[i][j], dp[i+1][j]+1); if(j \u003c m-1) dp[i][j] = min(dp[i][j], dp[i][j+1]+1); } } } return dp; } }; ","date":"2022-01-29","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/:3:0","tags":["leetcode每日一题——地图中的最高点"],"title":"leetcode每日一题——地图中的最高点","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E6%9C%80%E9%AB%98%E7%82%B9/"},{"categories":["算法——贪心"],"content":"leetcode每日一题-游戏中弱角色的数量","date":"2022-01-28","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/","tags":["利用排序基于遍历过程处理的贪心-游戏中弱角色的数量"],"title":"leetcode每日一题-游戏中弱角色的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——贪心"],"content":"题目 题目链接 ","date":"2022-01-28","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/:1:0","tags":["利用排序基于遍历过程处理的贪心-游戏中弱角色的数量"],"title":"leetcode每日一题-游戏中弱角色的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——贪心"],"content":"解题思路 一句话解决：以第一个字段为标准从大到小排序，然后再遍历数组，对比第二字段的最大值即可。 关键细节： 为了避免第一个字段相同的情况下被更新，所以在排序时采取，攻击力降序防御力升序的方式(关键)来进行。 这样就让第一个字段相同时，按照从左到右的遍历顺序是不可能把第一个字段相同的情况拿来更新 cnt 。 ","date":"2022-01-28","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/:2:0","tags":["利用排序基于遍历过程处理的贪心-游戏中弱角色的数量"],"title":"leetcode每日一题-游戏中弱角色的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——贪心"],"content":"解题代码 注意：golang 的代码中的断言型函数接口有点不一样。。它调用时用的是数组下标的形式来调用。 cpp version class Solution { public: //一句话解决：以第一个字段为标准从大到小排序，然后再遍历数组，对比第二字段的最大值即可。 //但为了避免第一个字段相同的情况下被更新，所以在排序时采取，攻击力降序防御力升序的方式(关键)来进行 //这样就让第一个字段相同时，按照从左到右的遍历顺序是不可能把第一个字段相同的情况拿来更新cnt。 int numberOfWeakCharacters(vector\u003cvector\u003cint\u003e\u003e\u0026 properties) { int n = properties.size(); auto cmp = [](vector\u003cint\u003e\u0026 t1,vector\u003cint\u003e\u0026t2){return t1[0]==t2[0]?t1[1]\u003ct2[1]:t1[0]\u003et2[0];}; sort(properties.begin(),properties.end(),cmp); int mx = INT_MIN; int cnt = 0; for(int i=0;i\u003cn;i++){ if(mx\u003eproperties[i][1]) cnt++; mx = max(mx,properties[i][1]); } return cnt; } }; java version class Solution { public int numberOfWeakCharacters(int[][] properties) { Arrays.sort(properties,(o1,o2)-\u003e o1[0]==o2[0]?o1[1]-o2[1]:o2[0]-o1[0]); int max = -1,cnt = 0; for(int[] p : properties){ if(p[1]\u003cmax) cnt++; max = Math.max(max,p[1]); } return cnt; } } golang version func numberOfWeakCharacters(properties [][]int) int { sort.Slice(properties,func (i int,j int) bool{//注意这个接口被写死只能用int型 p, q := properties[i], properties[j] return p[0] \u003e q[0] || p[0] == q[0] \u0026\u0026 p[1] \u003c q[1] }) var max = -1 cnt := 0 for _,v := range properties{ if max\u003ev[1] { cnt++ } max = int(math.Max(float64(max), float64(v[1]))) } return cnt } ","date":"2022-01-28","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/:3:0","tags":["利用排序基于遍历过程处理的贪心-游戏中弱角色的数量"],"title":"leetcode每日一题-游戏中弱角色的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——贪心"],"content":"收获 被坑了，往二分+哈希表方向去写了。完全没想到之间排序+遍历就能解决。。。 排序的处理非常之精髓。 ","date":"2022-01-28","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/:4:0","tags":["利用排序基于遍历过程处理的贪心-游戏中弱角色的数量"],"title":"leetcode每日一题-游戏中弱角色的数量","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%B8%B8%E6%88%8F%E4%B8%AD%E5%BC%B1%E8%A7%92%E8%89%B2%E7%9A%84%E6%95%B0%E9%87%8F/"},{"categories":["算法——计算几何"],"content":"leetcode每日一题-检测正方形","date":"2022-01-27","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/","tags":["leetcode每日一题-检测正方形"],"title":"leetcode每日一题-检测正方形","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/"},{"categories":["算法——计算几何"],"content":"题目 题目链接 ","date":"2022-01-27","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/:1:0","tags":["leetcode每日一题-检测正方形"],"title":"leetcode每日一题-检测正方形","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/"},{"categories":["算法——计算几何"],"content":"题目解析 注意此题为计算几何类型的题目，我认为这类问题最重要的就是把这个几何图形用好用的方法去表示出来。 三个重点： 点的表示：我们通过上下两点确定正方形的原则来表示，且点的存储方式一点不能用pair，这样效率及其低下，且难以有一定的自由度取操作 x，y 轴，我们采用哈希表套哈希表的方式取存储！ 点的记录：通过嵌套哈希表完成点的次数记录，比如： unordered_map\u003cint, unordered_map\u003cint, int\u003e\u003e cnt; cnt[y][x]++; 点的枚举：通过嵌套哈希表，可以很好的把 x，y 坐标的对应点数给限制住，所以我们要根据这点可以利用count方法直接把不是同一个纵轴的点给排除，比如： res += (colCnt.count(x) ? colCnt[x] : 0) * (yCnt.count(x + d) ? yCnt[x + d] : 0) * (colCnt.count(x + d)? colCnt[x + d] : 0); //一旦以上的count方法返回0，即表示该点不是同一个纵轴上的点，则得出结果0，整个res就相当于没有加任何数字。 再比如通过yCnt提前取出在同一个横轴的点： unordered_map\u003cint, int\u003e \u0026 yCnt = cnt[y]; //取和point在同一行的所有点 //这样后续的枚举过程直接可以套用yCnt[x+d]或者yCnt[x-d]来得到对应左右两种情况正方形的点个数。 最后一个优化：由于我们每次枚举同一纵轴上的点不确定是上面还是下面，实际上根本不重要，由于在计算时，我们都会把左右两种正方形的情况给计算完，而这个过程正好是两个相反的 +- 过程，所以无论枚举得到的正方形边长为正还是为负数，都不影响！ ","date":"2022-01-27","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/:2:0","tags":["leetcode每日一题-检测正方形"],"title":"leetcode每日一题-检测正方形","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/"},{"categories":["算法——计算几何"],"content":"解题代码 非常详细的注释了 class DetectSquares { public: unordered_map\u003cint, unordered_map\u003cint, int\u003e\u003e cnt; DetectSquares() { } void add(vector\u003cint\u003e point) { int x = point[0], y = point[1]; cnt[y][x]++; } int count(vector\u003cint\u003e point) { int res = 0; int x = point[0], y = point[1]; if (!cnt.count(y)) { return 0; } unordered_map\u003cint, int\u003e \u0026 yCnt = cnt[y]; //取和point在同一行的所有点 for (auto \u0026 [col, colCnt] : cnt) { if (col != y) {//由于我们构造正方形是根据上下两条边，故此处枚举的另外一点不能在同一行 // 根据对称性，这里可以不用取绝对值，具体而言就是所有情况根据+-已经包括 int d = col - y;//得到正方形边长，根据这个值可以直接取到对应的另外几个点(如果存在的话) //这里的colCnt.count(x)用于判断是否和当前点在同一个纵轴，这样才可能构造上下两边，故下面只要有1个为0，则加的都是0 //由于我们指定了必须是同一个纵轴上的点，所以不可能构造出两个相同的正方形！ res += (colCnt.count(x) ? colCnt[x] : 0) * (yCnt.count(x + d) ? yCnt[x + d] : 0) * (colCnt.count(x + d)? colCnt[x + d] : 0); res += (colCnt.count(x) ? colCnt[x] : 0) * (yCnt.count(x - d) ? yCnt[x - d] : 0) * (colCnt.count(x - d) ? colCnt[x - d] : 0); } } return res; } }; ","date":"2022-01-27","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/:3:0","tags":["leetcode每日一题-检测正方形"],"title":"leetcode每日一题-检测正方形","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/"},{"categories":["算法——计算几何"],"content":"总结 学到以下： 如何通过嵌套哈希表表示点，以及用它表示的而不用pair表示的好处。 重点好像也就上面那条。 ","date":"2022-01-27","objectID":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/:4:0","tags":["leetcode每日一题-检测正方形"],"title":"leetcode每日一题-检测正方形","uri":"/posts/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%A3%80%E6%B5%8B%E6%AD%A3%E6%96%B9%E5%BD%A2/"},{"categories":["JavaWeb笔记"],"content":"Java网络编程","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"Java网络编程 配套视频 在JavaSE阶段，我们学习了I/O流，既然I/O流如此强大，那么能否跨越不同的主机进行I/O操作呢？这就要提到Java的网络编程了。 注意：本章会涉及到计算机网络相关内容（只会讲解大致内容，不会完整的讲解计算机网络知识） 若没有计算机网络基础，实际上Java这里封装的Socket网络IO操作已经简单到完全不需要任何基础的，但是没有计算机网络相关的底层知识，以后的深入学习会非常不知所云。 ","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:0:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"计算机网络基础 利用通信线路和通信设备，将地理位置不同的、功能独立的多台计算机互连起来，以功能完善的网络软件来实现资源共享和信息传递，就构成了计算机网络系统。 比如我们家里的路由器，通过将我们的设备（手机、平板、电脑、电视剧）连接到路由器，来实现对互联网的访问。实际上，我们的路由器连接在互联网上，而我们的设备又连接了路由器，这样我们的设备就可以通过路由器访问到互联网了。通过网络，我们可以直接访问互联网上的另一台主机，比如我们要把QQ的消息发送给我们的朋友，或是通过远程桌面管理来操作另一台电脑，也可以是连接本地网络上的打印机。 既然我们可以通过网络访问其他计算机，那么如何区别不同的计算机呢？通过IP地址，我们就可以区分不同的计算机了： 每一台电脑在同一个网络上都有一个自己的IP地址，用于区别于其他的电脑，我们可以通过对方主机的IP地址对其进行访问。那么我手机连接的移动流量，能访问到连接家里路由器的电脑吗？（不能，因为他们不属于同一个网络） 而我们的电脑上可能运行着大量的程序，每一个程序可能都需要通过网络来访问其他计算机，那这时该如何区分呢？我们可以通过端口号来区分： 因此，我们一般看到的是这样的：192.168.0.11:8080，通过IP:端口的形式来访问目标主机上的一个应用程序服务。注意端口号只能是0-65535之间的值！ IP地址分为IPv4和IPv6，IPv4类似于192.168.0.11，我们上面提到的例子都是使用的IPv4，它一共有四组数字，每组数字占8个bit位，IPv4地址0.0.0.0表示为2进制就是：00000000.00000000.00000000.00000000，共32个bit，最大为255.255.255.255，实际上，IPv4能够表示的所有地址，早就已经被用完了。IPv6能够保存128个bit位，因此它也可以表示更多的IP地址，一个IPv6地址看起来像这样：1030::C9B4:FF12:48AA:1A2B，目前也正在向IPv6的阶段过度。 TCP和UDP是两种不同的传输层协议： TCP：当一台计算机想要与另一台计算机通讯时，两台计算机之间的通信需要畅通且可靠（会进行三次握手，断开也会进行四次挥手），这样才能保证正确收发数据，因此TCP更适合一些可靠的数据传输场景。 UDP：它是一种无连接协议，数据想发就发，而且不会建立可靠传输，也就是说传输过程中有可能会导致部分数据丢失，但是它比TCP传输更加简单高效，适合视频直播之类的。 ","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:1:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"了解Socket技术 通过Socket技术（它是计算机之间进行通信的一种约定或一种方式），我们就可以实现两台计算机之间的通信，Socket也被翻译为套接字，是操作系统底层提供的一项通信技术，它支持TCP和UDP。而Java就对socket底层支持进行了一套完整的封装，我们可以通过Java来实现Socket通信。 要实现Socket通信，我们必须创建一个数据发送者和一个数据接收者，也就是客户端和服务端，我们需要提前启动服务端，来等待客户端的连接，而客户端只需要随时启动去连接服务端即可！ //服务端 public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); Socket socket = server.accept(); //当没有客户端连接时，线程会阻塞，直到有客户端连接为止 System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); }catch (IOException e){ e.printStackTrace(); } } //客户端 public static void main(String[] args) { try (Socket socket = new Socket(\"localhost\", 8080)){ System.out.println(\"已连接到服务端！\"); }catch (IOException e){ System.out.println(\"服务端连接失败！\"); e.printStackTrace(); } } 实际上它就是一个TCP连接的建立过程： 一旦TCP连接建立，服务端和客户端之间就可以相互发送数据，直到客户端主动关闭连接。当然，服务端不仅仅只可以让一个客户端进行连接，我们可以尝试让服务端一直运行来不断接受客户端的连接： public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); while (true){ //无限循环等待客户端连接 Socket socket = server.accept(); System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); } }catch (IOException e){ e.printStackTrace(); } } 现在我们就可以多次去连接此服务端了。 ","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:2:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"使用Socket进行数据传输 通过Socket对象，我们就可以获取到对应的I/O流进行网络数据传输： public static void main(String[] args) { try (Socket socket = new Socket(\"localhost\", 8080); Scanner scanner = new Scanner(System.in)){ System.out.println(\"已连接到服务端！\"); OutputStream stream = socket.getOutputStream(); OutputStreamWriter writer = new OutputStreamWriter(stream); //通过转换流来帮助我们快速写入内容 System.out.println(\"请输入要发送给服务端的内容：\"); String text = scanner.nextLine(); writer.write(text+'\\n'); //因为对方是readLine()这里加个换行符 writer.flush(); System.out.println(\"数据已发送：\"+text); }catch (IOException e){ System.out.println(\"服务端连接失败！\"); e.printStackTrace(); }finally { System.out.println(\"客户端断开连接！\"); } } } public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); Socket socket = server.accept(); System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); //通过 System.out.print(\"接收到客户端数据：\"); System.out.println(reader.readLine()); socket.close(); //和服务端TCP连接完成之后，记得关闭socket }catch (IOException e){ e.printStackTrace(); } } 同理，既然服务端可以读取客户端的内容，客户端也可以在发送后等待服务端给予响应： public static void main(String[] args) { try (Socket socket = new Socket(\"localhost\", 8080); Scanner scanner = new Scanner(System.in)){ System.out.println(\"已连接到服务端！\"); OutputStream stream = socket.getOutputStream(); OutputStreamWriter writer = new OutputStreamWriter(stream); //通过转换流来帮助我们快速写入内容 System.out.println(\"请输入要发送给服务端的内容：\"); String text = scanner.nextLine(); writer.write(text+'\\n'); //因为对方是readLine()这里加个换行符 writer.flush(); System.out.println(\"数据已发送：\"+text); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); System.out.println(\"收到服务器返回：\"+reader.readLine()); }catch (IOException e){ System.out.println(\"服务端连接失败！\"); e.printStackTrace(); }finally { System.out.println(\"客户端断开连接！\"); } } public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); Socket socket = server.accept(); System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); //通过 System.out.print(\"接收到客户端数据：\"); System.out.println(reader.readLine()); OutputStreamWriter writer = new OutputStreamWriter(socket.getOutputStream()); writer.write(\"已收到！\"); writer.flush(); }catch (IOException e){ e.printStackTrace(); } } 我们可以手动关闭单向的流： socket.shutdownOutput(); //关闭输出方向的流 socket.shutdownInput(); //关闭输入方向的流 如果我们不希望服务端等待太长的时间，我们可以通过调用setSoTimeout()方法来设定IO超时时间： socket.setSoTimeout(3000); 当超过设定时间都依然没有收到客户端或是服务端的数据时，会抛出异常： java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) at java.io.InputStreamReader.read(InputStreamReader.java:184) at java.io.BufferedReader.fill(BufferedReader.java:161) at java.io.BufferedReader.readLine(BufferedReader.java:324) at java.io.BufferedReader.readLine(BufferedReader.java:389) at com.test.Main.main(Main.java:41) 我们之前使用的都是通过构造方法直接连接服务端，那么是否可以等到我们想要的时候再去连接呢？ try (Socket socket = new Socket(); //调用无参构造不会自动连接 Scanner scanner = new Scanner(System.in)){ socket.connect(new InetSocketAddress(\"localhost\", 8080), 1000); //手动调用connect方法进行连接 如果连接的双方发生意外而通知不到对方，导致一方还持有连接，这样就会占用资源，因此我们可以使用setKeepAlive()方法来防止此类情况发生： socket.setKeepAlive(true); 当客户端连接后，如果设置了keeplive为 true，当对方没有发送任何数据过来，超过一个时间(看系统内核参数配置)，那么我们这边会发送一个ack探测包发到对方，探测双方的TCP/IP连接是否有效。 TCP在传输过程中，实际上会有一个缓冲区用于数据的发送和接收： 此缓冲区大小为：8192，我们可以手动调整其大小来优化传输效率： socket.setReceiveBufferSize(25565); /","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:3:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"使用Socket传输文件 既然Socket为我们提供了IO流便于数据传输，那么我们就可以轻松地实现文件传输了。 ","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:4:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["JavaWeb笔记"],"content":"使用浏览器访问Socket服务器 在了解了如何使用Socket传输文件后，我们来看看，浏览器是如何向服务器发起请求的： public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); Socket socket = server.accept(); System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); InputStream in = socket.getInputStream(); //通过 System.out.println(\"接收到客户端数据：\"); while (true){ int i = in.read(); if(i == -1) break; System.out.print((char) i); } }catch (Exception e){ e.printStackTrace(); } } 我们现在打开浏览器，输入http://localhost:8080或是http://127.0.0.1:8080/，来连接我们本地开放的服务器。 我们发现浏览器是无法打开这个链接的，但是我们服务端却收到了不少的信息： GET / HTTP/1.1\rHost: 127.0.0.1:8080\rConnection: keep-alive\rCache-Control: max-age=0\rsec-ch-ua: \"Chromium\";v=\"94\", \"Google Chrome\";v=\"94\", \";Not A Brand\";v=\"99\"\rsec-ch-ua-mobile: ?0\rsec-ch-ua-platform: \"macOS\"\rUpgrade-Insecure-Requests: 1\rUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\rAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\rSec-Fetch-Site: none\rSec-Fetch-Mode: navigate\rSec-Fetch-User: ?1\rSec-Fetch-Dest: document\rAccept-Encoding: gzip, deflate, br\rAccept-Language: zh-CN,zh;q=0.9,und;q=0.8,en;q=0.7\r实际上这些内容都是Http协议规定的请求头内容。HTTP是一种应用层协议，全称为超文本传输协议，它本质也是基于TCP协议进行数据传输，因此我们的服务端能够读取HTTP请求。但是Http协议并不会保持长连接，在得到我们响应的数据后会立即关闭TCP连接。 既然使用的是Http连接，如果我们的服务器要支持响应HTTP请求，那么就需要按照HTTP协议的规则，返回一个规范的响应文本，首先是响应头，它至少要包含一个响应码： HTTP/1.1 200 Accpeted\r然后就是响应内容（注意一定要换行再写），我们尝试来编写一下支持HTTP协议的响应内容： public static void main(String[] args) { try(ServerSocket server = new ServerSocket(8080)){ //将服务端创建在端口8080上 System.out.println(\"正在等待客户端连接...\"); Socket socket = server.accept(); System.out.println(\"客户端已连接，IP地址为：\"+socket.getInetAddress().getHostAddress()); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); //通过 System.out.println(\"接收到客户端数据：\"); while (reader.ready()) System.out.println(reader.readLine()); //ready是判断当前流中是否还有可读内容 OutputStreamWriter writer = new OutputStreamWriter(socket.getOutputStream()); writer.write(\"HTTP/1.1 200 Accepted\\r\\n\"); //200是响应码，Http协议规定200为接受请求，400为错误的请求，404为找不到此资源（不止这些，还有很多） writer.write(\"\\r\\n\"); //在请求头写完之后还要进行一次换行，然后写入我们的响应实体（会在浏览器上展示的内容） writer.write(\"lbwnb!\"); writer.flush(); }catch (Exception e){ e.printStackTrace(); } } 我们可以打开浏览器的开发者模式（这里推荐使用Chrome/Edge浏览器，按下F12即可打开），我们来观察一下浏览器的实际请求过程。 ","date":"2022-01-26","objectID":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/:5:0","tags":["JavaWeb第一节"],"title":"Java网络编程","uri":"/posts/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"categories":["Linux网络编程"],"content":"TCP网络编程中connect、listen、accept三者之间的关系","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":" 基于 TCP 的网络编程开发分为服务器端和客户端两部分，常见的核心步骤和流程如下： 整个函数调用(image)\"\r整个函数调用(image)\r ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:0:0","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"accept()函数 对于客户端的 connect() 函数，该函数的功能为客户端主动连接服务器，建立连接是通过三次握手，而这个连接的过程是由内核完成，不是这个函数完成的，这个函数的作用仅仅是通知 Linux 内核，让 Linux 内核自动完成 TCP 三次握手连接，最后把连接的结果返回给这个函数的返回值（成功连接为0， 失败为-1）。 通常的情况，客户端的 connect() 函数默认会一直阻塞，直到三次握手成功或超时失败才返回（正常的情况，这个过程很快完成）。 ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:1:0","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"listen()函数 对于服务器，它是被动连接的。举一个生活中的例子，通常的情况下，移动的客服（相当于服务器）是等待着客户（相当于客户端）电话的到来。而这个过程，需要调用listen()函数。 #include\u003csys/socket.h\u003eint listen(int sockfd, int backlog); listen() 函数的主要作用就是将套接字( sockfd )变成被动的连接监听套接字（被动等待客户端的连接），至于参数 backlog 的作用是设置内核中连接队列的长度（这个长度有什么用，后面做详细的解释），TCP 三次握手也不是由这个函数完成，listen()的作用仅仅告诉内核一些信息。 这样的话，当有一个客户端主动连接 connect()，Linux 内核就自动完成TCP 三次握手，将建立好的链接自动存储到队列中，如此重复。 所以，只要 TCP 服务器调用了 listen()，客户端就可以通过 connect() 和服务器建立连接，而这个连接的过程是由内核完成。 三次握手\"\r三次握手\r 下面为测试的服务器和客户端代码，运行程序时，要先运行服务器，再运行客户端： 服务器： #include \u003cstdio.h\u003e#include \u003cstdlib.h\u003e#include \u003cstring.h\u003e #include \u003cunistd.h\u003e#include \u003csys/socket.h\u003e#include \u003cnetinet/in.h\u003e#include \u003carpa/inet.h\u003e int main(int argc, char *argv[]) { unsigned short port = 8000; int sockfd; sockfd = socket(AF_INET, SOCK_STREAM, 0);// 创建通信端点：套接字 if(sockfd \u003c 0) { perror(\"socket\"); exit(-1); } struct sockaddr_in my_addr; bzero(\u0026my_addr, sizeof(my_addr)); my_addr.sin_family = AF_INET; my_addr.sin_port = htons(port); my_addr.sin_addr.s_addr = htonl(INADDR_ANY); int err_log = bind(sockfd, (struct sockaddr*)\u0026my_addr, sizeof(my_addr)); if( err_log != 0) { perror(\"binding\"); close(sockfd); exit(-1); } err_log = listen(sockfd, 10); if(err_log != 0) { perror(\"listen\"); close(sockfd); exit(-1); } printf(\"listen client @port=%d...\\n\",port); sleep(10); // 延时10s system(\"netstat -an | grep 8000\"); // 查看连接状态 return 0; } 客户端 #include \u003cstdio.h\u003e#include \u003cunistd.h\u003e#include \u003cstring.h\u003e#include \u003cstdlib.h\u003e#include \u003carpa/inet.h\u003e#include \u003csys/socket.h\u003e#include \u003cnetinet/in.h\u003eint main(int argc, char *argv[]) { unsigned short port = 8000; // 服务器的端口号 char *server_ip = \"10.221.20.12\"; // 服务器ip地址 int sockfd; sockfd = socket(AF_INET, SOCK_STREAM, 0);// 创建通信端点：套接字 if(sockfd \u003c 0) { perror(\"socket\"); exit(-1); } struct sockaddr_in server_addr; bzero(\u0026server_addr,sizeof(server_addr)); // 初始化服务器地址 server_addr.sin_family = AF_INET; server_addr.sin_port = htons(port); inet_pton(AF_INET, server_ip, \u0026server_addr.sin_addr); int err_log = connect(sockfd, (struct sockaddr*)\u0026server_addr, sizeof(server_addr)); // 主动连接服务器 if(err_log != 0) { perror(\"connect\"); close(sockfd); exit(-1); } system(\"netstat -an | grep 8000\"); // 查看连接状态 while(1); return 0; } 运行结果： ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:2:0","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"三次握手的连接队列 这里详细的介绍一下 listen() 函数的第二个参数（ backlog）的作用：告诉内核连接队列的长度。 为了更好的理解 backlog 参数，我们必须认识到内核为任何一个给定的监听套接口维护两个队列： 未完成连接队列（incomplete connection queue），以某个客户发出并到达服务器，而服务器正在等待完成相应的 TCP 三次握手过程。这些端口都处于处于 SYN_RCVD 状态。 已完成连接队列（completed connection queue），这些端口处于 ESTABLISHED 状态。 当来自客户的 SYN 到达时，TCP 在未完成连接队列中创建一个新项，然后响应以三次握手的第二个分节：服务器的 SYN 响应，其中稍带对客户 SYN 的 ACK（即SYN+ACK），这一项一直保留在未完成连接队列中，直到三次握手的第三个分节（客户对服务器 SYN 的 ACK ）到达或者该项超时为止（源自Berkeley的实现为这些未完成连接的项设置的超时值为75秒）。 如果三次握手正常完成，该项就从未完成连接队列移到已完成连接队列的队尾。 backlog 参数历史上被定义为上面两个队列的大小之和，大多数实现默认值为 5，当服务器把这个完成连接队列的某个连接取走后，这个队列的位置又空出一个，这样来回实现动态平衡，但在高并发 web 服务器中此值显然不够。 ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:3:0","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"accept()函数 accept()函数功能是，从处于 established 状态的连接队列头部取出一个已经完成的连接，如果这个队列没有已经完成的连接，accept()函数就会阻塞，直到取出队列中已完成的用户连接为止。 如果，服务器不能及时调用 accept() 取走队列中已完成的连接，队列满掉后会怎样呢？UNP（《unix网络编程》）告诉我们，服务器的连接队列满掉后，服务器不会对再对建立新连接的syn进行应答，所以客户端的 connect 就会返回 ETIMEDOUT。但实际上Linux的并不是这样的！ ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:4:0","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"实验 下面为测试代码，服务器 listen() 函数只指定队列长度为 2，客户端有 6 个不同的套接字主动连接服务器，同时，保证客户端的 6 个 connect()函数都先调用完毕，服务器的 accpet() 才开始调用。 服务端： #include \u003cstdio.h\u003e#include \u003cstdlib.h\u003e#include \u003cstring.h\u003e #include \u003cunistd.h\u003e#include \u003csys/socket.h\u003e#include \u003cnetinet/in.h\u003e#include \u003carpa/inet.h\u003e int main(int argc, char *argv[]) { unsigned short port = 8000; int sockfd = socket(AF_INET, SOCK_STREAM, 0); if(sockfd \u003c 0) { perror(\"socket\"); exit(-1); } struct sockaddr_in my_addr; bzero(\u0026my_addr, sizeof(my_addr)); my_addr.sin_family = AF_INET; my_addr.sin_port = htons(port); my_addr.sin_addr.s_addr = htonl(INADDR_ANY); int err_log = bind(sockfd, (struct sockaddr*)\u0026my_addr, sizeof(my_addr)); if( err_log != 0) { perror(\"binding\"); close(sockfd); exit(-1); } err_log = listen(sockfd, 2); // 等待队列为2 if(err_log != 0) { perror(\"listen\"); close(sockfd); exit(-1); } printf(\"after listen\\n\"); sleep(20); //延时 20秒 printf(\"listen client @port=%d...\\n\",port); int i = 0; while(1) { struct sockaddr_in client_addr; char cli_ip[INET_ADDRSTRLEN] = \"\"; socklen_t cliaddr_len = sizeof(client_addr); int connfd; connfd = accept(sockfd, (struct sockaddr*)\u0026client_addr, \u0026cliaddr_len); if(connfd \u003c 0) { perror(\"accept\"); continue; } inet_ntop(AF_INET, \u0026client_addr.sin_addr, cli_ip, INET_ADDRSTRLEN); printf(\"-----------%d------\\n\", ++i); printf(\"client ip=%s,port=%d\\n\", cli_ip,ntohs(client_addr.sin_port)); char recv_buf[512] = {0}; while( recv(connfd, recv_buf, sizeof(recv_buf), 0) \u003e 0 ) { printf(\"recv data ==%s\\n\",recv_buf); break; } close(connfd); //关闭已连接套接字 //printf(\"client closed!\\n\"); } close(sockfd); //关闭监听套接字 return 0; } 客户端： #include \u003cstdio.h\u003e#include \u003cunistd.h\u003e#include \u003cstring.h\u003e#include \u003cstdlib.h\u003e#include \u003carpa/inet.h\u003e#include \u003csys/socket.h\u003e#include \u003cnetinet/in.h\u003e void test_connect() { unsigned short port = 8000; // 服务器的端口号 char *server_ip = \"10.221.20.12\"; // 服务器ip地址 int sockfd; sockfd = socket(AF_INET, SOCK_STREAM, 0);// 创建通信端点：套接字 if(sockfd \u003c 0) { perror(\"socket\"); exit(-1); } struct sockaddr_in server_addr; bzero(\u0026server_addr,sizeof(server_addr)); // 初始化服务器地址 server_addr.sin_family = AF_INET; server_addr.sin_port = htons(port); inet_pton(AF_INET, server_ip, \u0026server_addr.sin_addr); int err_log = connect(sockfd, (struct sockaddr*)\u0026server_addr, sizeof(server_addr)); // 主动连接服务器 if(err_log != 0) { perror(\"connect\"); close(sockfd); exit(-1); } printf(\"err_log ========= %d\\n\", err_log); char send_buf[100]=\"this is for test\"; send(sockfd, send_buf, strlen(send_buf), 0); // 向服务器发送信息 system(\"netstat -an | grep 8000\"); // 查看连接状态 //close(sockfd); } int main(int argc, char *argv[]) { pid_t pid; pid = fork(); if(0 == pid){ test_connect(); // 1 pid_t pid = fork(); if(0 == pid){ test_connect(); // 2 }else if(pid \u003e 0){ test_connect(); // 3 } }else if(pid \u003e 0){ test_connect(); // 4 pid_t pid = fork(); if(0 == pid){ test_connect(); // 5 }else if(pid \u003e 0){ test_connect(); // 6 } } while(1); return 0; } 服务器调用 accept()函数前延时了 20 秒。保证了客户端的 connect() 全部调用完毕后再调用 accept(),运行结果如下： 客户端运行效果图： 按照 UNP 的说法，连接队列满后（这里设置长度为 2，发了 6 个连接），以后再调用 connect() 应该统统超时失败，但实际上测试结果是：有的 connect()立刻成功返回了，有的经过明显延迟后成功返回了。对于服务器 accpet() 函数也是这样的结果：有的立马成功返回，有的延迟后成功返回。 对于上面服务器的代码，我们把lisen()的第二个参数改为 0 ，重新运行程序，发现： 客户端 connect() 全部返回连接成功（有些会延时）： 服务器 accpet() 函数却不能把连接队列的所有连接都取出来： 对于上面服务器的代码，我们把lisen()的第二个参数改为大于 6 的数(如 10)，重新运行程序，发现，客户端 connect() 立马返回连接成功， 服务器 accpet() 函数也立马返回成功。 ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:4:1","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["Linux网络编程"],"content":"总结 TCP 的连接队列满后，Linux 不会如书中所说的拒绝连接，只是有些会延时连接，但千万注意此时accept()就不一定能把已经建立好的连接全部取出来（如：当队列的长度指定为 0 ），写程序时服务器的 listen() 的第二个参数最好还是根据需要填写，写太大不好（具体可以看cat /proc/sys/net/core/somaxconn，默认最大值限制是 128），浪费资源，写太小也不好，延时建立连接。 ","date":"2022-01-26","objectID":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/:4:2","tags":["TCP网络编程中connect、listen、accept三者之间的关系"],"title":"TCP网络编程中connect、listen、accept三者之间的关系","uri":"/posts/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%ADconnectlistenaccept%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["现代C++语法"],"content":"C++与python文件系统对比","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"C++17 和 python 中好用的文件操作 | filesystem | os | shutil C++ 17 python 功能 filesystem::path::is_absolute() os.path.isabs() 判断是否为绝对路径 filesystem::path::parent_path() os.path.dirname() 路径分割 filesystem::path::filename() os.path.basename() 路径分割 filesystem::operator/() os.path.join() 路径拼接 filesystem::current_path() os.getcwd() 获取当前路径 filesystem::directory_iterator os.listdir() 返回指定目录下的所有文件/文件夹 filesystem::recursive_directory_iterator os.walk() 递归返回指定目录下的所有文件/文件夹 filesystem::exists() os.path.exists() 判断路径是否存在 filesystem::is_regular_file() os.path.isfile() 判断路径是文件还是目录 filesystem::is_directory() os.path.isdir() 判断路径是文件还是目录 filesystem::absolute() os.path.abspath() 返回绝对路径 filesystem::copy_file() shutil.copyfile() 文件拷贝 filesystem::remove() os.remove() 文件删除 filesystem::copy() shutil.copytree 路径拷贝 filesystem::remove_all shutil.rmtree() 路径删除 ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:0:0","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"filesystem::path vs. os.path filesystem::path是一个类，里面封装了很多方法，我们通过实例化之后直接调用方法。 os.path是一个模块，里面有很多函数，可以直接调用。 ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:1:0","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"判断是否为绝对路径 什么是绝对路径？我个人的理解是从根目录开始的就是绝对路径，例如/usr/local和C:\\\\Users，其余都是相对路径。可以发现，在不同操作系统中路径的分割符是不同的。同时在相对路径中./和../有特殊含义，./表示当前目录，../表示上一层目录，相应地，../../就是上两层目录。 1.filesystem::path中提供了判断是否为绝对路径/相对路径方法。 _LIBCPP_INLINE_VISIBILITY bool is_absolute() const { return has_root_directory(); } _LIBCPP_INLINE_VISIBILITY bool is_relative() const { return !is_absolute(); } 可以发现，判断相对路径的结果就是绝对路径取反。 void eg1_1() { /*判断是否为绝对路径*/ // std::filesystem::path abs_path = \"C:\\\\Users\"; std::filesystem::path abs_path = \"/usr/local\"; // 注意，实例化path的时候可以直接用等号 std::cout \u003c\u003c \"abs_path.is_absolute() : \" \u003c\u003c abs_path.is_absolute() \u003c\u003c std::endl; std::cout \u003c\u003c \"abs_path.is_relative() : \" \u003c\u003c abs_path.is_relative() \u003c\u003c std::endl; std::filesystem::path rel_path = \"../\"; std::cout \u003c\u003c \"rel_path.is_absolute() : \" \u003c\u003c rel_path.is_absolute() \u003c\u003c std::endl; std::cout \u003c\u003c \"rel_path.is_relative() : \" \u003c\u003c rel_path.is_relative() \u003c\u003c std::endl; } abs_path.is_absolute() : 1 abs_path.is_relative() : 0 rel_path.is_absolute() : 0 rel_path.is_relative() : 1 os.path中提供了isabs()函数用于判断是否为绝对路径。 def eg1_1(): \"\"\"判断是否为绝对路径\"\"\" # abs_path = \"C:\\\\Users\" abs_path = \"/usr/local\" print(\"os.path.isabs({}) : {}\".format(abs_path, os.path.isabs(abs_path))) rel_path = \"../\" print(\"os.path.isabs({}) : {}\".format(rel_path, os.path.isabs(rel_path))) os.path.isabs(/usr/local) : True os.path.isabs(../) : False ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:1:1","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"路径分割 filesystem::path中提供了路径分割的方法。 // decomposition _LIBCPP_INLINE_VISIBILITY path root_name() const { return string_type(__root_name()); } _LIBCPP_INLINE_VISIBILITY path root_directory() const { return string_type(__root_directory()); } _LIBCPP_INLINE_VISIBILITY path root_path() const { return root_name().append(string_type(__root_directory())); } _LIBCPP_INLINE_VISIBILITY path relative_path() const { return string_type(__relative_path()); } _LIBCPP_INLINE_VISIBILITY path parent_path() const { return string_type(__parent_path()); } _LIBCPP_INLINE_VISIBILITY path filename() const { return string_type(__filename()); } _LIBCPP_INLINE_VISIBILITY path stem() const { return string_type(__stem()); } _LIBCPP_INLINE_VISIBILITY path extension() const { return string_type(__extension()); } void eg1_2() { /*路径分割*/ std::filesystem::path path = \"../test_dir/1.txt\"; std::cout \u003c\u003c \"path.relative_path() : \" \u003c\u003c path.relative_path() \u003c\u003c std::endl; std::cout \u003c\u003c \"path.parent_path() : \" \u003c\u003c path.parent_path() \u003c\u003c std::endl; std::cout \u003c\u003c \"path.filename() : \" \u003c\u003c path.filename() \u003c\u003c std::endl; std::cout \u003c\u003c \"path.stem() : \" \u003c\u003c path.stem() \u003c\u003c std::endl; std::cout \u003c\u003c \"path.extension() : \" \u003c\u003c path.extension() \u003c\u003c std::endl; } path.relative_path() : \"../test_dir/1.txt\" path.parent_path() : \"../test_dir\" path.filename() : \"1.txt\" path.stem() : \"1\" path.extension() : \".txt\" os.path中提供了分割函数split()以及dirname()，basename()。 def eg1_2(): \"\"\"路径分割\"\"\" path = \"../test_dir/1.txt\" print(\"os.path.split(path) : {}\".format(os.path.split(path))) print(\"os.path.dirname(path) : {}\".format(os.path.dirname(path))) print(\"os.path.basename(path) : {}\".format(os.path.basename(path))) os.path.split(path) : ('../test_dir', '1.txt') os.path.dirname(path) : ../test_dir os.path.basename(path) : 1.txt ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:1:2","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"路径拼接 filesystem::path中重载了符号/和/=。 friend _LIBCPP_INLINE_VISIBILITY path operator/(const path\u0026 __lhs, const path\u0026 __rhs) { path __result(__lhs); __result /= __rhs; return __result; } void eg1_3() { /*路径拼接*/ std::filesystem::path path = \"../test_dir\"; std::filesystem::path txt1_path = path / \"1.txt\"; std::filesystem::path txt2_path = path / \"1_dir\" / \"2.txt\"; std::cout \u003c\u003c \"txt1_path : \" \u003c\u003c txt1_path \u003c\u003c std::endl; std::cout \u003c\u003c \"txt2_path : \" \u003c\u003c txt2_path \u003c\u003c std::endl; path /= \"1.txt\"; std::cout \u003c\u003c \"path : \" \u003c\u003c path \u003c\u003c std::endl; } txt1_path : \"../test_dir/1.txt\" txt2_path : \"../test_dir/1_dir/2.txt\" path : \"../test_dir/1.txt\" os.path中提供了join()函数。 def eg1_3(): \"\"\"路径拼接\"\"\" path = \"../test_dir\" txt1_path = os.path.join(path, \"1.txt\") txt2_path = os.path.join(path, \"1_dir\", \"2.txt\") print(\"txt1_path : {}\".format(txt1_path)) print(\"txt2_path : {}\".format(txt2_path)) txt1_path : ../test_dir/1.txt txt2_path : ../test_dir/1_dir/2.txt 注意，不同操作系统的分隔符不同，所以在Windows中运行结果如下。 txt1_path : ../test_dir\\1.txt txt2_path : ../test_dir\\1_dir\\2.txt ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:1:3","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"filesystem vs. os ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:0","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"获取当前工作目录 filesystem提供了获取当前路径的函数current_path()，注意返回的是绝对路径。 void eg2_1() { /*获取当前路径*/ std::filesystem::path current_path = std::filesystem::current_path(); std::cout \u003c\u003c \"current_path : \" \u003c\u003c current_path \u003c\u003c std::endl; } current_path : \"/Users/xxx/Github/intro_to_C-python/xxx/cmake-build-debug\" os提供了函数getcwd()。 def eg2_1(): \"\"\"获取当前路径\"\"\" current_path = os.getcwd() print(\"current_path : {}\".format(current_path)) current_path : /Users/xxx/Github/intro_to_C-python/xxx 这里先展示一下目录树，方便理解后边的例子。 └── test_dir ├── 1.txt └── 1_dir ├── 2.txt └── empty_dir ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:1","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"返回指定目录下的所有文件/文件夹 filesystem的类directory_iterator可以实现该功能。 void eg2_2() { /*返回指定目录下的所有文件/文件夹*/ std::filesystem::directory_iterator iter(\"../test_dir\"); for(const auto \u0026i : iter) { std::cout \u003c\u003c i.path() \u003c\u003cstd::endl; } } \"../test_dir/1.txt\" \"../test_dir/1_dir\" os中提供了函数listdir()。 def eg2_2(): \"\"\"返回指定目录下的所有文件/文件夹\"\"\" for i in os.listdir(\"./test_dir\"): print(i) 1.txt 1_dir ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:2","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"递归返回指定目录下的所有文件/文件夹 filesystem的类recursive_directory_iterator可以实现该功能。 void eg2_3() { /*递归返回指定目录下的所有文件/文件夹*/ std::filesystem::recursive_directory_iterator iter(\"../test_dir\"); for(const auto \u0026i : iter) { std::cout \u003c\u003c i.path() \u003c\u003cstd::endl; } } \"../test_dir/1.txt\" \"../test_dir/1_dir\" \"../test_dir/1_dir/empty_dir\" \"../test_dir/1_dir/2.txt\" os中提供了函数walk()。 def eg2_3(): \"\"\"递归返回指定目录下的所有文件/文件夹\"\"\" for root, dirs, files in os.walk(\"./test_dir\"): print(\"root : {}, dirs : {}, files : {}\".format(root, dirs, files)) root : ./test_dir, dirs : ['1_dir'], files : ['1.txt'] root : ./test_dir/1_dir, dirs : ['empty_dir'], files : ['2.txt'] root : ./test_dir/1_dir/empty_dir, dirs : [], files : [] ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:3","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"判断路径是否存在 filesystem提供了判断路径是否存在的函数exists()。 void eg2_4() { /*判断路径是否存在*/ bool exist = std::filesystem::exists(\"C:\\\\Users\"); std::cout \u003c\u003c \"exist : \" \u003c\u003c exist \u003c\u003c std::endl; } exist : 0 os.path中提供了函数exists()。 def eg2_4(): \"\"\"判断路径是否存在\"\"\" path = \"C:\\\\Users\" print(\"os.path.exists({}) : {}\".format(path, os.path.exists(path))) os.path.exists(C:\\Users) : False ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:4","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"判断路径是文件还是目录 filesystem提供了判断路径是文件还是目录的函数。（比较多） inline _LIBCPP_INLINE_VISIBILITY bool is_block_file(file_status __s) noexcept { return __s.type() == file_type::block; } inline _LIBCPP_INLINE_VISIBILITY bool is_block_file(const path\u0026 __p) { return is_block_file(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_block_file(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_block_file(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_character_file(file_status __s) noexcept { return __s.type() == file_type::character; } inline _LIBCPP_INLINE_VISIBILITY bool is_character_file(const path\u0026 __p) { return is_character_file(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_character_file(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_character_file(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_directory(file_status __s) noexcept { return __s.type() == file_type::directory; } inline _LIBCPP_INLINE_VISIBILITY bool is_directory(const path\u0026 __p) { return is_directory(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_directory(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_directory(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_empty(const path\u0026 __p) { return __fs_is_empty(__p); } inline _LIBCPP_INLINE_VISIBILITY bool is_empty(const path\u0026 __p, error_code\u0026 __ec) { return __fs_is_empty(__p, \u0026__ec); } inline _LIBCPP_INLINE_VISIBILITY bool is_fifo(file_status __s) noexcept { return __s.type() == file_type::fifo; } inline _LIBCPP_INLINE_VISIBILITY bool is_fifo(const path\u0026 __p) { return is_fifo(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_fifo(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_fifo(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_regular_file(file_status __s) noexcept { return __s.type() == file_type::regular; } inline _LIBCPP_INLINE_VISIBILITY bool is_regular_file(const path\u0026 __p) { return is_regular_file(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_regular_file(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_regular_file(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_socket(file_status __s) noexcept { return __s.type() == file_type::socket; } inline _LIBCPP_INLINE_VISIBILITY bool is_socket(const path\u0026 __p) { return is_socket(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_socket(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_socket(__status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_symlink(file_status __s) noexcept { return __s.type() == file_type::symlink; } inline _LIBCPP_INLINE_VISIBILITY bool is_symlink(const path\u0026 __p) { return is_symlink(__symlink_status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_symlink(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_symlink(__symlink_status(__p, \u0026__ec)); } inline _LIBCPP_INLINE_VISIBILITY bool is_other(file_status __s) noexcept { return exists(__s) \u0026\u0026 !is_regular_file(__s) \u0026\u0026 !is_directory(__s) \u0026\u0026 !is_symlink(__s); } inline _LIBCPP_INLINE_VISIBILITY bool is_other(const path\u0026 __p) { return is_other(__status(__p)); } inline _LIBCPP_INLINE_VISIBILITY bool is_other(const path\u0026 __p, error_code\u0026 __ec) noexcept { return is_other(__status(__p, \u0026__ec)); } void eg2_5() { /*判断路径是文件还是目录*/ std::filesystem::path file_path = \"../test_dir/1.txt\"; std::filesystem::path dir_path = \"../test_dir/1_dir\"; std::cout \u003c\u003c \"is_regular_file(file_path) : \" \u003c\u003c std::filesystem::is_regular_file(file_path) \u003c\u003c std::endl; std::cout \u003c\u003c \"is_directory(dir_path) : \" \u003c\u003c std::filesystem::is_directory(dir_path) \u003c\u003c std::endl; } is_regular_file(file_path) : 1 is_directory(dir_path) : 1 os.path中提供了函数isfile()和isdir()。 def eg2_5(): \"\"\"判断路径是文件还是目录\"\"\" file_path = \"./test_dir/1.txt\" dir_path = \"./test_dir/1_dir\" print(\"os.path.isfile({}) : {}\".format(file_path, os.path.isfile(file_path))) print(\"os.path.isdir({}) : {}\".format(dir_path, os.path.isdir(dir_path))) os.path.isfile(./test_dir/1.txt) : True os.path.i","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:5","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"返回绝对路径【神奇】 filesystem中提供了函数absolute()。 void eg2_6() { /*返回绝对路径*/ std::filesystem::path path = \"../test_dir\"; std::filesystem::path abs_path = std::filesystem::absolute(path); std::cout \u003c\u003c \"abs_path : \" \u003c\u003c abs_path \u003c\u003c std::endl; std::cout \u003c\u003c \"exists(abs_path) : \" \u003c\u003c std::filesystem::exists(abs_path) \u003c\u003c std::endl; } abs_path : \"/Users/xxx/Github/intro_to_C-python/xxx/cmake-build-debug/../test_dir\" exists(abs_path) : 1 在Windows系统中结果如下。 abs_path : \"D:\\\\GitHub\\\\intro_to_C-python\\\\xxx\\\\test_dir\" exists(abs_path) : 1 os.path中提供了函数abspath()。 def eg2_6(): \"\"\"返回绝对路径\"\"\" path = \"../../-PyTorch-\" abs_path = os.path.abspath(path) print(\"abs_path : {}\".format(abs_path)) print(\"os.path.exists({}) : {}\".format(abs_path, os.path.exists(abs_path))) abs_path : /Users/xxx/Github/-PyTorch- os.path.exists(/Users/xxx/Github/-PyTorch-) : True ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:2:6","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"filesystem vs. shutil shutil = shell + util，对os进行一些补充。 ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:3:0","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"文件拷贝 filesystem中提供了copy_file()函数，可以拷贝文件或空文件夹。 void eg3_1() { /*文件拷贝*/ std::cout \u003c\u003c \"~~~~~~before copy_file~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } std::filesystem::copy_file(\"../test_dir/1.txt\", \"../test_dir/eg3_1.txt\"); std::cout \u003c\u003c \"~~~~~~after copy_file~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } } ~~~~~~before copy_file~~~~~~ \"../test_dir/1.txt\" \"../test_dir/1_dir\" ~~~~~~after copy_file~~~~~~ \"../test_dir/eg3_1.txt\" \"../test_dir/1.txt\" \"../test_dir/1_dir\" shutil中提供函数copyfile()。 def eg3_1(): \"\"\"文件拷贝\"\"\" print(\"~~~~~~before copy_file~~~~~~\") for i in os.listdir(\"./test_dir\"): print(i) src_path = \"./test_dir/1.txt\" dst_path = \"./test_dir/eg3_1.txt\" shutil.copyfile(src_path, dst_path) print(\"~~~~~~after copy_file~~~~~~\") for i in os.listdir(\"./test_dir\"): print(i) ~~~~~~before copy_file~~~~~~ 1.txt 1_dir ~~~~~~after copy_file~~~~~~ eg3_1.txt 1.txt 1_dir ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:3:1","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"文件删除 filesystem中提供了remove()函数，可以删除文件或空文件夹。 void eg3_2() { /*文件删除*/ std::cout \u003c\u003c \"~~~~~~before remove~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } std::filesystem::remove(\"../test_dir/eg3_1.txt\"); std::cout \u003c\u003c \"~~~~~~after remove~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } } ~~~~~~before remove~~~~~~ \"../test_dir/eg3_1.txt\" \"../test_dir/1.txt\" \"../test_dir/1_dir\" ~~~~~~after remove~~~~~~ \"../test_dir/1.txt\" \"../test_dir/1_dir\" os中提供了函数remove()。 def eg3_2(): \"\"\"文件删除\"\"\" print(\"~~~~~~before remove~~~~~~\") for i in os.listdir(\"./test_dir\"): print(i) rm_path = \"./test_dir/eg3_1.txt\" os.remove(rm_path) print(\"~~~~~~after remove~~~~~~\") for i in os.listdir(\"./test_dir\"): print(i) ~~~~~~before remove~~~~~~ eg3_1.txt 1.txt 1_dir ~~~~~~after remove~~~~~~ 1.txt 1_dir ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:3:2","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"路径拷贝 filesystem中提供了copy()函数，可以按照选项（是否递归）拷贝文件或文件夹。 void eg3_3() { /*路径拷贝*/ std::cout \u003c\u003c \"~~~~~~before copy[1_dir]~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir/1_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } std::filesystem::copy(\"../test_dir/1_dir\", \"../test_dir/eg3_3_dir\", std::filesystem::copy_options::recursive); std::cout \u003c\u003c \"~~~~~~after copy[eg3_3_dir]~~~~~~\" \u003c\u003c std::endl; for (const auto \u0026i :std::filesystem::directory_iterator(\"../test_dir/eg3_3_dir\")) { std::cout \u003c\u003c i.path() \u003c\u003c std::endl; } } ~~~~~~before copy[1_dir]~~~~~~ \"../test_dir/1_dir/empty_dir\" \"../test_dir/1_dir/2.txt\" ~~~~~~after copy[eg3_3_dir]~~~~~~ \"../test_dir/eg3_3_dir/empty_dir\" \"../test_dir/eg3_3_dir/2.txt\" shutil提供了函数copytree()。 def eg3_3(): \"\"\"路径拷贝\"\"\" print(\"~~~~~~before copy[1_dir]~~~~~~\") for i in os.listdir(\"./test_dir/1_dir\"): print(i) src_path = \"./test_dir/1_dir\" dst_path = \"./test_dir/eg3_3_dir\" shutil.copytree(src_path, dst_path) print(\"~~~~~~after copy[eg3_3_dir]~~~~~~\") for i in os.listdir(\"./test_dir/eg3_3_dir\"): print(i) ~~~~~~before copy[1_dir]~~~~~~ empty_dir 2.txt ~~~~~~after copy[eg3_3_dir]~~~~~~ empty_dir 2.txt ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:3:3","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["现代C++语法"],"content":"递归删除 filesystem中提供了remove_all()函数，可以递归删除文件或文件夹。 void eg3_4() { /*递归删除*/ std::filesystem::path dir_path = \"../test_dir/eg3_3_dir\"; std::cout \u003c\u003c \"exists(dir_path) : \" \u003c\u003c std::filesystem::exists(dir_path) \u003c\u003c std::endl; std::filesystem::remove_all(dir_path); std::cout \u003c\u003c \"exists(dir_path) : \" \u003c\u003c std::filesystem::exists(dir_path) \u003c\u003c std::endl; } exists(dir_path) : 1 exists(dir_path) : 0 shutil提供了函数rmtree()。 def eg3_4(): \"\"\"递归删除\"\"\" dir_path = \"./test_dir/eg3_3_dir\" print(\"os.path.exists({}) : {}\".format(dir_path, os.path.exists(dir_path))) shutil.rmtree(dir_path) print(\"os.path.exists({}) : {}\".format(dir_path, os.path.exists(dir_path))) os.path.exists(./test_dir/eg3_3_dir) : True os.path.exists(./test_dir/eg3_3_dir) : False ","date":"2022-01-25","objectID":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/:3:4","tags":["C++与python文件系统对比"],"title":"C++与python文件系统对比","uri":"/posts/c++%E4%B8%8Epython%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94/"},{"categories":["JavaSE笔记"],"content":"Java I/O","date":"2022-01-23","objectID":"/posts/java-io/","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"Java I/O 配套视频 注意：这块会涉及到操作系统和计算机组成原理相关内容。 I/O简而言之，就是输入输出，那么为什么会有I/O呢？其实I/O无时无刻都在我们的身边，比如读取硬盘上的文件，网络文件传输，鼠标键盘输入，也可以是接受单片机发回的数据，而能够支持这些操作的设备就是I/O设备。 我们可以大致看一下整个计算机的总线结构： 常见的I/O设备一般是鼠标、键盘这类通过USB进行传输的外设或者是通过Sata接口或是M.2连接的硬盘。一般情况下，这些设备是由CPU发出指令通过南桥芯片间接进行控制，而不是由CPU直接操作。 而我们在程序中，想要读取这些外部连接的I/O设备中的内容，就需要将数据传输到内存中。而需要实现这样的操作，单单凭借一个小的程序是无法做到的，而操作系统（如：Windows/Linux/MacOS）就是专门用于控制和管理计算机硬件和软件资源的软件，我们需要读取一个IO设备的内容时，可以向操作系统发出请求，由操作系统帮助我们来和底层的硬件交互以完成我们的读取/写入请求。从读取硬盘文件的角度来说，不同的操作系统有着不同的文件系统（也就是文件在硬盘中的存储排列方式，如Windows就是NTFS、MacOS就是APFS），硬盘只能存储一个个0和1这样的二进制数据，至于0和1如何排列，各自又代表什么意思，就是由操作系统的文件系统来决定的。从网络通信角度来说，网络信号通过网卡等设备翻译为二进制信号，再交给系统进行读取，最后再由操作系统来给到程序。 JDK提供了一套用于IO操作的框架，根据流的传输方向和读取单位，分为字节流InputStream和OutputStream以及字符流Reader和Writer，当然，这里的Stream并不是前面集合框架认识的Stream，这里的流指的是数据流，通过流，我们就可以一直从流中读取数据，直到读取到尽头，或是不断向其中写入数据，直到我们写入完成。而这类IO就是我们所说的BIO， 字节流一次读取一个字节，也就是一个byte的大小，而字符流顾名思义，就是一次读取一个字符，也就是一个char的大小（在读取纯文本文件的时候更加适合），有关这两种流，会在后面详细介绍，这个章节我们需要学习16个关键的流。 ","date":"2022-01-23","objectID":"/posts/java-io/:0:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"文件流 要学习和使用IO，首先就要从最易于理解的读取文件开始说起。 ","date":"2022-01-23","objectID":"/posts/java-io/:1:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"文件字节流 首先介绍一下FileInputStream，通过它来获取文件的输入流。 public static void main(String[] args) { try { FileInputStream inputStream = new FileInputStream(\"路径\"); //路径支持相对路径和绝对路径 } catch (FileNotFoundException e) { e.printStackTrace(); } } 相对路径是在当前运行的路径下寻找文件，而绝对路径，是从根目录开始寻找。路径分割符支持使用/或是\\\\，但是不能写为\\因为它是转义字符！ 在使用完成一个流之后，必须关闭这个流来完成对资源的释放，否则资源会被一直占用！ public static void main(String[] args) { FileInputStream inputStream = null; //定义可以先放在try外部 try { inputStream = new FileInputStream(\"路径\"); } catch (FileNotFoundException e) { e.printStackTrace(); } finally { try { //建议在finally中进行，因为这个是任何情况都必须要执行的！ if(inputStream != null) inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } 虽然这样的写法才是最保险的，但是显得过于繁琐了，尤其是finally中再次嵌套了一个try-catch块，因此在JDK1.7新增了try-with-resource语法，用于简化这样的写法（本质上还是和这样的操作一致，只是换了个写法） public static void main(String[] args) { //注意，这种语法只支持实现了AutoCloseable接口的类！ try(FileInputStream inputStream = new FileInputStream(\"路径\")) { //直接在try()中定义要在完成之后释放的资源 } catch (IOException e) { //这里变成IOException是因为调用close()可能会出现，而FileNotFoundException是继承自IOException的 e.printStackTrace(); } //无需再编写finally语句块，因为在最后自动帮我们调用了close() } 之后为了方便，我们都使用此语法进行教学。 public static void main(String[] args) { //test.txt：a try(FileInputStream inputStream = new FileInputStream(\"test.txt\")) { //使用read()方法进行字符读取 System.out.println((char) inputStream.read()); //读取一个字节的数据（英文字母只占1字节，中文占2字节） System.out.println(inputStream.read()); //唯一一个字节的内容已经读完了，再次读取返回-1表示没有内容了 }catch (IOException e){ e.printStackTrace(); } } 使用read可以直接读取一个字节的数据，注意，流的内容是有限的，读取一个少一个！我们如果想一次性全部读取的话，可以直接使用一个while循环来完成： public static void main(String[] args) { //test.txt：abcd try(FileInputStream inputStream = new FileInputStream(\"test.txt\")) { int tmp; while ((tmp = inputStream.read()) != -1){ //通过while循环来一次性读完内容 System.out.println((char)tmp); } }catch (IOException e){ e.printStackTrace(); } } 使用方法能查看当前可读的剩余字节数量（注意：并不一定真实的数据量就是这么多，尤其是在网络I/O操作时，这个方法只能进行一个预估也可以说是暂时能一次性读取的数量） try(FileInputStream inputStream = new FileInputStream(\"test.txt\")) { System.out.println(inputStream.available()); //查看剩余数量 }catch (IOException e){ e.printStackTrace(); } 当然，一个一个读取效率太低了，那能否一次性全部读取呢？我们可以预置一个合适容量的byte[]数组来存放。 public static void main(String[] args) { //test.txt：abcd try(FileInputStream inputStream = new FileInputStream(\"test.txt\")) { byte[] bytes = new byte[inputStream.available()]; //我们可以提前准备好合适容量的byte数组来存放 System.out.println(inputStream.read(bytes)); //一次性读取全部内容（返回值是读取的字节数） System.out.println(new String(bytes)); //通过String(byte[])构造方法得到字符串 }catch (IOException e){ e.printStackTrace(); } } 也可以控制要读取数量： System.out.println(inputStream.read(bytes, 1, 2)); //第二个参数是从给定数组的哪个位置开始放入内容，第三个参数是读取流中的字节数 注意：一次性读取同单个读取一样，当没有任何数据可读时，依然会返回-1 通过skip()方法可以跳过指定数量的字节： public static void main(String[] args) { //test.txt：abcd try(FileInputStream inputStream = new FileInputStream(\"test.txt\")) { System.out.println(inputStream.skip(1)); System.out.println((char) inputStream.read()); //跳过了一个字节 }catch (IOException e){ e.printStackTrace(); } } 注意：FileInputStream是不支持reset()的，虽然有这个方法，但是这里先不提及。 既然有输入流，那么文件输出流也是必不可少的： public static void main(String[] args) { //输出流也需要在最后调用close()方法，并且同样支持try-with-resource try(FileOutputStream outputStream = new FileOutputStream(\"output.txt\")) { //注意：若此文件不存在，会直接创建这个文件！ }catch (IOException e){ e.printStackTrace(); } } 输出流没有read()操作而是write()操作，使用方法同输入流一样，只不过现在的方向变为我们向文件里写入内容： public static void main(String[] args) { try(FileOutputStream outputStream = new FileOutputStream(\"output.txt\")) { outputStream.write('c'); //同read一样，可以直接写入内容 outputStream.write(\"lbwnb\".getBytes()); //也可以直接写入byte[] outputStream.write(\"lbwnb\".getBytes(), 0, 1); //同上输入流 outputStream.flush(); //建议在最后执行一次刷新操作（强制写入）来保证数据正确写入到硬盘文件中 }catch (IOException e){ e.printStackTrace(); } } 那么如果是我只想在文件尾部进行追加写入数据呢？我们可以调用另一个构造方法来实现： public static void main(String[] args) { try(FileOutputStream outputStream = new FileOutputStream(\"output.txt\", true)) { outputStream.write(\"lb\".getBytes()); //现在只会进行追加写入，而不是直接替换原文件内容 outputStream.flush(); }catch (IOException e){ e.printStackTrace(); } } ","date":"2022-01-23","objectID":"/posts/java-io/:1:1","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"文件字符流 字符流不同于字节，字符流是以一个具体的字符进行读取，因此它只适合读纯文本的文件，如果是其他类型的文件不适用： public static void main(String[] args) { try(FileReader reader = new FileReader(\"test.txt\")){ reader.skip(1); //现在跳过的是一个字符 System.out.println((char) reader.read()); //现在是按字符进行读取，而不是字节，因此可以直接读取到中文字符 }catch (IOException e){ e.printStackTrace(); } } 同理，字符流只支持char[]类型作为存储： public static void main(String[] args) { try(FileReader reader = new FileReader(\"test.txt\")){ char[] str = new char[10]; reader.read(str); System.out.println(str); //直接读取到char[]中 }catch (IOException e){ e.printStackTrace(); } } 既然有了Reader肯定也有Writer： public static void main(String[] args) { try(FileWriter writer = new FileWriter(\"output.txt\")){ writer.getEncoding(); //支持获取编码（不同的文本文件可能会有不同的编码类型） writer.write('牛'); writer.append('牛'); //其实功能和write一样 writer.flush(); //刷新 }catch (IOException e){ e.printStackTrace(); } } 我们发现不仅有write()方法，还有一个append()方法，但是实际上他们效果是一样的，看源码： /** * Appends the specified character to this writer. * * \u003cp\u003e An invocation of this method of the form \u003ctt\u003eout.append(c)\u003c/tt\u003e * behaves in exactly the same way as the invocation * * \u003cpre\u003e * out.write(c) \u003c/pre\u003e * * @param c * The 16-bit character to append * * @return This writer * * @throws IOException * If an I/O error occurs * * @since 1.5 */ public Writer append(char c) throws IOException { write(c); return this; } append支持像StringBuilder那样的链式调用，返回的是Writer对象本身。 练习：尝试一下用Reader和Writer来拷贝纯文本文件 ","date":"2022-01-23","objectID":"/posts/java-io/:1:2","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"File类 File类专门用于表示一个文件或文件夹，只不过它只是代表这个文件，但并不是这个文件本身。通过File对象，可以更好地管理和操作硬盘上的文件。 public static void main(String[] args) { File file = new File(\"test.txt\"); //直接创建文件对象，可以是相对路径，也可以是绝对路径 System.out.println(file.exists()); //此文件是否存在 System.out.println(file.length()); //获取文件的大小 System.out.println(file.isDirectory()); //是否为一个文件夹 System.out.println(file.canRead()); //是否可读 System.out.println(file.canWrite()); //是否可写 System.out.println(file.canExecute()); //是否可执行 } 通过File对象，我们就能快速得到文件的所有信息，如果是文件夹，还可以获取文件夹内部的文件列表等内容： File file = new File(\"/\"); System.out.println(Arrays.toString(file.list())); //快速获取文件夹下的文件名称列表 for (File f : file.listFiles()){ //所有子文件的File对象 System.out.println(f.getAbsolutePath()); //获取文件的绝对路径 } 如果我们希望读取某个文件的内容，可以直接将File作为参数传入字节流或是字符流： File file = new File(\"test.txt\"); try (FileInputStream inputStream = new FileInputStream(file)){ //直接做参数 System.out.println(inputStream.available()); }catch (IOException e){ e.printStackTrace(); } 练习：尝试拷贝文件夹下的所有文件到另一个文件夹 ","date":"2022-01-23","objectID":"/posts/java-io/:1:3","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"缓冲流 虽然普通的文件流读取文件数据非常便捷，但是每次都需要从外部I/O设备去获取数据，由于外部I/O设备的速度一般都达不到内存的读取速度，很有可能造成程序反应迟钝，因此性能还不够高，而缓冲流正如其名称一样，它能够提供一个缓冲，提前将部分内容存入内存（缓冲区）在下次读取时，如果缓冲区中存在此数据，则无需再去请求外部设备。同理，当向外部设备写入数据时，也是由缓冲区处理，而不是直接向外部设备写入。 ","date":"2022-01-23","objectID":"/posts/java-io/:2:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"缓冲字节流 要创建一个缓冲字节流，只需要将原本的流作为构造参数传入BufferedInputStream即可： public static void main(String[] args) { try (BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"test.txt\"))){ //传入FileInputStream System.out.println((char) bufferedInputStream.read()); //操作和原来的流是一样的 }catch (IOException e){ e.printStackTrace(); } } 实际上进行I/O操作的并不是BufferedInputStream，而是我们传入的FileInputStream，而BufferedInputStream虽然有着同样的方法，但是进行了一些额外的处理然后再调用FileInputStream的同名方法，这样的写法称为装饰者模式 public void close() throws IOException { byte[] buffer; while ( (buffer = buf) != null) { if (bufUpdater.compareAndSet(this, buffer, null)) { //CAS无锁算法，并发会用到，暂时不管 InputStream input = in; in = null; if (input != null) input.close(); return; } // Else retry in case a new buf was CASed in fill() } } 实际上这种模式是父类FilterInputStream提供的规范，后面我们还会讲到更多FilterInputStream的子类。 我们可以发现在BufferedInputStream中还存在一个专门用于缓存的数组： /** * The internal buffer array where the data is stored. When necessary, * it may be replaced by another array of * a different size. */ protected volatile byte buf[]; I/O操作一般不能重复读取内容（比如键盘发送的信号，主机接收了就没了），而缓冲流提供了缓冲机制，一部分内容可以被暂时保存，BufferedInputStream支持reset()和mark()操作，首先我们来看看mark()方法的介绍： /** * Marks the current position in this input stream. A subsequent * call to the \u003ccode\u003ereset\u003c/code\u003e method repositions this stream at * the last marked position so that subsequent reads re-read the same bytes. * \u003cp\u003e * The \u003ccode\u003ereadlimit\u003c/code\u003e argument tells this input stream to * allow that many bytes to be read before the mark position gets * invalidated. * \u003cp\u003e * This method simply performs \u003ccode\u003ein.mark(readlimit)\u003c/code\u003e. * * @param readlimit the maximum limit of bytes that can be read before * the mark position becomes invalid. * @see java.io.FilterInputStream#in * @see java.io.FilterInputStream#reset() */ public synchronized void mark(int readlimit) { in.mark(readlimit); } 当调用mark()之后，输入流会以某种方式保留之后读取的readlimit数量的内容，当读取的内容数量超过readlimit则之后的内容不会被保留，当调用reset()之后，会使得当前的读取位置回到mark()调用时的位置。 public static void main(String[] args) { try (BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"test.txt\"))){ bufferedInputStream.mark(1); //只保留之后的1个字符 System.out.println((char) bufferedInputStream.read()); System.out.println((char) bufferedInputStream.read()); bufferedInputStream.reset(); //回到mark时的位置 System.out.println((char) bufferedInputStream.read()); System.out.println((char) bufferedInputStream.read()); }catch (IOException e) { e.printStackTrace(); } } 我们发现虽然后面的部分没有保存，但是依然能够正常读取，其实mark()后保存的读取内容是取readlimit和BufferedInputStream类的缓冲区大小两者中的最大值，而并非完全由readlimit确定。因此我们限制一下缓冲区大小，再来观察一下结果： public static void main(String[] args) { try (BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"test.txt\"), 1)){ //将缓冲区大小设置为1 bufferedInputStream.mark(1); //只保留之后的1个字符 System.out.println((char) bufferedInputStream.read()); System.out.println((char) bufferedInputStream.read()); //已经超过了readlimit，继续读取会导致mark失效 bufferedInputStream.reset(); //mark已经失效，无法reset() System.out.println((char) bufferedInputStream.read()); System.out.println((char) bufferedInputStream.read()); }catch (IOException e) { e.printStackTrace(); } } 了解完了BufferedInputStream之后，我们再来看看BufferedOutputStream，其实和BufferedInputStream原理差不多，只是反向操作： public static void main(String[] args) { try (BufferedOutputStream outputStream = new BufferedOutputStream(new FileOutputStream(\"output.txt\"))){ outputStream.write(\"lbwnb\".getBytes()); outputStream.flush(); }catch (IOException e) { e.printStackTrace(); } } 操作和FileOutputStream一致，这里就不多做介绍了。 ","date":"2022-01-23","objectID":"/posts/java-io/:2:1","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"缓冲字符流 缓存字符流和缓冲字节流一样，也有一个专门的缓冲区，BufferedReader构造时需要传入一个Reader对象： public static void main(String[] args) { try (BufferedReader reader = new BufferedReader(new FileReader(\"test.txt\"))){ System.out.println((char) reader.read()); }catch (IOException e) { e.printStackTrace(); } } 使用和reader也是一样的，内部也包含一个缓存数组： private char cb[]; 相比Reader更方便的是，它支持按行读取： public static void main(String[] args) { try (BufferedReader reader = new BufferedReader(new FileReader(\"test.txt\"))){ System.out.println(reader.readLine()); //按行读取 }catch (IOException e) { e.printStackTrace(); } } 读取后直接得到一个字符串，当然，它还能把每一行内容依次转换为集合类提到的Stream流： public static void main(String[] args) { try (BufferedReader reader = new BufferedReader(new FileReader(\"test.txt\"))){ reader .lines() .limit(2) .distinct() .sorted() .forEach(System.out::println); }catch (IOException e) { e.printStackTrace(); } } 它同样也支持mark()和reset()操作： public static void main(String[] args) { try (BufferedReader reader = new BufferedReader(new FileReader(\"test.txt\"))){ reader.mark(1); System.out.println((char) reader.read()); reader.reset(); System.out.println((char) reader.read()); }catch (IOException e) { e.printStackTrace(); } } BufferedReader处理纯文本文件时就更加方便了，BufferedWriter在处理时也同样方便： public static void main(String[] args) { try (BufferedWriter reader = new BufferedWriter(new FileWriter(\"output.txt\"))){ reader.newLine(); //使用newLine进行换行 reader.write(\"汉堡做滴彳亍不彳亍\"); //可以直接写入一个字符串 reader.flush(); //清空缓冲区 }catch (IOException e) { e.printStackTrace(); } } ","date":"2022-01-23","objectID":"/posts/java-io/:2:2","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"转换流 有时会遇到这样一个很麻烦的问题：我这里读取的是一个字符串或是一个个字符，但是我只能往一个OutputStream里输出，但是OutputStream又只支持byte类型，如果要往里面写入内容，进行数据转换就会很麻烦，那么能否有更加简便的方式来做这样的事情呢？ public static void main(String[] args) { try(OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(\"test.txt\"))){ //虽然给定的是FileOutputStream，但是现在支持以Writer的方式进行写入 writer.write(\"lbwnb\"); //以操作Writer的样子写入OutputStream }catch (IOException e){ e.printStackTrace(); } } 同样的，我们现在只拿到了一个InputStream，但是我们希望能够按字符的方式读取，我们就可以使用InputStreamReader来帮助我们实现： public static void main(String[] args) { try(InputStreamReader reader = new InputStreamReader(new FileInputStream(\"test.txt\"))){ //虽然给定的是FileInputStream，但是现在支持以Reader的方式进行读取 System.out.println((char) reader.read()); }catch (IOException e){ e.printStackTrace(); } } InputStreamReader和OutputStreamWriter本质也是Reader和Writer，因此可以直接放入BufferedReader来实现更加方便的操作。 ","date":"2022-01-23","objectID":"/posts/java-io/:3:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"打印流 打印流其实我们从一开始就在使用了，比如System.out就是一个PrintStream，PrintStream也继承自FilterOutputStream类因此依然是装饰我们传入的输出流，但是它存在自动刷新机制，例如当向PrintStream流中写入一个字节数组后自动调用flush()方法。PrintStream也永远不会抛出异常，而是使用内部检查机制checkError()方法进行错误检查。最方便的是，它能够格式化任意的类型，将它们以字符串的形式写入到输出流。 public final static PrintStream out = null; 可以看到System.out也是PrintStream，不过默认是向控制台打印，我们也可以让它向文件中打印： public static void main(String[] args) { try(PrintStream stream = new PrintStream(new FileOutputStream(\"test.txt\"))){ stream.println(\"lbwnb\"); //其实System.out就是一个PrintStream }catch (IOException e){ e.printStackTrace(); } } 我们平时使用的println方法就是PrintStream中的方法，它会直接打印基本数据类型或是调用对象的toString()方法得到一个字符串，并将字符串转换为字符，放入缓冲区再经过转换流输出到给定的输出流上。 因此实际上内部还包含这两个内容： /** * Track both the text- and character-output streams, so that their buffers * can be flushed without flushing the entire stream. */ private BufferedWriter textOut; private OutputStreamWriter charOut; 与此相同的还有一个PrintWriter，不过他们的功能基本一致，PrintWriter的构造方法可以接受一个Writer作为参数，这里就不再做过多阐述了。 ","date":"2022-01-23","objectID":"/posts/java-io/:4:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"数据流 数据流DataInputStream也是FilterInputStream的子类，同样采用装饰者模式，最大的不同是它支持基本数据类型的直接读取： public static void main(String[] args) { try (DataInputStream dataInputStream = new DataInputStream(new FileInputStream(\"test.txt\"))){ System.out.println(dataInputStream.readBoolean()); //直接将数据读取为任意基本数据类型 }catch (IOException e) { e.printStackTrace(); } } 用于写入基本数据类型： public static void main(String[] args) { try (DataOutputStream dataOutputStream = new DataOutputStream(new FileOutputStream(\"output.txt\"))){ dataOutputStream.writeBoolean(false); }catch (IOException e) { e.printStackTrace(); } } 注意，写入的是二进制数据，并不是写入的字符串，使用DataInputStream可以读取，一般他们是配合一起使用的。 ","date":"2022-01-23","objectID":"/posts/java-io/:5:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"对象流 既然基本数据类型能够读取和写入基本数据类型，那么能否将对象也支持呢？ObjectOutputStream不仅支持基本数据类型，通过对对象的序列化操作，以某种格式保存对象，来支持对象类型的IO，注意：它不是继承自FilterInputStream的。 public static void main(String[] args) { try (ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(\"output.txt\")); ObjectInputStream inputStream = new ObjectInputStream(new FileInputStream(\"output.txt\"))){ People people = new People(\"lbw\"); outputStream.writeObject(people); outputStream.flush(); people = (People) inputStream.readObject(); System.out.println(people.name); }catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } static class People implements Serializable{ //必须实现Serializable接口才能被序列化 String name; public People(String name){ this.name = name; } } 在我们后续的操作中，有可能会使得这个类的一些结构发生变化，而原来保存的数据只适用于之前版本的这个类，因此我们需要一种方法来区分类的不同版本： static class People implements Serializable{ private static final long serialVersionUID = 123456; //在序列化时，会被自动添加这个属性，它代表当前类的版本，我们也可以手动指定版本。 String name; public People(String name){ this.name = name; } } 当发生版本不匹配时，会无法反序列化为对象： java.io.InvalidClassException: com.test.Main$People; local class incompatible: stream classdesc serialVersionUID = 123456, local class serialVersionUID = 1234567 at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1850) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2160) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) at com.test.Main.main(Main.java:27) 如果我们不希望某些属性参与到序列化中进行保存，我们可以添加transient关键字： public static void main(String[] args) { try (ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(\"output.txt\")); ObjectInputStream inputStream = new ObjectInputStream(new FileInputStream(\"output.txt\"))){ People people = new People(\"lbw\"); outputStream.writeObject(people); outputStream.flush(); people = (People) inputStream.readObject(); System.out.println(people.name); //虽然能得到对象，但是name属性并没有保存，因此为null }catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } static class People implements Serializable{ private static final long serialVersionUID = 1234567; transient String name; public People(String name){ this.name = name; } } 其实我们可以看到，在一些JDK内部的源码中，也存在大量的transient关键字，使得某些属性不参与序列化，取消这些不必要保存的属性，可以节省数据空间占用以及减少序列化时间。 ","date":"2022-01-23","objectID":"/posts/java-io/:6:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"Java I/O编程实战 ","date":"2022-01-23","objectID":"/posts/java-io/:7:0","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"图书管理系统 要求实现一个图书管理系统（控制台），支持以下功能：保存书籍信息（要求持久化），查询、添加、删除、修改书籍信息。 参考代码： import java.io.*; import java.util.HashSet; import java.util.Objects; import java.util.Scanner; public class Library { static private HashSet\u003cBook\u003e MAP; public static void main(String[] args) { //TODO 注意先分配内存 Scanner scan = new Scanner(System.in); readData(); while (true) { System.out.println(\"============图书馆管理系统===========\"); System.out.println(\"1.插入数据\"); System.out.println(\"2.修改数据\"); System.out.println(\"3.查询图书列表\"); System.out.println(\"4.删除图书\"); System.out.println(\"(按下任意其他键退出程序)\"); String str = scan.nextLine(); switch (str) { case \"1\": insertBooks(scan); break; case \"2\": modifyBook(scan); break; case \"3\": showBooks(); break; case \"4\": deleteBooks(scan); break; default: saveData(); scan.close(); return; } } } //TODO 程序启动前，读取持久化数据 private static void readData(){ File file = new File(\"data\"); if(file.exists()){ try(ObjectInputStream inputStream = new ObjectInputStream(new FileInputStream(\"data\"))){ MAP = (HashSet\u003cBook\u003e) inputStream.readObject(); }catch (IOException|ClassNotFoundException e){ e.printStackTrace(); } }else { MAP = new HashSet\u003c\u003e(); } } //TODO 程序结束时，持久序列化保存数据 private static void saveData(){ try(ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(\"data\"))){ outputStream.writeObject(MAP); outputStream.flush(); }catch (IOException e){ e.printStackTrace(); } } //TODO 修改图书数据 private static void modifyBook(Scanner scan) { showBooks(); System.out.println(\"输入您要修改书籍的索书号：\"); int index = scan.nextInt(); scan.nextLine(); Book t = new Book().setIndex(index); if (!MAP.contains(t)) { System.out.println(\"输入错误图书馆内无该书籍!\"); } else { System.out.println(\"请输入您要更改的信息：书名，作者，价格\"); String name = scan.nextLine(); String author = scan.nextLine(); double price = scan.nextDouble(); scan.nextLine(); for (Book tt : MAP) { if (tt.equals(t)) { tt.setAuthor(author); tt.setName(name); tt.setPrice(price); break; } } } } //TODO 删除书籍 private static void deleteBooks(Scanner scan) { showBooks(); System.out.println(\"请输入要删除书籍的索书号：\"); int index = scan.nextInt(); scan.nextLine(); Book t = new Book().setIndex(index); if (!MAP.contains(t)) { System.out.println(\"索书号出错没有这样的书籍\"); } else { System.out.printf(\"删除索书号为:%d 的书籍成功!\\n\", index); MAP.remove(t); } } //TODO 显示图书信息 private static void showBooks() { MAP.forEach(System.out::println); } //TODO 插入书籍 private static void insertBooks(Scanner scan) { System.out.println(\"请输入 索书号 书名 作者 价格。每输入一个信息按下回车键确认！\"); MAP.add(new Book().setIndex(scan.nextInt()) .setName(scan.nextLine() + scan.nextLine()) .setAuthor(scan.nextLine()) .setPrice(scan.nextDouble()) ); scan.nextLine(); } //TODO 书籍类 private static class Book implements Serializable{ int index;//索书号 String name;//书名 String author;//作者 double price;//价格 public Book setName(String name) { this.name = name; return this; } public Book setIndex(int index) { this.index = index; return this; } public Book setAuthor(String author) { this.author = author; return this; } public Book setPrice(double price) { this.price = price; return this; } @Override public String toString() { return \"书籍{\" + \"索书号=\" + index + \", 书名='\" + name + '\\'' + \", 作者='\" + author + '\\'' + \", 价格=\" + price + '}'; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Book book = (Book) o; return index == book.index; } @Override public int hashCode() { return Objects.hash(index); } } } ","date":"2022-01-23","objectID":"/posts/java-io/:7:1","tags":["Java I/O"],"title":"Java I/O","uri":"/posts/java-io/"},{"categories":["JavaSE笔记"],"content":"Java反射与注解","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"Java反射和注解 配套视频 **注意：**本章节涉及到JVM相关底层原理，难度会有一些大。 反射就是把Java类中的各个成分映射成一个个的Java对象。即在运行状态中，对于任意一个类，都能够知道这个类所有的属性和方法，对于任意一个对象，都能调用它的任意一个方法和属性。这种动态获取信息及动态调用对象方法的功能叫Java的反射机制。 简而言之，我们可以通过反射机制，获取到类的一些属性，包括类里面有哪些字段，有哪些方法，继承自哪个类，甚至还能获取到泛型！它的权限非常高，慎重使用！ ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:0:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"Java类加载机制 在学习Java的反射机制之前，我们需要先了解一下类的加载机制，一个类是如何被加载和使用的： 在Java程序启动时，JVM会将一部分类（class文件）先加载（并不是所有的类都会在一开始加载），通过ClassLoader将类加载，在加载过程中，会将类的信息提取出来（存放在元空间中，JDK1.8之前存放在永久代），同时也会生成一个Class对象存放在内存（堆内存），注意此Class对象只会存在一个，与加载的类唯一对应！ **思考：**既然说和与加载的类唯一对应，那如果我们手动创建一个与JDK包名一样，同时类名也保持一致，那么JVM会加载这个类吗？ package java.lang; public class String { //JDK提供的String类也是 public static void main(String[] args) { System.out.println(\"我姓🐴，我叫🐴nb\"); } } 我们发现，会出现以下报错： 错误: 在类 java.lang.String 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args) 但是我们明明在自己写的String类中定义了main方法啊，为什么会找不到此方法呢？实际上这是ClassLoader的双亲委派机制在保护Java程序的正常运行： 实际上我们的类最开始是由BootstarpClassLoader进行加载，BootstarpClassLoader用于加载JDK提供的类，而我们自己编写的类实际上是AppClassLoader，只有BootstarpClassLoader都没有加载的类，才会让AppClassLoader来加载，因此我们自己编写的同名包同名类不会被加载，而实际要去启动的是真正的String类，也就自然找不到main方法了！ public class Main { public static void main(String[] args) { System.out.println(Main.class.getClassLoader()); //查看当前类的类加载器 System.out.println(Main.class.getClassLoader().getParent()); //父加载器 System.out.println(Main.class.getClassLoader().getParent().getParent()); //爷爷加载器 System.out.println(String.class.getClassLoader()); //String类的加载器 } } 由于BootstarpClassLoader是C++编写的，我们在Java中是获取不到的。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:1:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"Class对象 通过前面，我们了解了类的加载，同时会提取一个类的信息生成Class对象存放在内存中，而反射机制其实就是利用这些存放的类信息，来获取类的信息和操作类。那么如何获取到每个类对应的Class对象呢，我们可以通过以下方式： public static void main(String[] args) throws ClassNotFoundException { Class\u003cString\u003e clazz = String.class; //使用class关键字，通过类名获取 Class\u003c?\u003e clazz2 = Class.forName(\"java.lang.String\"); //使用Class类静态方法forName()，通过包名.类名获取，注意返回值是Class\u003c?\u003e Class\u003c?\u003e clazz3 = new String(\"cpdd\").getClass(); //通过实例对象获取 } 注意Class类也是一个泛型类，只有第一种方法，能够直接获取到对应类型的Class对象，而以下两种方法使用了?通配符作为返回值，但是实际上都和第一个返回的是同一个对象： Class\u003cString\u003e clazz = String.class; //使用class关键字，通过类名获取 Class\u003c?\u003e clazz2 = Class.forName(\"java.lang.String\"); //使用Class类静态方法forName()，通过包名.类名获取，注意返回值是Class\u003c?\u003e Class\u003c?\u003e clazz3 = new String(\"cpdd\").getClass(); System.out.println(clazz == clazz2); System.out.println(clazz == clazz3); 通过比较，验证了我们一开始的结论，在JVM中每个类始终只存在一个Class对象，无论通过什么方法获取，都是一样的。现在我们再来看看这个问题： public static void main(String[] args) { Class\u003c?\u003e clazz = int.class; //基本数据类型有Class对象吗？ System.out.println(clazz); } 迷了，不是每个类才有Class对象吗，基本数据类型又不是类，这也行吗？实际上，基本数据类型也有对应的Class对象（反射操作可能需要用到），而且我们不仅可以通过class关键字获取，其实本质上是定义在对应的包装类中的： /** * The {@code Class} instance representing the primitive type * {@code int}. * * @since JDK1.1 */ @SuppressWarnings(\"unchecked\") public static final Class\u003cInteger\u003e TYPE = (Class\u003cInteger\u003e) Class.getPrimitiveClass(\"int\"); /* * Return the Virtual Machine's Class object for the named * primitive type */ static native Class\u003c?\u003e getPrimitiveClass(String name); //C++实现，并非Java定义 每个包装类中（包括Void），都有一个获取原始类型Class方法，注意，getPrimitiveClass获取的是原始类型，并不是包装类型，只是可以使用包装类来表示。 public static void main(String[] args) { Class\u003c?\u003e clazz = int.class; System.out.println(Integer.TYPE == int.class); } 通过对比，我们发现实际上包装类型都有一个TYPE，其实也就是基本类型的Class，那么包装类的Class和基本类的Class一样吗？ public static void main(String[] args) { System.out.println(Integer.TYPE == Integer.class); } 我们发现，包装类型的Class对象并不是基本类型Class对象。数组类型也是一种类型，只是编程不可见，因此我们可以直接获取数组的Class对象： public static void main(String[] args) { Class\u003cString[]\u003e clazz = String[].class; System.out.println(clazz.getName()); //获取类名称（得到的是包名+类名的完整名称） System.out.println(clazz.getSimpleName()); System.out.println(clazz.getTypeName()); System.out.println(clazz.getClassLoader()); //获取它的类加载器 System.out.println(clazz.cast(new Integer(\"10\"))); //强制类型转换 } ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:2:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"再谈instanceof 正常情况下，我们使用instanceof进行类型比较： public static void main(String[] args) { String str = \"\"; System.out.println(str instanceof String); } 它可以判断一个对象是否为此接口或是类的实现或是子类，而现在我们有了更多的方式去判断类型： public static void main(String[] args) { String str = \"\"; System.out.println(str.getClass() == String.class); //直接判断是否为这个类型 } 如果需要判断是否为子类或是接口/抽象类的实现，我们可以使用asSubClass()方法： public static void main(String[] args) { Integer i = 10; i.getClass().asSubclass(Number.class); //当Integer不是Number的子类时，会产生异常 } ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:2:1","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"获取父类信息 通过getSuperclass()方法，我们可以获取到父类的Class对象： public static void main(String[] args) { Integer i = 10; System.out.println(i.getClass().getSuperclass()); } 也可以通过getGenericSuperclass()获取父类的原始类型的Type： public static void main(String[] args) { Integer i = 10; Type type = i.getClass().getGenericSuperclass(); System.out.println(type); System.out.println(type instanceof Class); } 我们发现Type实际上是Class类的父接口，但是获取到的Type的实现并不一定是Class。 同理，我们也可以像上面这样获取父接口： public static void main(String[] args) { Integer i = 10; for (Class\u003c?\u003e anInterface : i.getClass().getInterfaces()) { System.out.println(anInterface.getName()); } for (Type genericInterface : i.getClass().getGenericInterfaces()) { System.out.println(genericInterface.getTypeName()); } } ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:2:2","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"创建类对象 既然我们拿到了类的定义，那么是否可以通过Class对象来创建对象、调用方法、修改变量呢？当然是可以的，那我们首先来探讨一下如何创建一个类的对象： public static void main(String[] args) throws InstantiationException, IllegalAccessException { Class\u003cStudent\u003e clazz = Student.class; Student student = clazz.newInstance(); student.test(); } static class Student{ public void test(){ System.out.println(\"萨日朗\"); } } 通过使用newInstance()方法来创建对应类型的实例，返回泛型T，注意它会抛出InstantiationException和IllegalAccessException异常，那么什么情况下会出现异常呢？ public static void main(String[] args) throws InstantiationException, IllegalAccessException { Class\u003cStudent\u003e clazz = Student.class; Student student = clazz.newInstance(); student.test(); } static class Student{ public Student(String text){ } public void test(){ System.out.println(\"萨日朗\"); } } 当类默认的构造方法被带参构造覆盖时，会出现InstantiationException异常，因为newInstance()只适用于默认无参构造。 public static void main(String[] args) throws InstantiationException, IllegalAccessException { Class\u003cStudent\u003e clazz = Student.class; Student student = clazz.newInstance(); student.test(); } static class Student{ private Student(){} public void test(){ System.out.println(\"萨日朗\"); } } 当默认无参构造的权限不是public时，会出现IllegalAccessException异常，表示我们无权去调用默认构造方法。在JDK9之后，不再推荐使用newInstance()方法了，而是使用我们接下来要介绍到的，通过获取构造器，来实例化对象： public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { Class\u003cStudent\u003e clazz = Student.class; Student student = clazz.getConstructor(String.class).newInstance(\"what's up\"); student.test(); } static class Student{ public Student(String str){} public void test(){ System.out.println(\"萨日朗\"); } } 通过获取类的构造方法（构造器）来创建对象实例，会更加合理，我们可以使用getConstructor()方法来获取类的构造方法，同时我们需要向其中填入参数，也就是构造方法需要的类型，当然我们这里只演示了。那么，当访问权限不是public的时候呢？ public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { Class\u003cStudent\u003e clazz = Student.class; Student student = clazz.getConstructor(String.class).newInstance(\"what's up\"); student.test(); } static class Student{ private Student(String str){} public void test(){ System.out.println(\"萨日朗\"); } } 我们发现，当访问权限不足时，会无法找到此构造方法，那么如何找到非public的构造方法呢？ Class\u003cStudent\u003e clazz = Student.class; Constructor\u003cStudent\u003e constructor = clazz.getDeclaredConstructor(String.class); constructor.setAccessible(true); //修改访问权限 Student student = constructor.newInstance(\"what's up\"); student.test(); 使用getDeclaredConstructor()方法可以找到类中的非public构造方法，但是在使用之前，我们需要先修改访问权限，在修改访问权限之后，就可以使用非public方法了（这意味着，反射可以无视权限修饰符访问类的内容） ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:3:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"调用类的方法 我们可以通过反射来调用类的方法（本质上还是类的实例进行调用）只是利用反射机制实现了方法的调用，我们在包下创建一个新的类： package com.test; public class Student { public void test(String str){ System.out.println(\"萨日朗\"+str); } } 这次我们通过forName(String)来找到这个类并创建一个新的对象： public static void main(String[] args) throws ReflectiveOperationException { Class\u003c?\u003e clazz = Class.forName(\"com.test.Student\"); Object instance = clazz.newInstance(); //创建出学生对象 Method method = clazz.getMethod(\"test\", String.class); //通过方法名和形参类型获取类中的方法 method.invoke(instance, \"what's up\"); //通过Method对象的invoke方法来调用方法 } 通过调用getMethod()方法，我们可以获取到类中所有声明为public的方法，得到一个Method对象，我们可以通过Method对象的invoke()方法（返回值就是方法的返回值，因为这里是void，返回值为null）来调用已经获取到的方法，注意传参。 我们发现，利用反射之后，在一个对象从构造到方法调用，没有任何一处需要引用到对象的实际类型，我们也没有导入Student类，整个过程都是反射在代替进行操作，使得整个过程被模糊了，过多的使用反射，会极大地降低后期维护性。 同构造方法一样，当出现非public方法时，我们可以通过反射来无视权限修饰符，获取非public方法并调用，现在我们将test()方法的权限修饰符改为private： public static void main(String[] args) throws ReflectiveOperationException { Class\u003c?\u003e clazz = Class.forName(\"com.test.Student\"); Object instance = clazz.newInstance(); //创建出学生对象 Method method = clazz.getDeclaredMethod(\"test\", String.class); //通过方法名和形参类型获取类中的方法 method.setAccessible(true); method.invoke(instance, \"what's up\"); //通过Method对象的invoke方法来调用方法 } Method和Constructor都和Class一样，他们存储了方法的信息，包括方法的形式参数列表，返回值，方法的名称等内容，我们可以直接通过Method对象来获取这些信息： public static void main(String[] args) throws ReflectiveOperationException { Class\u003c?\u003e clazz = Class.forName(\"com.test.Student\"); Method method = clazz.getDeclaredMethod(\"test\", String.class); //通过方法名和形参类型获取类中的方法 System.out.println(method.getName()); //获取方法名称 System.out.println(method.getReturnType()); //获取返回值类型 } 当方法的参数为可变参数时，我们该如何获取方法呢？实际上，我们在之前就已经提到过，可变参数实际上就是一个数组，因此我们可以直接使用数组的class对象表示： Method method = clazz.getDeclaredMethod(\"test\", String[].class); 反射非常强大，尤其是我们提到的越权访问，但是请一定谨慎使用，别人将某个方法设置为private一定有他的理由，如果实在是需要使用别人定义为private的方法，就必须确保这样做是安全的，在没有了解别人代码的整个过程就强行越权访问，可能会出现无法预知的错误。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:4:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"修改类的属性 我们还可以通过反射访问一个类中定义的成员字段也可以修改一个类的对象中的成员字段值，通过getField()方法来获取一个类定义的指定字段： public static void main(String[] args) throws ReflectiveOperationException { Class\u003c?\u003e clazz = Class.forName(\"com.test.Student\"); Object instance = clazz.newInstance(); Field field = clazz.getField(\"i\"); //获取类的成员字段i field.set(instance, 100); //将类实例instance的成员字段i设置为100 Method method = clazz.getMethod(\"test\"); method.invoke(instance); } 在得到Field之后，我们就可以直接通过set()方法为某个对象，设定此属性的值，比如上面，我们就为instance对象设定值为100，当访问private字段时，同样可以按照上面的操作进行越权访问： public static void main(String[] args) throws ReflectiveOperationException { Class\u003c?\u003e clazz = Class.forName(\"com.test.Student\"); Object instance = clazz.newInstance(); Field field = clazz.getDeclaredField(\"i\"); //获取类的成员字段i field.setAccessible(true); field.set(instance, 100); //将类实例instance的成员字段i设置为100 Method method = clazz.getMethod(\"test\"); method.invoke(instance); } 现在我们已经知道，反射几乎可以把一个类的老底都给扒出来，任何属性，任何内容，都可以被反射修改，无论权限修饰符是什么，那么，如果我的字段被标记为final呢？现在在字段i前面添加final关键字，我们再来看看效果： private final int i = 10; 这时，当字段为final时，就修改失败了！当然，通过反射可以直接将final修饰符直接去除，去除后，就可以随意修改内容了，我们来尝试修改Integer的value值： public static void main(String[] args) throws ReflectiveOperationException { Integer i = 10; Field field = Integer.class.getDeclaredField(\"value\"); Field modifiersField = Field.class.getDeclaredField(\"modifiers\"); //这里要获取Field类的modifiers字段进行修改 modifiersField.setAccessible(true); modifiersField.setInt(field,field.getModifiers()\u0026~Modifier.FINAL); //去除final标记 field.setAccessible(true); field.set(i, 100); //强行设置值 System.out.println(i); } 我们可以发现，反射非常暴力，就连被定义为final字段的值都能强行修改，几乎能够无视一切阻拦。我们来试试看修改一些其他的类型： public static void main(String[] args) throws ReflectiveOperationException { List\u003cString\u003e i = new ArrayList\u003c\u003e(); Field field = ArrayList.class.getDeclaredField(\"size\"); field.setAccessible(true); field.set(i, 10); i.add(\"测试\"); //只添加一个元素 System.out.println(i.size()); //大小直接变成11 i.remove(10); //瞎移除都不带报错的，淦 } 实际上，整个ArrayList体系由于我们的反射操作，导致被破坏，因此它已经无法正常工作了！ 再次强调，在进行反射操作时，必须注意是否安全，虽然拥有了创世主的能力，但是我们不能滥用，我们只能把它当做一个不得已才去使用的工具！ ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:5:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"自定义ClassLoader加载类 我们可以自己手动将class文件加载到JVM中吗？先写好我们定义的类： package com.test; public class Test { public String text; public void test(String str){ System.out.println(text+\" \u003e 我是测试方法！\"+str); } } 通过javac命令，手动编译一个.class文件： nagocoler@NagodeMacBook-Pro HelloWorld % javac src/main/java/com/test/Test.java 编译后，得到一个class文件，我们把它放到根目录下，然后编写一个我们自己的ClassLoader，因为普通的ClassLoader无法加载二进制文件，因此我们编写一个自己的来让它支持： //定义一个自己的ClassLoader static class MyClassLoader extends ClassLoader{ public Class\u003c?\u003e defineClass(String name, byte[] b){ return defineClass(name, b, 0, b.length); //调用protected方法，支持载入外部class文件 } } public static void main(String[] args) throws IOException { MyClassLoader classLoader = new MyClassLoader(); FileInputStream stream = new FileInputStream(\"Test.class\"); byte[] bytes = new byte[stream.available()]; stream.read(bytes); Class\u003c?\u003e clazz = classLoader.defineClass(\"com.test.Test\", bytes); //类名必须和我们定义的保持一致 System.out.println(clazz.getName()); //成功加载外部class文件 } 现在，我们就将此class文件读取并解析为Class了，现在我们就可以对此类进行操作了（注意，我们无法在代码中直接使用此类型，因为它是我们直接加载的），我们来试试看创建一个此类的对象并调用其方法： try { Object obj = clazz.newInstance(); Method method = clazz.getMethod(\"test\", String.class); //获取我们定义的test(String str)方法 method.invoke(obj, \"哥们这瓜多少钱一斤？\"); }catch (Exception e){ e.printStackTrace(); } 我们来试试看修改成员字段之后，再来调用此方法： try { Object obj = clazz.newInstance(); Field field = clazz.getField(\"text\"); //获取成员变量 String text; field.set(obj, \"华强\"); Method method = clazz.getMethod(\"test\", String.class); //获取我们定义的test(String str)方法 method.invoke(obj, \"哥们这瓜多少钱一斤？\"); }catch (Exception e){ e.printStackTrace(); } 通过这种方式，我们就可以实现外部加载甚至是网络加载一个类，只需要把类文件传递即可，这样就无需再将代码写在本地，而是动态进行传递，不仅可以一定程度上防止源代码被反编译（只是一定程度上，想破解你代码有的是方法），而且在更多情况下，我们还可以对byte[]进行加密，保证在传输过程中的安全性。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:6:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"注解 其实我们在之前就接触到注解了，比如@Override表示重写父类方法（当然不加效果也是一样的，此注解在编译时会被自动丢弃）注解本质上也是一个类，只不过它的用法比较特殊。 注解可以被标注在任意地方，包括方法上、类名上、参数上、成员属性上、注解定义上等，就像注释一样，它相当于我们对某样东西的一个标记。而与注释不同的是，注解可以通过反射在运行时获取，注解也可以选择是否保留到运行时。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:7:0","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"预设注解 JDK预设了以下注解，作用于代码： @Override - 检查（仅仅是检查，不保留到运行时）该方法是否是重写方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。 @Deprecated - 标记过时方法。如果使用该方法，会报编译警告。 @SuppressWarnings - 指示编译器去忽略注解中声明的警告（仅仅编译器阶段，不保留到运行时） @FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。 @SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:7:1","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"元注解 元注解是作用于注解上的注解，用于我们编写自定义的注解： @Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问。 @Documented - 标记这些注解是否包含在用户文档中。 @Target - 标记这个注解应该是哪种 Java 成员。 @Inherited - 标记这个注解是继承于哪个注解类(默认 注解并没有继承于任何子类) @Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。 看了这么多预设的注解，你们肯定眼花缭乱了，那我们来看看@Override是如何定义的： @Target(ElementType.METHOD) @Retention(RetentionPolicy.SOURCE) public @interface Override { } 该注解由@Target限定为只能作用于方法上，ElementType是一个枚举类型，用于表示此枚举的作用域，一个注解可以有很多个作用域。@Retention表示此注解的保留策略，包括三种策略，在上述中有写到，而这里定义为只在代码中。一般情况下，自定义的注解需要定义1个@Retention和1-n个@Target。 既然了解了元注解的使用和注解的定义方式，我们就来尝试定义一个自己的注解： @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface Test { } 这里我们定义一个Test注解，并将其保留到运行时，同时此注解可以作用于方法或是类上： @Test public class Main { @Test public static void main(String[] args) { } } 这样，一个最简单的注解就被我们创建了。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:7:2","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"注解的使用 我们还可以在注解中定义一些属性，注解的属性也叫做成员变量，注解只有成员变量，没有方法。注解的成员变量在注解的定义中以“无形参的方法”形式来声明，其方法名定义了该成员变量的名字，其返回值定义了该成员变量的类型： @Target({ElementType.METHOD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface Test { String value(); } 默认只有一个属性时，我们可以将其名字设定为value，否则，我们需要在使用时手动指定注解的属性名称，使用value则无需填入： @Target({ElementType.METHOD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface Test { String test(); } public class Main { @Test(test = \"\") public static void main(String[] args) { } } 我们也可以使用default关键字来为这些属性指定默认值： @Target({ElementType.METHOD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface Test { String value() default \"都看到这里了，给个三连吧！\"; } 当属性存在默认值时，使用注解的时候可以不用传入属性值。当属性为数组时呢？ @Target({ElementType.METHOD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface Test { String[] value(); } 当属性为数组，我们在使用注解传参时，如果数组里面只有一个内容，我们可以直接传入一个值，而不是创建一个数组： @Test(\"关注点了吗\") public static void main(String[] args) { } public class Main { @Test({\"value1\", \"value2\"}) //多个值时就使用花括号括起来 public static void main(String[] args) { } } ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:7:3","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"反射获取注解 既然我们的注解可以保留到运行时，那么我们来看看，如何获取我们编写的注解，我们需要用到反射机制： public static void main(String[] args) { Class\u003cStudent\u003e clazz = Student.class; for (Annotation annotation : clazz.getAnnotations()) { System.out.println(annotation.annotationType()); //获取类型 System.out.println(annotation instanceof Test); //直接判断是否为Test Test test = (Test) annotation; System.out.println(test.value()); //获取我们在注解中写入的内容 } } 通过反射机制，我们可以快速获取到我们标记的注解，同时还能获取到注解中填入的值，那么我们来看看，方法上的标记是不是也可以通过这种方式获取注解： public static void main(String[] args) throws NoSuchMethodException { Class\u003cStudent\u003e clazz = Student.class; for (Annotation annotation : clazz.getMethod(\"test\").getAnnotations()) { System.out.println(annotation.annotationType()); //获取类型 System.out.println(annotation instanceof Test); //直接判断是否为Test Test test = (Test) annotation; System.out.println(test.value()); //获取我们在注解中写入的内容 } } 无论是方法、类、还是字段，都可以使用getAnnotations()方法（还有几个同名的）来快速获取我们标记的注解。 所以说呢，这玩意学来有啥用？丝毫get不到这玩意的用处。其实不是，现阶段你们还体会不到注解带来的快乐，在接触到Spring和SpringBoot等大型框架后，就能感受到注解带来的魅力了。 ","date":"2022-01-23","objectID":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/:7:4","tags":["Java反射与注解"],"title":"Java反射与注解","uri":"/posts/java%E5%8F%8D%E5%B0%84%E4%B8%8E%E6%B3%A8%E5%B0%84/"},{"categories":["JavaSE笔记"],"content":"Java多线程","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"Java多线程 配套视频 **注意：**本章节会涉及到 操作系统 相关知识。 在了解多线程之前，让我们回顾一下操作系统中提到的进程概念： 进程是程序执行的实体，每一个进程都是一个应用程序（比如我们运行QQ、浏览器、LOL、网易云音乐等软件），都有自己的内存空间，CPU一个核心同时只能处理一件事情，当出现多个进程需要同时运行时，CPU一般通过时间片轮转调度算法，来实现多个进程的同时运行。 在早期的计算机中，进程是拥有资源和独立运行的最小单位，也是程序执行的最小单位。但是，如果我希望两个任务同时进行，就必须运行两个进程，由于每个进程都有一个自己的内存空间，进程之间的通信就变得非常麻烦（比如要共享某些数据）而且执行不同进程会产生上下文切换，非常耗时，那么能否实现在一个进程中就能够执行多个任务呢？ 后来，线程横空出世，一个进程可以有多个线程，线程是程序执行中一个单一的顺序控制流程，现在线程才是程序执行流的最小单元，各个线程之间共享程序的内存空间（也就是所在进程的内存空间），上下文切换速度也高于进程。 在Java中，我们从开始，一直以来编写的都是单线程应用程序（运行main()方法的内容），也就是说只能同时执行一个任务（无论你是调用方法、还是进行计算，始终都是依次进行的，也就是同步的），而如果我们希望同时执行多个任务（两个方法同时在运行或者是两个计算同时在进行，也就是异步的），就需要用到Java多线程框架。实际上一个Java程序启动后，会创建很多线程，不仅仅只运行一个主线程： public static void main(String[] args) { ThreadMXBean bean = ManagementFactory.getThreadMXBean(); long[] ids = bean.getAllThreadIds(); ThreadInfo[] infos = bean.getThreadInfo(ids); for (ThreadInfo info : infos) { System.out.println(info.getThreadName()); } } 关于除了main线程默认以外的线程，涉及到JVM相关底层原理，在这里不做讲解，了解就行。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:0:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程的创建和启动 通过创建Thread对象来创建一个新的线程，Thread构造方法中需要传入一个Runnable接口的实现（其实就是编写要在另一个线程执行的内容逻辑）同时Runnable只有一个未实现方法，因此可以直接使用lambda表达式： @FunctionalInterface public interface Runnable { /** * When an object implementing interface \u003ccode\u003eRunnable\u003c/code\u003e is used * to create a thread, starting the thread causes the object's * \u003ccode\u003erun\u003c/code\u003e method to be called in that separately executing * thread. * \u003cp\u003e * The general contract of the method \u003ccode\u003erun\u003c/code\u003e is that it may * take any action whatsoever. * * @see java.lang.Thread#run() */ public abstract void run(); } 创建好后，通过调用start()方法来运行此线程： public static void main(String[] args) { Thread t = new Thread(() -\u003e { //直接编写逻辑 System.out.println(\"我是另一个线程！\"); }); t.start(); //调用此方法来开始执行此线程 } 可能上面的例子看起来和普通的单线程没两样，那我们先来看看下面这段代码的运行结果： public static void main(String[] args) { Thread t = new Thread(() -\u003e { System.out.println(\"我是线程：\"+Thread.currentThread().getName()); System.out.println(\"我正在计算 0-10000 之间所有数的和...\"); int sum = 0; for (int i = 0; i \u003c= 10000; i++) { sum += i; } System.out.println(\"结果：\"+sum); }); t.start(); System.out.println(\"我是主线程！\"); } 我们发现，这段代码执行输出结果并不是按照从上往下的顺序了，因为他们分别位于两个线程，他们是同时进行的！如果你还是觉得很疑惑，我们接着来看下面的代码运行结果： public static void main(String[] args) { Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 50; i++) { System.out.println(\"我是一号线程：\"+i); } }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 50; i++) { System.out.println(\"我是二号线程：\"+i); } }); t1.start(); t2.start(); } 我们可以看到打印实际上是在交替进行的，也证明了他们是在同时运行！ 注意：我们发现还有一个run方法，也能执行线程里面定义的内容，但是run是直接在当前线程执行，并不是创建一个线程执行！ 实际上，线程和进程差不多，也会等待获取CPU资源，一旦获取到，就开始按顺序执行我们给定的程序，当需要等待外部IO操作（比如Scanner获取输入的文本），就会暂时处于休眠状态，等待通知，或是调用sleep()方法来让当前线程休眠一段时间： public static void main(String[] args) throws InterruptedException { System.out.println(\"l\"); Thread.sleep(1000); //休眠时间，以毫秒为单位，1000ms = 1s System.out.println(\"b\"); Thread.sleep(1000); System.out.println(\"w\"); Thread.sleep(1000); System.out.println(\"nb!\"); } 我们也可以使用stop()方法来强行终止此线程： public static void main(String[] args) throws InterruptedException { Thread t = new Thread(() -\u003e { Thread me = Thread.currentThread(); //获取当前线程对象 for (int i = 0; i \u003c 50; i++) { System.out.println(\"打印:\"+i); if(i == 20) me.stop(); //此方法会直接终止此线程 } }); t.start(); } 虽然stop()方法能够终止此线程，但是并不是所推荐的做法，有关线程中断相关问题，我们会在后面继续了解。 思考：猜猜以下程序输出结果： private static int value = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) value++; System.out.println(\"线程1完成\"); }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) value++; System.out.println(\"线程2完成\"); }); t1.start(); t2.start(); Thread.sleep(1000); //主线程停止1秒，保证两个线程执行完成 System.out.println(value); } 我们发现，value最后的值并不是我们理想的结果，有关为什么会出现这种问题，在我们学习到线程锁的时候，再来探讨。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程的休眠和中断 我们前面提到，一个线程处于运行状态下，线程的下一个状态会出现以下情况： 当CPU给予的运行时间结束时，会从运行状态回到就绪（可运行）状态，等待下一次获得CPU资源。 当线程进入休眠 / 阻塞(如等待IO请求) / 手动调用wait()方法时，会使得线程处于等待状态，当等待状态结束后会回到就绪状态。 当线程出现异常或错误 / 被stop() 方法强行停止 / 所有代码执行结束时，会使得线程的运行终止。 而这个部分我们着重了解一下线程的休眠和中断，首先我们来了解一下如何使得线程进如休眠状态： public static void main(String[] args) { Thread t = new Thread(() -\u003e { try { System.out.println(\"l\"); Thread.sleep(1000); //sleep方法是Thread的静态方法，它只作用于当前线程（它知道当前线程是哪个） System.out.println(\"b\"); //调用sleep后，线程会直接进入到等待状态，直到时间结束 } catch (InterruptedException e) { e.printStackTrace(); } }); t.start(); } 通过调用sleep()方法来将当前线程进入休眠，使得线程处于等待状态一段时间。我们发现，此方法显示声明了会抛出一个InterruptedException异常，那么这个异常在什么时候会发生呢？ public static void main(String[] args) { Thread t = new Thread(() -\u003e { try { Thread.sleep(10000); //休眠10秒 } catch (InterruptedException e) { e.printStackTrace(); } }); t.start(); try { Thread.sleep(3000); //休眠3秒，一定比线程t先醒来 t.interrupt(); //调用t的interrupt方法 } catch (InterruptedException e) { e.printStackTrace(); } } 我们发现，每一个Thread对象中，都有一个interrupt()方法，调用此方法后，会给指定线程添加一个中断标记以告知线程需要立即停止运行或是进行其他操作，由线程来响应此中断并进行相应的处理，我们前面提到的stop()方法是强制终止线程，这样的做法虽然简单粗暴，但是很有可能导致资源不能完全释放，而类似这样的发送通知来告知线程需要中断，让线程自行处理后续，会更加合理一些，也是更加推荐的做法。我们来看看interrupt的用法： public static void main(String[] args) { Thread t = new Thread(() -\u003e { System.out.println(\"线程开始运行！\"); while (true){ //无限循环 if(Thread.currentThread().isInterrupted()){ //判断是否存在中断标志 break; //响应中断 } } System.out.println(\"线程被中断了！\"); }); t.start(); try { Thread.sleep(3000); //休眠3秒，一定比线程t先醒来 t.interrupt(); //调用t的interrupt方法 } catch (InterruptedException e) { e.printStackTrace(); } } 通过isInterrupted()可以判断线程是否存在中断标志，如果存在，说明外部希望当前线程立即停止，也有可能是给当前线程发送一个其他的信号，如果我们并不是希望收到中断信号就是结束程序，而是通知程序做其他事情，我们可以在收到中断信号后，复位中断标记，然后继续做我们的事情： public static void main(String[] args) { Thread t = new Thread(() -\u003e { System.out.println(\"线程开始运行！\"); while (true){ if(Thread.currentThread().isInterrupted()){ //判断是否存在中断标志 System.out.println(\"发现中断信号，复位，继续运行...\"); Thread.interrupted(); //复位中断标记（返回值是当前是否有中断标记，这里不用管） } } }); t.start(); try { Thread.sleep(3000); //休眠3秒，一定比线程t先醒来 t.interrupt(); //调用t的interrupt方法 } catch (InterruptedException e) { e.printStackTrace(); } } 复位中断标记后，会立即清除中断标记。那么，如果现在我们想暂停线程呢？我们希望线程暂时停下，比如等待其他线程执行完成后，再继续运行，那这样的操作怎么实现呢？ public static void main(String[] args) { Thread t = new Thread(() -\u003e { System.out.println(\"线程开始运行！\"); Thread.currentThread().suspend(); //暂停此线程 System.out.println(\"线程继续运行！\"); }); t.start(); try { Thread.sleep(3000); //休眠3秒，一定比线程t先醒来 t.resume(); //恢复此线程 } catch (InterruptedException e) { e.printStackTrace(); } } 虽然这样很方便地控制了线程的暂停状态，但是这两个方法我们发现实际上也是不推荐的做法，它很容易导致死锁！有关为什么被弃用的原因，我们会在线程锁继续探讨。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程的优先级 实际上，Java程序中的每个线程并不是平均分配CPU时间的，为了使得线程资源分配更加合理，Java采用的是抢占式调度方式，优先级越高的线程，优先使用CPU资源！我们希望CPU花费更多的时间去处理更重要的任务，而不太重要的任务，则可以先让出一部分资源。线程的优先级一般分为以下三种： MIN_PRIORITY 最低优先级 MAX_PRIORITY 最高优先级 NOM_PRIORITY 常规优先级 public static void main(String[] args) { Thread t = new Thread(() -\u003e { System.out.println(\"线程开始运行！\"); }); t.start(); t.setPriority(Thread.MIN_PRIORITY); //通过使用setPriority方法来设定优先级 } 优先级越高的线程，获得CPU资源的概率会越大，并不是说一定优先级越高的线程越先执行！ ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:3:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程的礼让和加入 我们还可以在当前线程的工作不重要时，将CPU资源让位给其他线程，通过使用yield()方法来将当前资源让位给其他同优先级线程： public static void main(String[] args) { Thread t1 = new Thread(() -\u003e { System.out.println(\"线程1开始运行！\"); for (int i = 0; i \u003c 50; i++) { if(i % 5 == 0) { System.out.println(\"让位！\"); Thread.yield(); } System.out.println(\"1打印：\"+i); } System.out.println(\"线程1结束！\"); }); Thread t2 = new Thread(() -\u003e { System.out.println(\"线程2开始运行！\"); for (int i = 0; i \u003c 50; i++) { System.out.println(\"2打印：\"+i); } }); t1.start(); t2.start(); } 观察结果，我们发现，在让位之后，尽可能多的在执行线程2的内容。 当我们希望一个线程等待另一个线程执行完成后再继续进行，我们可以使用join()方法来实现线程的加入： public static void main(String[] args) { Thread t1 = new Thread(() -\u003e { System.out.println(\"线程1开始运行！\"); for (int i = 0; i \u003c 50; i++) { System.out.println(\"1打印：\"+i); } System.out.println(\"线程1结束！\"); }); Thread t2 = new Thread(() -\u003e { System.out.println(\"线程2开始运行！\"); for (int i = 0; i \u003c 50; i++) { System.out.println(\"2打印：\"+i); if(i == 10){ try { System.out.println(\"线程1加入到此线程！\"); t1.join(); //在i==10时，让线程1加入，先完成线程1的内容，在继续当前内容 } catch (InterruptedException e) { e.printStackTrace(); } } } }); t1.start(); t2.start(); } 我们发现，线程1加入后，线程2等待线程1待执行的内容全部执行完成之后，再继续执行的线程2内容。注意，线程的加入只是等待另一个线程的完成，并不是将另一个线程和当前线程合并！我们来看看： public static void main(String[] args) { Thread t1 = new Thread(() -\u003e { System.out.println(Thread.currentThread().getName()+\"开始运行！\"); for (int i = 0; i \u003c 50; i++) { System.out.println(Thread.currentThread().getName()+\"打印：\"+i); } System.out.println(\"线程1结束！\"); }); Thread t2 = new Thread(() -\u003e { System.out.println(\"线程2开始运行！\"); for (int i = 0; i \u003c 50; i++) { System.out.println(\"2打印：\"+i); if(i == 10){ try { System.out.println(\"线程1加入到此线程！\"); t1.join(); //在i==10时，让线程1加入，先完成线程1的内容，在继续当前内容 } catch (InterruptedException e) { e.printStackTrace(); } } } }); t1.start(); t2.start(); } 实际上，t2线程只是暂时处于等待状态，当t1执行结束时，t2才开始继续执行，只是在效果上看起来好像是两个线程合并为一个线程在执行而已。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:4:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程锁和线程同步 在开始讲解线程同步之前，我们需要先了解一下多线程情况下Java的内存管理： 线程之间的共享变量（比如之前悬念中的value变量）存储在主内存（main memory）中，每个线程都有一个私有的工作内存（本地内存），工作内存中存储了该线程以读/写共享变量的副本。它类似于我们在计算机组成原理中学习的多处理器高速缓存机制： 高速缓存通过保存内存中数据的副本来提供更加快速的数据访问，但是如果多个处理器的运算任务都涉及同一块内存区域，就可能导致各自的高速缓存数据不一致，在写回主内存时就会发生冲突，这就是引入高速缓存引发的新问题，称之为：缓存一致性。 实际上，Java的内存模型也是这样类似设计的，当我们同时去操作一个共享变量时，如果仅仅是读取还好，但是如果同时写入内容，就会出现问题！好比说一个银行，如果我和我的朋友同时在银行取我账户里面的钱，难道取1000还可能吐2000出来吗？我们需要一种更加安全的机制来维持秩序，保证数据的安全性！ ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"悬念破案 我们再来回顾一下之前留给大家的悬念： private static int value = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) value++; System.out.println(\"线程1完成\"); }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) value++; System.out.println(\"线程2完成\"); }); t1.start(); t2.start(); Thread.sleep(1000); //主线程停止1秒，保证两个线程执行完成 System.out.println(value); } 实际上，当两个线程同时读取value的时候，可能会同时拿到同样的值，而进行自增操作之后，也是同样的值，再写回主内存后，本来应该进行2次自增操作，实际上只执行了一次！ 那么要去解决这样的问题，我们就必须采取某种同步机制，来限制不同线程对于共享变量的访问！我们希望的是保证共享变量value自增操作的原子性（原子性是指一个操作或多个操作要么全部执行，且执行的过程不会被任何因素打断，包括其他线程，要么就都不执行） ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:1","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"线程锁 通过synchronized关键字来创造一个线程锁，首先我们来认识一下synchronized代码块，它需要在括号中填入一个内容，必须是一个对象或是一个类，我们在value自增操作外套上同步代码块： private static int value = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) { synchronized (Main.class){ value++; } } System.out.println(\"线程1完成\"); }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) { synchronized (Main.class){ value++; } } System.out.println(\"线程2完成\"); }); t1.start(); t2.start(); Thread.sleep(1000); //主线程停止1秒，保证两个线程执行完成 System.out.println(value); } 我们发现，现在得到的结果就是我们想要的内容了，因为在同步代码块执行过程中，拿到了我们传入对象或类的锁（传入的如果是对象，就是对象锁，不同的对象代表不同的对象锁，如果是类，就是类锁，类锁只有一个，实际上类锁也是对象锁，是Class类实例，但是Class类实例同样的类无论怎么获取都是同一个），但是注意两个线程必须使用同一把锁！ 当一个线程进入到同步代码块时，会获取到当前的锁，而这时如果其他使用同样的锁的同步代码块也想执行内容，就必须等待当前同步代码块的内容执行完毕，在执行完毕后会自动释放这把锁，而其他的线程才能拿到这把锁并开始执行同步代码块里面的内容。（实际上synchronized是一种悲观锁，随时都认为有其他线程在对数据进行修改，后面有机会我们还会讲到乐观锁，如CAS算法） 那么我们来看看，如果使用的是不同对象的锁，那么还能顺利进行吗？ private static int value = 0; public static void main(String[] args) throws InterruptedException { Main main1 = new Main(); Main main2 = new Main(); Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) { synchronized (main1){ value++; } } System.out.println(\"线程1完成\"); }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) { synchronized (main2){ value++; } } System.out.println(\"线程2完成\"); }); t1.start(); t2.start(); Thread.sleep(1000); //主线程停止1秒，保证两个线程执行完成 System.out.println(value); } 当对象不同时，获取到的是不同的锁，因此并不能保证自增操作的原子性，最后也得不到我们想要的结果。 synchronized关键字也可以作用于方法上，调用此方法时也会获取锁： private static int value = 0; private static synchronized void add(){ value++; } public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) add(); System.out.println(\"线程1完成\"); }); Thread t2 = new Thread(() -\u003e { for (int i = 0; i \u003c 10000; i++) add(); System.out.println(\"线程2完成\"); }); t1.start(); t2.start(); Thread.sleep(1000); //主线程停止1秒，保证两个线程执行完成 System.out.println(value); } 我们发现实际上效果是相同的，只不过这个锁不用你去给，如果是静态方法，就是使用的类锁，而如果是普通成员方法，就是使用的对象锁。通过灵活的使用synchronized就能很好地解决我们之前提到的问题了！ ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:2","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"死锁 其实死锁的概念在操作系统中也有提及，它是指两个线程相互持有对方需要的锁，但是又迟迟不释放，导致程序卡住： 我们发现，线程A和线程B都需要对方的锁，但是又被对方牢牢把握，由于线程被无限期地阻塞，因此程序不可能正常终止。我们来看看以下这段代码会得到什么结果： public static void main(String[] args) throws InterruptedException { Object o1 = new Object(); Object o2 = new Object(); Thread t1 = new Thread(() -\u003e { synchronized (o1){ try { Thread.sleep(1000); synchronized (o2){ System.out.println(\"线程1\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }); Thread t2 = new Thread(() -\u003e { synchronized (o2){ try { Thread.sleep(1000); synchronized (o1){ System.out.println(\"线程2\"); } } catch (InterruptedException e) { e.printStackTrace(); } } }); t1.start(); t2.start(); } 那么我们如何去检测死锁呢？我们可以利用jstack命令来检测死锁，首先利用jps找到我们的java进程： nagocoler@NagodeMacBook-Pro ~ % jps 51592 Launcher 51690 Jps 14955 51693 Main nagocoler@NagodeMacBook-Pro ~ % jstack 51693 ... Java stack information for the threads listed above: =================================================== \"Thread-1\": at com.test.Main.lambda$main$1(Main.java:46) - waiting to lock \u003c0x000000076ad27fc0\u003e (a java.lang.Object) - locked \u003c0x000000076ad27fd0\u003e (a java.lang.Object) at com.test.Main$$Lambda$2/1867750575.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) \"Thread-0\": at com.test.Main.lambda$main$0(Main.java:34) - waiting to lock \u003c0x000000076ad27fd0\u003e (a java.lang.Object) - locked \u003c0x000000076ad27fc0\u003e (a java.lang.Object) at com.test.Main$$Lambda$1/396873410.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock. jstack自动帮助我们找到了一个死锁，并打印出了相关线程的栈追踪信息。 不推荐使用 suspend() 去挂起线程的原因，是因为suspend()在使线程暂停的同时，并不会去释放任何锁资源。其他线程都无法访问被它占用的锁。直到对应的线程执行resume()方法后，被挂起的线程才能继续，从而其它被阻塞在这个锁的线程才可以继续执行。但是，如果resume()操作出现在suspend()之前执行，那么线程将一直处于挂起状态，同时一直占用锁，这就产生了死锁。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:3","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"wait和notify方法 其实我们之前可能就发现了，Object类还有三个方法我们从来没有使用过，分别是wait()、notify()以及notifyAll()，他们其实是需要配合synchronized来使用的，只有在同步代码块中才能使用这些方法，我们来看看他们的作用是什么： public static void main(String[] args) throws InterruptedException { Object o1 = new Object(); Thread t1 = new Thread(() -\u003e { synchronized (o1){ try { System.out.println(\"开始等待\"); o1.wait(); //进入等待状态并释放锁 System.out.println(\"等待结束！\"); } catch (InterruptedException e) { e.printStackTrace(); } } }); Thread t2 = new Thread(() -\u003e { synchronized (o1){ System.out.println(\"开始唤醒！\"); o1.notify(); //唤醒处于等待状态的线程 for (int i = 0; i \u003c 50; i++) { System.out.println(i); } //唤醒后依然需要等待这里的锁释放之前等待的线程才能继续 } }); t1.start(); Thread.sleep(1000); t2.start(); } 我们可以发现，对象的wait()方法会暂时使得此线程进入等待状态，同时会释放当前代码块持有的锁，这时其他线程可以获取到此对象的锁，当其他线程调用对象的notify()方法后，会唤醒刚才变成等待状态的线程（这时并没有立即释放锁）。注意，必须是在持有锁（同步代码块内部）的情况下使用，否则会抛出异常！ notifyAll其实和notify一样，也是用于唤醒，但是前者是唤醒所有调用wait()后处于等待的线程，而后者是看运气随机选择一个。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:4","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"ThreadLocal的使用 既然每个线程都有一个自己的工作内存，那么能否只在自己的工作内存中创建变量仅供线程自己使用呢？ 我们可以是ThreadLocal类，来创建工作内存中的变量，它将我们的变量值存储在内部（只能存储一个变量），不同的变量访问到ThreadLocal对象时，都只能获取到自己线程所属的变量。 public static void main(String[] args) throws InterruptedException { ThreadLocal\u003cString\u003e local = new ThreadLocal\u003c\u003e(); //注意这是一个泛型类，存储类型为我们要存放的变量类型 Thread t1 = new Thread(() -\u003e { local.set(\"lbwnb\"); //将变量的值给予ThreadLocal System.out.println(\"变量值已设定！\"); System.out.println(local.get()); //尝试获取ThreadLocal中存放的变量 }); Thread t2 = new Thread(() -\u003e { System.out.println(local.get()); //尝试获取ThreadLocal中存放的变量 }); t1.start(); Thread.sleep(3000); //间隔三秒 t2.start(); } 上面的例子中，我们开启两个线程分别去访问ThreadLocal对象，我们发现，第一个线程存放的内容，第一个线程可以获取，但是第二个线程无法获取，我们再来看看第一个线程存入后，第二个线程也存放，是否会覆盖第一个线程存放的内容： public static void main(String[] args) throws InterruptedException { ThreadLocal\u003cString\u003e local = new ThreadLocal\u003c\u003e(); //注意这是一个泛型类，存储类型为我们要存放的变量类型 Thread t1 = new Thread(() -\u003e { local.set(\"lbwnb\"); //将变量的值给予ThreadLocal System.out.println(\"线程1变量值已设定！\"); try { Thread.sleep(2000); //间隔2秒 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"线程1读取变量值：\"); System.out.println(local.get()); //尝试获取ThreadLocal中存放的变量 }); Thread t2 = new Thread(() -\u003e { local.set(\"yyds\"); //将变量的值给予ThreadLocal System.out.println(\"线程2变量值已设定！\"); }); t1.start(); Thread.sleep(1000); //间隔1秒 t2.start(); } 我们发现，即使线程2重新设定了值，也没有影响到线程1存放的值，所以说，不同线程向ThreadLocal存放数据，只会存放在线程自己的工作空间中，而不会直接存放到主内存中，因此各个线程直接存放的内容互不干扰。 我们发现在线程中创建的子线程，无法获得父线程工作内存中的变量： public static void main(String[] args) { ThreadLocal\u003cString\u003e local = new ThreadLocal\u003c\u003e(); Thread t = new Thread(() -\u003e { local.set(\"lbwnb\"); new Thread(() -\u003e { System.out.println(local.get()); }).start(); }); t.start(); } 我们可以使用InheritableThreadLocal来解决： public static void main(String[] args) { ThreadLocal\u003cString\u003e local = new InheritableThreadLocal\u003c\u003e(); Thread t = new Thread(() -\u003e { local.set(\"lbwnb\"); new Thread(() -\u003e { System.out.println(local.get()); }).start(); }); t.start(); } 在InheritableThreadLocal存放的内容，会自动向子线程传递。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:5","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"定时器 我们有时候会有这样的需求，我希望定时执行任务，比如3秒后执行，其实我们可以通过使用Thread.sleep()来实现： public static void main(String[] args) { new TimerTask(() -\u003e System.out.println(\"我是定时任务！\"), 3000).start(); //创建并启动此定时任务 } static class TimerTask{ Runnable task; long time; public TimerTask(Runnable runnable, long time){ this.task = runnable; this.time = time; } public void start(){ new Thread(() -\u003e { try { Thread.sleep(time); task.run(); //休眠后再运行 } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } 我们通过自行封装一个TimerTask类，并在启动时，先休眠3秒钟，再执行我们传入的内容。那么现在我们希望，能否循环执行一个任务呢？比如我希望每隔1秒钟执行一次代码，这样该怎么做呢？ public static void main(String[] args) { new TimerLoopTask(() -\u003e System.out.println(\"我是定时任务！\"), 3000).start(); //创建并启动此定时任务 } static class TimerLoopTask{ Runnable task; long loopTime; public TimerLoopTask(Runnable runnable, long loopTime){ this.task = runnable; this.loopTime = loopTime; } public void start(){ new Thread(() -\u003e { try { while (true){ //无限循环执行 Thread.sleep(loopTime); task.run(); //休眠后再运行 } } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } 现在我们将单次执行放入到一个无限循环中，这样就能一直执行了，并且按照我们的间隔时间进行。 但是终究是我们自己实现，可能很多方面还没考虑到，Java也为我们提供了一套自己的框架用于处理定时任务： public static void main(String[] args) { Timer timer = new Timer(); //创建定时器对象 timer.schedule(new TimerTask() { //注意这个是一个抽象类，不是接口，无法使用lambda表达式简化，只能使用匿名内部类 @Override public void run() { System.out.println(Thread.currentThread().getName()); //打印当前线程名称 } }, 1000); //执行一个延时任务 } 我们可以通过创建一个Timer类来让它进行定时任务调度，我们可以通过此对象来创建任意类型的定时任务，包延时任务、循环定时任务等。我们发现，虽然任务执行完成了，但是我们的程序并没有停止，这是因为Timer内存维护了一个任务队列和一个工作线程： public class Timer { /** * The timer task queue. This data structure is shared with the timer * thread. The timer produces tasks, via its various schedule calls, * and the timer thread consumes, executing timer tasks as appropriate, * and removing them from the queue when they're obsolete. */ private final TaskQueue queue = new TaskQueue(); /** * The timer thread. */ private final TimerThread thread = new TimerThread(queue); ... } TimerThread继承自Thread，是一个新创建的线程，在构造时自动启动： public Timer(String name) { thread.setName(name); thread.start(); } 而它的run方法会循环地读取队列中是否还有任务，如果有任务依次执行，没有的话就暂时处于休眠状态： public void run() { try { mainLoop(); } finally { // Someone killed this Thread, behave as if Timer cancelled synchronized(queue) { newTasksMayBeScheduled = false; queue.clear(); // Eliminate obsolete references } } } /** * The main timer loop. (See class comment.) */ private void mainLoop() { try { TimerTask task; boolean taskFired; synchronized(queue) { // Wait for queue to become non-empty while (queue.isEmpty() \u0026\u0026 newTasksMayBeScheduled) //当队列为空同时没有被关闭时，会调用wait()方法暂时处于等待状态，当有新的任务时，会被唤醒。 queue.wait(); if (queue.isEmpty()) break; //当被唤醒后都没有任务时，就会结束循环，也就是结束工作线程 ... } newTasksMayBeScheduled实际上就是标记当前定时器是否关闭，当它为false时，表示已经不会再有新的任务到来，也就是关闭，我们可以通过调用cancel()方法来关闭它的工作线程： public void cancel() { synchronized(queue) { thread.newTasksMayBeScheduled = false; queue.clear(); queue.notify(); //唤醒wait使得工作线程结束 } } 因此，我们可以在使用完成后，调用Timer的cancel()方法以正常退出我们的程序： public static void main(String[] args) { Timer timer = new Timer(); timer.schedule(new TimerTask() { @Override public void run() { System.out.println(Thread.currentThread().getName()); timer.cancel(); //结束 } }, 1000); } ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:6:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"守护线程 不要把守护进程和守护线程相提并论！守护进程在后台运行运行，不需要和用户交互，本质和普通进程类似。而守护线程就不一样了，当其他所有的非守护线程结束之后，守护线程是自动结束，也就是说，Java中所有的线程都执行完毕后，守护线程自动结束，因此守护线程不适合进行IO操作，只适合打打杂： public static void main(String[] args) throws InterruptedException{ Thread t = new Thread(() -\u003e { while (true){ try { System.out.println(\"程序正常运行中...\"); Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } }); t.setDaemon(true); //设置为守护线程（必须在开始之前，中途是不允许转换的） t.start(); for (int i = 0; i \u003c 5; i++) { Thread.sleep(1000); } } 在守护线程中产生的新线程也是守护的： public static void main(String[] args) throws InterruptedException{ Thread t = new Thread(() -\u003e { Thread it = new Thread(() -\u003e { while (true){ try { System.out.println(\"程序正常运行中...\"); Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } }); it.start(); }); t.setDaemon(true); //设置为守护线程（必须在开始之前，中途是不允许转换的） t.start(); for (int i = 0; i \u003c 5; i++) { Thread.sleep(1000); } } ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"再谈集合类并行方法 其实我们之前在讲解集合类的根接口时，就发现有这样一个方法： default Stream\u003cE\u003e parallelStream() { return StreamSupport.stream(spliterator(), true); } 并行流，其实就是一个多线程执行的流，它通过默认的ForkJoinPool实现（这里不讲解原理），它可以提高你的多线程任务的速度。 public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(Arrays.asList(1, 4, 5, 2, 9, 3, 6, 0)); list .parallelStream() //获得并行流 .forEach(i -\u003e System.out.println(Thread.currentThread().getName()+\" -\u003e \"+i)); } 我们发现，forEach操作的顺序，并不是我们实际List中的顺序，同时每次打印也是不同的线程在执行！我们可以通过调用forEachOrdered()方法来使用单线程维持原本的顺序： public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(Arrays.asList(1, 4, 5, 2, 9, 3, 6, 0)); list .parallelStream() //获得并行流 .forEachOrdered(System.out::println); } 我们之前还发现，在Arrays数组工具类中，也包含大量的并行方法： public static void main(String[] args) { int[] arr = new int[]{1, 4, 5, 2, 9, 3, 6, 0}; Arrays.parallelSort(arr); //使用多线程进行并行排序，效率更高 System.out.println(Arrays.toString(arr)); } 更多地使用并行方法，可以更加充分地发挥现代计算机多核心的优势，但是同时需要注意多线程产生的异步问题！ public static void main(String[] args) { int[] arr = new int[]{1, 4, 5, 2, 9, 3, 6, 0}; Arrays.parallelSetAll(arr, i -\u003e { System.out.println(Thread.currentThread().getName()); return arr[i]; }); System.out.println(Arrays.toString(arr)); } 通过对Java多线程的了解，我们就具备了利用多线程解决问题的思维！ ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:8:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"Java多线程编程实战 这是整个教程最后一个编程实战内容了，下一章节为反射一般开发者使用比较少，属于选学内容，不编排编程实战课程。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:9:0","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"生产者与消费者 所谓的生产者消费者模型，是通过一个容器来解决生产者和消费者的强耦合问题。通俗的讲，就是生产者在不断的生产，消费者也在不断的消费，可是消费者消费的产品是生产者生产的，这就必然存在一个中间容器，我们可以把这个容器想象成是一个货架，当货架空的时候，生产者要生产产品，此时消费者在等待生产者往货架上生产产品，而当货架有货物的时候，消费者可以从货架上拿走商品，生产者此时等待货架出现空位，进而补货，这样不断的循环。 通过多线程编程，来模拟一个餐厅的2个厨师和3个顾客，假设厨师炒出一个菜的时间为3秒，顾客吃掉菜品的时间为4秒。 ","date":"2022-01-23","objectID":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/:9:1","tags":["Java多线程"],"title":"Java多线程","uri":"/posts/java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":["JavaSE笔记"],"content":"java面向对象","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"Java对象和多态 （面向对象） 配套视频 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:0:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"面向对象基础 面向对象程序设计(Object Oriented Programming) 对象基于类创建，类相当于一个模板，对象就是根据模板创建出来的实体（就像做月饼，我们要做一个月饼首先需要一个模具，模具就是我们的类，而做出来的月饼，就是类的实现，也叫做对象），类是抽象的数据类型，并不能代表某一个具体的事物，类是对象的一个模板。类具有自己的属性，包括成员变量、成员方法等，我们可以调用类的成员方法来让类进行一些操作。 Scanner sc = new Scanner(System.in); String str = sc.nextLine(); System.out.println(\"你输入了：\"+str); sc.close(); 所有的对象，都需要通过new关键字创建，基本数据类型不是对象！Java不是纯面对对象语言！ 不是基本类型的变量，都是引用类型，引用类型变量代表一个对象，而基本数据类型变量，保存的是基本数据类型的值，我们可以通过引用来对对象进行操作。（最好不要理解为引用指向对象的地址，初学者不要谈内存，学到JVM时再来讨论） 对象占用的内存由JVM统一管理，不需要手动释放内存，当一个对象不再使用时（比如失去引用或是离开了作用域）会被JVM自动清理，内存管理更方便！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:1:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"类的基本结构 为了快速掌握，我们自己创建一个自己的类，创建的类文件名称应该和类名一致。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"成员变量 在类中，可以包含许多的成员变量，也叫成员属性，成员字段(field)通过.来访问我们类中的成员变量，我们可以通过类创建的对象来访问和修改这些变量。成员变量是属于对象的！ public class Test { int age; String name; } public static void main(String[] args) { Test test = new Test(); test.name = \"奥利给\"; System.out.println(test.name); } 成员变量默认带有初始值，也可以自己定义初始值。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"成员方法 我们之前的学习中接触过方法(Method)吗？主方法！ public static void main(String[] args) { //Body } 方法是语句的集合，是为了完成某件事情而存在的。完成某件事情，可以有结果，也可以做了就做了，不返回结果。比如计算两个数字的和，我们需要得到计算后的结果，所以说方法需要有返回值；又比如，我们只想吧数字打印在控制台，只需要打印就行，不用给我结果，所以说方法不需要有返回值。 方法的定义和使用 在类中，我们可以定义自己的方法，格式如下： [返回值类型] 方法名称([参数]){ //方法体 return 结果; } 返回值类型：可以是引用类型和基本类型，还可以是void，表示没有返回值 方法名称：和标识符的规则一致，和变量一样，规范小写字母开头！ 参数：例如方法需要计算两个数的和，那么我们就要把两个数到底是什么告诉方法，那么它们就可以作为参数传入方法 方法体：方法具体要干的事情 结果：方法执行的结果通过return返回（如果返回类型为void，可以省略return） 非void方法中，return关键字不一定需要放在最后，但是一定要保证方法在任何情况下都具有返回值！ int test(int a){ if(a \u003e 0){ //缺少retrun语句！ }else{ return 0; } } return也能用来提前结束整个方法，无论此时程序执行到何处，无论return位于哪里，都会立即结束个方法！ void main(String[] args) { for (int i = 0; i \u003c 10; i++) { if(i == 1) return; //在循环内返回了！和break区别？ } System.out.println(\"淦\"); //还会到这里吗？ } 传入方法的参数，如果是基本类型，会在调用方法的时候，对参数的值进行复制，方法中的参数变量，不是我们传入的变量本身！ public static void main(String[] args) { int a = 10, b = 20; new Test().swap(a, b); System.out.println(\"a=\"+a+\", b=\"+b); } public class Test{ void swap(int a, int b){ //传递的仅仅是值而已！ int temp = a; a = b; b = temp; } } 传入方法的参数，如果是引用类型，那么传入的依然是该对象的引用！（类似于C语言的指针） public class B{ String name; } public class A{ void test(B b){ //传递的是对象的引用，而不是值 System.out.println(b.name); } } public static void main(String[] args) { int a = 10, b = 20; B b = new B(); b.name = \"lbw\"; new A().test(b); System.out.println(\"a=\"+a+\", b=\"+b); } 方法之间可以相互调用 void a(){ //xxxx } void b(){ a(); } 当方法在自己内部调用自己时，称为递归调用（递归很危险，慎重！） int a(){ return a(); } 成员方法和成员变量一样，是属于对象的，只能通过对象去调用！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"对象设计练习 学生应该具有以下属性：名字、年龄 学生应该具有以下行为：学习、运动、说话 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"方法的重载 一个类中可以包含多个同名的方法，但是需要的形式参数不一样。（补充：形式参数就是定义方法需要的参数，实际参数就传入的参数）方法的返回类型，可以相同，也可以不同，但是仅返回类型不同，是不允许的！ public class Test { int a(){ //原本的方法 return 1; } int a(int i){ //ok，形参不同 return i; } void a(byte i){ //ok，返回类型和形参都不同 } void a(){ //错误，仅返回值类型名称不同不能重载 } } 现在我们就可以使用不同的参数，但是支持调用同样的方法，执行一样的逻辑： public class Test { int sum(int a, int b){ //只有int支持，不灵活！ return a+b; } double sum(double a, double b){ //重写一个double类型的，就支持小数计算了 return a+b; } } 现在我们有很多种重写的方法，那么传入实参后，到底进了哪个方法呢？ public class Test { void a(int i){ System.out.println(\"调用了int\"); } void a(short i){ System.out.println(\"调用了short\"); } void a(long i){ System.out.println(\"调用了long\"); } void a(char i){ System.out.println(\"调用了char\"); } void a(double i){ System.out.println(\"调用了double\"); } void a(float i){ System.out.println(\"调用了float\"); } public static void main(String[] args) { Test test = new Test(); test.a(1); //直接输入整数 test.a(1.0); //直接输入小数 short s = 2; test.a(s); //会对号入座吗？ test.a(1.0F); } } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:4","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"构造方法 构造方法（构造器）没有返回值，也可以理解为，返回的是当前对象的引用！每一个类都默认自带一个无参构造方法。 //反编译结果 package com.test; public class Test { public Test() { //即使你什么都不编写，也自带一个无参构造方法，只是默认是隐藏的 } } 反编译其实就是把我们编译好的class文件变回Java源代码。 Test test = new Test(); //实际上存在Test()这个的方法，new关键字就是用来创建并得到引用的 // new + 你想要使用的构造方法 这种方法没有写明返回值，但是每个类都必须具有这个方法！只有调用类的构造方法，才能创建类的对象！ 类要在一开始准备的所有东西，都会在构造方法里面执行，完成构造方法的内容后，才能创建出对象！ 一般最常用的就是给成员属性赋初始值： public class Student { String name; Student(){ name = \"伞兵一号\"; } } 我们可以手动指定有参构造，当遇到名称冲突时，需要用到this关键字 public class Student { String name; Student(String name){ //形参和类成员变量冲突了，Java会优先使用形式参数定义的变量！ this.name = name; //通过this指代当前的对象属性，this就代表当前对象 } } //idea 右键快速生成！ 注意，this只能用于指代当前对象的内容，因此，只有属于对象拥有的部分才可以使用this，也就是说，只能在类的成员方法中使用this，不能在静态方法中使用this关键字。 在我们定义了新的有参构造之后，默认的无参构造会被覆盖！ //反编译后依然只有我们定义的有参构造！ 如果同时需要有参和无参构造，那么就需要用到方法的重载！手动再去定义一个无参构造。 public class Student { String name; Student(){ } Student(String name){ this.name = name; } } 成员变量的初始化始终在构造方法执行之前 public class Student { String a = \"sadasa\"; Student(){ System.out.println(a); } public static void main(String[] args) { Student s = new Student(); } } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:5","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"静态变量和静态方法 静态变量和静态方法是类具有的属性（后面还会提到静态类、静态代码块），也可以理解为是所有对象共享的内容。我们通过使用static关键字来声明一个变量或一个方法为静态的，一旦被声明为静态，那么通过这个类创建的所有对象，操作的都是同一个目标，也就是说，对象再多，也只有这一个静态的变量或方法。那么，一个对象改变了静态变量的值，那么其他的对象读取的就是被改变的值。 public class Student { static int a; } public static void main(String[] args) { Student s1 = new Student(); s1.a = 10; Student s2 = new Student(); System.out.println(s2.a); } 不推荐使用对象来调用，被标记为静态的内容，可以直接通过类名.xxx的形式访问 public static void main(String[] args) { Student.a = 10; System.out.println(Student.a); } 简述类加载机制 类并不是在一开始就全部加载好，而是在需要时才会去加载（提升速度）以下情况会加载类： 访问类的静态变量，或者为静态变量赋值 new 创建类的实例（隐式加载） 调用类的静态方法 子类初始化时 其他的情况会在讲到反射时介绍 所有被标记为静态的内容，会在类刚加载的时候就分配，而不是在对象创建的时候分配，所以说静态内容一定会在第一个对象初始化之前完成加载。 public class Student { static int a = test(); //直接调用静态方法，只能调用静态方法 Student(){ System.out.println(\"构造类对象\"); } static int test(){ //静态方法刚加载时就有了 System.out.println(\"初始化变量a\"); return 1; } } 思考：下面这种情况下，程序能正常运行吗？如果能，会输出什么内容？ public class Student { static int a = test(); static int test(){ return a; } public static void main(String[] args) { System.out.println(Student.a); } } 定义和赋值是两个阶段，在定义时会使用默认值（上面讲的，类的成员变量会有默认值）定义出来之后，如果发现有赋值语句，再进行赋值，而这时，调用了静态方法，所以说会先去加载静态方法，静态方法调用时拿到a，而a这时仅仅是刚定义，所以说还是初始值，最后得到0 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:6","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"代码块和静态代码块 代码块在对象创建时执行，也是属于类的内容，但是它在构造方法执行之前执行（和成员变量初始值一样），且每创建一个对象时，只执行一次！（相当于构造之前的准备工作） public class Student { { System.out.println(\"我是代码块\"); } Student(){ System.out.println(\"我是构造方法\"); } } 静态代码块和上面的静态方法和静态变量一样，在类刚加载时就会调用； public class Student { static int a; static { a = 10; } public static void main(String[] args) { System.out.println(Student.a); } } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:7","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"String和StringBuilder类 字符串类是一个比较特殊的类，他是Java中唯一重载运算符的类！(Java不支持运算符重载，String是特例) String的对象直接支持使用+或+=运算符来进行拼接，并形成新的String对象！（String的字符串是不可变的！） String a = \"dasdsa\", b = \"dasdasdsa\"; String l = a+b; System.out.println(l); 大量进行字符串的拼接似乎不太好，编译器是很聪明的，String的拼接有可能会被编译器优化为StringBuilder来减少对象创建（对象频繁创建时很费时间同时占内存的！） String result=\"String\"+\"and\"; //会被优化成一句！ String str1=\"String\"; String str2=\"and\"; String result=str1+str2; //变量随时可变，在编译时无法确定result的值，那么只能在运行时再去确定 String str1=\"String\"; String str2=\"and\"; String result=(new StringBuilder(String.valueOf(str1))).append(str2).toString(); //使用StringBuilder，会采用类似于第一种实现，显然会更快！ StringBuilder也是一个类，但是它能够存储可变长度的字符串！ StringBuilder builder = new StringBuilder(); builder .append(\"a\") .append(\"bc\") .append(\"d\"); //链式调用 String str = builder.toString(); System.out.println(str); ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:2:8","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"包和访问控制 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:3:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"包声明和导入 包其实就是用来区分类位置的东西，也可以用来将我们的类进行分类，类似于C++中的namespace！ package com.test; public class Test{ } 包其实是文件夹，比如com.test就是一个com文件夹中包含一个test文件夹，再包含我们Test类。 一般包按照个人或是公司域名的规则倒过来写 顶级域名.一级域名.二级域名 com.java.xxxx 如果需要使用其他包里面的类，那么我们需要import（类似于C/C++中的include） import com.test.Student; 也可以导入包下的全部（一般导入会由编译器自带帮我们补全，但是一定要记得我们需要导包！） import com.test.* Java默认为我们导入了以下的包，不需要去声明 import java.lang.* ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:3:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"静态导入 静态导入可以直接导入某个类的静态方法或者是静态变量，导入后，相当于这个方法或是类在定义在当前类中，可以直接调用该方法。 import static com.test.ui.Student.test; public class Main { public static void main(String[] args) { test(); } } 静态导入不会进行类的初始化！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:3:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"访问控制 Java支持对类属性访问的保护，也就是说，不希望外部类访问类中的属性或是方法，只允许内部调用，这种情况下我们就需要用到权限控制符。 ![image-20210819160939950](C:/Users/Alone/Users/nagocoler/Library/Application Support/typora-user-images/image-20210819160939950.png) 权限控制符可以声明在方法、成员变量、类前面，一旦声明private，只能类内部访问！ public class Student { private int a = 10; //具有私有访问权限，只能类内部访问 } public static void main(String[] args) { Student s = new Student(); System.out.println(s.a); //还可以访问吗？ } 和文件名称相同的类，只能是public，并且一个java文件中只能有一个public class！ // Student.java public class Student { } class Test{ //不能添加权限修饰符！只能是default } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:3:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"数组类型 假设出现一种情况，我想记录100个数字，定义100个变量还可行吗？ 我们可以使用到数组，数组是相同类型数据的有序集合。数组可以代表任何相同类型的一组内容（包括引用类型和基本类型）其中存放的每一个数据称为数组的一个元素，数组的下标是从0开始，也就是第一个元素的索引是0！ int[] arr = new int[10]; //需要new关键字来创建！ String[] arr2 = new String[10]; 数组本身也是类（编程不可见，C++写的），不是基本数据类型！ int[] arr = new int[10]; System.out.println(arr.length); //数组有成员变量！ System.out.println(arr.toString()); //数组有成员方法！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"一维数组 一维数组中，元素是依次排列的（线性），每个数组元素可以通过下标来访问！声明格式如下： 类型[] 变量名称 = new 类型[数组大小]; 类型 变量名称n = new 类型[数组大小]; //支持C语言样式，但不推荐！ 类型[] 变量名称 = new 类型[]{...}; //静态初始化（直接指定值和大小） 类型[] 变量名称 = {...}; //同上，但是只能在定义时赋值 创建出来的数组每个元素都有默认值（规则和类的成员变量一样，C语言创建的数组需要手动设置默认值），我们可以通过下标去访问： int[] arr = new int[10]; arr[0] = 626; System.out.println(arr[0]); System.out.println(arr[1]); 我们可以通过数组变量名称.length来获取当前数组长度： int[] arr = new int[]{1, 2, 3}; System.out.println(arr.length); //打印length成员变量的值 数组在创建时，就固定长度，不可更改！访问超出数组长度的内容，会出现错误！ String[] arr = new String[10]; System.out.println(arr[10]); //出现异常！ //Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 11 // at com.test.Application.main(Application.java:7) 思考：能不能直接修改length的值来实现动态扩容呢？ int[] arr = new int[]{1, 2, 3}; arr.length = 10; 数组做实参，因为数组也是类，所以形参得到的是数组的引用而不是复制的数组，操作的依然是数组对象本身 public static void main(String[] args) { int[] arr = new int[]{1, 2, 3}; test(arr); System.out.println(arr[0]); } private static void test(int[] arr){ arr[0] = 2934; } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"数组的遍历 如果我们想要快速打印数组中的每一个元素，又怎么办呢？ 传统for循环 我们很容易就联想到for循环 int[] arr = new int[]{1, 2, 3}; for (int i = 0; i \u003c arr.length; i++) { System.out.println(arr[i]); } foreach 传统for循环虽然可控性高，但是不够省事，要写一大堆东西，有没有一种省事的写法呢？ int[] arr = new int[]{1, 2, 3}; for (int i : arr) { System.out.println(i); } foreach属于增强型的for循环，它使得代码更简洁，同时我们能直接拿到数组中的每一个数字。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"二维数组 二维数组其实就是存放数组的数组，每一个元素都存放一个数组的引用，也就相当于变成了一个平面。 //三行两列 int[][] arr = { {1, 2}, {3, 4}, {5, 6}}; System.out.println(arr[2][1]); 二维数组的遍历同一维数组一样，只不过需要嵌套循环！ int[][] arr = new int[][]{ {1, 2}, {3, 4}, {5, 6}}; for (int i = 0; i \u003c 3; i++) { for (int j = 0; j \u003c 2; j++) { System.out.println(arr[i][j]); } } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"多维数组 不止二维数组，还存在三维数组，也就是存放数组的数组的数组，原理同二维数组一样，逐级访问即可。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:4","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"可变长参数 可变长参数其实就是数组的一种应用，我们可以指定方法的形参为一个可变长参数，要求实参可以根据情况动态填入0个或多个，而不是固定的数量 public static void main(String[] args) { test(\"AAA\", \"BBB\", \"CCC\"); //可变长，最后都会被自动封装成一个数组 } private static void test(String... test){ System.out.println(test[0]); //其实参数就是一个数组 } 由于是数组，所以说只能使用一种类型的可变长参数，并且可变长参数只能放在最后一位！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:5","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"实战：三大基本排序算法 现在我们有一个数组，但是数组里面的数据是乱序排列的，如何使它变得有序？ int[] arr = {8, 5, 0, 1, 4, 9, 2, 3, 6, 7}; 排序是编程的一个重要技能，掌握排序算法，你的技术才能更上一层楼，很多的项目都需要用到排序！三大排序算法： 冒泡排序 冒泡排序就是冒泡，其实就是不断使得我们无序数组中的最大数向前移动，经历n轮循环逐渐将每一个数推向最前。 插入排序 插入排序其实就跟我们打牌是一样的，我们在摸牌的时候，牌堆是乱序的，但是我们一张一张摸到手中进行排序，使得它变成了有序的！ 选择排序 选择排序其实就是每次都选择当前数组中最大的数排到最前面！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:4:6","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"封装、继承和多态 封装、继承和多态是面向对象编程的三大特性。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:5:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"封装 封装的目的是为了保证变量的安全性，使用者不必在意具体实现细节，而只是通过外部接口即可访问类的成员，如果不进行封装，类中的实例变量可以直接查看和修改，可能给整个代码带来不好的影响，因此在编写类时一般将成员变量私有化，外部类需要同getter和setter方法来查看和设置变量。 设想：学生小明已经创建成功，正常情况下能随便改他的名字和年龄吗？ public class Student { private String name; private int age; public Student(String name, int age) { this.name = name; this.age = age; } public int getAge() { return age; } public String getName() { return name; } } 也就是说，外部现在只能通过调用我定义的方法来获取成员属性，而我们可以在这个方法中进行一些额外的操作，比如小明可以修改名字，但是名字中不能包含\"小\"这个字。 public void setName(String name) { if(name.contains(\"小\")) return; this.name = name; } 单独给外部开放设置名称的方法，因为我还需要做一些额外的处理，所以说不能给外部直接操作成员变量的权限！ 封装思想其实就是把实现细节给隐藏了，外部只需知道这个方法是什么作用，而无需关心实现。 封装就是通过访问权限控制来实现的。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:5:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"继承 继承属于非常重要的内容，在定义不同类的时候存在一些相同属性，为了方便使用可以将这些共同属性抽象成一个父类，在定义其他子类时可以继承自该父类，减少代码的重复定义，子类可以使用父类中非私有的成员。 现在学生分为两种，艺术生和体育生，他们都是学生的分支，但是他们都有自己的方法： public class SportsStudent extends Student{ //通过extends关键字来继承父类 public SportsStudent(String name, int age) { super(name, age); //必须先通过super关键字（指代父类），实现父类的构造方法！ } public void exercise(){ System.out.println(\"我超勇的！\"); } } public class ArtStudent extends Student{ public ArtStudent(String name, int age) { super(name, age); } public void art(){ System.out.println(\"随手画个毕加索！\"); } } 子类具有父类的全部属性，protected可见但外部无法使用（包括private属性，不可见，无法使用），同时子类还能有自己的方法。继承只能继承一个父类，不支持多继承！ 每一个子类必须定义一个实现父类构造方法的构造方法，也就是需要在构造方法开始使用super()，如果父类使用的是默认构造方法，那么子类不用手动指明。 所有类都默认继承自Object类，除非手动指定类型，但是依然改变不了最顶层的父类是Object类。所有类都包含Object类中的方法，比如： public static void main(String[] args) { Object obj = new Object; System.out.println(obj.hashCode()); //求对象的hashcode，默认是对象的内存地址 System.out.println(obj.equals(obj)); //比较对象是否相同，默认比较的是对象的内存地址，也就是等同于 == System.out.println(obj.toString()); //将对象转换为字符串，默认生成对象的类名称+hashcode } 关于Object类的其他方法，我们会在Java多线程中再来提及。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:5:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"多态 多态是同一个行为具有多个不同表现形式或形态的能力。也就是同样的方法，由于实现类不同，执行的结果也不同！ 方法的重写 我们之前学习了方法的重载，方法的重写和重载是不一样的，重载是原有的方法逻辑不变的情况下，支持更多参数的实现，而重写是直接覆盖原有方法！ //父类中的study public void study(){ System.out.println(\"学习\"); } //子类中的study @Override //声明这个方法是重写的，但是可以不要，我们现阶段不接触 public void study(){ System.out.println(\"给你看点好康的\"); } 再次定义同样的方法后，父类的方法就被覆盖！子类还可以给父类方法提升访问权限！ public static void main(String[] args) { SportsStudent student = new SportsStudent(\"lbw\", 20); student.study(); //输出子类定义的内容 } 思考：静态方法能被重写吗？ 当我们在重写方法时，不仅想使用我们自己的逻辑，同时还希望执行父类的逻辑（也就是调用父类的方法）怎么办呢？ public void study(){ super.study(); System.out.println(\"给你看点好康的\"); } 同理，如果想访问父类的成员变量，也可以使用super关键字来访问，注意，子类可以具有和父类相同的成员变量！而在方法中访问的默认是 形参列表中 \u003e 当前类的成员变量 \u003e 父类成员变量 public void setTest(int test){ test = 1; this.test = 1; super.test = 1; } 再谈类型转换 我们曾经学习过基本数据类型的类型转换，支持一种数据类型转换为另一种数据类型，而我们的类也是支持类型转换的（仅限于存在亲缘关系的类之间进行转换）比如子类可以直接向上转型： Student student = new SportsStudent(\"lbw\", 20); //父类变量引用子类实例 student.study(); //得到依然是具体实现的结果，而不是当前类型的结果 我们也可以把已经明确是由哪个类实现的父类引用，强制转换为对应的类型： Student student = new SportsStudent(\"lbw\", 20); //是由SportsStudent进行实现的 //... do something... SportsStudent ps = (SportsStudent)student; //让它变成一个具体的子类 ps.sport(); //调用具体实现类的方法 这样的类型转换称为向下转型。 instanceof关键字 那么我们如果只是得到一个父类引用，但是不知道它到底是哪一个子类的实现怎么办？我们可以使用instanceof关键字来实现，它能够进行类型判断！ private static void test(Student student){ if (student instanceof SportsStudent){ SportsStudent sportsStudent = (SportsStudent) student; sportsStudent.sport(); }else if (student instanceof ArtStudent){ ArtStudent artStudent = (ArtStudent) student; artStudent.art(); } } 通过进行类型判断，我们就可以明确类的具体实现到底是哪个类！ 思考：student instanceof Student的结果是什么？ 再谈final关键字 我们目前只知道final关键字能够使得一个变量的值不可更改，那么如果在类前面声明final，会发生什么？ public final class Student { //类被声明为终态，那么它还能被继承吗 } 类一旦被声明为终态，将无法再被继承，不允许子类的存在！而方法被声明为final呢？ public final void study(){ //还能重写吗 System.out.println(\"学习\"); } 如果类的成员属性被声明为final，那么必须在构造方法中或是在定义时赋初始值！ private final String name; //引用类型不允许再指向其他对象 private final int age; //基本类型值不允许发生改变 public Student(String name, int age) { this.name = name; this.age = age; } 学习完封装继承和多态之后，我们推荐在不会再发生改变的成员属性上添加final关键字，JVM会对添加了final关键字的属性进行优化！ 抽象类 类本身就是一种抽象，而抽象类，把类还要抽象，也就是说，抽象类可以只保留特征，而不保留具体呈现形态，比如方法可以定义好，但是我可以不去实现它，而是交由子类来进行实现！ public abstract class Student { //抽象类 public abstract void test(); //抽象方法 } 通过使用abstract关键字来表明一个类是一个抽象类，抽象类可以使用abstract关键字来表明一个方法为抽象方法，也可以定义普通方法，抽象方法不需要编写具体实现（无方法体）但是必须由子类实现（除非子类也是一个抽象类）！ 抽象类由于不是具体的类定义，因此无法直接通过new关键字来创建对象！ Student s = new Student(){ //只能直接创建带实现的匿名内部类！ public void test(){ } } 因此，抽象类一般只用作继承使用！抽象类使得继承关系之间更加明确： public void study(){ //现在只能由子类编写，父类没有定义，更加明确了多态的定义！同一个方法多种实现！ System.out.println(\"给你看点好康的\"); } 接口 接口甚至比抽象类还抽象，他只代表某个确切的功能！也就是只包含方法的定义，甚至都不是一个类！接口包含了一些列方法的具体定义，类可以实现这个接口，表示类支持接口代表的功能（类似于一个插件，只能作为一个附属功能加在主体上，同时具体实现还需要由主体来实现） public interface Eat { void eat(); } 通过使用interface关键字来表明是一个接口（注意，这里class关键字被替换为了interface）接口只能包含public权限的抽象方法！（Java8以后可以有默认实现）我们可以通过声明default关键字来给抽象方法一个默认实现： public interface Eat { default void eat(){ //do something... } } 接口中定义的变量，默认为public static final public interface Eat { int a = 1; void eat(); } 一个类可以实现很多个接口，但是不能理解为多继承！（实际上实现接口是附加功能，和继承的概念有一定出入，顶多说是多继承的一种替代方案）一个类可以附加很多个功能！ public class SportsStudent extends Student implements Eat, ...{ @Override public void eat() { } } 类通过implements关键字来声明实现的接口！每个接口之间用逗号隔开！ 实现接口的类也能通过instanceof关键字判断，也支持向上和向下转型！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:5:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"内部类 类中可以存在一个类！各种各样的长相怪异的代码就是从这里开始出现的！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:6:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"成员内部类 我们的类中可以在嵌套一个类： public class Test { class Inner{ //类中定义的一个内部类 } } 成员内部类和成员变量和成员方法一样，都是属于对象的，也就是说，必须存在外部对象，才能创建内部类的对象！ public static void main(String[] args) { Test test = new Test(); Test.Inner inner = test.new Inner(); //写法有那么一丝怪异，但是没毛病！ } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:6:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"静态内部类 静态内部类其实就和类中的静态变量和静态方法一样，是属于类拥有的，我们可以直接通过类名.去访问: public class Test { static class Inner{ } } public static void main(String[] args) { Test.Inner inner = new Test.Inner(); //不用再创建外部类对象了！ } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:6:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"局部内部类 对，你没猜错，就是和局部变量一样哒~ public class Test { public void test(){ class Inner{ } Inner inner = new Inner(); } } 反正我是没用过！内部类 -\u003e 累不累 -\u003e 反正我累了！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:6:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"匿名内部类 匿名内部类才是我们的重点，也是实现lambda表达式的原理！匿名内部类其实就是在new的时候，直接对接口或是抽象类的实现： public static void main(String[] args) { Eat eat = new Eat() { @Override public void eat() { //DO something... } }; } 我们不用单独去创建一个类来实现，而是可以直接在new的时候写对应的实现！但是，这样写，无法实现复用，只能在这里使用！ lambda表达式 读作λ表达式，它其实就是我们接口匿名实现的简化，比如说： public static void main(String[] args) { Eat eat = new Eat() { @Override public void eat() { //DO something... } }; } public static void main(String[] args) { Eat eat = () -\u003e {}; //等价于上述内容 } lambda表达式（匿名内部类）只能访问外部的final类型或是隐式final类型的局部变量！ 为了方便，JDK默认就为我们提供了专门写函数式的接口，这里只介绍Consumer ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:6:4","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"枚举类 假设现在我们想给小明添加一个状态（跑步、学习、睡觉），外部可以实时获取小明的状态： public class Student { private final String name; private final int age; private String status; //... public void setStatus(String status) { this.status = status; } public String getStatus() { return status; } } 但是这样会出现一个问题，如果我们仅仅是存储字符串，似乎外部可以不按照我们规则，传入一些其他的字符串。这显然是不够严谨的！ 有没有一种办法，能够更好地去实现这样的状态标记呢？我们希望开发者拿到使用的就是我们定义好的状态，我们可以使用枚举类！ public enum Status { RUNNING, STUDY, SLEEP //直接写每个状态的名字即可，分号可以不打，但是推荐打上 } 使用枚举类也非常方便，我们只需要直接访问即可 public class Student { private final String name; private final int age; private Status status; //... public void setStatus(Status status) { //不再是String，而是我们指定的枚举类型 this.status = status; } public Status getStatus() { return status; } } public static void main(String[] args) { Student student = new Student(\"小明\", 18); student.setStatus(Status.RUNNING); System.out.println(student.getStatus()); } 枚举类型使用起来就非常方便了，其实枚举类型的本质就是一个普通的类，但是它继承自Enum类，我们定义的每一个状态其实就是一个public static final的Status类型成员变量！ // Compiled from \"Status.java\" public final class com.test.Status extends java.lang.Enum\u003ccom.test.Status\u003e { public static final com.test.Status RUNNING; public static final com.test.Status STUDY; public static final com.test.Status SLEEP; public static com.test.Status[] values(); public static com.test.Status valueOf(java.lang.String); static {}; } 既然枚举类型是普通的类，那么我们也可以给枚举类型添加独有的成员方法 public enum Status { RUNNING(\"睡觉\"), STUDY(\"学习\"), SLEEP(\"睡觉\"); //无参构造方法被覆盖，创建枚举需要添加参数（本质就是调用的构造方法！） private final String name; //枚举的成员变量 Status(String name){ //覆盖原有构造方法（默认private，只能内部使用！） this.name = name; } public String getName() { //获取封装的成员变量 return name; } } public static void main(String[] args) { Student student = new Student(\"小明\", 18); student.setStatus(Status.RUNNING); System.out.println(student.getStatus().getName()); } 枚举类还自带一些继承下来的实用方法 Status.valueOf(\"\") //将名称相同的字符串转换为枚举 Status.values() //快速获取所有的枚举 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:7:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"基本类型包装类 Java并不是纯面向对象的语言，虽然Java语言是一个面向对象的语言，但是Java中的基本数据类型却不是面向对象的。在学习泛型和集合之前，基本类型的包装类是一定要讲解的内容！ 我们的基本类型，如果想通过对象的形式去使用他们，Java提供的基本类型包装类，使得Java能够更好的体现面向对象的思想，同时也使得基本类型能够支持对象操作！ byte -\u003e Byte boolean -\u003e Boolean short -\u003e Short char -\u003e Character int -\u003e Integer long -\u003e Long float -\u003e Float double -\u003e Double 包装类实际上就行将我们的基本数据类型，封装成一个类（运用了封装的思想） private final int value; //Integer内部其实本质还是存了一个基本类型的数据，但是我们不能直接操作 public Integer(int value) { this.value = value; } 现在我们操作的就是Integer对象而不是一个int基本类型了！ public static void main(String[] args) { Integer i = 1; //包装类型可以直接接收对应类型的数据，并变为一个对象！ System.out.println(i + i); //包装类型可以直接被当做一个基本类型进行操作！ } 自动装箱和拆箱 那么为什么包装类型能直接使用一个具体值来赋值呢？其实依靠的是自动装箱和拆箱机制 Integer i = 1; //其实这里只是简写了而已 Integer i = Integer.valueOf(1); //编译后真正的样子 调用valueOf来生成一个Integer对象！ public static Integer valueOf(int i) { if (i \u003e= IntegerCache.low \u0026\u0026 i \u003c= IntegerCache.high) //注意，Java为了优化，有一个缓存机制，如果是在-128~127之间的数，会直接使用已经缓存好的对象，而不是再去创建新的！（面试常考） return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); //返回一个新创建好的对象 } 而如果使用包装类来进行运算，或是赋值给一个基本类型变量，会进行自动拆箱： public static void main(String[] args) { Integer i = Integer.valueOf(1); int a = i; //简写 int a = i.intValue(); //编译后实际的代码 long c = i.longValue(); //其他类型也有！ } 既然现在是包装类型了，那么我们还能使用==来判断两个数是否相等吗？ public static void main(String[] args) { Integer i1 = 28914; Integer i2 = 28914; System.out.println(i1 == i2); //实际上判断是两个对象是否为同一个对象（内存地址是否相同） System.out.println(i1.equals(i2)); //这个才是真正的值判断！ } 注意IntegerCache带来的影响！ 思考：下面这种情况结果会是什么？ public static void main(String[] args) { Integer i1 = 28914; Integer i2 = 28914; System.out.println(i1+1 == i2+1); } 在集合类的学习中，我们还会继续用到我们的包装类型！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:8:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"面向对象编程实战 虽然我们学习了编程，但是我们不能一股脑的所有问题都照着编程的思维去解决，编程只是解决问题的一种手段，灵活的运用我们所学的知识，才是解决问题的最好办法！比如，求1到100所有数的和： public static void main(String[] args) { int sum = 0; for (int i = 1; i \u003c= 100; i++) { //for循环暴力求解，简单，但是效率似乎低了一些 sum += i; } System.out.println(sum); } public static void main(String[] args) { System.out.println((1 + 100) * 50); //高斯求和公式，利用数学，瞬间计算结果！ } 说到最后，其实数学和逻辑思维才是解决问题的最终办法！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:9:0","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"对象设计（面向对象、多态运用） 设计一个Person抽象类，包含吃饭运动学习三种行为，分为工人、学生、老师三种职业。 设计设计一个接口考试，只有老师和学生会考试。 设计一个方法，模拟让人类进入考场，要求只有会考试的人才能进入，并且考试。 ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:9:1","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"二分搜索（搜索算法） 现在有一个有序数组（从小到大，数组长度 0 \u003c n \u003c 1000000）如何快速寻找我们想要的数在哪个位置，如果存在请返回下标，不存在返回-1即可。 int[] arr = new int[]{1, 4, 5, 6, 7, 10, 12, 14, 20, 22, 26}; //测试用例 private static int test(int[] arr, int target){ //请在这里实现搜索算法 } ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:9:2","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"快速排序（排序算法、递归分治） （开始之前先介绍一下递归！）快速排序其实是一种排序执行效率很高的排序算法，它利用分治法来对待排序序列进行分治排序，它的思想主要是通过一趟排序将待排记录分隔成独立的两部分，其中的一部分比关键字小，后面一部分比关键字大，然后再对这前后的两部分分别采用这种方式进行排序，通过递归的运算最终达到整个序列有序。 快速排序就像它的名字一样，快速！在极端情况下，会退化成冒泡排序！ ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:9:3","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"0/1背包问题（回溯法、剪枝/动态规划优化） 给定 n 件物品，每一个物品的重量为 w[n]，每个物品的价值为 v[n]。现挑选物品放入背包中，假定背包能承受的最大重量为 capacity，求装入物品的最大价值是多少? int[] w = {2, 3, 4, 5}; int[] v = {3, 4, 5, 6}; int capacity = 8; ","date":"2022-01-23","objectID":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/:9:4","tags":["Java对象和多态"],"title":"Java对象和多态","uri":"/posts/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"categories":["JavaSE笔记"],"content":"Java异常处理机制","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"Java异常机制 配套视频 在理想的情况下，我们的程序会按照我们的思路去运行，按理说是不会出现问题的，但是，代码实际编写后并不一定是完美的，可能会有我们没有考虑到的情况，如果这些情况能够正常得到一个错误的结果还好，但是如果直接导致程序运行出现问题了呢？ public static void main(String[] args) { test(1, 0); //当b为0的时候，还能正常运行吗？ } private static int test(int a, int b){ return a/b; //没有任何的判断而是直接做计算 } Exception in thread \"main\" java.lang.ArithmeticException: / by zero at com.test.Application.test(Application.java:9) at com.test.Application.main(Application.java:5) 当程序运行出现我们没有考虑到的情况时，就有可能出现异常或是错误！ ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:0:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"异常 我们在之前其实已经接触过一些异常了，比如数组越界异常，空指针异常，算术异常等，他们其实都是异常类型，我们的每一个异常也是一个类，他们都继承自Exception类！异常类型本质依然类的对象，但是异常类型支持在程序运行出现问题时抛出（也就是上面出现的红色报错）也可以提前声明，告知使用者需要处理可能会出现的异常！ ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:1:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"运行时异常 异常的第一种类型是运行时异常，如上述的列子，在编译阶段无法感知代码是否会出现问题，只有在运行的时候才知道会不会出错（正常情况下是不会出错的），这样的异常称为运行时异常。所有的运行时异常都继承自RuntimeException。 ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:1:1","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"编译时异常 异常的另一种类型是编译时异常，编译时异常是明确会出现的异常，在编译阶段就需要进行处理的异常（捕获异常）如果不进行处理，将无法通过编译！默认继承自Exception类的异常都是编译时异常。 File file = new File(\"my.txt\"); file.createNewFile(); //要调用此方法，首先需要处理异常 ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:1:2","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"错误 错误比异常更严重，异常就是不同寻常，但不一定会导致致命的问题，而错误是致命问题，一般出现错误可能JVM就无法继续正常运行了，比如OutOfMemoryError就是内存溢出错误（内存占用已经超出限制，无法继续申请内存了） int[] arr = new int[Integer.MAX_VALUE]; //能创建如此之大的数组吗？ 运行后得到以下内容： Exception in thread \"main\" java.lang.OutOfMemoryError: Requested array size exceeds VM limit at com.test.Main.main(Main.java:14) 错误都继承自Error类，一般情况下，程序中只能处理异常，错误是很难进行处理的，Error和Execption都继承自Throwable类。当程序中出现错误或异常时又没有进行处理时，程序（当前线程）将终止运行： int[] arr = new int[Integer.MAX_VALUE]; System.out.println(\"lbwnb\"); //还能正常打印吗？ ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:2:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"异常的处理 当程序没有按照我们想要的样子运行而出现异常时（默认会交给JVM来处理，JVM发现任何异常都会立即终止程序运行，并在控制台打印栈追踪信息），我们希望能够自己处理出现的问题，让程序继续运行下去，就需要对异常进行捕获，比如： int[] arr = new int[5]; arr[5] = 1; //我们需要处理这种情况，保证后面的代码正常运行！ System.out.println(\"lbwnb\"); 我们可以使用try和catch语句块来处理： int[] arr = new int[5]; try{ //在try块中运行代码 arr[5] = 1; //当代码出现异常时，异常会被捕获，并在catch块中得到异常类型的对象 }catch (ArrayIndexOutOfBoundsException e){ //捕获的异常类型 System.out.println(\"程序运行出现异常！\"); //出现异常时执行 } //后面的代码会正常运行 System.out.println(\"lbwnb\"); 当异常被捕获后，就由我们自己进行处理（不再交给JVM处理），因此就不会导致程序终止运行。 我们可以通过使用e.printStackTrace()来打印栈追踪信息，定位我们的异常出现位置： java.lang.ArrayIndexOutOfBoundsException: 5 at com.test.Main.main(Main.java:7) //Main类的第7行出现问题 程序运行出现异常！ lbwnb 运行时异常在编译时可以不用捕获，但是编译时异常必须进行处理： File file = new File(\"my.txt\"); try { file.createNewFile(); } catch (IOException e) { //捕获声明的异常类型 e.printStackTrace(); } 可以捕获到类型不止是Exception的子类，只要是继承自Throwalbe的类，都能被捕获，也就是说，Error也能被捕获，但是不建议这样做，因为错误一般是虚拟机相关的问题，出现Error应该从问题的根源去解决。 ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:3:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"异常的抛出 当别人调用我们的方法时，如果传入了错误的参数导致程序无法正常运行，这时我们就需要手动抛出一个异常来终止程序继续运行下去，同时告知上一级方法执行出现了问题： public static void main(String[] args) { try { test(1, 0); } catch (Exception e) { //捕获方法中会出现的异常 e.printStackTrace(); } } private static int test(int a, int b) throws Exception { //声明抛出的异常类型 if(b == 0) throw new Exception(\"0不能做除数！\"); //创建异常对象并抛出异常 return a/b; //抛出异常会终止代码运行 } 通过throw关键字抛出异常（抛出异常后，后面的代码不再执行）当程序运行到这一行时，就会终止执行，并出现一个异常。 如果方法中抛出了非运行时异常，但是不希望在此方法内处理，而是交给调用者来处理异常，就需要在方法定义后面显式声明抛出的异常类型！如果抛出的是运行时异常，则不需要在方法后面声明异常类型，调用时也无需捕获，但是出现异常时同样会导致程序终止（出现运行时异常同时未被捕获会默认交给JVM处理，也就是直接中止程序并在控制台打印栈追踪信息） 如果想要调用声明编译时异常的方法，但是依然不想去处理，可以同样的在方法上声明throws来继续交给上一级处理。 public static void main(String[] args) throws Exception { //出现异常就再往上抛，而不是在此方法内处理 test(1, 0); } private static int test(int a, int b) throws Exception { //声明抛出的异常类型 if(b == 0) throw new Exception(\"0不能做除数！\"); //创建异常对象并抛出异常 return a/b; } 当main方法都声明抛出异常时，出现异常就由JVM进行处理，也就是默认的处理方式（直接中止程序并在控制台打印栈追踪信息） 异常只能被捕获一次，当异常捕获出现嵌套时，只会在最内层被捕获： public static void main(String[] args) throws Exception { try{ test(1, 0); }catch (Exception e){ System.out.println(\"外层\"); } } private static int test(int a, int b){ try{ if(b == 0) throw new Exception(\"0不能做除数！\"); }catch (Exception e){ System.out.println(\"内层\"); return 0; } return a/b; } ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:4:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"自定义异常 JDK为我们已经提前定义了一些异常了，但是可能对我们来说不够，那么就需要自定义异常： public class MyException extends Exception { //直接继承即可 } public static void main(String[] args) throws MyException { throw new MyException(); //直接使用 } 也可以使用父类的带描述的构造方法： public class MyException extends Exception { public MyException(String message){ super(message); } } public static void main(String[] args) throws MyException { throw new MyException(\"出现了自定义的错误\"); } 捕获异常指定的类型，会捕获其所有子异常类型： try { throw new MyException(\"出现了自定义的错误\"); } catch (Exception e) { //捕获父异常类型 System.out.println(\"捕获到异常\"); } ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:5:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"多重异常捕获和finally关键字 当代码可能出现多种类型的异常时，我们希望能够分不同情况处理不同类型的异常，就可以使用多重异常捕获： try { //.... } catch (NullPointerException e) { } catch (IndexOutOfBoundsException e){ } catch (RuntimeException e){ } 注意，类似于if-else if的结构，父异常类型只能放在最后！ try { //.... } catch (RuntimeException e){ //父类型在前，会将子类的也捕获 } catch (NullPointerException e) { //永远都不会被捕获 } catch (IndexOutOfBoundsException e){ //永远都不会被捕获 } 如果希望把这些异常放在一起进行处理： try { //.... } catch (NullPointerException | IndexOutOfBoundsException e) { //用|隔开每种类型即可 } 当我们希望，程序运行时，无论是否出现异常，都会在最后执行的任务，可以交给finally语句块来处理： try { //.... }catch (Exception e){ }finally { System.out.println(\"lbwnb\"); //无论是否出现异常，都会在最后执行 } try语句块至少要配合catch或finally中的一个： try { int a = 10; a /= 0; }finally { //不捕获异常，程序会终止，但在最后依然会执行下面的内容 System.out.println(\"lbwnb\"); } 思考：try、catch和finally执行顺序： private static int test(int a){ try{ return a; }catch (Exception e){ return 0; }finally { a = a + 1; } } ","date":"2022-01-23","objectID":"/posts/java%E5%BC%82%E5%B8%B8/:6:0","tags":["Java异常处理"],"title":"Java异常处理机制","uri":"/posts/java%E5%BC%82%E5%B8%B8/"},{"categories":["JavaSE笔记"],"content":"Java泛型与集合类","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"Java泛型与集合类 配套视频 在前面我们学习了最重要的类和对象，了解了面向对象编程的思想，注意，非常重要，面向对象是必须要深入理解和掌握的内容，不能草草结束。在本章节，我们会继续深入了解，从我们的泛型开始，再到我们的数据结构，最后再开始我们的集合类学习。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:0:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"走进泛型 为了统计学生成绩，要求设计一个Score对象，包括课程名称、课程号、课程成绩，但是成绩分为两种，一种是以优秀、良好、合格 来作为结果，还有一种就是 60.0、75.5、92.5 这样的数字分数，那么现在该如何去设计这样的一个Score类呢？现在的问题就是，成绩可能是String类型，也可能是Integer类型，如何才能很好的去存可能出现的两种类型呢？ public class Score { String name; String id; Object score; //因为Object是所有类型的父类，因此既可以存放Integer也能存放String public Score(String name, String id, Object score) { this.name = name; this.id = id; this.score = score; } } 以上的方法虽然很好地解决了多种类型存储问题，但是Object类型在编译阶段并不具有良好的类型判断能力，很容易出现以下的情况： public static void main(String[] args) { Score score = new Score(\"数据结构与算法基础\", \"EP074512\", \"优秀\"); //是String类型的 //.... Integer number = (Integer) score.score; //获取成绩需要进行强制类型转换，虽然并不是一开始的类型，但是编译不会报错 } //运行时出现异常！ Exception in thread \"main\" java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer at com.test.Main.main(Main.java:14) 使用Object类型作为引用，取值只能进行强制类型转换，显然无法在编译期确定类型是否安全，项目中代码量非常之大，进行类型比较又会导致额外的开销和增加代码量，如果不经比较就很容易出现类型转换异常，代码的健壮性有所欠缺！（此方法虽然可行，但并不是最好的方法） 为了解决以上问题，JDK1.5新增了泛型，它能够在编译阶段就检查类型安全，大大提升开发效率。 public class Score\u003cT\u003e { //将Score转变为泛型类\u003cT\u003e String name; String id; T score; //T为泛型，根据用户提供的类型自动变成对应类型 public Score(String name, String id, T score) { //提供的score类型即为T代表的类型 this.name = name; this.id = id; this.score = score; } } public static void main(String[] args) { //直接确定Score的类型是字符串类型的成绩 Score\u003cString\u003e score = new Score\u003cString\u003e(\"数据结构与算法基础\", \"EP074512\", \"优秀\"); Integer i = score.score; //编译不通过，因为成员变量score类型被定为String！ } 泛型将数据类型的确定控制在了编译阶段，在编写代码的时候就能明确泛型的类型！如果类型不符合，将无法通过编译！ 泛型本质上也是一个语法糖（并不是JVM所支持的语法，编译后会转成编译器支持的语法，比如之前的foreach就是），在编译后会被擦除，变回上面的Object类型调用，但是类型转换由编译器帮我们完成，而不是我们自己进行转换（安全） //反编译后的代码 public static void main(String[] args) { Score score = new Score(\"数据结构与算法基础\", \"EP074512\", \"优秀\"); String i = (String)score.score; //其实依然会变为强制类型转换，但是这是由编译器帮我们完成的 } 像这样在编译后泛型的内容消失转变为Object的情况称为类型擦除（重要，需要完全理解），所以泛型只是为了方便我们在编译阶段确定类型的一种语法而已，并不是JVM所支持的。 综上，泛型其实就是一种类型参数，用于指定类型。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:1:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"泛型的使用 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"泛型类 上一节我们已经提到泛型类的定义，实际上就是普通的类多了一个类型参数，也就是在使用时需要指定具体的泛型类型。泛型的名称一般取单个大写字母，比如T代表Type，也就是类型的英文单词首字母，当然也可以添加数字和其他的字符。 public class Score\u003cT\u003e { //将Score转变为泛型类\u003cT\u003e String name; String id; T score; //T为泛型，根据用户提供的类型自动变成对应类型 public Score(String name, String id, T score) { //提供的score类型即为T代表的类型 this.name = name; this.id = id; this.score = score; } } 在一个普通类型中定义泛型，泛型T称为参数化类型，在定义泛型类的引用时，需要明确指出类型： Score\u003cString\u003e score = new Score\u003cString\u003e(\"数据结构与算法基础\", \"EP074512\", \"优秀\"); 此时类中的泛型T已经被替换为String了，在我们获取此对象的泛型属性时，编译器会直接告诉我们类型： Integer i = score.score; //编译不通过，因为成员变量score明确为String类型 注意，泛型只能用于对象属性，也就是非静态的成员变量才能使用： static T score; //错误，不能在静态成员上定义 由此可见，泛型是只有在创建对象后编译器才能明确泛型类型，而静态类型是类所具有的属性，不足以使得编译器完成类型推断。 泛型无法使用基本类型，如果需要基本类型，只能使用基本类型的包装类进行替换！ Score\u003cdouble\u003e score = new Score\u003cdouble\u003e(\"数据结构与算法基础\", \"EP074512\", 90.5); //编译不通过 那么为什么泛型无法使用基本类型呢？回想上一节提到的类型擦除，其实就很好理解了。由于JVM没有泛型概念，因此泛型最后还是会被编译器编译为Object，并采用强制类型转换的形式进行类型匹配，而我们的基本数据类型和引用类型之间无法进行类型转换，所以只能使用基本类型的包装类来处理。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:1","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"类的泛型方法 泛型方法的使用也很简单，我们只需要把它当做一个未知的类型来使用即可： public T getScore() { //若方法的返回值类型为泛型，那么编译器会自动进行推断 return score; } public void setScore(T score) { //若方法的形式参数为泛型，那么实参只能是定义时的类型 this.score = score; } Score\u003cString\u003e score = new Score\u003cString\u003e(\"数据结构与算法基础\", \"EP074512\", \"优秀\"); score.setScore(10); //编译不通过，因为只接受String类型 同样地，静态方法无法直接使用类定义的泛型（注意是无法直接使用，静态方法可以使用泛型） ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:2","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"自定义泛型方法 那么如果我想在静态方法中使用泛型呢？首先我们要明确之前为什么无法使用泛型，因为之前我们的泛型定义是在类上的，只有明确具体的类型才能开始使用，也就是创建对象时完成类型确定，但是静态方法不需要依附于对象，那么只能在使用时再来确定了，所以静态方法可以使用泛型，但是需要单独定义： public static \u003cE\u003e void test(E e){ //在方法定义前声明泛型 System.out.println(e); } 同理，成员方法也能自行定义泛型，在实际使用时再进行类型确定： public \u003cE\u003e void test(E e){ System.out.println(e); } 其实，无论是泛型类还是泛型方法，再使用时一定要能够进行类型推断，明确类型才行。 注意一定要区分类定义的泛型和方法前定义的泛型！ ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:3","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"泛型引用 可以看到我们在定义一个泛型类的引用时，需要在后面指出此类型： Score\u003cInteger\u003e score; //声明泛型为Integer类型 如果不希望指定类型，或是希望此引用类型可以引用任意泛型的Score类对象，可以使用?通配符，来表示自动匹配任意的可用类型： Score\u003c?\u003e score; //score可以引用任意的Score类型对象了！ 那么使用通配符之后，得到的泛型成员变量会是什么类型呢？ Object o = score.getScore(); //只能变为Object 因为使用了通配符，编译器就无法进行类型推断，所以只能使用原始类型。 在学习了泛型的界限后，我们还会继续了解通配符的使用。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:4","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"泛型的界限 现在有一个新的需求，现在没有String类型的成绩了，但是成绩依然可能是整数，也可能是小数，这时我们不希望用户将泛型指定为除数字类型外的其他类型，我们就需要使用到泛型的上界定义： public class Score\u003cT extends Number\u003e { //设定泛型上界，必须是Number的子类 private final String name; private final String id; private T score; public Score(String name, String id, T score) { this.name = name; this.id = id; this.score = score; } public T getScore() { return score; } } 通过extends关键字进行上界限定，只有指定类型或指定类型的子类才能作为类型参数。 同样的，泛型通配符也支持泛型的界限： Score\u003c? extends Number\u003e score; //限定为匹配Number及其子类的类型 同理，既然泛型有上限，那么也有下限： Score\u003c? super Integer\u003e score; //限定为匹配Integer及其父类 通过super关键字进行下界限定，只有指定类型或指定类型的父类才能作为类型参数。 图解如下： 那么限定了上界后，我们再来使用这个对象的泛型成员，会变成什么类型呢？ Score\u003c? extends Number\u003e score = new Score\u003c\u003e(\"数据结构与算法基础\", \"EP074512\", 10); Number o = score.getScore(); //得到的结果为上界类型 也就是说，一旦我们指定了上界后，编译器就将范围从原始类型Object提升到我们指定的上界Number，但是依然无法明确具体类型。思考：那如果定义下限呢？ 那么既然我们可以给泛型类限定上界，现在我们来看编译后结果呢： //使用javap -l 进行反编译 public class com.test.Score\u003cT extends java.lang.Number\u003e { public com.test.Score(java.lang.String, java.lang.String, T); LineNumberTable: line 8: 0 line 9: 4 line 10: 9 line 11: 14 line 12: 19 LocalVariableTable: Start Length Slot Name Signature 0 20 0 this Lcom/test/Score; 0 20 1 name Ljava/lang/String; 0 20 2 id Ljava/lang/String; 0 20 3 score Ljava/lang/Number; //可以看到score的类型直接被编译为Number类 public T getScore(); LineNumberTable: line 15: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/test/Score; } 因此，一旦确立上限后，编译器会自动将类型提升到上限类型。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:5","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"钻石运算符 我们发现，每次创建泛型对象都需要在前后都标明类型，但是实际上后面的类型声明是可以去掉的，因为我们在传入参数时或定义泛型类的引用时，就已经明确了类型，因此JDK1.7提供了钻石运算符来简化代码： Score\u003cInteger\u003e score = new Score\u003cInteger\u003e(\"数据结构与算法基础\", \"EP074512\", 10); //1.7之前 Score\u003cInteger\u003e score = new Score\u003c\u003e(\"数据结构与算法基础\", \"EP074512\", 10); //1.7之后 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:6","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"泛型与多态 泛型不仅仅可以可以定义在类上，同时也能定义在接口上： public interface ScoreInterface\u003cT\u003e { T getScore(); void setScore(T t); } 当实现此接口时，我们可以选择在实现类明确泛型类型或是继续使用此泛型，让具体创建的对象来确定类型。 public class Score\u003cT\u003e implements ScoreInterface\u003cT\u003e{ //将Score转变为泛型类\u003cT\u003e private final String name; private final String id; private T score; public Score(String name, String id, T score) { this.name = name; this.id = id; this.score = score; } public T getScore() { return score; } @Override public void setScore(T score) { this.score = score; } } public class StringScore implements ScoreInterface\u003cString\u003e{ //在实现时明确类型 @Override public String getScore() { return null; } @Override public void setScore(String s) { } } 抽象类同理，这里就不多做演示了。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:7","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"多态类型擦除 思考一个问题，既然继承后明确了泛型类型，那么为什么@Override不会出现错误呢，重写的条件是需要和父类的返回值类型、形式参数一致，而泛型默认的原始类型是Object类型，子类明确后变为Number类型，这显然不满足重写的条件，但是为什么依然能编译通过呢？ class A\u003cT\u003e{ private T t; public T get(){ return t; } public void set(T t){ this.t=t; } } class B extends A\u003cNumber\u003e{ private Number n; @Override public Number get(){ //这并不满足重写的要求，因为只能重写父类同样返回值和参数的方法，但是这样却能够通过编译！ return t; } @Override public void set(Number t){ this.t=t; } } 通过反编译进行观察，实际上是编译器帮助我们生成了两个桥接方法用于支持重写： @Override public Object get(){ return this.get();//调用返回Number的那个方法 } @Override public void set(Object t ){ this.set((Number)t ); //调用参数是Number的那个方法 } ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:2:8","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"数据结构基础 警告！本章最难的部分！ 学习集合类之前，我们还有最关键的内容需要学习，同第一章一样，自底向上才是最佳的学习方向，比起直接带大家认识集合类，不如先了解一下数据结构，只有了解了数据结构基础，才能更好地学习集合类，同时，数据结构也是你以后深入学习JDK源码的必备条件！（学习不要快餐式！）当然，我们主要是讲解Java，数据结构作为铺垫作用，所以我们只会讲解关键的部分，其他部分可以下去自行了解。 在计算机科学中，数据结构是一种数据组织、管理和存储的格式,它可以帮助我们实现对数据高效的访问和修改。更准确地说,数据结构是数据值的集合，可以体现数据值之间的关系，以及可以对数据进行应用的函数或操作。 通俗地说，我们需要去学习在计算机中如何去更好地管理我们的数据，才能让我们对我们的数据控制更加灵活！ ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:3:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"线性表 线性表是最基本的一种数据结构，它是表示一组相同类型数据的有限序列，你可以把它与数组进行参考，但是它并不是数组，线性表是一种表结构，它能够支持数据的插入、删除、更新、查询等，同时数组可以随意存放在数组中任意位置，而线性表只能依次有序排列，不能出现空隙，因此，我们需要进一步的设计。 顺序表 将数据依次存储在连续的整块物理空间中，这种存储结构称为顺序存储结构，而以这种方式实现的线性表，我们称为顺序表。 同样的，表中的每一个个体都被称为元素，元素左边的元素（上一个元素），称为前驱，同理，右边的元素（后一个元素）称为后驱。 我们设计线性表的目标就是为了去更好地管理我们的数据，也就是说，我们可以基于数组，来进行封装，实现增删改查！既然要存储一组数据，那么很容易联想到我们之前学过的数组，数组就能够容纳一组同类型的数据。 目标：以数组为底层，编写以下抽象类的具体实现 /** * 线性表抽象类 * @param \u003cE\u003e 存储的元素(Element)类型 */ public abstract class AbstractList\u003cE\u003e { /** * 获取表的长度 * @return 顺序表的长度 */ public abstract int size(); /** * 添加一个元素 * @param e 元素 * @param index 要添加的位置(索引) */ public abstract void add(E e, int index); /** * 移除指定位置的元素 * @param index 位置 * @return 移除的元素 */ public abstract E remove(int index); /** * 获取指定位置的元素 * @param index 位置 * @return 元素 */ public abstract E get(int index); } 链表 数据分散的存储在物理空间中，通过一根线保存着它们之间的逻辑关系，这种存储结构称为链式存储结构 实际上，就是每一个结点存放一个元素和一个指向下一个结点的引用（C语言里面是指针，Java中就是对象的引用，代表下一个结点对象） 利用这种思想，我们再来尝试实现上面的抽象类，从实际的代码中感受！ 比较：顺序表和链表的优异？ 顺序表优缺点： 访问速度快，随机访问性能高 插入和删除的效率低下，极端情况下需要变更整个表 不易扩充，需要复制并重新创建数组 链表优缺点： 插入和删除效率高，只需要改变连接点的指向即可 动态扩充容量，无需担心容量问题 访问元素需要依次寻找，随机访问元素效率低下 链表只能指向后面，能不能指向前面呢？双向链表！ 栈和队列实际上就是对线性表加以约束的一种数据结构，如果前面的线性表的掌握已经ok，那么栈和队列就非常轻松了！ 栈 栈遵循先入后出原则，只能在线性表的一端添加和删除元素。我们可以把栈看做一个杯子，杯子只有一个口进出，最低处的元素只能等到上面的元素离开杯子后，才能离开。 向栈中插入一个元素时，称为入栈（压栈），移除栈顶元素称为出栈，我们需要尝试实现以下抽象类型： /** * 抽象类型栈，待实现 * @param \u003cE\u003e 元素类型 */ public abstract class AbstractStack\u003cE\u003e { /** * 出栈操作 * @return 栈顶元素 */ public abstract E pop(); /** * 入栈操作 * @param e 元素 */ public abstract void push(E e); } 其实，我们的JVM在处理方法调用时，也是一个栈操作： 所以说，如果玩不好递归，就会像这样： public class Main { public static void main(String[] args) { go(); } private static void go(){ go(); } } Exception in thread \"main\" java.lang.StackOverflowError at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) at com.test.Main.go(Main.java:13) ... 栈的深度是有限制的，如果达到限制，将会出现StackOverflowError错误（注意是错误！说明是JVM出现了问题） 队列 队列同样也是受限制的线性表，不过队列就像我们排队一样，只能从队尾开始排，从队首出。 所以我们要实现以下内容： /** * * @param \u003cE\u003e */ public abstract class AbstractQueue\u003cE\u003e { /** * 进队操作 * @param e 元素 */ public abstract void offer(E e); /** * 出队操作 * @return 元素 */ public abstract E poll(); } ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:3:1","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"二叉树 本版块主要学习的是二叉树，树也是一种数据结构，但是它使用起来更加的复杂。 树 我们前面已经学习过链表了，我们知道链表是单个结点之间相连，也就是一种一对一的关系，而树则是一个结点连接多个结点，也就是一对多的关系。 一个结点可以有N个子结点，就像上图一样，看起来就像是一棵树。而位于最顶端的结点（没有父结点）我们称为根结点，而结点拥有的子节点数量称为度，每向下一级称为一个层次，树中出现的最大层次称为树的深度(高度)。 二叉树 二叉树是一种特殊的树，每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点，位于两边的子结点称为左右子树（注意，左右子树是明确区分的，是左就是左，是右就是右） 数学性质： 在二叉树的第i层上最多有2^(i-1) 个节点。 二叉树中如果深度为k，那么最多有2^k-1个节点。 设计一个二叉树结点类： public class TreeNode\u003cE\u003e { public E e; //当前结点数据 public TreeNode\u003cE\u003e left; //左子树 public TreeNode\u003cE\u003e right; //右子树 } 二叉树的遍历 顺序表的遍历其实就是依次有序去访问表中每一个元素，而像二叉树这样的复杂结构，我们有四种遍历方式，他们是：前序遍历、中序遍历、后序遍历以及层序遍历，本版块我们主要讨论前三种遍历方式： 前序遍历：从二叉树的根结点出发，到达结点时就直接输出结点数据，按照先向左在向右的方向访问。ABCDEF 中序遍历：从二叉树的根结点出发，优先输出左子树的节点的数据，再输出当前节点本身，最后才是右子树。CBDAEF 后序遍历：从二叉树的根结点出发，优先遍历其左子树，再遍历右子树，最后在输出当前节点本身。CDBFEA 满二叉树和完全二叉树 满二叉树和完全二叉树其实就是特殊情况下的二叉树，满二叉树左右的所有叶子节点都在同一层，也就是说，完全把每一个层级都给加满了结点。完全二叉树与满二叉树不同的地方在于，它的最下层叶子节点可以不满，但是最下层的叶子节点必须靠左排布。 其实满二叉树和完全二叉树就是有一定规律的二叉树，很容易理解。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:3:2","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"快速查找 我们之前提到的这些数据结构，很好地帮我们管理了数据，但是，如果需要查找某一个元素是否存在于数据结构中，如何才能更加高效的去完成呢？ 哈希表 通过前面的学习，我们发现，顺序表虽然查询效率高，但是插入删除有严重表更新的问题，而链表虽然弥补了更新问题，但是查询效率实在是太低了，能否有一种折中方案？哈希表！ 不知大家在之前的学习中是否发现，我们的Object类中，定义了一个叫做hashcode()的方法？而这个方法呢，就是为了更好地支持哈希表的实现。hashcode()默认得到的是对象的内存地址，也就是说，每个对象的hashCode都不一样。 哈希表，其实本质上就是一个存放链表的数组，那么它是如何去存储数据的呢？我们先来看看长啥样： 数组中每一个元素都是一个头结点，用于保存数据，那我们怎么确定数据应该放在哪一个位置呢？通过hash算法，我们能够瞬间得到元素应该放置的位置。 //假设hash表长度为16，hash算法为： private int hash(int hashcode){ return hashcode % 16; } 设想这样一个问题，如果计算出来的hash值和之前已经存在的元素相同了呢？这种情况我们称为hash碰撞，这也是为什么要将每一个表元素设置为一个链表的头结点的原因，一旦发现重复，我们可以往后继续添加节点。 当然，以上的hash表结构只是一种设计方案，在面对大额数据时，是不够用的，在JDK1.8中，集合类使用的是数组+二叉树的形式解决的（这里的二叉树是经过加强的二叉树，不是前面讲得简单二叉树，我们下一节就会开始讲） 二叉排序树 我们前面学习的二叉树效率是不够的，我们需要的是一种效率更高的二叉树，因此，基于二叉树的改进，提出了二叉查找树，可以看到结构像下面这样： 不难发现，每个节点的左子树，一定小于当前节点的值，每个节点的右子树，一定大于当前节点的值，这样的二叉树称为二叉排序树。利用二分搜索的思想，我们就可以快速查找某个节点！ 平衡二叉树 在了解了二叉查找树之后，我们发现，如果根节点为10，现在加入到结点的值从9开始，依次减小到1，那么这个表就会很奇怪，就像下面这样： 显然，当所有的结点都排列到一边，这种情况下，查找效率会直接退化为最原始的二叉树！因此我们需要维持二叉树的平衡，才能维持原有的查找效率。 现在我们对二叉排序树加以约束，要求每个结点的左右两个子树的高度差的绝对值不超过1，这样的二叉树称为平衡二叉树，同时要求每个结点的左右子树都是平衡二叉树，这样，就不会因为一边的疯狂增加导致失衡。我们来看看以下几种情况： 左左失衡 右右失衡 左右失衡 右左失衡 通过以上四种情况的处理，最终得到维护平衡二叉树的算法。 红黑树 红黑树也是二叉排序树的一种改进，同平衡二叉树一样，红黑树也是一种维护平衡的二叉排序树，但是没有平衡二叉树那样严格（平衡二叉树每次插入新结点时，可能会出现大量的旋转，而红黑树保证不超过三次），红黑树降低了对于旋转的要求，因此效率有一定的提升同时实现起来也更加简单。但是红黑树的效率却高于平衡二叉树，红黑树也是JDK1.8中使用的数据结构！ 红黑树的特性: （1）每个节点或者是黑色，或者是红色。 （2）根节点是黑色。 （3）每个叶子节点的两边也需要表示（虽然没有，但是null也需要表示出来）是黑色。 （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 我们来看看一个节点，是如何插入到红黑树中的： 基本的 插入规则和平衡二叉树一样，但是在插入后： 将新插入的节点标记为红色 如果 X 是根结点(root)，则标记为黑色 如果 X 的 parent 不是黑色，同时 X 也不是 root: 3.1 如果 X 的 uncle (叔叔) 是红色 3.1.1 将 parent 和 uncle 标记为黑色 3.1.2 将 grand parent (祖父) 标记为红色 3.1.3 让 X 节点的颜色与 X 祖父的颜色相同，然后重复步骤 2、3 3.2 如果 X 的 uncle (叔叔) 是黑色，我们要分四种情况处理 3.2.1 左左 (P 是 G 的左孩子，并且 X 是 P 的左孩子) 3.2.2 左右 (P 是 G 的左孩子，并且 X 是 P 的右孩子) 3.2.3 右右 (P 是 G 的右孩子，并且 X 是 P 的右孩子) 3.2.4 右左 (P 是 G 的右孩子，并且 X 是 P 的左孩子) 其实这种情况下处理就和我们的平衡二叉树一样了 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:3:3","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"认识集合类 集合表示一组对象，称为其元素。一些集合允许重复的元素，而另一些则不允许。一些集合是有序的，而其他则是无序的。 集合类其实就是为了更好地组织、管理和操作我们的数据而存在的，包括列表、集合、队列、映射等数据结构。从这一块开始，我们会从源码角度给大家讲解（数据结构很重要！），不仅仅是教会大家如何去使用。 集合类最顶层不是抽象类而是接口，因为接口代表的是某个功能，而抽象类是已经快要成形的类型，不同的集合类的底层实现是不相同的，同时一个集合类可能会同时具有两种及以上功能（既能做队列也能做列表），所以采用接口会更加合适，接口只需定义支持的功能即可。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:4:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"数组与集合 相同之处： 它们都是容器，都能够容纳一组元素。 不同之处： 数组的大小是固定的，集合的大小是可变的。 数组可以存放基本数据类型，但集合只能存放对象。 数组存放的类型只能是一种，但集合可以有不同种类的元素。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:4:1","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"集合根接口Collection 本接口中定义了全部的集合基本操作，我们可以在源码中看看。 我们再来看看List和Set以及Queue接口。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:4:2","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"集合类的使用 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"List列表 首先介绍ArrayList，它的底层是用数组实现的，内部维护的是一个可改变大小的数组，也就是我们之前所说的线性表！跟我们之前自己写的ArrayList相比，它更加的规范，同时继承自List接口。 先看看ArrayList的源码！ 基本操作 List\u003cString\u003e list = new ArrayList\u003c\u003e(); //默认长度的列表 List\u003cString\u003e listInit = new ArrayList\u003c\u003e(100); //初始长度为100的列表 向列表中添加元素： List\u003cString\u003e list = new ArrayList\u003c\u003e(); list.add(\"lbwnb\"); list.add(\"yyds\"); list.contains(\"yyds\"); //是否包含某个元素 System.out.println(list); 移除元素： public static void main(String[] args) { List\u003cString\u003e list = new ArrayList\u003c\u003e(); list.add(\"lbwnb\"); list.add(\"yyds\"); list.remove(0); //按下标移除元素 list.remove(\"yyds\"); //移除指定元素 System.out.println(list); } 也支持批量操作： public static void main(String[] args) { ArrayList\u003cString\u003e list = new ArrayList\u003c\u003e(); list.addAll(new ArrayList\u003c\u003e()); //在尾部批量添加元素 list.removeAll(new ArrayList\u003c\u003e()); //批量移除元素（只有给定集合中存在的元素才会被移除） list.retainAll(new ArrayList\u003c\u003e()); //只保留某些元素 System.out.println(list); } 我们再来看LinkedList，其实本质就是一个链表！我们来看看源码。 其实与我们之前编写的LinkedList不同之处在于，它内部使用的是一个双向链表： private static class Node\u003cE\u003e { E item; Node\u003cE\u003e next; Node\u003cE\u003e prev; Node(Node\u003cE\u003e prev, E element, Node\u003cE\u003e next) { this.item = element; this.next = next; this.prev = prev; } } 当然，我们发现它还实现了Queue接口，所以LinkedList也能被当做一个队列或是栈来使用。 public static void main(String[] args) { LinkedList\u003cString\u003e list = new LinkedList\u003c\u003e(); list.offer(\"A\"); //入队 System.out.println(list.poll()); //出队 list.push(\"A\"); list.push(\"B\"); //进栈 list.push(\"C\"); System.out.println(list.pop()); System.out.println(list.pop()); //出栈 System.out.println(list.pop()); } 利用代码块来快速添加内容 前面我们学习了匿名内部类，我们就可以利用代码块，来快速生成一个自带元素的List List\u003cString\u003e list = new LinkedList\u003cString\u003e(){{ //初始化时添加 this.add(\"A\"); this.add(\"B\"); }}; 如果是需要快速生成一个只读的List，后面我们会讲解Arrays工具类。 集合的排序 List\u003cInteger\u003e list = new LinkedList\u003cInteger\u003e(){ //Java9才支持匿名内部类使用钻石运算符 { this.add(10); this.add(2); this.add(5); this.add(8); } }; list.sort((a, b) -\u003e { //排序已经由JDK实现，现在只需要填入自定义规则，完成Comparator接口实现 return a - b; //返回值小于0，表示a应该在b前面，返回值大于0，表示b应该在a后面，等于0则不进行交换 }); System.out.println(list); ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:1","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"迭代器 集合的遍历 所有的集合类，都支持foreach循环！ public static void main(String[] args) { List\u003cInteger\u003e list = new LinkedList\u003cInteger\u003e(){ //Java9才支持匿名内部类使用钻石运算符 { this.add(10); this.add(2); this.add(5); this.add(8); } }; for (Integer integer : list) { System.out.println(integer); } } 当然，也可以使用JDK1.8新增的forEach方法，它接受一个Consumer接口实现： list.forEach(i -\u003e { System.out.println(i); }); 从JDK1.8开始，lambda表达式开始逐渐成为主流，我们需要去适应函数式编程的这种语法，包括批量替换，也是用到了函数式接口来完成的。 list.replaceAll((i) -\u003e { if(i == 2) return 3; //将所有的2替换为3 else return i; //不是2就不变 }); System.out.println(list); Iterable和Iterator接口 我们之前学习数据结构时，已经得知，不同的线性表实现，在获取元素时的效率也不同，因此我们需要一种更好地方式来统一不同数据结构的遍历。 由于ArrayList对于随机访问的速度更快，而LinkedList对于顺序访问的速度更快，因此在上述的传统for循环遍历操作中，ArrayList的效率更胜一筹，因此我们要使得LinkedList遍历效率提升，就需要采用顺序访问的方式进行遍历，如果没有迭代器帮助我们统一标准，那么我们在应对多种集合类型的时候，就需要对应编写不同的遍历算法，很显然这样会降低我们的开发效率，而迭代器的出现就帮助我们解决了这个问题。 我们先来看看迭代器里面方法： public interface Iterator\u003cE\u003e { //... } 每个集合类都有自己的迭代器，通过iterator()方法来获取： Iterator\u003cInteger\u003e iterator = list.iterator(); //生成一个新的迭代器 while (iterator.hasNext()){ //判断是否还有下一个元素 Integer i = iterator.next(); //获取下一个元素（获取一个少一个） System.out.println(i); } 迭代器生成后，默认指向第一个元素，每次调用next()方法，都会将指针后移，当指针移动到最后一个元素之后，调用hasNext()将会返回false，迭代器是一次性的，用完即止，如果需要再次使用，需要调用iterator()方法。 ListIterator\u003cInteger\u003e iterator = list.listIterator(); //List还有一个更好地迭代器实现ListIterator ListIterator是List中独有的迭代器，在原有迭代器基础上新增了一些额外的操作。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:2","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"Set集合 我们之前已经看过Set接口的定义了，我们发现接口中定义的方法都是Collection中直接继承的，因此，Set支持的功能其实也就和Collection中定义的差不多，只不过使用方法上稍有不同。 Set集合特点： 不允许出现重复元素 不支持随机访问（不允许通过下标访问） 首先认识一下HashSet，它的底层就是采用哈希表实现的（我们在这里先不去探讨实现原理，因为底层实质上维护的是一个HashMap，我们学习了Map之后再来讨论） public static void main(String[] args) { HashSet\u003cInteger\u003e set = new HashSet\u003c\u003e(); set.add(120); //支持插入元素，但是不支持指定位置插入 set.add(13); set.add(11); for (Integer integer : set) { System.out.println(integer); } } 运行上面代码发现，最后Set集合中存在的元素顺序，并不是我们的插入顺序，这是因为HashSet底层是采用哈希表来实现的，实际的存放顺序是由Hash算法决定的。 那么我们希望数据按照我们插入的顺序进行保存该怎么办呢？我们可以使用LinkedHashSet： public static void main(String[] args) { LinkedHashSet\u003cInteger\u003e set = new LinkedHashSet\u003c\u003e(); //会自动保存我们的插入顺序 set.add(120); set.add(13); set.add(11); for (Integer integer : set) { System.out.println(integer); } } LinkedHashSet底层维护的不再是一个HashMap，而是LinkedHashMap，它能够在插入数据时利用链表自动维护顺序，因此这样就能够保证我们插入顺序和最后的迭代顺序一致了。 还有一种Set叫做TreeSet，它会在元素插入时进行排序： public static void main(String[] args) { TreeSet\u003cInteger\u003e set = new TreeSet\u003c\u003e(); set.add(1); set.add(3); set.add(2); System.out.println(set); } 可以看到最后得到的结果并不是我们插入顺序，而是按照数字的大小进行排列。当然，我们也可以自定义排序规则： public static void main(String[] args) { TreeSet\u003cInteger\u003e set = new TreeSet\u003c\u003e((a, b) -\u003e b - a); //在创建对象时指定规则即可 set.add(1); set.add(3); set.add(2); System.out.println(set); } 现在的结果就是我们自定义的排序规则了。 虽然Set集合只是粗略的进行了讲解，但是学习Map之后，我们还会回来看我们Set的底层实现，所以说最重要的还是Map。本节只需要记住Set的性质、使用即可。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:3","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"Map映射 什么是映射 我们在高中阶段其实已经学习过映射了，映射指两个元素的之间相互“对应”的关系，也就是说，我们的元素之间是两两对应的，是以键值对的形式存在。 Map接口 Map就是为了实现这种数据结构而存在的，我们通过保存键值对的形式来存储映射关系。 我们先来看看Map接口中定义了哪些操作。 HashMap和LinkedHashMap HashMap的实现过程，相比List，就非常地复杂了，它并不是简简单单的表结构，而是利用哈希表存放映射关系，我们来看看HashMap是如何实现的，首先回顾我们之前学习的哈希表，它长这样： 哈希表的本质其实就是一个用于存放后续节点的头结点的数组，数组里面的每一个元素都是一个头结点（也可以说就是一个链表），当要新插入一个数据时，会先计算该数据的哈希值，找到数组下标，然后创建一个新的节点，添加到对应的链表后面。 而HashMap就是采用的这种方式，我们可以看到源码中同样定义了这样的一个结构： /** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node\u003cK,V\u003e[] table; 这个表会在第一次使用时初始化，同时在必要时进行扩容，并且它的大小永远是2的倍数！ /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 \u003c\u003c 4; // aka 16 我们可以看到默认的大小为2的4次方，每次都需要是2的倍数，也就是说，下一次增长之后，大小会变成2的5次方。 我们现在需要思考一个问题，当我们表中的数据不断增加之后，链表会变得越来越长，这样会严重导致查询速度变慢，首先想到办法就是，我们可以对数组的长度进行扩容，来存放更多的链表，那么什么情况下会进行扩容呢？ /** * The load factor for the hash table. * * @serial */ final float loadFactor; 我们还发现HashMap源码中有这样一个变量，也就是负载因子，那么它是干嘛的呢？ 负载因子其实就是用来衡量当前情况是否需要进行扩容的标准。我们可以看到默认的负载因子是0.75 /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; 那么负载因子是怎么控制扩容的呢？0.75的意思是，在插入新的结点后，如果当前数组的占用率达到75%则进行扩容。在扩容时，会将所有的数据，重新计算哈希值，得到一个新的下标，组成新的哈希表。 但是这样依然有一个问题，链表过长的情况还是有可能发生，所以，为了从根源上解决这个问题，在JDK1.8时，引入了红黑树这个数据结构。 当链表的长度达到8时，会自动将链表转换为红黑树，这样能使得原有的查询效率大幅度降低！当使用红黑树之后，我们就可以利用二分搜索的思想，快速地去寻找我们想要的结果，而不是像链表一样挨个去看。 /** * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */ static final class TreeNode\u003cK,V\u003e extends LinkedHashMap.Entry\u003cK,V\u003e { 除了Node以外，HashMap还有TreeNode，很明显这就是为了实现红黑树而设计的内部类。不过我们发现，TreeNode并不是直接继承Node，而是使用了LinkedHashMap中的Entry实现，它保存了前后节点的顺序（也就是我们的插入顺序）。 /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry\u003cK,V\u003e extends HashMap.Node\u003cK,V\u003e { Entry\u003cK,V\u003e before, after; Entry(int hash, K key, V value, Node\u003cK,V\u003e next) { super(hash, key, value, next); } } LinkedHashMap是直接继承自HashMap，具有HashMap的全部性质，同时得益于每一个节点都是一个双向链表，保存了插入顺序，这样我们在遍历LinkedHashMap时，顺序就同我们的插入顺序一致。当然，也可以使用访问顺序，也就是说对于刚访问过的元素，会被排到最后一位。 public static void main(String[] args) { LinkedHashMap\u003cInteger, String\u003e map = new LinkedHashMap\u003c\u003e(16, 0.75f, true); //以访问顺序 map.put(1, \"A\"); map.put(2, \"B\"); map.put(3, \"C\"); map.get(2); System.out.println(map); } 观察结果，我们发现，刚访问的结果被排到了最后一位。 TreeMap TreeMap其实就是自动维护顺序的一种Map，就和我们前面提到的TreeSet一样： /** * The comparator used to maintain order in this tree map, or * null if it uses the natural ordering of its keys. * * @serial */ private final Comparator\u003c? super K\u003e comparator; private transient Entry\u003cK,V\u003e root; /** * Node in the Tree. Doubles as a means to pass key-value pairs back to * user (see Map.Entry). */ static final class Entry\u003cK,V\u003e implements Map.Entry\u003cK,V\u003e { 我们发现它的内部直接维护了一个红黑树，就像它的名字一样，就是一个Tree，因为它默认就是有序的，所以说直接采用红黑树会更好。我们在创建时，直接给予一个比较规则即可。 Map的使用 我们首先来看看Map的一些基本操作： public static void main(String[] args) { Map\u003cInteger, String\u003e map = new HashMap\u003c\u003e(); map.put(1, \"A\"); map.put(2, \"B\"); map.put(3, \"C\"); System.out.println(map.get(1)); //获取Key为1的值 System.out.println(map.getOrDefault(0, \"K\")); //不存在就返回K map.remove(1); //移除这个Key的键值对 } 由于Map并未实现迭代器接口，因此不支持foreach，但是JDK1.8为我们提供了forEach方法使用： public static void main(String[] args) { Map\u003cInteger, String\u003e map = new HashMap\u003c\u003e(); map.put(1, \"A\"); map.put(2, \"B\"); map.put(3, \"C\"); map.forEach((k, v) -\u003e System.out.println(k+\"-\u003e\"+v)); for (Map.Entry\u003cInteger, String\u003e entry : map.entrySet()) { //也可以获取所有的Entry来foreach int key = entry.getKey(); String value = entry.getValue(); System.out.println(key+\" -\u003e \"+value); } } 我们也可以单独获取所有的值或者是键： public static void main(String[] args) { Map\u003cInteger, String\u003e map = new HashMap\u003c\u003e(); map.put(1, \"A\"); map.put(2, \"B\"); map.put(3, \"C\"); System.out.println(map.keySet()); //直接获取所有的key System.out.println(map.values()); //直接获取所有的值 } 再谈Set原理 通过观察Hash","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:4","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"集合的嵌套 既然集合类型中的元素类型是泛型，那么能否嵌套存储呢？ public static void main(String[] args) { Map\u003cString, List\u003cInteger\u003e\u003e map = new HashMap\u003c\u003e(); //每一个映射都是 字符串\u003c-\u003e列表 map.put(\"卡布奇诺今犹在\", new LinkedList\u003c\u003e()); map.put(\"不见当年倒茶人\", new LinkedList\u003c\u003e()); System.out.println(map.keySet()); System.out.println(map.values()); } 通过Key获取到对应的值后，就是一个列表： map.get(\"卡布奇诺今犹在\").add(10); System.out.println(map.get(\"卡布奇诺今犹在\").get(0)); 让套娃继续下去： public static void main(String[] args) { Map\u003cInteger, Map\u003cInteger, Map\u003cInteger, String\u003e\u003e\u003e map = new HashMap\u003c\u003e(); } 你也可以使用List来套娃别的： public static void main(String[] args) { List\u003cMap\u003cString, Set\u003cString\u003e\u003e\u003e list = new LinkedList\u003c\u003e(); } ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:5","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"流Stream和Optional的使用 Java 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。 它看起来就像一个工厂的流水线一样！我们就可以把一个Stream当做流水线处理： public static void main(String[] args) { List\u003cString\u003e list = new ArrayList\u003c\u003e(); list.add(\"A\"); list.add(\"B\"); list.add(\"C\"); //移除为B的元素 Iterator\u003cString\u003e iterator = list.iterator(); while (iterator.hasNext()){ if(iterator.next().equals(\"B\")) iterator.remove(); } //Stream操作 list = list //链式调用 .stream() //获取流 .filter(e -\u003e !e.equals(\"B\")) //只允许所有不是B的元素通过流水线 .collect(Collectors.toList()); //将流水线中的元素重新收集起来，变回List System.out.println(list); } 可能从上述例子中还不能感受到流处理带来的便捷，我们通过下面这个例子来感受一下： public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(); list.add(1); list.add(2); list.add(3); list.add(3); list = list .stream() .distinct() //去重（使用equals判断） .sorted((a, b) -\u003e b - a) //进行倒序排列 .map(e -\u003e e+1) //每个元素都要执行+1操作 .limit(2) //只放行前两个元素 .collect(Collectors.toList()); System.out.println(list); } 当遇到大量的复杂操作时，我们就可以使用Stream来快速编写代码，这样不仅代码量大幅度减少，而且逻辑也更加清晰明了（如果你学习过SQL的话，你会发现它更像一个Sql语句） 注意：不能认为每一步是直接依次执行的！ List\u003cInteger\u003e list = new ArrayList\u003c\u003e(); list.add(1); list.add(2); list.add(3); list.add(3); list = list .stream() .distinct() //断点 .sorted((a, b) -\u003e b - a) .map(e -\u003e { System.out.println(\"\u003e\u003e\u003e \"+e); //断点 return e+1; }) .limit(2) //断点 .collect(Collectors.toList()); //实际上，stream会先记录每一步操作，而不是直接开始执行内容，当整个链式调用完成后，才会依次进行！ 接下来，我们用一堆随机数来进行更多流操作的演示： public static void main(String[] args) { Random random = new Random(); //Random是一个随机数工具类 random .ints(-100, 100) //生成-100~100之间的，随机int型数字（本质上是一个IntStream） .limit(10) //只获取前10个数字（这是一个无限制的流，如果不加以限制，将会无限进行下去！） .filter(i -\u003e i \u003c 0) //只保留小于0的数字 .sorted() //默认从小到大排序 .forEach(System.out::println); //依次打印 } 我们可以生成一个统计实例来帮助我们快速进行统计： public static void main(String[] args) { Random random = new Random(); //Random是一个随机数工具类 IntSummaryStatistics statistics = random .ints(0, 100) .limit(100) .summaryStatistics(); //获取语法统计实例 System.out.println(statistics.getMax()); //快速获取最大值 System.out.println(statistics.getCount()); //获取数量 System.out.println(statistics.getAverage()); //获取平均值 } 普通的List只需要一个方法就可以直接转换到方便好用的IntStream了： public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(); list.add(1); list.add(1); list.add(2); list.add(3); list.add(4); list.stream() .mapToInt(i -\u003e i) //将每一个元素映射为Integer类型（这里因为本来就是Integer） .summaryStatistics(); } 我们还可以通过flat来对整个流进行进一步细分： public static void main(String[] args) { List\u003cString\u003e list = new ArrayList\u003c\u003e(); list.add(\"A,B\"); list.add(\"C,D\"); list.add(\"E,F\"); //我们想让每一个元素通过,进行分割，变成独立的6个元素 list = list .stream() //生成流 .flatMap(e -\u003e Arrays.stream(e.split(\",\"))) //分割字符串并生成新的流 .collect(Collectors.toList()); //汇成新的List System.out.println(list); //得到结果 } 我们也可以只通过Stream来完成所有数字的和，使用reduce方法： public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(); list.add(1); list.add(2); list.add(3); int sum = list .stream() .reduce((a, b) -\u003e a + b) //计算规则为：a是上一次计算的值，b是当前要计算的参数，这里是求和 .get(); //我们发现得到的是一个Optional类实例，不是我们返回的类型，通过get方法返回得到的值 System.out.println(sum); } 通过上面的例子，我们发现，Stream不喜欢直接给我们返回一个结果，而是通过Optinal的方式，那么什么是Optional呢？ Optional类是Java8为了解决null值判断问题，使用Optional类可以避免显式的null值判断（null的防御性检查），避免null导致的NPE（NullPointerException）。总而言之，就是对控制的一个判断，为了避免空指针异常。 public static void main(String[] args) { String str = null; if(str != null){ //当str不为空时添加元素到List中 list.add(str); } } 有了Optional之后，我们就可以这样写： public static void main(String[] args) { String str = null; Optional\u003cString\u003e optional = Optional.ofNullable(str); //转换为Optional optional.ifPresent(System.out::println); //当存在时再执行方法 } 就类似于Kotlin中的： var str : String? = null str?.upperCase() 我们可以选择直接get或是当值为null时，获取备选值： public static void main(String[] args) { String str = null; Optional optional = Optional.ofNullable(str); //转换为Optional（可空） System.out.println(optional.orElse(\"lbwnb\")); // S","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:6","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"Arrays和Collections的使用 Arrays是一个用于操作数组的工具类，它给我们提供了大量的工具方法： /** * This class contains various methods for manipulating arrays (such as * sorting and searching). This class also contains a static factory * that allows arrays to be viewed as lists. \u003c- 注意，这句话很关键 * * @author Josh Bloch * @author Neal Gafter * @author John Rose * @since 1.2 */ public class Arrays { 由于操作数组并不像集合那样方便，因此JDK提供了Arrays类来增强对数组操作，比如： public static void main(String[] args) { int[] array = {1, 5, 2, 4, 7, 3, 6}; Arrays.sort(array); //直接进行排序（底层原理：进行判断，元素少使用插入排序，大量元素使用双轴快速/归并排序） System.out.println(array); //由于int[]是一个对象类型，而数组默认是没有重写toString()方法，因此无法打印到想要的结果 System.out.println(Arrays.toString(array)); //我们可以使用Arrays.toString()来像集合一样直接打印每一个元素出来 } public static void main(String[] args) { int[] array = {1, 5, 2, 4, 7, 3, 6}; Arrays.sort(array); System.out.println(\"排序后的结果：\"+Arrays.toString(array)); System.out.println(\"目标元素3位置为：\"+Arrays.binarySearch(array, 3)); //二分搜素，必须是已经排序好的数组！ } public static void main(String[] args) { int[] array = {1, 5, 2, 4, 7, 3, 6}; Arrays .stream(array) //将数组转换为流进行操作 .sorted() .forEach(System.out::println); } public static void main(String[] args) { int[] array = {1, 5, 2, 4, 7, 3, 6}; int[] array2 = Arrays.copyOf(array, array.length); //复制一个一模一样的数组 System.out.println(Arrays.toString(array2)); System.out.println(Arrays.equals(array, array2)); //比较两个数组是否值相同 Arrays.fill(array, 0); //将数组的所有值全部填充为指定值 System.out.println(Arrays.toString(array)); Arrays.setAll(array2, i -\u003e array2[i] + 2); //依次计算每一个元素（注意i是下标位置） System.out.println(Arrays.toString(array2)); //这里计算让每个元素值+2 } 思考：当二维数组使用Arrays.equals()进行比较以及Arrays.toString()进行打印时，还会得到我们想要的结果吗？ public static void main(String[] args) { Integer[][] array = {{1, 5}, {2, 4}, {7, 3}, {6}}; Integer[][] array2 = {{1, 5}, {2, 4}, {7, 3}, {6}}; System.out.println(Arrays.toString(array)); //这样还会得到我们想要的结果吗？ System.out.println(Arrays.equals(array2, array)); //这样还会得到true吗？ System.out.println(Arrays.deepToString(array)); //使用deepToString就能到打印多维数组 System.out.println(Arrays.deepEquals(array2, array)); //使用deepEquals就能比较多维数组 } 那么，一开始提到的当做List进行操作呢？我们可以使用Arrays.asList()来将数组转换为一个 固定长度的List public static void main(String[] args) { Integer[] array = {1, 5, 2, 4, 7, 3, 6}; List\u003cInteger\u003e list = Arrays.asList(array); //不支持基本类型数组，必须是对象类型数组 Arrays.asList(\"A\", \"B\", \"C\"); //也可以逐个添加，因为是可变参数 list.add(1); //此List实现是长度固定的，是Arrays内部单独实现的一个类型，因此不支持添加操作 list.remove(0); //同理，也不支持移除 list.set(0, 8); //直接设置指定下标的值就可以 list.sort(Comparator.reverseOrder()); //也可以执行排序操作 System.out.println(list); //也可以像List那样直接打印 } 文字游戏：allows arrays to be viewed as lists，实际上只是当做List使用，本质还是数组，因此数组的属性依然存在！因此如果要将数组快速转换为实际的List，可以像这样： public static void main(String[] args) { Integer[] array = {1, 5, 2, 4, 7, 3, 6}; List\u003cInteger\u003e list = new ArrayList\u003c\u003e(Arrays.asList(array)); } 通过自行创建一个真正的ArrayList并在构造时将Arrays的List值传递。 既然数组操作都这么方便了，集合操作能不能也安排点高级的玩法呢？那必须的，JDK为我们准备的Collocations类就是专用于集合的工具类： public static void main(String[] args) { List\u003cInteger\u003e list = new ArrayList\u003c\u003e(); Collections.max(list); Collections.min(list); } 当然，Collections提供的内容相比Arrays会更多，希望大家下去自行了解，这里就不多做介绍了。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:5:7","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"集合类编程实战 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:6:0","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"反转链表 1 \u003c- 3 \u003c- 5 \u003c- 7 \u003c- 9 转换为 1 \u003c- 3 \u003c- 5 \u003c- 7 \u003c- 9 现在有一个单链表，尝试将其所有节点倒序排列 public class Main { public static void main(String[] args) { Node head = new Node(1); head.next = new Node(3); head.next.next = new Node(5); head.next.next.next = new Node(7); head.next.next.next.next = new Node(9); head = reverse(head); while (head != null){ System.out.println(head.value+\" \"); head = head.next; } } public static class Node { public int value; public Node next; public Node(int data) { this.value = data; } } public static Node reverse(Node head) { //在这里实现 } } ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:6:1","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"重建二叉树 现在知道二叉树的前序: GDAFEMHZ，以及中序: ADEFGHMZ，请根据已知信息还原这颗二叉树。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:6:2","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"实现计算器 实现一个计算器，要求输入一个计算公式（含加减乘除运算符，没有负数但是有小数），得到结果，比如输入：1+4*3/1.321，得到结果为：2.2 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:6:3","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["JavaSE笔记"],"content":"字符串匹配（KMP算法） 现在给定一个主字符串和一个子字符串，请判断主字符串是否包含子字符串，例如主字符串：ABCABCDHI，子字符串：ABCD，因此主字符串包含此子字符串；主字符串：ABCABCUISA，子字符串：ABCD，则不包含。 ","date":"2022-01-23","objectID":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/:6:4","tags":["Java泛型与集合类"],"title":"Java泛型与集合类","uri":"/posts/java%E6%B3%9B%E5%9E%8B%E4%B8%8E%E9%9B%86%E5%90%88%E7%B1%BB/"},{"categories":["C++实战"],"content":"”一起来领略C++模板的奥义“","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"想要实现PrintLn，关键在于支持无限个参数的打印函数，所以我大致总结下C++能够如何去实现它！ ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:0:0","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"方式一：用初始化列表实现PrintLn() 【C++11】 ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:1:0","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"版本一：朴素初始化列表版本版本 函数版本： #include\u003ciostream\u003eusing namespace std; void PrintLn(std::initializer_list\u003cint\u003e args){ for(auto arg:args){ cout\u003c\u003carg\u003c\u003c\", \"; } cout\u003c\u003cstd::endl; } int main() { PrintLn({3,23,2,12}); return 0; } 类的构造器版本(可去掉小括号)： #include\u003ciostream\u003eusing namespace std; class PrintLn { public: PrintLn(std::initializer_list\u003cint\u003e args) { for (auto arg:args) { cout \u003c\u003c arg \u003c\u003c \", \"; } cout \u003c\u003c std::endl; } }; int main() { auto t = PrintLn{3, 23, 2, 12}; return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:1:1","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"版本二：加模板参数版本 实际上和上面的基本没太大区别，除了上面确定为了int类型，二加了模板后，可以为任意类型，但是实际上传入的还是只能是同一个类型。所以初始化列表实现是非常的不好用的。 #incldue\u003ciostream\u003e using namespace std; template\u003ctypename T\u003e class PrintLn { public: PrintLn(std::initializer_list\u003cT\u003e args) { for (auto arg:args) { cout \u003c\u003c arg \u003c\u003c \", \"; }# PrintLn函数实现 想要实现PrintLn，关键在于支持无限个参数的打印函数，所以我大致总结下C++能够如何去实现它！ ## 方式一：用初始化列表实现PrintLn() 【C++11】 ### 版本一：朴素初始化列表版本版本 \u003e 函数版本： ```cpp #include\u003ciostream\u003eusing namespace std; void PrintLn(std::initializer_list\u003cint\u003e args){ for(auto arg:args){ cout\u003c\u003carg\u003c\u003c\", \"; } cout\u003c\u003cstd::endl; } int main() { PrintLn({3,23,2,12}); return 0; } 类的构造器版本(可去掉小括号)： #include\u003ciostream\u003eusing namespace std; class PrintLn { public: PrintLn(std::initializer_list\u003cint\u003e args) { for (auto arg:args) { cout \u003c\u003c arg \u003c\u003c \", \"; } cout \u003c\u003c std::endl; } }; int main() { auto t = PrintLn{3, 23, 2, 12}; return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:1:2","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"版本二：加模板参数版本 实际上和上面的基本没太大区别，除了上面确定为了int类型，而加了模板后，可以为任意类型，但是实际上传入的还是只能是同一个类型。所以初始化列表的方式实现Println只能说形似而神不似。 #incldue\u003ciostream\u003e using namespace std; template\u003ctypename T\u003e class PrintLn { public: PrintLn(std::initializer_list\u003cT\u003e args) { for (auto arg:args) { cout \u003c\u003c arg \u003c\u003c \", \"; } cout \u003c\u003c std::endl; } }; int main() { auto t = PrintLn\u003cint\u003e{3, 23, 2, 12}; return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:1:3","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"方式二：用可变参模板实现 【C++11/17】 如果有了解过C的可变参函数和可变参的宏，那么这个可变参模板与它有些类型，只不过C里面的va_start,va_list,va_arg,va_end这一系列实现可变参数的宏用起来非常麻烦，而且无法确定每个参数的类型，而可变参的模板则带有模板的泛型性质，所以是能确定类型的，甚至由于模板可以传值，后面还可直接传值使用。 以下简单描述可变参模板的使用方式： typenam... 算C++的一个新的关键字，它可以用来定义一个可变参的模板类型，而这个类型在其他地方定义使用的时候也要在后面带上 ... 表示拆包，否则会报错。 例如： template\u003ctypename... T\u003e void f(T... t){//TODO 这种类型或变量在任何地方作为参数定义或者传递的时候都需要加上...表示拆包 f(t...) } 在C++17出现fold expression之前，这个拆包过程只能借助另一个模板参数来得到模板参数包里面的内容。 注意以上两点，那么可以开始编写泛型模板，实现可变参数的完全打印过程了。 ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:2:0","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"C++11版本实现 错误实现版本：如果你直接像下面这样进行拆包，那么编译是会报错的，因为拆包过程相当于一个递归的过程，而你这个递归的过程没有一个跳出的条件，比如args如果为0个参数时，继续在往下就无法展开了，所以需要实现一个没有参数的版本让拆包过程停止。 template\u003ctypename T,typename... Args\u003e void PrintLn(T firstArg,Args... args){ cout\u003c\u003cfirstArg\u003c\u003c\", \"; PrintLn(args...); } //拆包过程：PrintLn(3,1,3,4)-\u003e // PrintLn(firstArg:3,args(1,3,4)); // PrintLn(firstArg:1,args(3,4)); // PrintLn(firstArg:3,args(4)); // PrintLn(firstArg:4,args(null)) // 由于到了上面的第四行还要继续往下拆包 // 而此时只有0个参数，没有对应的PrintLn版本可以调用，故报错！ 以下为正确修改版本： #include\u003ciostream\u003eusing namespace std; void PrintLn(){ } template\u003ctypename T,typename... Args\u003e void PrintLn(T firstArg,Args... args){ cout\u003c\u003cfirstArg\u003c\u003c\", \"; PrintLn(args...); } int main() { PrintLn(3, 23, 2, 12); return 0; 当然也可以控制只剩一个参数时就停止拆包。 #include\u003ciostream\u003eusing namespace std; template\u003ctypename T\u003e void PrintLn(T arg){ cout\u003c\u003carg\u003c\u003cendl; } template\u003ctypename T,typename... Args\u003e void PrintLn(T firstArg,Args... args){ cout\u003c\u003cfirstArg\u003c\u003c\", \"; PrintLn(args...); } int main() { PrintLn(3, 23, 2, 12); return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:2:1","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"C++17版本实现 上面的实现流程实际上在C++17中可以用 if constexpr()+sizeof… 在编译期间来进行流程控制。 首先来讲一讲为什么普通的 if + sizeof… 来实现可变参数的长度控制流程会报错呢？ 因为整个模板推断和拆包解包过程是在编译期完成的，而if的控制流程在编译期是完全不清楚的，所以会报错，但是有了if constexpr之后，就能控制编译期的模板拆包过程了！ 如上面实现PrintLn，可以直接简化成下面这样： #include\u003ciostream\u003eusing namespace std; template\u003ctypename T,typename... Args\u003e void PrintLn(T firstArg,Args... args){ if constexpr(sizeof...(args)==0){//当参数个数为0个的时候就不继续拆包了 cout\u003c\u003cfirstArg\u003c\u003cendl; }else{ cout\u003c\u003cfirstArg\u003c\u003c\", \"; PrintLn(args...);//往下继续拆包 } } int main() { PrintLn(3, 23, 2, 12); return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:2:2","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"方式三：可变参模板的fold expression展开 【C++17】 在C++17中，加入了一个fold expression的语法，让可变参数模板可以不通过递归的方式来解包，直接把每个包解开放入一个表达式，然后剩余的包都以该表达式解开，基本的语法如下： ((expression)op...); expression : 表示希望每个解开的参数所执行的表达式。 op : 你指定的操作符。 ... : 一直不断的解包，由于此处放的位置是右边，所以往右边解包，如果放左边则往左边解包。 示例代码： #include \u003cbits/stdc++.h\u003eusing namespace std; template\u003ctypename... Args\u003e double sum(Args... args){ return (args+...);//等价于3+23+1+3.32 } template\u003cauto... val\u003e//可变的传值的模板参数 constexpr int sum(){ return (val+...); } int main() { cout\u003c\u003csum\u003c3,23,1,32\u003e()\u003c\u003cendl;//传值模板参数不支持浮点类型，所以全用的int类型 cout\u003c\u003csum(3,23,1,3.32); return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:3:0","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"简单的利用fold expr实现 基于以上对fold expr的使用，我们来正式实现PrintLn，值得一提的是，这个fold expr的性能肯定是比之前递归解包的性能要好的，因为只是迭代的拓宽而已。 我们可以将拆开的包用 ',' 展开 #include\u003ciostream\u003eusing namespace std; template\u003ctypename... Args\u003e void PrintLn(Args... args){ ((cout\u003c\u003cargs\u003c\u003c\", \"),...)\u003c\u003cendl; } int main() { PrintLn(1,2.3,\"LB\",\"hhh\"); return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:3:1","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"加上流程控制实现 通过更复杂的流程控制把最后一个打印出来的逗号去掉。 通过延申三元运算符，使得运行时能够正确的打印最后一次。 反正我这里编译期只负责文本替换，所以被fold expr展开的表达式并不会有什么要是编译期常量的要求。 这一切都看作简单宏替换即可。 #include\u003ciostream\u003eusing namespace std; template\u003ctypename... Args\u003e void PrintLn(Args... args){ int lastIndex = sizeof...(args)-1;//得到传入的参数长度 int i = 0; ((i++==lastIndex?cout\u003c\u003cargs\u003c\u003cendl:cout\u003c\u003cargs\u003c\u003c\", \"),...); } int main() { PrintLn(1,2.3,\"LB\",\"hhh\"); return 0; } ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:3:2","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"更多fold expr运用… 利用与或表达式展开，然后利用它们的短路性质，实现得到拆包元素的精准打击（获得包里的第几个元素）。 #include\u003ciostream\u003eusing namespace std; template\u003ctypename... Args\u003e auto GetNth(int n, Args... args) { int i = 0; using CommonType = common_type_t\u003cArgs...\u003e; CommonType ret; ((i++ == n \u0026\u0026 (ret = args, true))||...); return ret; } int main() { cout \u003c\u003c GetNth(3, 2, 1, 2.3, 32.2); return 0; } 上面为了存储不确定的类型用了common_type_t，这个可以帮助你得到一个公共可用的类型，而这个类型必须是公共可用，比如int了float型可以进行相互转化所以有公共类型，而 char* 和int类型则没有，所以这个GetNth中的元素不能传递 char* 类型的同时传递int类型。 ","date":"2022-01-20","objectID":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/:3:3","tags":["C++模板"],"title":"C++变参模板运用实战——实现PrintLn","uri":"/posts/c++%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%86%99println/"},{"categories":["C++实战"],"content":"”Socket基本操作的C++封装“","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"封装过程 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:1:0","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"接口类的实现(抽象类) _public_socket.h 该头文件用于包含所有该系统平台socket所需要依赖的库。 windows平台 #ifndef MY_TINY_STL__PUBLIC_SOCKET_H #define MY_TINY_STL__PUBLIC_SOCKET_H #include \u003cwinsock2.h\u003e#pragma comment (lib, \"ws2_32.lib\") //加载 ws2_32.dll #endif //MY_TINY_STL__PUBLIC_SOCKET_H Linux平台 #include \u003cunistd.h\u003e#include \u003carpa/inet.h\u003e#include \u003csys/socket.h\u003e#include \u003cnetinet/in.h\u003eTCP_INTERFACE.h(作用于win平台) 由于该接口由服务器端和客户端继承，而两者同样的函数成员也就是这些了，设计客户端和服务器端时就只需要考虑各自的套接字以及其余操作的成员函数，也不需要管理DLL的开关。 还有一个erro_die()成员函数用于阻断错误并打印对应情况。 // // Created by Alone on 2021/8/17. // #ifndef MY_TINY_STL_TCP_INTERFACE_H #define MY_TINY_STL_TCP_INTERFACE_H #include \u003ccstdio\u003e#include \"_public_socket.h\" class TCP_INTERFACE { public: TCP_INTERFACE() { //初始化 DLL WSADATA wsaData; WSAStartup(MAKEWORD(2, 2), \u0026wsaData); } //返回值小于等于0时发生错误 virtual int Send(SOCKET clnt, const void *buf, const int buflen) = 0; virtual int Recv(SOCKET clnt, void *buf, const int buflen) = 0; //closesocket返回值不为0则发生错误 virtual void Close(SOCKET clnt) = 0; virtual void error_die(const char *str) = 0; ~TCP_INTERFACE() { WSACleanup(); } }; #endif //MY_TINY_STL_TCP_INTERFACE_H ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:1:1","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"服务器端封装 这次修改了下逻辑，还是用accept返回一个套接字进行发送和接收操作比较好。类的底层没有再保留用于和某个客户端通信的套接字了，自己控制各个客户端套接字的关闭和使用。 TCP_SOCKET_SERVER.h // // Created by Alone on 2021/8/16. // #ifndef MY_TINY_STL_TCP_SOCKET_SERVER_H #define MY_TINY_STL_TCP_SOCKET_SERVER_H #include \"TCP_INTERFACE.h\" class TCP_SOCKET_SERVER : public TCP_INTERFACE { public: TCP_SOCKET_SERVER(); ~TCP_SOCKET_SERVER(); void Bind(int port); void Listen(); SOCKET Accept(); int Send(SOCKET clnt, const void *buf, const int buflen); int Recv(SOCKET clnt, void *buf, const int buflen); void Close(SOCKET clnt); void error_die(const char *str); private: SOCKET servSock; sockaddr_in sockAddr; }; #endif //MY_TINY_STL_TCP_SOCKET_SERVER_H TCP_SOCKET_SERVER.cpp // // Created by Alone on 2021/8/16. // #include \"TCP_SOCKET_SERVER.h\" //初始化操作 TCP_SOCKET_SERVER::TCP_SOCKET_SERVER() : servSock(0) { memset(\u0026sockAddr, 0, sizeof(sockAddr)); //每个字节都用0填充 } //绑定操作 void TCP_SOCKET_SERVER::Bind(int port) { servSock = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP); sockAddr.sin_family = PF_INET; //使用IPv4地址 sockAddr.sin_addr.s_addr = htonl(INADDR_ANY); //具体的IP地址 sockAddr.sin_port = htons(port); //端口 if (bind(servSock, (SOCKADDR *) \u0026sockAddr, sizeof(SOCKADDR)) != 0) { error_die(\"bind\"); } } //置于监听状态 void TCP_SOCKET_SERVER::Listen() { if (servSock == 0) error_die(\"listen\"); if (listen(servSock, SOMAXCONN) != 0) { error_die(\"listen\"); } } //利用套接字的监听串口，接收客户端的请求，建立新的套接字进行存储信息 SOCKET TCP_SOCKET_SERVER::Accept() { SOCKADDR t; int nSize = sizeof(SOCKADDR); //后面两个参数为可选 SOCKET clnt = accept(servSock, \u0026t, \u0026nSize); if (clnt \u003c= 0)error_die(\"accept\"); return clnt; } //返回的是发送到缓冲区的字节长度 int TCP_SOCKET_SERVER::Send(SOCKET clnt, const void *buf, const int buflen) { return send(clnt, (const char *) buf, buflen, 0); } //返回已经接收的字节长度 int TCP_SOCKET_SERVER::Recv(SOCKET clnt, void *buf, const int buflen) { return recv(clnt, (char *) buf, buflen, 0); } //析构函数关闭socket TCP_SOCKET_SERVER::~TCP_SOCKET_SERVER() { if (servSock != 0)closesocket(servSock); } void TCP_SOCKET_SERVER::Close(SOCKET clnt) { if (closesocket(clnt) != 0) { error_die(\"closesocket\"); } } void TCP_SOCKET_SERVER::error_die(const char *str) { printf(\"[hint]%s failed:%d\", str, WSAGetLastError()); exit(-1); } ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:1:2","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"客户端的封装 TCP_SOCKET_CLIENT.h 增加了利用域名查询ip地址的成员函数gethostbyname(),挺好玩的！此次增加了erro_die函数，且发送和接收都操作套接字。在类的内部还是保留了套接字的备份，用于忘记关闭套接字时，析构函数进行关闭。 // // Created by Alone on 2021/8/18. // #ifndef MY_TINY_STL_TCP_SOCKET_CLIENT_H #define MY_TINY_STL_TCP_SOCKET_CLIENT_H #include \"TCP_INTERFACE.h\"#include \u003ciostream\u003e class TCP_SOCKET_CLIENT : public TCP_INTERFACE { public: TCP_SOCKET_CLIENT(); ~TCP_SOCKET_CLIENT(); SOCKET Connect(const char *IPAdrr, u_short port); //用于利用URL(域名)查询IP地址 void Gethostbyname(const char *URL); //接口必须实现的函数 int Send(SOCKET clnt,const void *buf, const int bufSize); int Recv(SOCKET clnt,void *buf, const int bufSize); void Close(SOCKET clnt); void error_die(const char *str); private: //由于一般客户端只需要一个套接字实现连接,然后还需要一个socketadrr_in用于连接内容的赋值 SOCKET clntSock; sockaddr_in sockAddr; }; #endif //MY_TINY_STL_TCP_SOCKET_CLIENT_H TCP_SOCKET_CLIENT.cpp // // Created by Alone on 2021/8/17. // #include \"TCP_SOCKET_CLIENT.h\" //初始化 TCP_SOCKET_CLIENT::TCP_SOCKET_CLIENT() : clntSock(0) {} //关闭套接字操作 void TCP_SOCKET_CLIENT::Close(SOCKET clnt) { if (closesocket(clnt) != 0) error_die(\"close\"); clntSock = 0; } //连接服务器操作 SOCKET TCP_SOCKET_CLIENT::Connect(const char *IPAdrr, u_short port) { memset(\u0026sockAddr, 0, sizeof sockAddr); clntSock = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP); sockAddr.sin_family = PF_INET; sockAddr.sin_addr.s_addr = inet_addr(IPAdrr); sockAddr.sin_port = htons(port); if (connect(clntSock, (SOCKADDR *) \u0026sockAddr, sizeof(sockAddr)) != 0) { error_die(\"connect\"); } return clntSock; } //发送信息操作 int TCP_SOCKET_CLIENT::Send(SOCKET clnt,const void *buf, const int bufSize) { return send(clnt, (const char *) buf, bufSize, 0); } //接收信息操作 int TCP_SOCKET_CLIENT::Recv(SOCKET clnt,void *buf, const int bufSize) { return recv(clnt, (char *) buf, bufSize, 0); } //根据域名获取ip地址等信息 void TCP_SOCKET_CLIENT::Gethostbyname(const char *URL) { hostent *host = gethostbyname(URL); if (!host) { std::cout \u003c\u003c \"Get IP address error!\\n\"; return; } //打印本命 std::cout \u003c\u003c URL \u003c\u003c std::endl; //别名 for (int i = 0; host-\u003eh_aliases[i]; i++) { printf(\"Aliases %d: %s\\n\", i + 1, host-\u003eh_aliases[i]); } //地址类型 printf(\"Address type: %s\\n\", (host-\u003eh_addrtype == AF_INET) ? \"AF_INET\" : \"AF_INET6\"); //IP地址,其中inet_ntoa()函数是将网络字节序转为本地的字节序，方便打印看懂 for (int i = 0; host-\u003eh_addr_list[i]; i++) { printf(\"IP addr %d: %s\\n\", i + 1, inet_ntoa(*(struct in_addr *) host-\u003eh_addr_list[i])); } } //析构时需要确保所有东西已经关闭 TCP_SOCKET_CLIENT::~TCP_SOCKET_CLIENT() { if (clntSock != 0) closesocket(clntSock); } void TCP_SOCKET_CLIENT::error_die(const char *str) { printf(\"[hint]%s failed:%d\", str, WSAGetLastError()); exit(-1); } ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:1:3","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实例讲解 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:2:0","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实例一：回声程序通信 服务器回声程序 绑定本地1234端口，进入监听状态等待请求，如果通信对象关闭了通信，也不慌，重新goto到等待请求得到新的通信套接字 #include \u003ciostream\u003e#include \"TCP_SOCKET_SERVER.h\" #define BUF_SIZE 1000 using namespace std; int main() { TCP_SOCKET_SERVER a; a.Bind(1234); a.Listen(); restart: SOCKET clnt = a.Accept(); while (1) { char *x = new char[BUF_SIZE]; memset(x, 0, BUF_SIZE); int size = a.Recv(clnt,x, BUF_SIZE); if (size \u003c= 0) break; if (a.Send(clnt,x, size) \u003c= 0) break; } a.Close(clnt); cout \u003c\u003c \"connect is over.Waiting for a new connection!\\n\"; goto restart; } 客户端通信程序 为保持持续通信，一旦客户端拒绝了请求，那么弹出循环重新连接，并设置连接超时操作。 #include \"TCP_SOCKET_CLIENT.h\"#define BUF_SIZE 100 int main(){ TCP_SOCKET_CLIENT t; const char* to = \"127.0.0.1\"; restart: SOCKET clnt = t.Connect(to,1234); while(1){ std::cout\u003c\u003c\"\\nInput your message:\\n\"; char buf[BUF_SIZE] = {0}; std::cin.getline(buf,99); int size = t.Send(clnt,buf,BUF_SIZE); if(size\u003c=0) break; memset(buf,0,sizeof buf); if(t.Recv(clnt,buf,size)\u003c=0) break; printf(\"received from %s is:\\n\",to); std::cout\u003c\u003cbuf; } t.Close(clnt); std::cout\u003c\u003c\"The Server is disconnected,and socket has been cleaned up,socket connection has been re-established\\n\"; goto restart; return 0; } 回声效果 客户端收到的结果 服务器端一直运行着，只要不关闭，但每次只能和一个客户端进行通信，通信完后重新等待连接。 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:2:1","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实例二：文件操作，传送图片(掌握重复传包) 分析待传图片 看看这百万大小的字节，一次肯定是传不完的，所以我们需要发送端不断的续传，直到传送完毕。 发送端程序 #include \"TCP_SOCKET_CLIENT.h\"#include \u003cfstream\u003eint main(){ TCP_SOCKET_CLIENT t; const char* to = \"127.0.0.1\"; restart: SOCKET clnt = t.Connect(to,1234); //图片写入buf(这几百万字节大小，得亏是new动态分配 std::ifstream img(\"D:/DesktopBackground/L-69.png\",std::ios::in|std::ios::binary); //设置文件指针用于求文件内容长度 img.seekg(0,std::ios::end); int len = img.tellg(); img.seekg(0,std::ios::beg); if(len\u003e0){printf(\"read OK\\n\");} else {printf(\"file is empty!\");return 0;} //填补buf char * buf = new char[len]; img.read(buf,len); //发送数据到服务器,一次肯定发送不完，所以多次 int sum = 0; while(sum\u003clen){ int sendlen = t.Send(clnt,buf,len); if(sendlen\u003c=0){ printf(\"Send Erro!\"); return 0; } sum += sendlen; } t.Close(clnt); printf(\"Send OK!\"); return 0; } 接收端程序 #include \u003ciostream\u003e#include \"TCP_SOCKET_SERVER.h\"#include \u003cfstream\u003e#define BUF_SIZE 100 using namespace std; int main() { TCP_SOCKET_SERVER a; a.Bind(1234); a.Listen(); //等待连接，连接成功便可建立通讯 SOCKET clnt = a.Accept(); //创建文件用于写入图片数据 ofstream t(\"test.png\",ios::binary|ios::out); //由于要接收的图片文件较大，需要分多次包进行传输数据，所以需要不断循环接收 while(1){ char buf[BUF_SIZE]; int sz = a.Recv(clnt,buf,BUF_SIZE); //直到发送端发送数据完毕断开连接后，便可判断为接收完毕 if(sz\u003c=0){ cout\u003c\u003c\"Finish !\"; return 0; } //每次调整文件指针位置到最后续写 t.seekp(0,ios::end); t.write(buf,sz); } a.Close(clnt); } 接收结果 一模一样毫无偏差 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:2:2","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实例三：Web通信(浏览器访问服务器) ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:3:0","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"Web服务器程序 我这个web服务器也算是及其简单了。。并没有对客户端的http请求进行解析然后发送对应的文件给客户端，而是单纯的我客户端想怎么发就怎么发。。另外这个程序虽然是对图片进行了判断，但并未写出对应的图片发送程序(二进制文件读写是不一样的)，所以实际只能发送文本文件，如html代码。所以后面看到的课程表都无法显示图片的原因是客户端程序压根就没在乎过客户端的请求🤣 #include \u003ciostream\u003e#include \u003cfstream\u003e#include \"TCP_SOCKET_SERVER.h\" void sendfileToWeb(SOCKET clnt, TCP_SOCKET_SERVER \u0026a, const char *filename); int main() { TCP_SOCKET_SERVER a; a.Bind(8086); a.Listen(); SOCKET clnt = a.Accept(); while (1) { sendfileToWeb(clnt, a, \"D:/Html/schedule/schedule.html\"); a.Close(clnt); clnt = a.Accept(); } } //反馈请求，发送文件代码或者图片等二进制信息。 void sendfileToWeb(SOCKET clnt, TCP_SOCKET_SERVER \u0026a, const char *filename) { //写入返回头信息：包括状态和内容类型 char *type = nullptr; if (strstr(filename, \".html\")) type = \"text/html\"; else if (strstr(filename, \".jpg\")) type = \"image/jpg\"; else if (strstr(filename, \".png\")) type = \"image/png\"; char x[100] = {0}; sprintf(x, \"HTTP/1.1 200 ok\\r\\nContent-Type: %s\\r\\n\\r\\n\", type); //发送返回头信息,每次发送间隔需要一定时间，否则浏览器可能接收没这么快 int sz1 = a.Send(clnt, x, strlen(x)); if (sz1 \u003c= 0)return; //发送文件内容到客户端 std::ifstream ss(filename, std::ios::in); char html[1024] = {0}; while (ss.getline(html, 1024)) { int szz = a.Send(clnt, html, strlen(html)); if (szz \u003c= 0) return; Sleep(1); } ss.close(); } ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:3:1","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"接收结果 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:3:2","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"总结 收获： 了解到网络通讯过程到底是怎么样的。 了解到底层socket通信是如何进行的。 封装了socket操作，增强了代码的复用性。 对基本的http请求过程有所了解： 基本上就是浏览器(客户端)对相应的IP地址发起请求，其对应的服务器返回给你这个网页的主页，然后根据你鼠标的点击，又会触发http请求，其对应的服务器对你的请求进行解析，得出你想要的文件，然后发送给你，循环往复一直如此。。。 ","date":"2022-01-20","objectID":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/:4:0","tags":["socket通信"],"title":"Socket基本操作的C++封装","uri":"/posts/socket%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cc++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"”简单学习下基本的类封装“","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"为什么需要大数加减类？ 对于计算机而言，基本的数据类型一般最多为64位数据表示范围，这个范围是有限的，没法无限的表示所有的数据，那么有没有一种方式能够表示所有的大数，并完成加减乘除呢？ 答案肯定是有的，由于数据都是由一位一位的数字所组成，我们只需要用数组中的每一位表示一位数字，便可完成对大数的模拟了。 那么我们说明时候需要用到大数模拟呢？对竞赛人而言，有很多题目实际上就是高精度大数模拟类型，而对于普通的程序员而言，大数模拟也仅是在做某个逻辑运算而保证不会溢出的最佳策略，那么大家难道不好奇如何实现一个大数模拟类吗？ 现在就从封装一个简单的加减类开始了解这样一个大数模拟是怎么实现的👀 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:1:0","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"大数加减类实现详解 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:2:0","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"一、流程图总览 如图总体来说分为五部分： 静态成员函数：属于类的公共接口(核心) 构造和析构函数：构造对象以及析构对象 成员数据：用于运算的数据以及表示对象的数据 运算符重载：用于自定义运算方式(核心) 内部成员函数：属于对象的公共接口 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:2:1","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"二、成员数据和构造函数详解 成员数据 bool f; //是否是负数的标记 char *nums; //存储非符号的大数各个位 int length; //nums的数据长度 int capacity; //nums的可用容量 构造和析构 //缺省构造函数 BigInteger() : length(0), capacity(1), f(false) { nums = new char[capacity]; } //用于转化普通字符串的构造函数 BigInteger(const char *n) : length(strlen(n)), f(false) { int start = 0; if (n[0] == '-') { f = true; start++; } while (start\u003clength\u0026\u0026n[start] == '0')start++; capacity = length * 10; nums = new char[capacity]; std::copy(n + start, n + length, nums); length = length - start; } //拷贝构造函数 BigInteger(BigInteger \u0026a) { capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); } //移动构造函数：这里调用了等于号，根据a的类型来决定用哪个等于号 BigInteger(BigInteger \u0026\u0026a) :length(0){ *this = a; } //析构函数 ~BigInteger() { delete[] nums; } ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:2:2","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"三、(算法核心)静态成员函数和运算符重载详解 static Swap() //调用std的swap实现对基本数据的交换 static void Swap(BigInteger \u0026a, BigInteger \u0026b) { std::swap(a.length, b.length); std::swap(a.capacity, b.capacity); std::swap(a.f, b.f); std::swap(a.nums, b.nums); } static compare() //不看符号比较nums的大小：表示a是否比b大 static bool compare(const BigInteger \u0026a,const BigInteger \u0026b) { //比较纯nums大小(不看符号 int n1 = a.length; int n2 = b.length; if (n1 != n2)return n1 \u003e n2; //返回a和b哪个大，true则位a大 int i = 0; while (i \u003c n1 \u0026\u0026 a.nums[i] == b.nums[i])i++; //a b一样长的情况下，比较两个的值 if (i == n1) return false; return a.nums[i] \u003e b.nums[i]; } static isEqual() //表示a和b是否相等 bool isEqual(BigInteger \u0026a, BigInteger \u0026b) { if (a.f != b.f || (a.length != b.length))return false; int i = 0; while (i \u003c a.length \u0026\u0026 a.nums[i] == b.nums[i])i++; return i == a.length \u0026\u0026 a.f == b.f; } (*核心算法)static add() 不看符号的加法，符号这方面由重载加法运算符控制。 static BigInteger add(BigInteger \u0026a, BigInteger \u0026b) { //不看符号的加法 a.reverse();//尾端对齐 b.reverse(); BigInteger t; int up = 0; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta + tb + up; t.push_back(base % 10 + '0'); up = base / 10; } if (up) t.push_back(up + '0'); t.reverse();//返回原位 a.reverse(); b.reverse(); return t; } (*核心算法)static minus() 不看符号的减法,默认了a的nums大小(不看符号)是比b大的。 static BigInteger minus(BigInteger \u0026a, BigInteger \u0026b) { a.reverse(); b.reverse(); BigInteger t; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta - tb; if (base \u003c 0) { base += 10; a[i + 1]--; } t.push_back(base + '0'); } t.reverse(); a.reverse(); b.reverse(); return t; } char \u0026operator[] char \u0026operator[](int i) { return nums[i]; } BigInteger \u0026operator= 用了两个版本–右值引用和左值引用版本 右值引用延长寿命，用交换的方式实现(毕竟是将亡值 BigInteger \u0026operator=(BigInteger\u0026\u0026 a) { //Swap\u0026Copy方式实现右值赋值重载 Swap(*this, a); return *this; } 左值引用深拷贝 BigInteger \u0026operator=(const BigInteger \u0026a) {//深拷贝 if (length != 0)//如果不是初始化调用的 = ，则肯定需要先把原来的内存delete掉 delete[]nums; capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); return *this; } bool operator\u003c 重载了小于号，好处在于可以直接利用stl进行各种排序操作了。 注意：一定要写成const版本的成员函数，不然STL库无法调用，因为STL库中的所有比较都是基于const对象。 bool operator\u003c(const BigInteger \u0026a) const { if (f \u0026\u0026 !a.f) { //其中一个为负数，则那个是更小的 return true; } else if (!f \u0026\u0026 a.f) return false; if (f) { //两者都为负数的情况，左边的值要更大则为true return compare(*this, a); }//两者均为正数，则值更小的在左边为true return compare(a, *this); } (*核心算法)BigInteger operator+ 利用静态成员函数完成无符号的加减，然后在这里进行判断各种符号情况，根据不同的符号情况进行不同的加法处理。 注意在调用minus之前需要比较两个数的nums谁更大，更大的放在第一个参数上！ BigInteger operator+(BigInteger \u0026a) { BigInteger res; bool flag; if (a.f \u0026\u0026 f) { //同为负数情况，直接相加，再改符号 res = add(*this, a); flag = true; } else if (a.f \u0026\u0026 !f) {//左正右负 if (compare(a, *this)) { //看负数对应的nums是否比正数大 flag = true; res = minus(a, *this); } else { flag = false; res = minus(*this, a); } } else if (!a.f \u0026\u0026 f) { if (compare(*this, a)) { //与上一个相同 flag = true; res = minus(*this, a); } else { flag = false; res = minus(a, *this); } } else { //同时为正数就是最简单的加法 flag = false; res = add(*this, a); } res.f = flag; return res; } (*核心算法)BigInteger operator- 同样是分类讨论，同样是根据不同的类型调用minus和add函数。 与正数不同的处理在于对符号的处理，如果同为负数，则需要判断两者是否相等，防止两数相等相减后减为 0，而被处理为 -0 。 BigInteger operator-(BigInteger \u0026a) { BigInteger res; bool flag; if (a.f \u0026\u0026 f) { //同为负数情况--左边-右边==(不看符号)右边-(不看符号)左边 if (compare(a, *this)) { flag = false; res = minus(a, *this); } else { if (isEqual(*this, a)) flag = false; else flag = true; res = minus(*this, a); } } else if (a.f \u0026\u0026 !f) { //左边为正，右边为负--左边-右边==左边+右边 flag = false; res = add(a, *this); } else if (!a.f \u0026\u0026 f) { //右边为正，左边为负--左边-右边==两边为负的加法 flag = true; res = add(a, *this); } else { //同时为正数--左边-右边==左边-右边(分类讨论正负 if (compare(a, *this)) { //右边\u003e左边,符号为负 res = minus(a, *this); flag = true; } else { //右边\u003c左边，符号为正 res = minus(*this, a); flag = false; } } res.f = flag; return res; } ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:2:3","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"四、其他内部成员函数详解 向外提供的get接口 int getCap() { return capacity; } int getLength() { return length; } bool isNegative() { return f; } bool isEmpty() { return length == 0; } 进行赋值操作所必备的push_back和reverse函数 void push_back(char x) { if (length \u003e= capacity) {//扩容操作 capacity *= 2; char *t = nums; nums = new char[capacity]; std::copy(t, t + length, nums); delete[]t; } nums[length++] = x; } void reverse() {//反转操作 int l = 0, r = length - 1; while (l \u003c r) { std::swap(nums[l], nums[r]); l++; r--; } } 无关紧要的 read() 输入接口 和 print() 输出测试接口 void print() { if (f) printf(\"-\"); nums[length] = '\\0'; int i = 0; while (nums[i] == '0')i++; printf(\"%s\", nums + i); } void read() {//利用getchar()给对象赋上数据 char c = getchar(); if (c == '-') { f = true; c = getchar(); } while (c == '0') c = getchar();//将前导0消耗掉 while (c != '\\n') { push_back(c);//不断的调用push_back即可 c = getchar(); } } ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:2:4","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"整理代码 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:3:0","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":".h声明文件 如果在声明类的同时进行定义，则内部的成员函数默认就是内联的。所以我们一般把短小的代码进行内联，以下的实现均是以该规律进行。 // // Created by Alone on 2021/10/7. // #ifndef MY_TINY_STL_BIGINTEGER_H #define MY_TINY_STL_BIGINTEGER_H #include \u003calgorithm\u003e#include \u003ciostream\u003e#include \u003ccstring\u003e class BigInteger { bool f; char *nums; int length; int capacity; public: //构造函数 BigInteger() : length(0), capacity(1), f(false) { //缺省构造函数 nums = new char[capacity]; } BigInteger(const char *n); BigInteger(const BigInteger \u0026a); BigInteger(BigInteger \u0026\u0026a); ~BigInteger() { //析构函数 delete[] nums; } public: //静态函数 static void Swap(BigInteger \u0026a, BigInteger \u0026b); static bool compare(const BigInteger \u0026a, const BigInteger \u0026b); bool isEqual(BigInteger \u0026a, BigInteger \u0026b); static BigInteger add(BigInteger \u0026a, BigInteger \u0026b); static BigInteger minus(BigInteger \u0026a, BigInteger \u0026b); public: //运算符重载 char \u0026operator[](int i) { return nums[i]; } BigInteger \u0026operator=(BigInteger \u0026\u0026a) { //Swap\u0026Copy方式实现右值赋值重载 Swap(*this, a); return *this; } BigInteger \u0026operator=(const BigInteger \u0026a); bool operator\u003c(const BigInteger \u0026a) const; BigInteger operator+(BigInteger \u0026a); BigInteger operator-(BigInteger \u0026a); public: //对象的基本成员函数 int getCap() { return capacity; } int getLength() { return length; } bool isNegative() { return f; } bool isEmpty() { return length == 0; } void reverse(); void push_back(char x); void print(); void read(); }; #endif //MY_TINY_STL_BIGINTEGER_H ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:3:1","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":".cpp定义并实现 // // Created by Alone on 2021/10/7. // #include \"BigInteger.h\" //@构造函数实现 BigInteger::BigInteger(const char *n) : length(strlen(n)), f(false) { //用于初始值的构造函数 int start = 0; if (n[0] == '-') { f = true; start++; } while (start \u003c length \u0026\u0026 n[start] == '0')start++; capacity = length * 10; nums = new char[capacity]; std::copy(n + start, n + length, nums); length = length - start; } BigInteger::BigInteger(const BigInteger \u0026a) { //拷贝构造函数 capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); } BigInteger::BigInteger(BigInteger \u0026\u0026a) : length(0) { //移动构造函数 *this = a; } //@静态函数实现 void BigInteger::Swap(BigInteger \u0026a, BigInteger \u0026b) { std::swap(a.length, b.length); std::swap(a.capacity, b.capacity); std::swap(a.f, b.f); std::swap(a.nums, b.nums); } bool BigInteger::compare(const BigInteger \u0026a, const BigInteger \u0026b) { int n1 = a.length; int n2 = b.length; if (n1 != n2)return n1 \u003e n2; //返回a和b哪个大，true则位a大 int i = 0; while (i \u003c n1 \u0026\u0026 a.nums[i] == b.nums[i])i++; //a b一样长的情况下，比较两个的值 if (i == n1) return false; return a.nums[i] \u003e b.nums[i]; } bool BigInteger::isEqual(BigInteger \u0026a, BigInteger \u0026b) { if (a.f != b.f || (a.length != b.length))return false; int i = 0; while (i \u003c a.length \u0026\u0026 a.nums[i] == b.nums[i])i++; return i == a.length \u0026\u0026 a.f == b.f; } BigInteger BigInteger::add(BigInteger \u0026a, BigInteger \u0026b) { a.reverse();//尾端对齐 b.reverse(); BigInteger t; int up = 0; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta + tb + up; t.push_back(base % 10 + '0'); up = base / 10; } if (up) t.push_back(up + '0'); t.reverse();//返回原位 a.reverse(); b.reverse(); return t; } BigInteger BigInteger::minus(BigInteger \u0026a, BigInteger \u0026b) { a.reverse(); b.reverse(); BigInteger t; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta - tb; if (base \u003c 0) { base += 10; a[i + 1]--; } t.push_back(base + '0'); } t.reverse(); a.reverse(); b.reverse(); return t; } //@运算符重载实现 BigInteger \u0026BigInteger::operator=(const BigInteger \u0026a) { if (length != 0)//如果不是初始化调用的 = ，则肯定需要先把原来的内存delete掉 delete[]nums; capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); return *this; } BigInteger BigInteger::operator+(BigInteger \u0026a) { BigInteger res; bool flag; if (a.f \u0026\u0026 f) { //同为负数情况，直接相加，再改符号 res = add(*this, a); flag = true; } else if (a.f \u0026\u0026 !f) {//左正右负 if (compare(a, *this)) { //看负数对应的nums是否比正数大 flag = true; res = minus(a, *this); } else { flag = false; res = minus(*this, a); } } else if (!a.f \u0026\u0026 f) { if (compare(*this, a)) { //与上一个相同 flag = true; res = minus(*this, a); } else { flag = false; res = minus(a, *this); } } else { //同时为正数就是最简单的加法 flag = false; res = add(*this, a); } res.f = flag; return res; } BigInteger BigInteger::operator-(BigInteger \u0026a) { BigInteger res; bool flag; if (a.f \u0026\u0026 f) { //同为负数情况--左边-右边==(不看符号)右边-(不看符号)左边 if (compare(a, *this)) { flag = false; res = minus(a, *this); } else { if (isEqual(*this, a)) flag = false; else flag = true; res = minus(*this, a); } } else if (a.f \u0026\u0026 !f) { //左边为正，右边为负--左边-右边==左边+右边 flag = false; res = add(a, *this); } else if (!a.f \u0026\u0026 f) { //右边为正，左边为负--左边-右边==两边为负的加法 flag = true; res = add(a, *this); } else { //同时为正数--左边-右边==左边-右边(分类讨论正负 if (compare(a, *this)) { //右边\u003e左边,符号为负 res = minus(a, *this); flag = true; } else { //右边\u003c左边，符号为正 res = minus(*this, a); flag = false; } } res.f = flag; return res; } bool BigInteger::operator\u003c(const BigInteger \u0026a) const { if (f \u0026\u0026 !a.f) { //其中一个为负数，则那个是更小的 return true; } else if (!f \u0026\u0026 a.f) return false; if (f) { //两者都为负数的情况，左边的值要更大则为true return compare(*this, a); }//两者均为正数，则值更小的在左边为true return compare(a, *this); } //@基本成员函数 void BigInteger::reverse() { int l = 0, r = length - 1; while (l \u003c r) { std::swap(nums[l], nums[r]); l++","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:3:2","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"功能测试 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:4:0","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"一、基本的加减测试 运行的测试代码： 打印输出 python输出 总结 与python输出无异，故通过测试。但碍于测试数据太过少，不是很有说服力，还有后面的解题测试。 ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:4:1","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"二、存储个人输入的数据+排序测试 测试代码(方便测试只输入了10个数据)： 排序输出： ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:4:2","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"三、解题测试 正好最近刷的PAT甲级就涉及到大数的加减hhh！ 题目描述 OJ平台 解题代码 #include \"bits/stdc++.h\" class BigInteger { bool f; char *nums; int length; int capacity; public://构造函数 BigInteger() : length(0), capacity(1), f(false) { //缺省构造函数 nums = new char[capacity]; } BigInteger(const char *n) : length(strlen(n)), f(false) { //用于初始值的构造函数 int start = 0; if (n[0] == '-') { f = true; start++; } while (start \u003c length \u0026\u0026 n[start] == '0')start++; capacity = length * 10; nums = new char[capacity]; std::copy(n + start, n + length, nums); length = length - start; } BigInteger(const BigInteger \u0026a) { //拷贝构造函数 capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); } BigInteger(BigInteger \u0026\u0026a) : length(0) { //移动构造函数 *this = a; } ~BigInteger() { //析构函数 delete[] nums; } public://静态成员函数 static void Swap(BigInteger \u0026a, BigInteger \u0026b) { std::swap(a.length, b.length); std::swap(a.capacity, b.capacity); std::swap(a.f, b.f); std::swap(a.nums, b.nums); } static bool compare(const BigInteger \u0026a, const BigInteger \u0026b) { //比较纯nums大小(不看符号 int n1 = a.length; int n2 = b.length; if (n1 != n2)return n1 \u003e n2; //返回a和b哪个大，true则位a大 int i = 0; while (i \u003c n1 \u0026\u0026 a.nums[i] == b.nums[i])i++; //a b一样长的情况下，比较两个的值 if (i == n1) return false; return a.nums[i] \u003e b.nums[i]; } bool isEqual(BigInteger \u0026a, BigInteger \u0026b) { if (a.f != b.f || (a.length != b.length))return false; int i = 0; while (i \u003c a.length \u0026\u0026 a.nums[i] == b.nums[i])i++; return i == a.length \u0026\u0026 a.f == b.f; } static BigInteger add(BigInteger \u0026a, BigInteger \u0026b) { //不看符号的加法 a.reverse();//尾端对齐 b.reverse(); BigInteger t; int up = 0; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta + tb + up; t.push_back(base % 10 + '0'); up = base / 10; } if (up) t.push_back(up + '0'); t.reverse();//返回原位 a.reverse(); b.reverse(); return t; } static BigInteger minus(BigInteger \u0026a, BigInteger \u0026b) { //不看符号的减法,默认了a的长度或者大小是比b要大的(所以外界不要乱调用 a.reverse(); b.reverse(); BigInteger t; int len = a.length \u003e b.length ? a.length : b.length; for (int i = 0; i \u003c len; i++) { int ta = i \u003c a.length ? a[i] - '0' : 0; int tb = i \u003c b.length ? b[i] - '0' : 0; int base = ta - tb; if (base \u003c 0) { base += 10; a[i + 1]--; } t.push_back(base + '0'); } t.reverse(); a.reverse(); b.reverse(); return t; } public://成员函数和重载运算符 char \u0026operator[](int i) { return nums[i]; } BigInteger \u0026operator=(BigInteger \u0026\u0026a) { //Swap\u0026Copy方式实现右值赋值重载 Swap(*this, a); return *this; } BigInteger \u0026operator=(const BigInteger \u0026a) {//深拷贝 if (length != 0)//如果不是初始化调用的 = ，则肯定需要先把原来的内存delete掉 delete[]nums; capacity = a.capacity; length = a.length; f = a.f; nums = new char[capacity]; std::copy(a.nums, a.nums + length, nums); return *this; } int getCap() { return capacity; } int getLength() { return length; } bool isNegative() { return f; } bool isEmpty() { return length == 0; } void reverse() { int l = 0, r = length - 1; while (l \u003c r) { std::swap(nums[l], nums[r]); l++; r--; } } void push_back(char x) { if (length \u003e= capacity) { capacity *= 2; char *t = nums; nums = new char[capacity]; std::copy(t, t + length, nums); delete[]t; } nums[length++] = x; } bool operator\u003c(const BigInteger \u0026a) const { if (f \u0026\u0026 !a.f) { //其中一个为负数，则那个是更小的 return true; } else if (!f \u0026\u0026 a.f) return false; if (f) { //两者都为负数的情况，左边的值要更大则为true return compare(*this, a); }//两者均为正数，则值更小的在左边为true return compare(a, *this); } BigInteger operator+(BigInteger \u0026a) { BigInteger res; bool flag; if (a.f \u0026\u0026 f) { //同为负数情况，直接相加，再改符号 res = add(*this, a); flag = true; } else if (a.f \u0026\u0026 !f) {//左正右负 if (compare(a, *this)) { //看负数对应的nums是否比正数大 flag = true; res = minus(a, *this); } else { flag = false; res = minus(*this, a); } } else if (!a.f \u0026\u0026 f) { if (compare(*this, a)) { //与上一个相同 flag = true; res = minus(*this, a); } else { flag = false; res = minus(a, *this); } } else { //同时为正数就是最简单的加法 flag = false; res = add(*this, a); } res.f = flag; return res; } BigInteger operator-(BigInteger \u0026a","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:4:3","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"总结 很多人，可能觉得做项目一定得是那种高大上，又或者是那种贪吃蛇小游戏、扫雷小游戏类型，实际上只要你有兴趣，任何一个东西都能成为你的练手项目，并且收获收获也许比你跟风去弄几个小游戏更大。 做这个小项目，我的收获是，对C++的语法更加的了解了，关于移动构造器、拷贝构造器、赋值重载这块弄得更清楚了，最大的收获在于强化了一个类的设计思路，这个是最重要的。 多写一些类的实现，不仅有利于对算法和语言语法的理解，更大的收获在于对各个功能的设计思路，作为程序员，我们最需要的就是这样的逻辑思维，从接到需求开始，我们应该能迅速的抽象出各种实现方案，然后进行不断的优化，得出属于自己的代码！ ","date":"2022-01-20","objectID":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/:5:0","tags":["C++类的封装"],"title":"大数加减类的实现(C++实现)✨","uri":"/posts/%E5%A4%A7%E6%95%B4%E6%95%B0%E5%8A%A0%E5%87%8F/"},{"categories":["C++实战"],"content":"矩阵快速幂的C++封装","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实现源码在线查看 如果对于类的设计已经非常清楚，只是进来想看看我的这个泛型模板源代码，那么直接点到下面这个链接进行查看： 源码链接 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:0:0","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"什么是矩阵快速幂？ 关于快速幂，就是利用二进制进行求解某个数的幂的快速方法。 后面会对快速幂的原理进行简单讲解，如果还是不懂，请自行百度。 相信有很多小伙伴是初入大学的世界，可能还没学过线性代数（比如我），于是乎不知道矩阵是什么，我推荐一个网站去看看矩阵的乘法是怎么运算的，下面是网站链接：（话说how to这个网站还真是牛批，什么东西都有教程，而且质量还贼高！😂） 矩阵乘法的计算方式 那么如何用代码表示矩阵以及他的乘法呢？ 其实很简单，就是三层循环进行控制即可。 如：我这里是C++的重载运算符 Matrix 是我定义的一个类。 Matrix\u0026 operator*(Matrix\u0026 b){ assert(b.date!=NULL \u0026\u0026 date!=NULL \u0026\u0026 m==b.n); ll tmp[n][b.m]; for(int i=0;i\u003cn;i++){ for(int j=0;j\u003cb.m;j++){ ll sum = 0; for(int k=0;k\u003cm;k++){ sum = (sum + date[i][k]*b.date[k][j])%MOD; } tmp[i][j] = sum; } } this-\u003em = b.m; for(int i=0;i\u003cn;i++){ for (int j = 0; j \u003c m; ++j) { date[i][j] = tmp[i][j]; } } return *this; } ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:1:0","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"怎么进行矩阵快速幂的运算？ 关于如何矩阵快速幂，我们先了解一下简单的快速幂。 说是快速幂就是通过位运算实现快速的同数累乘。 简述一下快速幂的原理： 原理就是，如果要求x的3次幂，那么可以转化为求 x*x 的 2 次幂，而求一个数的 2^n 幂是很简单的，比如进行一次 x *= x 便得到 x 的二次方。而再进行一次 x *= x 就得到了 4 次方，继续便可得到 8/16... 总之是 log2N 的时间。 代码如下： int QuickPow(int x,int n){ int c = n; int res = 1; while(c!=0){ if(c\u00261!=0){ res *= x; } c \u003e\u003e= 1; x *= x; } return res; } 那么矩阵的快速幂如何进行？ 把上述的 int 类型换成自己定义的矩阵就是矩阵的快速幂了。 我直接贴上C++实现的重载运算符后的类的快速幂写法： 这里的quickPow表示的是一个类的成员函数，所以可以直接用到这个矩阵里的数据进行运算。this表示指向这个对象的指针。init() 成员函数表示初始化为单位矩阵。 void quickPow(ll c){ if(c==1||c\u003c0)return; if(c==0){ init(); return; } Matrix tmp(*this); init(); while (c){ if(c\u00261){ *this = *this * tmp; } c \u003e\u003e= 1; tmp = tmp*tmp; } } ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:1:1","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"为什么突然想写这个模板？ 主要是因为最近做了几道快速幂的题目，被坑的很惨，然后就突然想设计一个模板了，主要是 my_tiny_stl 这个仓库也好久没更新了😂 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:2:0","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"题目 OJ网站 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:2:1","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"我是如何被坑的 首先拿到这道题，我便马上开始简单的O(n)递推法实现，然后提交，然后。。超时。。 定眼一看，数据量原来这么大！ 后面一想，肯定是矩阵快速幂了，先想出以下矩阵的递推式子： 进而题目便可得到求解。 然后我就利用 C++ 的类简单的封装了一个矩阵类，里面重载了乘法和 quickpow 方法，然后比较悠闲的准备提交，还没提交前就遇到C++的语法陷阱、、 语法陷阱（建议非C++党绕道） 由于类用的都是堆内存，所以我写了析构函数，我遇到的问题出在重载乘法时我返回的是左值，而且我也没有对 ‘=’ 进行重载，所以 ‘=’ 就是直接的成员变量拷贝，这导致一个结果就是两个对象的 date 指向同一片内存空间，而之前的那片内存空间泄露了，且最后这两个对象肯定都会调用析构函数，这又导致了析构函数调用了两次！ 如何解决这个问题？如果是 C++98 ，那么这个问题很大，基本上就是两种方法解决： 逃避问题，乘法的左操作数必须是当前赋值对象，这样就避免了最后赋值语句将原本对象内的指针直接改变。 解决问题，解决这类问题无论是 C++11 还是 C++98 最直接的方式就是重载 ‘=’ 号，重载 '=' 号的实现根据具体的情况进行，而具体实现赋值的重载，我们需要考虑两件事：第一，需尽可能的减少内存的申请和使用（具体而言就是判断两个对象的指针所指向的是否为同一片空间，即便不是同一片空间，为了增加空间利用率还可以判断两个空间是否大小一致，然后进行拷贝即可）。第二，如果是临时对象则需要把它的指针置空（防止编译器未优化临时变量的析构函数，从而调用了析构函数多次析构同一内存）。 由于 C++11 开始有了右值引用和它配套的移动赋值构造器，所以可以把临时变量直接调用移动构造器变成具名对象，然后进行操作，一般就是把它的指针所有权进行转移，然后把它的指针置空防止析构错误，在我的理解下，右值引用的出现就是为了捕捉到匿名对象然后给程序员进行适当的性能优化操作，没有右值引用前，匿名对象的内存根本就没法去使用，只能用来简单的赋值拷贝操作后才能使用，这样就很消耗内存了，右值引用出现后，我们可以通过右值引用对匿名对象进行捕捉，然后操作它的底层内存。还有一个很大的内存相关的更新就是有了一个 nullptr 关键字，这个关键字使得空指针不再会有歧义，所以 delete nullptr 是安全的。所以防止多次 delete 同一片空间产生错误可以将它赋值为 nullptr 即可。 那么基于 C++11 这个问题该如何解决呢？解决方法和C++98没差，就是能够更加得心应手的进行内存的管理了，如果等号右边是一个右值，那么它肯定是一个临时对象，所以我们可以在 ‘=’ 号的重载中直接了当的用它的内存，并把它的指针置空。如果没有右值类型进行捕获，编译器默认也是会对临时对象进行优化的，也能防止产生多个对象的赋值拷贝，但只能在对象初始化的时候进行优化！而在其他时候则还是会调用析构函数，这个时候如果还是用编译器默认产生的 ‘=’ 重载，则会发生被析构的空间的指针被赋值的情况，而我们的右值引用版本的赋值重载便是针对此现象的。这样便于内存管理，将右值和左值进行分开处理。右值是临时变量只需要用一会儿，所以可以直接把它的内存拿过来继续用，也不会对程序逻辑造成影响，而左值则不一样，它还需要存活很长一段时间，所以我们需要另创空间进行拷贝。 特别提醒：如果是做算法题，则完全不用去考虑内存的管理，析构函数也不要去写，毕竟只需要单次调用使用，对象最多也就存在一会儿。 做题陷阱 在必要的时候千万不要舍不得开long long！！！！ 这道题的数据量无论是幂的次方还是整个记录过程的数据都要开long long！！！ 我被这个陷阱坑了无数回了，这一次也不另外😅 开始写完这种之后过了前5个，然后后面5个报错，我还以为我设计的这个类有问题，还特意去写了好几个普通C语言版本😂最后发现原来的没开long long。以下为更改long long后的代码通过版本，我用宏定义写了几个版本。。。 这个Matrix类的设计还是很多地方没有考虑到位，比如上一个陷阱的问题只是通过方法一得到解决，并未去重载赋值操作符。。。所以后面痛定思痛，设计一个较为可用的Matrix类！ 效率时快时慢的，这主要取决于编译器是否进行优化。 // // Created by Alone on 2021/11/19. // #include \u003cbits/stdc++.h\u003eusing namespace std; //#define ELSE_MAIN #define MY_MAIN #define MAT #ifdef MAT typedef long long ll; class Matrix{ ll** date; int m; int n; public: static const int MOD; public: Matrix(ll** rec,int n,int m):date(rec),n(n),m(m){}//C风格的初始化 Matrix():date(NULL),m(0),n(0){} //缺省 Matrix(Matrix\u0026 b):n(b.n),m(b.m){//拷贝构造 assert(b.date!=NULL \u0026\u0026 b.n\u003e0 \u0026\u0026 b.m\u003e0); date = new ll*[n]; copy(b.date,b.date+n,date); for(int i=0;i\u003cn;i++){ date[i] = new ll[m]; copy(b.date[i],b.date[i]+m,date[i]); } } ~Matrix(){//析构函数实现 assert(date!=NULL \u0026\u0026 n\u003e0 \u0026\u0026 m\u003e0); for (int i = n-1; i \u003e=0 ; --i) { delete [] date[i]; } delete[] date; } Matrix\u0026 operator*(Matrix\u0026 b){ assert(b.date!=NULL \u0026\u0026 date!=NULL \u0026\u0026 m==b.n); ll tmp[n][b.m]; for(int i=0;i\u003cn;i++){ for(int j=0;j\u003cb.m;j++){ ll sum = 0; for(int k=0;k\u003cm;k++){ sum = (sum + date[i][k]*b.date[k][j])%MOD; } tmp[i][j] = sum; } } this-\u003em = b.m; for(int i=0;i\u003cn;i++){ for (int j = 0; j \u003c m; ++j) { date[i][j] = tmp[i][j]; } } return *this; } void init(){//重新初始化为单位矩阵 assert(date!=NULL \u0026\u0026 n\u003e0 \u0026\u0026 m\u003e0); for (int i = 0; i \u003c n; ++i) { for (int j = 0; j \u003c m; ++j) { if(i==j)date[i][j] = 1; else date[i][j] = 0; } } } void quickPow(ll c){ if(c==1||c\u003c0)return; if(c==0){ init(); return; } Matrix tmp(*this); init(); while (c){ if(c\u00261){ *this = *this * tmp; } c \u003e\u003e= 1; tmp = tmp*tmp; } } void print(){ for(int i=0;i\u003cn;i++){ for(int j=0;j\u003cm;j++){ cout\u003c\u003cdate[i][j]\u003c\u003c' '; } cout\u003c\u003cendl; } } int get(int x,int y){ assert(date!=NULL \u0026\u0026 x\u003cn \u0026\u0026 y\u003cm); return date[x][y]; } }; const int Matrix::MOD = 1e9+7; #endif #ifdef MY_MAIN int main(){ ll c; cin\u003e\u003ec; ll** matrix = new ll*[2]; matrix[0] = new ll[2]{1,1}; matrix[1] = new ll[2]{1,0}; Matrix mat(matrix,2,2); mat.quickPow(c-1); //mat.print(); ll** res = new ll*[2]; res[0] = new ll[1]; res[1] = new ll[1]; res[0][0] = res[1][0] = 1; Matrix fib(res,2,1); //这里有个内存分配错误，mat*fib返回的是左值，而=没有重载默认直接赋值成员变量。 //直接导致了fib失去了之前的变量所有权，和mat共同有一个内存空间，这样导致同一片空间被free两次 //通过重载 = 号解决，防止直接的内存没有被释放就重新绑定同一片内存 Matrix ret(mat*fib); cout\u003c\u003cret.get(0,0); return 0; } #endif #ifdef TEST_MAIN typedef long long ll ; const int MOD = 1e9+7; ll a[2][2]{{1,1},{1,0}};ll b[2]{1,1}; void selfMut(){ ll tmp[2][2]; for(int i=0;i\u003c2;i","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:2:2","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"教你设计Matrix类 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:3:0","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"如何设计一个类？ 结构： 内部数据抽象：对象的运算数据存储 用二级指针 date 进行矩阵的二维空间的内存管理，以及n和m表示矩阵的行高和列宽。 行为抽象：对对象的行为描述 构造函数（缺省、自定义、拷贝、移动） 和 析构函数（调用destory成员函数）。 setter 和 getter。提供函数接口去设置和得到数据。 功能函数：比如重载的乘法运算符和快速幂函数这些都算功能函数。 对象的属性抽象： 数据和行为是否对外，对外封装数据的意义在于防止对对象行为描述的破坏，而对外开放的某个行为通常需要内部多个函数进行重复调用来实现。这个时候需要用到 public 和 private 关键字进行修饰。 数据和行为是否可继承，对于某些行为（函数），我们不想被外部调用，但是对子类又很有用，这个时候我们可以采取 protect 关键字进行修饰。 数据和行为是否能重复利用，为了节省不必要的内存开销，可以设计不需要产生具体对象的通用型函数，可以使用 static 关键字进行修饰，这样可以避免我想使用某个函数前还得去申请一片毫不相关的内存空间，而对于某个对象的数据也可以采用 static 进行修饰，这样一来，这个数据在对象的创建过程中就不需要再进行申请和赋值了。 那么接下来就来确定行为（函数）的属性了，数据肯定是不对外开放的（private），否则面向对象将毫无意义。 构造函数 和 析构函数，这两个是对象的创建和销毁的关键，所以如果不是希望对象不被创建或者是不被销毁，都应该使用 public 修饰。 setter 和 getter。很明显是对外开放的接口函数，所以肯定也是 public 修饰。 功能函数：矩阵快速幂的函数，很明显我们希望设置一个对外通用的情况，那么这个时候就应该不需要创建对象便可进行调用，所以最好用 static 进行修饰（这个一般为外部通用接口，内部还需实现一个方便类调用的版本，这也很简单，直接传参调用该函数即可），重载的乘法肯定也是对外的所以需要 public 修饰，destroy 函数用于处理内存的回收，很明显这是一个内部通用的函数，但是外界完全是不需要它的！所以把它设置为 private 属性即可。 以上便是对整个类的设计思路，当然真正动手设计的时候，还需要具体到函数的参数和返回值类型，因为这牵扯到 C++ 的具体语法了，比如我应该在重载乘法的时候返回一个什么样的类型？最好是返回一个右值！而重载赋值运算符则最好是返回一个左值。一般需要考虑返回值类型的取舍时，最难的就是如果返回一个对象，我该返回左值还是右值。 写了这么久C++，我感觉用C++写的Java屏蔽重载运算符的特性主要就是重载运算符时需要考虑的过程太多了，C++菜鸡（我就是这个菜鸟😂）写出及其低效且不安全的代码，而只有老鸟才能写出优雅高效的代码。 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:3:1","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"类的具体抽象结构（代码的具体规划图） 按照属性对类的各个部分进行分类的，毕竟属性也基本就代表了这个方法的使用场景了。 最后可以利用基本的断言或者异常，让代码变得更为健壮，使得更容易定位错误发生的位置和原因。 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:3:2","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"实现矩阵泛型模板类 源代码实现 我先是画出规划图进行实现，然后在实现的过程中，发现可以新增一些特性，比如重载下标运算符，比如用print函数打印出来方便验证。 具体实现过程，为了方便简单的定位可能发生的错误，使用了大量的assert进行断言检查。如果想代码的健壮性更强，可以使用抛出异常的方式。 源代码对应的GitHub仓库地址：仓库链接，还有更多模板的实现，包括少量STL 更好的源码阅读体验：源码在线阅读 直接阅读下面的代码有点不太好查看，推荐去上面的GitHub1s里面查看源码。 // // Created by L_B__ on 2021/11/20. // #ifndef LQTEST_MATRIX_H #define LQTEST_MATRIX_H #include \u003ccassert\u003e#include \u003calgorithm\u003e#include \u003ciostream\u003e #define _MOD template\u003ctypename T\u003e class Matrix { /*Type define*/ typedef T data_t; typedef int ssize_t; /*data source*/ data_t **data; ssize_t n; ssize_t m; public: static const data_t MOD; public: /*default construct*/ Matrix() : m(0), n(0), data(nullptr) {} /*custom construct*/ Matrix(data_t **mat, ssize_t n, ssize_t m) : data(mat), n(n), m(m) {}//外部申请内存传入内部 Matrix(ssize_t n, ssize_t m) : data(nullptr), n(n), m(m) {//外部指定矩阵的行和列即可，内存初始化在内部进行 assert(n \u003e 0 \u0026\u0026 m \u003e 0); data = new data_t *[n]; for (int i = 0; i \u003c n; i++) { data[i] = new data_t[m]; } init(data, n, m); } /*copy construct*/ Matrix(Matrix \u0026src) : data(nullptr), n(src.n), m(src.m)//也可用const \u0026引用类型，但这样很多右值的情况都不会调用移动构造了 { assert(n \u003e 0 \u0026\u0026 m \u003e 0); data = new data_t *[n]; for (int i = 0; i \u003c n; ++i) { data[i] = new data_t[m]; std::copy(src.data[i], src.data[i] + m, data[i]); } } /*move construct*/ Matrix(Matrix\u003cdata_t\u003e \u0026\u0026src) : n(src.n), m(src.m), data(nullptr) { assert(src.data != nullptr \u0026\u0026 n \u003e 0 \u0026\u0026 m \u003e 0); data = src.data; src.data = nullptr; src.n = src.m = 0; } /*destruct*/ ~Matrix() { destroy(); } /*overload*/ //加上MOD的特殊版本 #ifdef _MOD Matrix operator*(const Matrix\u003cdata_t\u003e \u0026src)//建议返回右值，返回左值的后续坑比较多，参数const \u0026既可以接受左值也可接受右值 { assert(data != nullptr \u0026\u0026 src.data != nullptr \u0026\u0026 m == src.n \u0026\u0026 m \u003e 0 \u0026\u0026 n \u003e 0 \u0026\u0026 src.m \u003e 0);//进行矩阵乘法的必要条件 data_t **tmp = new data_t *[n]; //为tmp申请动态内存，因为是传出参数 for (int i = 0; i \u003c n; ++i) { tmp[i] = new data_t[m]; } for (int i = 0; i \u003c n; ++i)//开始更新tmp { for (int j = 0; j \u003c src.m; ++j) { data_t sum = 0; for (int k = 0; k \u003c m; ++k) { sum = (sum + data[i][k] * src.data[k][j]) % MOD; } tmp[i][j] = sum; } } //直接构造匿名对象返回 return Matrix(tmp, n, src.m);; } Matrix \u0026operator*=(const Matrix\u003cdata_t\u003e \u0026src)//*= 想一想我们平时的使用，就是返回一个左值 { assert(data != nullptr \u0026\u0026 src.data != nullptr \u0026\u0026 m == src.n \u0026\u0026 m \u003e 0 \u0026\u0026 n \u003e 0 \u0026\u0026 src.m \u003e 0);//进行矩阵乘法的必要条件 data_t tmp[n][src.m];//静态内存就行，毕竟只是临时存数据的 for (int i = 0; i \u003c n; ++i)//开始更新tmp { for (int j = 0; j \u003c src.m; ++j) { data_t sum = 0; for (int k = 0; k \u003c m; ++k) { sum = (sum + data[i][k] * src.data[k][j]) % MOD; } tmp[i][j] = sum; } } //由于此时的date内存可能不够容下tmp数据，所以可能需要重新进行内存申请 //注意重新申请列内存的时候，需要把之前的内存释放 if (m != src.m) { for (int i = 0; i \u003c n; ++i) { delete[]data[i]; data[i] = nullptr; data[i] = new data_t[src.m]; } } for (int i = 0; i \u003c n; ++i) { assert(data[i] != nullptr); std::copy(tmp[i], tmp[i] + src.m, data[i]); } return *this; } #endif #ifndef _MOD Matrix operator*(const Matrix\u003cdata_t\u003e\u0026src)//建议返回右值，返回左值的后续坑比较多，参数const \u0026既可以接受左值也可接受右值 { assert(data!= nullptr\u0026\u0026src.data!= nullptr\u0026\u0026m==src.n\u0026\u0026m\u003e0\u0026\u0026n\u003e0\u0026\u0026src.m\u003e0);//进行矩阵乘法的必要条件 data_t** tmp = new data_t *[n]; //为tmp申请动态内存，因为是传出参数 for (int i = 0; i \u003c n; ++i) { tmp[i] = new data_t [m]; } for(int i=0;i\u003cn;++i)//开始更新tmp { for (int j = 0; j \u003c src.m; ++j) { data_t sum = 0; for (int k = 0; k \u003c m; ++k) { sum = sum + data[i][k]*src.data[k][j]; } tmp[i][j] = sum; } } //直接构造匿名对象返回 return Matrix (tmp,n,src.m); } /*与乘法的唯一区别在于乘法是构造一个新的对象，而*=返回的是this*/ Matrix \u0026operator*=(const Matrix\u003cdata_t\u003e \u0026src)//*= 想一想我们平时的使用，就是返回一个左值 { assert(data!= nullptr\u0026\u0026src.data!= nullptr\u0026\u0026m==src.n\u0026\u0026m\u003e0\u0026\u0026n\u003e0\u0026\u0026src.m\u003e0);//进行矩阵乘法的必要条件 data_t tmp[n][src.m];//静态内存就行，毕竟只是临时存数据的 for(int i=0;i\u003cn;++i)//开始更新tmp { for (int j = 0; j \u003c src.m; ++j) { data_t sum = 0; for (int k = 0; k \u003c m; ++k) { sum = sum + data[i][k]*src.data[k][j]; } tmp[i][j] = sum; } } //由于此时的date内存可能不够容下tmp数据，所以可能需要重新进行内存申请 //注意重新申请列内存的时候，需要把之前的内存释放 if(m!=src.m){ for (int i = 0; i \u003c n; ++i) { delete []data[i]; data[i] = nullptr; data[i] = new data_t [src.m]; } } for (int i = 0; i \u003c n; ++i) { assert(data[i] != nullptr); std::copy(tmp[i],tmp[i]+src.m,data[i]); } return *this; } #endif //赋值号的重载","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:3:3","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":["C++实战"],"content":"总结 由于用的是 C++11 的语法进行实现的模板类，所以低于这个版本的编译器都无法正常使用。我查阅了相关资料，实际上蓝桥杯比赛的时候可以用 C++11，而acm更是不用说，早就能用C++11了。 简单复盘： 实现一个这样的类，主要能学到以下几点： 类的设计技巧。 对C++的左右值有了更深入的理解。 各种构造器的设计和实现已经达到炉火纯青的地步了。 ","date":"2022-01-20","objectID":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/:4:0","tags":["矩阵快速幂"],"title":"矩阵快速幂的C++封装","uri":"/posts/%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82%E7%9A%84c++%E5%B0%81%E8%A3%85/"},{"categories":null,"content":"手写http协议解析库 ","date":"0001-01-01","objectID":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/:0:0","tags":null,"title":"","uri":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/"},{"categories":null,"content":"http协议的组成 http请求报文如下： http响应报文如下： ","date":"0001-01-01","objectID":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/:1:0","tags":null,"title":"","uri":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/"},{"categories":null,"content":"状态机设计 请求报文解析 响应报文解析 ","date":"0001-01-01","objectID":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/:2:0","tags":null,"title":"","uri":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/"},{"categories":null,"content":"代码结构设计 基础结构类：Response和Request，其中都包含一个Url类，用于解析得到路径和Query参数。 工具类：HttpParser，HttpParser用于解析纯http报文然后得到对应的Response或Request，组合成字符串信息只需要调用Request或者Response对应的to_string()方法。 ","date":"0001-01-01","objectID":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/:3:0","tags":null,"title":"","uri":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/"},{"categories":null,"content":"代码使用示例 对于head和body的设置均可通过直接调用head()和body()方法来设置，这个方法返回的是一个左值。 #include\"http-parser/Parser.h\" int main(){ http::Parser parser; auto req = parser.ToRequest(buffer); std::cout \u003c\u003c req.to_string(); //根据request内容获取对应的http报文 req.head()[\"dfasf\"] = \"fda\"; //随意设置request的header req.body() = \"fdsafsadf\"; //设置request的body部分 //request的特殊字段（GET的FORM和POST的form auto v = req.Query(\"test\"); //获取第一个值 auto v1 = req.PostQuery(\"test\"); //获取post表单里的第一个query值 req.PostMultiPart(\"test\"); //返回form-data的键值（可以传入文件 http::Response response; response.SetStatus(http::OK); response.SetContentType(http::ACCEPT_CONTENT_TYPE::T_JSON); response.SetConnection(false); response.body() = R\"({\"hello world!\":2323})\"; auto response_text = response.to_string(); } ","date":"0001-01-01","objectID":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/:4:0","tags":null,"title":"","uri":"/posts/%E6%89%8B%E5%86%99http%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E5%99%A8/"}]